13b, 4k, gbs=512: dp=8, tp=2, pp=1, mbs=1
LOCAL_IP = 10.64.24.52
DP=8, MP=2, PP=1
[2024-02-12 20:29:30,513] torch.distributed.run: [WARNING] 
[2024-02-12 20:29:30,513] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 20:29:30,513] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 20:29:30,513] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...

 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.522 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.524 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.530 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.576 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.039 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.134 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.144 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.137 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (443.22, 552.47)
    train/valid/test-data-iterators-setup ..........: (0.02, 11282.85)
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 30372.9 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.028312E+01 | loss scale: 1.0 | grad norm: 718.929 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (30134.34, 30140.93)
    forward-compute ................................: (12588.05, 12965.32)
    backward-compute ...............................: (17103.35, 17500.94)
    batch-generator ................................: (255.92, 380.40)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.04)
    grads-reduce-scatter ...........................: (132.22, 132.46)
    params-all-gather ..............................: (68.92, 69.03)
    optimizer-copy-to-main-grad ....................: (0.42, 0.64)
    optimizer-clip-main-grad .......................: (7.04, 7.06)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.13, 9.35)
    optimizer-copy-main-to-model-params ............: (2.62, 2.69)
    optimizer ......................................: (20.22, 20.29)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 29717.5 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 2.067744E+00 | loss scale: 1.0 | grad norm: 13.816 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (29481.50, 29487.49)
    forward-compute ................................: (11998.60, 12389.35)
    backward-compute ...............................: (17017.37, 17448.07)
    batch-generator ................................: (55.73, 93.45)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (131.88, 132.36)
    params-all-gather ..............................: (68.93, 69.04)
    optimizer-copy-to-main-grad ....................: (0.40, 0.68)
    optimizer-clip-main-grad .......................: (4.18, 4.20)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.89, 9.01)
    optimizer-copy-main-to-model-params ............: (2.62, 2.69)
    optimizer ......................................: (17.09, 17.16)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 29935.3 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.435967E+00 | loss scale: 1.0 | grad norm: 4.122 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (29700.61, 29706.50)
    forward-compute ................................: (12231.25, 12632.88)
    backward-compute ...............................: (17026.34, 17434.60)
    batch-generator ................................: (55.92, 82.74)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (131.88, 132.27)
    params-all-gather ..............................: (68.92, 69.04)
    optimizer-copy-to-main-grad ....................: (0.40, 0.60)
    optimizer-clip-main-grad .......................: (4.18, 4.20)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.90, 9.01)
    optimizer-copy-main-to-model-params ............: (2.62, 2.76)
    optimizer ......................................: (16.98, 17.13)

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<36600> with error 12, opcode 129, len 0, vendor err 129

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<60770> with error 12, opcode 129, len 0, vendor err 129

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<54316> with error 12, opcode 129, len 0, vendor err 129

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<36600> with error 5, opcode 129, len 0, vendor err 249

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<60770> with error 5, opcode 129, len 0, vendor err 249

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<60770> with error 5, opcode 129, len 0, vendor err 249

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<60770> with error 5, opcode 129, len 0, vendor err 249

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<54316> with error 5, opcode 129, len 0, vendor err 249

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<54316> with error 5, opcode 129, len 0, vendor err 249

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<54316> with error 5, opcode 129, len 0, vendor err 249

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<54316> with error 5, opcode 129, len 0, vendor err 244

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<36600> with error 5, opcode 129, len 0, vendor err 249

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<36600> with error 5, opcode 129, len 0, vendor err 249

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<42690> with error 5, opcode 129, len 0, vendor err 244

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<36600> with error 5, opcode 129, len 0, vendor err 244

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<54316> with error 5, opcode 129, len 0, vendor err 249

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<42690> with error 12, opcode 129, len 0, vendor err 129

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<60770> with error 5, opcode 129, len 0, vendor err 244

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<42690> with error 5, opcode 129, len 0, vendor err 249

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<36600> with error 5, opcode 129, len 0, vendor err 249

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<60770> with error 5, opcode 129, len 0, vendor err 249

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<42690> with error 5, opcode 129, len 0, vendor err 249

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<42690> with error 5, opcode 129, len 0, vendor err 249

SYM206-GPU-A0206-P2-Node52:416466:417580 [2] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.49<42690> with error 5, opcode 129, len 0, vendor err 249
[E ProcessGroupNCCL.cpp:467] [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=494, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600538 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=494, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600543 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 15] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=494, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600602 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=494, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600661 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 13] NCCL watchdog thread terminated with exception: [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=494, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600538 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 13] NCCL watchdog thread terminated with exception: [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=494, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600538 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 9] NCCL watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=494, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600543 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 9] NCCL watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=494, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600543 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 15] NCCL watchdog thread terminated with exception: [Rank 15] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=494, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600602 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 15] NCCL watchdog thread terminated with exception: [Rank 15] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=494, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600602 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 11] NCCL watchdog thread terminated with exception: [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=494, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600661 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 11] NCCL watchdog thread terminated with exception: [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=494, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600661 milliseconds before timing out.
[2024-02-12 21:00:17,395] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 416464 closing signal SIGTERM
[2024-02-12 21:00:17,396] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 416466 closing signal SIGTERM
[2024-02-12 21:00:17,397] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 416467 closing signal SIGTERM
[2024-02-12 21:00:17,397] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 416468 closing signal SIGTERM
[2024-02-12 21:00:17,397] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 416470 closing signal SIGTERM
[2024-02-12 21:00:18,627] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 1 (pid: 416465) of binary: /usr/bin/python
benchmark/test_padding.sh.two.bak-222: line 138: 416390 Killed                  torchrun $DISTRIBUTED_ARGS pretrain_gpt.py $GPT_ARGS $DATA_ARGS $OUTPUT_ARGS --distributed-backend nccl
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (363846): No such process
kill: (363852): No such process
kill: (363858): No such process
kill: (363864): No such process
10.64.24.49 kill done.
13b, 4k, gbs=512: dp=4, tp=2, pp=2, mbs=1
LOCAL_IP = 10.64.24.52
DP=4, MP=2, PP=2
[2024-02-12 21:02:46,455] torch.distributed.run: [WARNING] 
[2024-02-12 21:02:46,455] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 21:02:46,455] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 21:02:46,455] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 3275816960
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 3275816960
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.178 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.270 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.326 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.390 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.936 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.001 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.073 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.092 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (920.57, 965.30)
    train/valid/test-data-iterators-setup ..........: (0.02, 12642.46)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 29429.7 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 8.099643E+00 | loss scale: 1.0 | grad norm: 300.724 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 9] (after 10 iterations) memory (MB) | allocated: 28300.5654296875 | max allocated: 38174.34423828125 | reserved: 40114.0 | max reserved: 40114.0[Rank 8] (after 10 iterations) memory (MB) | allocated: 28300.5654296875 | max allocated: 38174.34423828125 | reserved: 40034.0 | max reserved: 40034.0

(min, max) time across ranks (ms):
    forward-backward ...............................: (28789.84, 28923.12)
    forward-compute ................................: (9434.64, 10639.04)
    backward-compute ...............................: (16563.14, 17643.22)
    batch-generator ................................: (269.46, 333.92)
    forward-recv ...................................: (288.99, 316.80)
    forward-send ...................................: (3.68, 6.99)
    backward-recv ..................................: (80.23, 84.47)
    backward-send ..................................: (1.07, 1.11)
    forward-send-backward-recv .....................: (2252.19, 2768.63)
    backward-send-forward-recv .....................: (505.13, 621.32)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (11.25, 11.71)
    grads-reduce-scatter ...........................: (36.21, 445.26)
    params-all-gather ..............................: (19.53, 19.76)
    optimizer-copy-to-main-grad ....................: (0.35, 0.51)
    optimizer-clip-main-grad .......................: (7.46, 7.48)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.33, 9.52)
    optimizer-copy-main-to-model-params ............: (2.68, 2.76)
    optimizer ......................................: (21.05, 21.29)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 28737.5 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 2.247444E+00 | loss scale: 1.0 | grad norm: 48.028 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (28508.91, 28642.60)
    forward-compute ................................: (9441.88, 10447.74)
    backward-compute ...............................: (16506.14, 17597.57)
    batch-generator ................................: (97.34, 135.83)
    forward-recv ...................................: (69.20, 70.49)
    forward-send ...................................: (1.03, 1.05)
    backward-recv ..................................: (80.64, 83.91)
    backward-send ..................................: (1.06, 1.26)
    forward-send-backward-recv .....................: (2086.79, 2552.02)
    backward-send-forward-recv .....................: (743.56, 754.84)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (11.15, 12.01)
    grads-reduce-scatter ...........................: (36.35, 36.69)
    params-all-gather ..............................: (19.54, 19.82)
    optimizer-copy-to-main-grad ....................: (0.33, 0.49)
    optimizer-clip-main-grad .......................: (4.29, 4.31)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.11, 9.33)
    optimizer-copy-main-to-model-params ............: (2.68, 2.85)
    optimizer ......................................: (17.28, 17.44)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 28473.6 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.508074E+00 | loss scale: 1.0 | grad norm: 6.762 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (28246.71, 28380.29)
    forward-compute ................................: (9193.06, 10441.90)
    backward-compute ...............................: (16525.33, 17607.15)
    batch-generator ................................: (96.36, 130.58)
    forward-recv ...................................: (68.97, 69.69)
    forward-send ...................................: (1.03, 1.05)
    backward-recv ..................................: (80.66, 83.99)
    backward-send ..................................: (1.07, 1.11)
    forward-send-backward-recv .....................: (2066.51, 2519.01)
    backward-send-forward-recv .....................: (482.20, 496.17)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (11.22, 11.62)
    grads-reduce-scatter ...........................: (36.25, 36.76)
    params-all-gather ..............................: (19.58, 19.76)
    optimizer-copy-to-main-grad ....................: (0.34, 0.49)
    optimizer-clip-main-grad .......................: (4.29, 4.31)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.10, 9.31)
    optimizer-copy-main-to-model-params ............: (2.68, 2.75)
    optimizer ......................................: (17.22, 17.30)
Mon Feb 12 21:22:36 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             454W / 700W |  43708MiB / 81559MiB |     41%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   48C    P0             449W / 700W |  43788MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   49C    P0             479W / 700W |  43678MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0             505W / 700W |  43698MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   41C    P0             464W / 700W |  43694MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             433W / 700W |  43714MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             455W / 700W |  43396MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   41C    P0             401W / 700W |  43476MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 28790.6 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.194631E+00 | loss scale: 1.0 | grad norm: 2.636 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (28471.19, 28604.64)
    forward-compute ................................: (9428.25, 10411.08)
    backward-compute ...............................: (16508.76, 17575.76)
    batch-generator ................................: (96.78, 131.22)
    forward-recv ...................................: (69.20, 69.55)
    forward-send ...................................: (1.04, 1.05)
    backward-recv ..................................: (80.64, 83.26)
    backward-send ..................................: (1.05, 1.12)
    forward-send-backward-recv .....................: (2047.58, 2526.31)
    backward-send-forward-recv .....................: (744.40, 752.68)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (11.23, 11.99)
    grads-reduce-scatter ...........................: (36.39, 36.82)
    params-all-gather ..............................: (19.56, 19.74)
    optimizer-copy-to-main-grad ....................: (0.34, 0.50)
    optimizer-clip-main-grad .......................: (4.28, 4.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.11, 9.26)
    optimizer-copy-main-to-model-params ............: (2.67, 2.75)
    optimizer ......................................: (17.16, 17.24)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 28446.4 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.306318E+00 | loss scale: 1.0 | grad norm: 3.483 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (28219.27, 28352.27)
    forward-compute ................................: (9152.42, 10416.36)
    backward-compute ...............................: (16495.30, 17585.90)
    batch-generator ................................: (98.69, 131.86)
    forward-recv ...................................: (68.98, 69.60)
    forward-send ...................................: (1.03, 1.06)
    backward-recv ..................................: (85.60, 89.53)
    backward-send ..................................: (1.07, 1.11)
    forward-send-backward-recv .....................: (2062.02, 2564.12)
    backward-send-forward-recv .....................: (482.24, 488.88)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (11.25, 11.69)
    grads-reduce-scatter ...........................: (36.21, 36.71)
    params-all-gather ..............................: (19.57, 20.13)
    optimizer-copy-to-main-grad ....................: (0.34, 0.50)
    optimizer-clip-main-grad .......................: (4.27, 4.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.12, 9.24)
    optimizer-copy-main-to-model-params ............: (2.68, 2.74)
    optimizer ......................................: (17.12, 17.19)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 28685.5 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.165246E+00 | loss scale: 1.0 | grad norm: 2.660 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (28457.92, 28590.79)
    forward-compute ................................: (9446.19, 10399.13)
    backward-compute ...............................: (16537.26, 17553.53)
    batch-generator ................................: (98.89, 129.94)
    forward-recv ...................................: (69.16, 69.84)
    forward-send ...................................: (1.02, 1.07)
    backward-recv ..................................: (80.02, 83.89)
    backward-send ..................................: (1.07, 1.12)
    forward-send-backward-recv .....................: (2030.77, 2473.24)
    backward-send-forward-recv .....................: (752.14, 755.78)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (11.23, 11.87)
    grads-reduce-scatter ...........................: (36.26, 36.86)
    params-all-gather ..............................: (19.56, 19.72)
    optimizer-copy-to-main-grad ....................: (0.34, 0.49)
    optimizer-clip-main-grad .......................: (4.27, 4.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.11, 9.20)
    optimizer-copy-main-to-model-params ............: (2.68, 2.75)
    optimizer ......................................: (17.12, 17.44)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 28456.2 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.151540E+00 | loss scale: 1.0 | grad norm: 1.435 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (28229.93, 28362.40)
    forward-compute ................................: (9167.71, 10406.48)
    backward-compute ...............................: (16528.62, 17600.45)
    batch-generator ................................: (96.90, 129.02)
    forward-recv ...................................: (68.90, 69.49)
    forward-send ...................................: (1.02, 1.06)
    backward-recv ..................................: (80.37, 83.56)
    backward-send ..................................: (1.06, 1.36)
    forward-send-backward-recv .....................: (2068.84, 2531.49)
    backward-send-forward-recv .....................: (484.76, 486.94)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (11.22, 11.61)
    grads-reduce-scatter ...........................: (36.32, 36.81)
    params-all-gather ..............................: (19.56, 19.77)
    optimizer-copy-to-main-grad ....................: (0.34, 0.51)
    optimizer-clip-main-grad .......................: (4.27, 4.29)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.11, 9.20)
    optimizer-copy-main-to-model-params ............: (2.67, 2.74)
    optimizer ......................................: (17.11, 17.18)
Mon Feb 12 21:41:40 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             492W / 700W |  43708MiB / 81559MiB |     12%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   47C    P0             505W / 700W |  43788MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   48C    P0             469W / 700W |  43678MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0             475W / 700W |  43698MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   40C    P0             477W / 700W |  43694MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   48C    P0             437W / 700W |  43714MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   46C    P0             492W / 700W |  43396MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   40C    P0             457W / 700W |  43476MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 28804.1 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.169269E+00 | loss scale: 1.0 | grad norm: 1.805 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (28489.51, 28621.95)
    forward-compute ................................: (9424.03, 10421.02)
    backward-compute ...............................: (16523.91, 17585.51)
    batch-generator ................................: (97.34, 132.34)
    forward-recv ...................................: (69.16, 69.79)
    forward-send ...................................: (1.03, 1.05)
    backward-recv ..................................: (80.85, 83.35)
    backward-send ..................................: (1.05, 1.10)
    forward-send-backward-recv .....................: (2039.06, 2537.74)
    backward-send-forward-recv .....................: (749.69, 757.03)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (11.19, 11.93)
    grads-reduce-scatter ...........................: (36.28, 36.79)
    params-all-gather ..............................: (19.56, 19.74)
    optimizer-copy-to-main-grad ....................: (0.34, 0.50)
    optimizer-clip-main-grad .......................: (4.27, 4.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.11, 9.21)
    optimizer-copy-main-to-model-params ............: (2.68, 2.75)
    optimizer ......................................: (17.12, 17.19)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 27910.1 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.127309E+00 | loss scale: 1.0 | grad norm: 0.918 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (27683.30, 27816.75)
    forward-compute ................................: (9148.13, 9857.50)
    backward-compute ...............................: (16503.97, 17579.86)
    batch-generator ................................: (96.48, 133.19)
    forward-recv ...................................: (69.06, 69.79)
    forward-send ...................................: (1.03, 1.05)
    backward-recv ..................................: (80.42, 84.08)
    backward-send ..................................: (1.06, 1.12)
    forward-send-backward-recv .....................: (1495.95, 2028.37)
    backward-send-forward-recv .....................: (484.87, 489.97)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (11.23, 11.69)
    grads-reduce-scatter ...........................: (36.37, 36.83)
    params-all-gather ..............................: (19.52, 19.74)
    optimizer-copy-to-main-grad ....................: (0.33, 0.50)
    optimizer-clip-main-grad .......................: (3.54, 3.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.10, 9.21)
    optimizer-copy-main-to-model-params ............: (2.68, 2.75)
    optimizer ......................................: (16.39, 16.46)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 28705.9 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.089611E+00 | loss scale: 1.0 | grad norm: 1.586 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (28479.53, 28612.16)
    forward-compute ................................: (9437.59, 10382.64)
    backward-compute ...............................: (16558.45, 17575.87)
    batch-generator ................................: (93.93, 127.53)
    forward-recv ...................................: (331.23, 334.72)
    forward-send ...................................: (1.03, 1.06)
    backward-recv ..................................: (80.92, 83.35)
    backward-send ..................................: (1.05, 1.11)
    forward-send-backward-recv .....................: (2030.89, 2481.32)
    backward-send-forward-recv .....................: (484.64, 492.40)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (11.19, 11.94)
    grads-reduce-scatter ...........................: (36.37, 36.84)
    params-all-gather ..............................: (19.55, 19.75)
    optimizer-copy-to-main-grad ....................: (0.34, 0.50)
    optimizer-clip-main-grad .......................: (4.03, 4.05)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.12, 9.21)
    optimizer-copy-main-to-model-params ............: (2.67, 2.74)
    optimizer ......................................: (16.87, 16.94)
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (365654): No such process
kill: (365660): No such process
kill: (365666): No such process
kill: (365672): No such process
10.64.24.49 kill done.
13b, 4k, gbs=512: dp=4, tp=4, pp=1, mbs=2
LOCAL_IP = 10.64.24.52
DP=4, MP=4, PP=1
[2024-02-12 21:53:21,257] torch.distributed.run: [WARNING] 
[2024-02-12 21:53:21,257] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 21:53:21,257] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 21:53:21,257] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.259 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.362 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.939 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.055 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (455.27, 533.06)
    train/valid/test-data-iterators-setup ..........: (0.02, 10752.07)
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 31592.1 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 8.203804E+00 | loss scale: 1.0 | grad norm: 285.752 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (31397.01, 31400.95)
    forward-compute ................................: (12556.23, 12745.35)
    backward-compute ...............................: (18557.57, 18802.60)
    batch-generator ................................: (440.71, 504.89)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.04)
    grads-reduce-scatter ...........................: (107.61, 107.65)
    params-all-gather ..............................: (55.98, 56.11)
    optimizer-copy-to-main-grad ....................: (0.75, 1.06)
    optimizer-clip-main-grad .......................: (7.33, 7.35)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.30, 9.48)
    optimizer-copy-main-to-model-params ............: (2.90, 2.99)
    optimizer ......................................: (21.48, 21.57)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 30867.6 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 2.308295E+00 | loss scale: 1.0 | grad norm: 47.868 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (30676.45, 30680.23)
    forward-compute ................................: (11920.39, 12096.04)
    backward-compute ...............................: (18484.82, 18718.18)
    batch-generator ................................: (63.48, 105.43)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (107.26, 107.31)
    params-all-gather ..............................: (55.93, 56.06)
    optimizer-copy-to-main-grad ....................: (0.71, 1.00)
    optimizer-clip-main-grad .......................: (4.43, 4.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.04, 9.16)
    optimizer-copy-main-to-model-params ............: (2.90, 2.99)
    optimizer ......................................: (17.95, 18.05)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 30898.6 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.522049E+00 | loss scale: 1.0 | grad norm: 6.811 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (30707.21, 30710.89)
    forward-compute ................................: (11928.41, 12119.69)
    backward-compute ...............................: (18490.91, 18739.45)
    batch-generator ................................: (63.84, 103.48)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (107.26, 107.53)
    params-all-gather ..............................: (55.96, 56.07)
    optimizer-copy-to-main-grad ....................: (0.74, 0.98)
    optimizer-clip-main-grad .......................: (4.41, 4.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.04, 9.16)
    optimizer-copy-main-to-model-params ............: (2.90, 2.99)
    optimizer ......................................: (17.97, 18.07)
Mon Feb 12 22:14:42 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   42C    P0             483W / 700W |  65060MiB / 81559MiB |     53%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   48C    P0             484W / 700W |  65508MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   49C    P0             518W / 700W |  65106MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0             510W / 700W |  65302MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   41C    P0             457W / 700W |  64850MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             446W / 700W |  65032MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             438W / 700W |  65092MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   41C    P0             453W / 700W |  64796MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 31479.3 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.189737E+00 | loss scale: 1.0 | grad norm: 3.478 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (31196.88, 31200.39)
    forward-compute ................................: (12442.22, 12651.06)
    backward-compute ...............................: (18473.51, 18715.84)
    batch-generator ................................: (61.14, 89.53)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (107.25, 108.04)
    params-all-gather ..............................: (55.92, 56.29)
    optimizer-copy-to-main-grad ....................: (0.73, 0.88)
    optimizer-clip-main-grad .......................: (4.43, 4.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.04, 9.13)
    optimizer-copy-main-to-model-params ............: (2.90, 2.99)
    optimizer ......................................: (17.86, 17.95)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 30873.3 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.306657E+00 | loss scale: 1.0 | grad norm: 2.744 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (30680.19, 30684.32)
    forward-compute ................................: (11921.21, 12157.63)
    backward-compute ...............................: (18476.76, 18721.41)
    batch-generator ................................: (62.70, 77.00)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (107.26, 107.94)
    params-all-gather ..............................: (55.97, 56.22)
    optimizer-copy-to-main-grad ....................: (0.73, 0.89)
    optimizer-clip-main-grad .......................: (4.40, 4.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.02, 9.09)
    optimizer-copy-main-to-model-params ............: (2.90, 2.99)
    optimizer ......................................: (17.87, 17.96)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 30862.0 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.163260E+00 | loss scale: 1.0 | grad norm: 1.903 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (30670.01, 30674.45)
    forward-compute ................................: (11921.17, 12159.53)
    backward-compute ...............................: (18464.50, 18711.70)
    batch-generator ................................: (60.66, 78.49)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (107.25, 107.53)
    params-all-gather ..............................: (55.92, 56.17)
    optimizer-copy-to-main-grad ....................: (0.72, 0.86)
    optimizer-clip-main-grad .......................: (4.41, 4.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.03, 9.09)
    optimizer-copy-main-to-model-params ............: (2.90, 2.99)
    optimizer ......................................: (17.78, 17.87)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 31410.8 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.145893E+00 | loss scale: 1.0 | grad norm: 1.414 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (31218.58, 31223.15)
    forward-compute ................................: (12442.99, 12684.49)
    backward-compute ...............................: (18488.24, 18738.01)
    batch-generator ................................: (62.53, 77.20)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (107.25, 107.84)
    params-all-gather ..............................: (55.96, 56.06)
    optimizer-copy-to-main-grad ....................: (0.72, 0.90)
    optimizer-clip-main-grad .......................: (4.41, 4.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.03, 9.10)
    optimizer-copy-main-to-model-params ............: (2.90, 2.99)
    optimizer ......................................: (17.98, 18.07)
Mon Feb 12 22:35:23 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   42C    P0             547W / 700W |  65060MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   49C    P0             557W / 700W |  65508MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   50C    P0             516W / 700W |  65106MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0             507W / 700W |  65302MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   42C    P0             516W / 700W |  64850MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   50C    P0             508W / 700W |  65032MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   48C    P0             490W / 700W |  65092MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             433W / 700W |  64796MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 30963.5 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.159434E+00 | loss scale: 1.0 | grad norm: 1.070 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (30684.99, 30688.34)
    forward-compute ................................: (11914.14, 12141.46)
    backward-compute ...............................: (18497.71, 18731.76)
    batch-generator ................................: (61.13, 76.96)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (107.27, 107.49)
    params-all-gather ..............................: (55.92, 56.28)
    optimizer-copy-to-main-grad ....................: (0.74, 0.85)
    optimizer-clip-main-grad .......................: (3.67, 3.69)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.02, 9.09)
    optimizer-copy-main-to-model-params ............: (2.90, 2.99)
    optimizer ......................................: (17.11, 17.21)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 30624.0 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.126977E+00 | loss scale: 1.0 | grad norm: 1.432 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (30432.23, 30436.40)
    forward-compute ................................: (11659.18, 11897.36)
    backward-compute ...............................: (18488.86, 18735.24)
    batch-generator ................................: (61.41, 77.13)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (107.25, 107.53)
    params-all-gather ..............................: (55.96, 56.13)
    optimizer-copy-to-main-grad ....................: (0.73, 0.92)
    optimizer-clip-main-grad .......................: (3.67, 3.69)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.02, 9.53)
    optimizer-copy-main-to-model-params ............: (2.90, 2.99)
    optimizer ......................................: (17.63, 17.72)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 30614.8 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.094430E+00 | loss scale: 1.0 | grad norm: 1.994 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (30424.52, 30428.04)
    forward-compute ................................: (11646.75, 11889.24)
    backward-compute ...............................: (18489.35, 18739.38)
    batch-generator ................................: (61.96, 77.78)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (107.25, 107.30)
    params-all-gather ..............................: (55.92, 56.05)
    optimizer-copy-to-main-grad ....................: (0.72, 0.85)
    optimizer-clip-main-grad .......................: (4.15, 4.17)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.03, 9.09)
    optimizer-copy-main-to-model-params ............: (2.90, 2.99)
    optimizer ......................................: (17.53, 17.62)
[2024-02-12 22:47:20,730] torch.distributed.elastic.agent.server.api: [ERROR] Error waiting on exit barrier. Elapsed: 96.22877359390259 seconds
benchmark/test_padding.sh.two.bak-222: line 138: 419953 Killed                  torchrun $DISTRIBUTED_ARGS pretrain_gpt.py $GPT_ARGS $DATA_ARGS $OUTPUT_ARGS --distributed-backend nccl
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (366720): No such process
kill: (366726): No such process
kill: (366732): No such process
kill: (366738): No such process
10.64.24.49 kill done.
13b, 4k, gbs=512: dp=4, tp=4, pp=1, mbs=1
LOCAL_IP = 10.64.24.52
DP=4, MP=4, PP=1
[2024-02-12 22:47:57,636] torch.distributed.run: [WARNING] 
[2024-02-12 22:47:57,636] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 22:47:57,636] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 22:47:57,636] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.254 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.470 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.960 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  4.985 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (449.91, 553.10)
    train/valid/test-data-iterators-setup ..........: (0.02, 10799.99)
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 36115.3 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 8.205168E+00 | loss scale: 1.0 | grad norm: 285.762 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (35921.75, 35923.45)
    forward-compute ................................: (15397.22, 15566.86)
    backward-compute ...............................: (20269.85, 20442.30)
    batch-generator ................................: (502.90, 543.72)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.03, 0.04)
    grads-reduce-scatter ...........................: (107.58, 107.73)
    params-all-gather ..............................: (55.98, 56.26)
    optimizer-copy-to-main-grad ....................: (0.77, 0.96)
    optimizer-clip-main-grad .......................: (7.43, 7.46)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.28, 9.41)
    optimizer-copy-main-to-model-params ............: (2.91, 3.00)
    optimizer ......................................: (21.35, 21.43)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 35321.5 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 2.307964E+00 | loss scale: 1.0 | grad norm: 47.711 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (35131.54, 35133.06)
    forward-compute ................................: (14686.27, 14836.08)
    backward-compute ...............................: (20209.57, 20368.59)
    batch-generator ................................: (113.95, 147.41)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (107.26, 107.31)
    params-all-gather ..............................: (55.95, 56.97)
    optimizer-copy-to-main-grad ....................: (0.77, 0.95)
    optimizer-clip-main-grad .......................: (4.43, 4.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.03, 9.10)
    optimizer-copy-main-to-model-params ............: (2.91, 3.00)
    optimizer ......................................: (17.98, 18.07)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 35080.6 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.520849E+00 | loss scale: 1.0 | grad norm: 7.078 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (34891.07, 34892.64)
    forward-compute ................................: (14418.79, 14580.70)
    backward-compute ...............................: (20223.04, 20395.72)
    batch-generator ................................: (113.71, 148.43)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (107.24, 107.50)
    params-all-gather ..............................: (55.96, 56.07)
    optimizer-copy-to-main-grad ....................: (0.77, 0.95)
    optimizer-clip-main-grad .......................: (4.43, 4.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.02, 9.08)
    optimizer-copy-main-to-model-params ............: (2.91, 2.99)
    optimizer ......................................: (18.05, 18.14)
Mon Feb 12 23:12:16 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   39C    P0             117W / 700W |  46980MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   46C    P0             124W / 700W |  47278MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   47C    P0             129W / 700W |  47246MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0             119W / 700W |  46978MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             118W / 700W |  46940MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   47C    P0             124W / 700W |  47454MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   45C    P0             124W / 700W |  47474MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             121W / 700W |  47134MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 36080.3 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.190322E+00 | loss scale: 1.0 | grad norm: 3.266 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (35669.90, 35671.55)
    forward-compute ................................: (15214.68, 15367.45)
    backward-compute ...............................: (20214.16, 20378.92)
    batch-generator ................................: (115.45, 147.49)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (107.25, 107.32)
    params-all-gather ..............................: (55.95, 56.09)
    optimizer-copy-to-main-grad ....................: (0.78, 0.91)
    optimizer-clip-main-grad .......................: (4.43, 4.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.02, 9.08)
    optimizer-copy-main-to-model-params ............: (2.91, 2.99)
    optimizer ......................................: (17.91, 18.12)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 35586.1 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.305866E+00 | loss scale: 1.0 | grad norm: 3.264 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (35396.92, 35398.28)
    forward-compute ................................: (14942.52, 15098.64)
    backward-compute ...............................: (20208.68, 20377.60)
    batch-generator ................................: (114.96, 148.41)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (107.25, 107.45)
    params-all-gather ..............................: (55.97, 56.07)
    optimizer-copy-to-main-grad ....................: (0.78, 0.90)
    optimizer-clip-main-grad .......................: (4.41, 4.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.02, 9.08)
    optimizer-copy-main-to-model-params ............: (2.91, 2.99)
    optimizer ......................................: (17.90, 17.98)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 35047.7 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.168461E+00 | loss scale: 1.0 | grad norm: 4.378 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (34858.97, 34860.23)
    forward-compute ................................: (14412.60, 14569.29)
    backward-compute ...............................: (20200.61, 20367.54)
    batch-generator ................................: (116.76, 146.45)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (107.25, 107.30)
    params-all-gather ..............................: (55.96, 56.08)
    optimizer-copy-to-main-grad ....................: (0.78, 0.96)
    optimizer-clip-main-grad .......................: (4.44, 4.46)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.02, 9.08)
    optimizer-copy-main-to-model-params ............: (2.90, 2.99)
    optimizer ......................................: (17.94, 18.02)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 35598.4 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.159173E+00 | loss scale: 1.0 | grad norm: 3.150 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (35408.73, 35410.31)
    forward-compute ................................: (14940.87, 15090.82)
    backward-compute ...............................: (20229.92, 20390.75)
    batch-generator ................................: (114.77, 147.21)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (107.28, 107.67)
    params-all-gather ..............................: (55.98, 56.30)
    optimizer-copy-to-main-grad ....................: (0.78, 0.95)
    optimizer-clip-main-grad .......................: (4.42, 4.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.02, 9.09)
    optimizer-copy-main-to-model-params ............: (2.90, 2.99)
    optimizer ......................................: (17.99, 18.07)
Mon Feb 12 23:35:56 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   40C    P0             513W / 700W |  46980MiB / 81559MiB |     67%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   47C    P0             527W / 700W |  47278MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   48C    P0             507W / 700W |  47246MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   40C    P0             463W / 700W |  46978MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   40C    P0             474W / 700W |  46940MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   48C    P0             467W / 700W |  47454MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   46C    P0             476W / 700W |  47474MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   40C    P0             484W / 700W |  47134MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 35685.3 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.177473E+00 | loss scale: 1.0 | grad norm: 2.292 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (35405.91, 35407.31)
    forward-compute ................................: (14939.91, 15091.74)
    backward-compute ...............................: (20226.18, 20389.01)
    batch-generator ................................: (115.35, 148.14)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (107.25, 107.31)
    params-all-gather ..............................: (55.95, 56.06)
    optimizer-copy-to-main-grad ....................: (0.76, 0.92)
    optimizer-clip-main-grad .......................: (4.42, 4.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.02, 9.09)
    optimizer-copy-main-to-model-params ............: (2.91, 3.00)
    optimizer ......................................: (17.94, 18.04)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 35335.2 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.135840E+00 | loss scale: 1.0 | grad norm: 1.931 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (35145.60, 35147.35)
    forward-compute ................................: (14686.13, 14826.79)
    backward-compute ...............................: (20229.22, 20382.21)
    batch-generator ................................: (114.28, 147.20)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (107.25, 107.57)
    params-all-gather ..............................: (55.97, 56.08)
    optimizer-copy-to-main-grad ....................: (0.75, 0.91)
    optimizer-clip-main-grad .......................: (4.44, 4.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.02, 9.08)
    optimizer-copy-main-to-model-params ............: (2.91, 2.99)
    optimizer ......................................: (17.93, 18.01)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 35322.7 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.098289E+00 | loss scale: 1.0 | grad norm: 0.821 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (35133.99, 35135.28)
    forward-compute ................................: (14677.28, 14825.09)
    backward-compute ...............................: (20221.20, 20375.61)
    batch-generator ................................: (115.24, 149.00)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (107.25, 107.55)
    params-all-gather ..............................: (55.95, 56.08)
    optimizer-copy-to-main-grad ....................: (0.78, 0.91)
    optimizer-clip-main-grad .......................: (4.00, 4.02)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.01, 9.07)
    optimizer-copy-main-to-model-params ............: (2.91, 2.99)
    optimizer ......................................: (17.58, 17.66)
benchmark/test_padding.sh.two.bak-222: line 138: 420940 Killed                  torchrun $DISTRIBUTED_ARGS pretrain_gpt.py $GPT_ARGS $DATA_ARGS $OUTPUT_ARGS --distributed-backend nccl
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (367786): No such process
kill: (367792): No such process
kill: (367798): No such process
kill: (367804): No such process
10.64.24.49 kill done.
13b, 4k, gbs=512: dp=4, tp=1, pp=4, mbs=1
LOCAL_IP = 10.64.24.52
DP=4, MP=1, PP=4
[2024-02-12 23:48:31,905] torch.distributed.run: [WARNING] 
[2024-02-12 23:48:31,905] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 23:48:31,905] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 23:48:31,905] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 3146393600
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 3403960320
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...

> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...

 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.612 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.780 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.792 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.789 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.789 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.799 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.800 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.850 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.952 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.056 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.123 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.243 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.227 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.297 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.300 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.308 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (44.37, 624.17)
    train/valid/test-data-iterators-setup ..........: (9869.86, 11402.14)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 26219.8 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 8.816486E+00 | loss scale: 1.0 | grad norm: 491.192 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 27149.71484375 | max allocated: 41223.484375 | reserved: 42060.0 | max reserved: 42060.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 29362.8388671875 | max allocated: 38215.04443359375 | reserved: 38564.0 | max reserved: 38564.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (25364.94, 25704.90)
    forward-compute ................................: (7233.25, 9014.01)
    backward-compute ...............................: (14266.22, 16030.20)
    batch-generator ................................: (119.88, 192.26)
    forward-recv ...................................: (158.78, 382.69)
    forward-send ...................................: (3.61, 118.99)
    backward-recv ..................................: (77.69, 237.36)
    backward-send ..................................: (1.07, 3.05)
    forward-send-backward-recv .....................: (2979.73, 3512.14)
    backward-send-forward-recv .....................: (180.58, 242.85)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 22.43)
    grads-reduce-scatter ...........................: (37.91, 439.90)
    params-all-gather ..............................: (18.47, 20.08)
    optimizer-copy-to-main-grad ....................: (0.20, 0.29)
    optimizer-clip-main-grad .......................: (7.13, 7.35)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.93, 9.71)
    optimizer-copy-main-to-model-params ............: (2.46, 2.70)
    optimizer ......................................: (20.73, 20.97)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 25218.8 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 2.230921E+00 | loss scale: 1.0 | grad norm: 41.801 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (24771.85, 25111.71)
    forward-compute ................................: (7160.35, 8720.98)
    backward-compute ...............................: (14202.66, 16012.20)
    batch-generator ................................: (92.82, 116.70)
    forward-recv ...................................: (56.66, 165.47)
    forward-send ...................................: (0.98, 4.12)
    backward-recv ..................................: (77.44, 234.92)
    backward-send ..................................: (1.07, 3.04)
    forward-send-backward-recv .....................: (2692.06, 3185.53)
    backward-send-forward-recv .....................: (172.97, 223.23)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 22.74)
    grads-reduce-scatter ...........................: (35.06, 38.40)
    params-all-gather ..............................: (18.44, 20.10)
    optimizer-copy-to-main-grad ....................: (0.19, 0.27)
    optimizer-clip-main-grad .......................: (4.08, 4.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 9.46)
    optimizer-copy-main-to-model-params ............: (2.45, 2.70)
    optimizer ......................................: (16.92, 17.17)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 25700.7 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.503548E+00 | loss scale: 1.0 | grad norm: 9.849 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (25254.87, 25593.35)
    forward-compute ................................: (7171.96, 8836.79)
    backward-compute ...............................: (14204.41, 15991.23)
    batch-generator ................................: (92.88, 117.43)
    forward-recv ...................................: (56.59, 165.54)
    forward-send ...................................: (0.99, 4.58)
    backward-recv ..................................: (77.85, 235.45)
    backward-send ..................................: (1.07, 3.06)
    forward-send-backward-recv .....................: (2683.80, 3275.27)
    backward-send-forward-recv .....................: (447.83, 728.64)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 22.41)
    grads-reduce-scatter ...........................: (34.97, 38.27)
    params-all-gather ..............................: (18.44, 20.09)
    optimizer-copy-to-main-grad ....................: (0.19, 0.28)
    optimizer-clip-main-grad .......................: (4.08, 4.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 9.47)
    optimizer-copy-main-to-model-params ............: (2.45, 2.70)
    optimizer ......................................: (16.93, 17.17)
Tue Feb 13 00:06:09 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   40C    P0             400W / 700W |  45160MiB / 81559MiB |     56%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   47C    P0             422W / 700W |  45014MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   48C    P0             381W / 700W |  45208MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0             343W / 700W |  44774MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   41C    P0             349W / 700W |  41750MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             339W / 700W |  42348MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             356W / 700W |  42336MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   41C    P0             312W / 700W |  42072MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 25283.6 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.199638E+00 | loss scale: 1.0 | grad norm: 3.018 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (24748.48, 25087.03)
    forward-compute ................................: (7166.45, 8718.17)
    backward-compute ...............................: (14193.94, 15998.59)
    batch-generator ................................: (90.97, 114.10)
    forward-recv ...................................: (56.17, 165.79)
    forward-send ...................................: (0.98, 4.26)
    backward-recv ..................................: (77.32, 234.88)
    backward-send ..................................: (1.06, 3.03)
    forward-send-backward-recv .....................: (2683.07, 3144.34)
    backward-send-forward-recv .....................: (171.60, 225.46)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.62)
    grads-reduce-scatter ...........................: (34.94, 38.32)
    params-all-gather ..............................: (18.44, 20.08)
    optimizer-copy-to-main-grad ....................: (0.19, 0.28)
    optimizer-clip-main-grad .......................: (4.08, 4.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.70, 9.46)
    optimizer-copy-main-to-model-params ............: (2.45, 2.70)
    optimizer ......................................: (16.94, 17.19)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 25727.1 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.327136E+00 | loss scale: 1.0 | grad norm: 5.057 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (25281.33, 25620.08)
    forward-compute ................................: (7158.43, 9250.00)
    backward-compute ...............................: (14198.50, 15960.14)
    batch-generator ................................: (91.89, 113.52)
    forward-recv ...................................: (56.20, 165.61)
    forward-send ...................................: (0.99, 4.63)
    backward-recv ..................................: (78.13, 234.34)
    backward-send ..................................: (1.06, 3.04)
    forward-send-backward-recv .....................: (3229.93, 3679.82)
    backward-send-forward-recv .....................: (172.29, 225.58)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.45)
    grads-reduce-scatter ...........................: (34.99, 38.33)
    params-all-gather ..............................: (18.45, 20.12)
    optimizer-copy-to-main-grad ....................: (0.19, 0.28)
    optimizer-clip-main-grad .......................: (4.17, 4.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 9.46)
    optimizer-copy-main-to-model-params ............: (2.45, 2.70)
    optimizer ......................................: (17.00, 17.24)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 25781.3 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.184406E+00 | loss scale: 1.0 | grad norm: 2.167 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (25335.55, 25674.35)
    forward-compute ................................: (7188.25, 8966.70)
    backward-compute ...............................: (14205.73, 15962.29)
    batch-generator ................................: (92.08, 110.91)
    forward-recv ...................................: (56.57, 165.79)
    forward-send ...................................: (0.97, 4.36)
    backward-recv ..................................: (76.17, 232.94)
    backward-send ..................................: (1.06, 3.05)
    forward-send-backward-recv .....................: (2997.90, 3723.23)
    backward-send-forward-recv .....................: (172.24, 517.12)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 22.51)
    grads-reduce-scatter ...........................: (34.94, 38.22)
    params-all-gather ..............................: (18.42, 20.11)
    optimizer-copy-to-main-grad ....................: (0.19, 0.27)
    optimizer-clip-main-grad .......................: (4.08, 4.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.70, 9.58)
    optimizer-copy-main-to-model-params ............: (2.45, 2.71)
    optimizer ......................................: (17.10, 17.43)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 25192.4 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.159892E+00 | loss scale: 1.0 | grad norm: 2.561 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (24745.27, 25085.56)
    forward-compute ................................: (7155.32, 8705.95)
    backward-compute ...............................: (14204.60, 15976.82)
    batch-generator ................................: (92.78, 109.69)
    forward-recv ...................................: (56.65, 165.58)
    forward-send ...................................: (0.98, 4.37)
    backward-recv ..................................: (77.35, 235.68)
    backward-send ..................................: (1.06, 3.03)
    forward-send-backward-recv .....................: (2673.52, 3135.15)
    backward-send-forward-recv .....................: (172.03, 223.54)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 22.50)
    grads-reduce-scatter ...........................: (35.00, 38.42)
    params-all-gather ..............................: (18.44, 20.11)
    optimizer-copy-to-main-grad ....................: (0.19, 0.27)
    optimizer-clip-main-grad .......................: (4.08, 4.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.70, 9.46)
    optimizer-copy-main-to-model-params ............: (2.45, 2.70)
    optimizer ......................................: (17.04, 17.75)
Tue Feb 13 00:23:09 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             360W / 700W |  45160MiB / 81559MiB |     63%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   47C    P0             402W / 700W |  45014MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   49C    P0             390W / 700W |  45208MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0             376W / 700W |  44774MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   41C    P0             321W / 700W |  41750MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             323W / 700W |  42348MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             318W / 700W |  42336MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   41C    P0             270W / 700W |  42072MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 25283.9 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.177801E+00 | loss scale: 1.0 | grad norm: 2.650 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (24746.22, 25085.65)
    forward-compute ................................: (7148.87, 8727.69)
    backward-compute ...............................: (14200.92, 16002.46)
    batch-generator ................................: (93.35, 113.50)
    forward-recv ...................................: (56.43, 166.11)
    forward-send ...................................: (0.98, 4.41)
    backward-recv ..................................: (77.53, 234.60)
    backward-send ..................................: (1.06, 3.02)
    forward-send-backward-recv .....................: (2678.88, 3157.66)
    backward-send-forward-recv .....................: (172.38, 221.95)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 22.60)
    grads-reduce-scatter ...........................: (34.82, 38.32)
    params-all-gather ..............................: (18.44, 20.10)
    optimizer-copy-to-main-grad ....................: (0.19, 0.27)
    optimizer-clip-main-grad .......................: (4.09, 4.96)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.70, 9.46)
    optimizer-copy-main-to-model-params ............: (2.45, 2.70)
    optimizer ......................................: (17.76, 18.00)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 25474.0 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.154570E+00 | loss scale: 1.0 | grad norm: 1.168 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (25027.39, 25367.48)
    forward-compute ................................: (7142.69, 8717.37)
    backward-compute ...............................: (14191.61, 16006.41)
    batch-generator ................................: (92.82, 113.26)
    forward-recv ...................................: (56.58, 165.64)
    forward-send ...................................: (0.98, 4.10)
    backward-recv ..................................: (78.30, 235.87)
    backward-send ..................................: (1.06, 3.04)
    forward-send-backward-recv .....................: (2701.69, 3435.77)
    backward-send-forward-recv .....................: (173.01, 506.32)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 22.46)
    grads-reduce-scatter ...........................: (34.93, 38.30)
    params-all-gather ..............................: (18.44, 20.08)
    optimizer-copy-to-main-grad ....................: (0.19, 0.27)
    optimizer-clip-main-grad .......................: (3.84, 4.04)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 9.46)
    optimizer-copy-main-to-model-params ............: (2.45, 2.70)
    optimizer ......................................: (16.65, 16.89)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 25198.8 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.105313E+00 | loss scale: 1.0 | grad norm: 1.052 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (24753.82, 25093.13)
    forward-compute ................................: (7142.37, 8739.47)
    backward-compute ...............................: (14185.49, 16006.61)
    batch-generator ................................: (92.75, 119.18)
    forward-recv ...................................: (56.49, 165.32)
    forward-send ...................................: (0.98, 4.39)
    backward-recv ..................................: (77.40, 234.60)
    backward-send ..................................: (1.07, 3.09)
    forward-send-backward-recv .....................: (2684.44, 3148.15)
    backward-send-forward-recv .....................: (172.19, 222.92)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 22.61)
    grads-reduce-scatter ...........................: (35.06, 38.32)
    params-all-gather ..............................: (18.43, 20.08)
    optimizer-copy-to-main-grad ....................: (0.19, 0.28)
    optimizer-clip-main-grad .......................: (2.88, 2.99)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 9.46)
    optimizer-copy-main-to-model-params ............: (2.45, 2.70)
    optimizer ......................................: (15.60, 15.86)
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (370064): No such process
kill: (370070): No such process
kill: (370076): No such process
kill: (370082): No such process
10.64.24.49 kill done.
13b, 4k, gbs=512: dp=2, tp=4, pp=2, mbs=2
LOCAL_IP = 10.64.24.52
DP=2, MP=4, PP=2
[2024-02-13 00:33:51,306] torch.distributed.run: [WARNING] 
[2024-02-13 00:33:51,306] torch.distributed.run: [WARNING] *****************************************
[2024-02-13 00:33:51,306] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-13 00:33:51,306] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1638548480
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 1638548480
 > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 1638548480
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1638548480
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.266 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.328 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.160 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.263 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (937.75, 977.85)
    train/valid/test-data-iterators-setup ..........: (0.02, 11027.09)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 33382.7 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 8.175494E+00 | loss scale: 1.0 | grad norm: 251.099 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 11] (after 10 iterations) memory (MB) | allocated: 18995.4521484375 | max allocated: 33354.9248046875 | reserved: 36940.0 | max reserved: 36940.0[Rank 9] (after 10 iterations) memory (MB) | allocated: 18995.4521484375 | max allocated: 33354.9248046875 | reserved: 36940.0 | max reserved: 36940.0

[Rank 8] (after 10 iterations) memory (MB) | allocated: 18995.4521484375 | max allocated: 33354.9248046875 | reserved: 36940.0 | max reserved: 36940.0[Rank 10] (after 10 iterations) memory (MB) | allocated: 18995.4521484375 | max allocated: 33354.9248046875 | reserved: 36920.0 | max reserved: 36920.0

(min, max) time across ranks (ms):
    forward-backward ...............................: (32965.28, 33109.67)
    forward-compute ................................: (11926.15, 12974.54)
    backward-compute ...............................: (18289.90, 18953.30)
    batch-generator ................................: (506.92, 530.74)
    forward-recv ...................................: (514.07, 531.32)
    forward-send ...................................: (3.99, 9.22)
    backward-recv ..................................: (96.51, 97.33)
    backward-send ..................................: (1.96, 2.03)
    forward-send-backward-recv .....................: (2593.44, 2737.43)
    backward-send-forward-recv .....................: (672.09, 678.85)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.98, 6.33)
    grads-reduce-scatter ...........................: (15.31, 229.91)
    params-all-gather ..............................: (8.46, 8.81)
    optimizer-copy-to-main-grad ....................: (0.73, 0.86)
    optimizer-clip-main-grad .......................: (7.41, 7.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.44, 9.69)
    optimizer-copy-main-to-model-params ............: (2.94, 3.07)
    optimizer ......................................: (21.81, 21.97)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 32520.3 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 2.270918E+00 | loss scale: 1.0 | grad norm: 39.436 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (32320.71, 32464.88)
    forward-compute ................................: (11734.74, 12614.92)
    backward-compute ...............................: (18200.07, 18871.44)
    batch-generator ................................: (104.45, 132.65)
    forward-recv ...................................: (88.58, 88.79)
    forward-send ...................................: (1.93, 1.95)
    backward-recv ..................................: (96.52, 97.46)
    backward-send ..................................: (1.96, 2.04)
    forward-send-backward-recv .....................: (2255.82, 2376.87)
    backward-send-forward-recv .....................: (905.75, 913.20)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.91, 6.54)
    grads-reduce-scatter ...........................: (15.46, 15.88)
    params-all-gather ..............................: (8.34, 8.78)
    optimizer-copy-to-main-grad ....................: (0.73, 0.84)
    optimizer-clip-main-grad .......................: (4.48, 4.52)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.20, 9.36)
    optimizer-copy-main-to-model-params ............: (2.94, 3.06)
    optimizer ......................................: (18.19, 18.32)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 32184.4 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.458384E+00 | loss scale: 1.0 | grad norm: 13.925 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (31985.03, 32129.39)
    forward-compute ................................: (11480.26, 12533.79)
    backward-compute ...............................: (18215.27, 18850.35)
    batch-generator ................................: (103.62, 131.79)
    forward-recv ...................................: (88.64, 88.68)
    forward-send ...................................: (1.92, 1.95)
    backward-recv ..................................: (96.04, 97.07)
    backward-send ..................................: (1.95, 2.04)
    forward-send-backward-recv .....................: (2152.02, 2281.79)
    backward-send-forward-recv .....................: (648.07, 654.84)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.86, 6.25)
    grads-reduce-scatter ...........................: (15.37, 15.98)
    params-all-gather ..............................: (8.30, 8.81)
    optimizer-copy-to-main-grad ....................: (0.69, 0.85)
    optimizer-clip-main-grad .......................: (4.46, 4.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.20, 9.34)
    optimizer-copy-main-to-model-params ............: (2.94, 3.06)
    optimizer ......................................: (18.14, 18.27)
Tue Feb 13 00:56:11 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   40C    P0             547W / 700W |  40614MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   46C    P0             472W / 700W |  40662MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   47C    P0             493W / 700W |  40642MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   40C    P0             492W / 700W |  40422MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             409W / 700W |  40626MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   47C    P0             416W / 700W |  41110MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   45C    P0             445W / 700W |  41050MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             368W / 700W |  40810MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 32496.2 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.169185E+00 | loss scale: 1.0 | grad norm: 2.028 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (32203.89, 32347.83)
    forward-compute ................................: (11738.08, 12735.93)
    backward-compute ...............................: (18200.89, 18823.16)
    batch-generator ................................: (103.19, 131.49)
    forward-recv ...................................: (88.53, 88.70)
    forward-send ...................................: (1.93, 1.96)
    backward-recv ..................................: (96.60, 97.61)
    backward-send ..................................: (1.96, 2.04)
    forward-send-backward-recv .....................: (2107.87, 2253.24)
    backward-send-forward-recv .....................: (653.72, 914.96)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.86, 6.50)
    grads-reduce-scatter ...........................: (15.23, 15.82)
    params-all-gather ..............................: (8.41, 8.80)
    optimizer-copy-to-main-grad ....................: (0.72, 0.85)
    optimizer-clip-main-grad .......................: (4.48, 4.51)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.19, 9.35)
    optimizer-copy-main-to-model-params ............: (2.95, 3.07)
    optimizer ......................................: (18.10, 18.23)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 32153.0 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.299810E+00 | loss scale: 1.0 | grad norm: 1.961 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (31953.70, 32097.40)
    forward-compute ................................: (11474.73, 12504.00)
    backward-compute ...............................: (18202.24, 18831.11)
    batch-generator ................................: (102.30, 131.40)
    forward-recv ...................................: (88.60, 88.74)
    forward-send ...................................: (1.92, 1.98)
    backward-recv ..................................: (96.41, 97.58)
    backward-send ..................................: (1.95, 2.03)
    forward-send-backward-recv .....................: (2122.40, 2268.95)
    backward-send-forward-recv .....................: (647.57, 654.89)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.86, 6.35)
    grads-reduce-scatter ...........................: (15.32, 15.98)
    params-all-gather ..............................: (8.43, 8.77)
    optimizer-copy-to-main-grad ....................: (0.72, 0.87)
    optimizer-clip-main-grad .......................: (4.55, 4.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.20, 9.35)
    optimizer-copy-main-to-model-params ............: (2.94, 3.06)
    optimizer ......................................: (18.27, 18.39)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 32174.1 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.169570E+00 | loss scale: 1.0 | grad norm: 1.724 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (31973.22, 32118.27)
    forward-compute ................................: (11516.89, 12518.50)
    backward-compute ...............................: (18202.88, 18823.54)
    batch-generator ................................: (104.53, 157.42)
    forward-recv ...................................: (88.65, 88.73)
    forward-send ...................................: (1.92, 1.96)
    backward-recv ..................................: (96.09, 97.87)
    backward-send ..................................: (1.97, 2.03)
    forward-send-backward-recv .....................: (2149.72, 2232.16)
    backward-send-forward-recv .....................: (653.91, 686.88)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.88, 6.43)
    grads-reduce-scatter ...........................: (15.35, 15.95)
    params-all-gather ..............................: (8.36, 8.80)
    optimizer-copy-to-main-grad ....................: (0.71, 0.96)
    optimizer-clip-main-grad .......................: (4.50, 4.54)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.20, 9.36)
    optimizer-copy-main-to-model-params ............: (2.94, 3.06)
    optimizer ......................................: (18.26, 18.39)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 32796.3 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.157664E+00 | loss scale: 1.0 | grad norm: 1.561 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (32596.59, 32741.00)
    forward-compute ................................: (11769.08, 12887.38)
    backward-compute ...............................: (18213.62, 18822.81)
    batch-generator ................................: (104.50, 157.51)
    forward-recv ...................................: (88.59, 88.80)
    forward-send ...................................: (1.92, 1.96)
    backward-recv ..................................: (96.33, 97.05)
    backward-send ..................................: (1.96, 2.05)
    forward-send-backward-recv .....................: (2421.25, 2525.91)
    backward-send-forward-recv .....................: (908.09, 1038.29)
    layernorm-grads-all-reduce .....................: (0.02, 0.04)
    embedding-grads-all-reduce .....................: (5.89, 6.26)
    grads-reduce-scatter ...........................: (15.44, 15.93)
    params-all-gather ..............................: (8.34, 8.82)
    optimizer-copy-to-main-grad ....................: (0.70, 0.95)
    optimizer-clip-main-grad .......................: (4.51, 4.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.18, 9.43)
    optimizer-copy-main-to-model-params ............: (2.94, 3.07)
    optimizer ......................................: (18.41, 18.53)
Tue Feb 13 01:17:46 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             492W / 700W |  40614MiB / 81559MiB |     29%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   48C    P0             487W / 700W |  41058MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   49C    P0             470W / 700W |  41038MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0             485W / 700W |  40818MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   41C    P0             493W / 700W |  40626MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             515W / 700W |  41110MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             436W / 700W |  41050MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   41C    P0             412W / 700W |  40810MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 32340.7 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.170182E+00 | loss scale: 1.0 | grad norm: 1.297 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (32050.49, 32195.81)
    forward-compute ................................: (11511.01, 12603.61)
    backward-compute ...............................: (18229.40, 18867.00)
    batch-generator ................................: (100.45, 159.85)
    forward-recv ...................................: (88.54, 88.80)
    forward-send ...................................: (1.92, 1.97)
    backward-recv ..................................: (96.93, 97.80)
    backward-send ..................................: (1.95, 2.06)
    forward-send-backward-recv .....................: (2185.79, 2255.76)
    backward-send-forward-recv .....................: (646.00, 711.37)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.80, 6.55)
    grads-reduce-scatter ...........................: (15.17, 15.85)
    params-all-gather ..............................: (8.33, 8.78)
    optimizer-copy-to-main-grad ....................: (0.70, 0.92)
    optimizer-clip-main-grad .......................: (4.23, 5.27)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.19, 9.36)
    optimizer-copy-main-to-model-params ............: (2.94, 3.06)
    optimizer ......................................: (18.96, 19.09)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 32308.7 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.133055E+00 | loss scale: 1.0 | grad norm: 0.899 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (32108.47, 32253.70)
    forward-compute ................................: (11773.02, 12412.22)
    backward-compute ...............................: (18239.67, 18878.26)
    batch-generator ................................: (99.59, 156.32)
    forward-recv ...................................: (88.58, 88.72)
    forward-send ...................................: (1.92, 1.97)
    backward-recv ..................................: (96.50, 97.23)
    backward-send ..................................: (1.96, 2.04)
    forward-send-backward-recv .....................: (1917.41, 2019.19)
    backward-send-forward-recv .....................: (911.52, 1030.07)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.84, 6.29)
    grads-reduce-scatter ...........................: (15.23, 15.86)
    params-all-gather ..............................: (8.31, 8.83)
    optimizer-copy-to-main-grad ....................: (0.70, 0.93)
    optimizer-clip-main-grad .......................: (4.23, 4.26)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.20, 9.33)
    optimizer-copy-main-to-model-params ............: (2.94, 3.06)
    optimizer ......................................: (17.95, 18.08)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 32250.5 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.097836E+00 | loss scale: 1.0 | grad norm: 0.946 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (32050.35, 32195.19)
    forward-compute ................................: (11509.31, 12606.64)
    backward-compute ...............................: (18232.92, 18861.71)
    batch-generator ................................: (99.82, 157.98)
    forward-recv ...................................: (88.62, 88.77)
    forward-send ...................................: (1.92, 1.96)
    backward-recv ..................................: (96.56, 412.02)
    backward-send ..................................: (1.96, 2.06)
    forward-send-backward-recv .....................: (1901.06, 2254.05)
    backward-send-forward-recv .....................: (647.17, 711.68)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.81, 6.54)
    grads-reduce-scatter ...........................: (15.48, 15.93)
    params-all-gather ..............................: (8.32, 8.80)
    optimizer-copy-to-main-grad ....................: (0.70, 0.91)
    optimizer-clip-main-grad .......................: (4.22, 4.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.18, 9.35)
    optimizer-copy-main-to-model-params ............: (2.94, 3.06)
    optimizer ......................................: (18.05, 18.18)
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (371178): No such process
kill: (371184): No such process
kill: (371190): No such process
kill: (371196): No such process
10.64.24.49 kill done.
13b, 4k, gbs=512: dp=2, tp=4, pp=2, mbs=1
LOCAL_IP = 10.64.24.52
DP=2, MP=4, PP=2
[2024-02-13 01:30:47,502] torch.distributed.run: [WARNING] 
[2024-02-13 01:30:47,502] torch.distributed.run: [WARNING] *****************************************
[2024-02-13 01:30:47,502] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-13 01:30:47,502] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 1638548480
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1638548480
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1638548480
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 1638548480
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.207 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  6.806 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.998 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.300 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (914.32, 961.51)
    train/valid/test-data-iterators-setup ..........: (0.02, 12588.77)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 36653.9 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 8.177289E+00 | loss scale: 1.0 | grad norm: 252.192 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 18915.4521484375 | max allocated: 25896.1064453125 | reserved: 27540.0 | max reserved: 27540.0[Rank 11] (after 10 iterations) memory (MB) | allocated: 18915.4521484375 | max allocated: 25896.1064453125 | reserved: 27408.0 | max reserved: 27408.0

[Rank 9] (after 10 iterations) memory (MB) | allocated: 18915.4521484375 | max allocated: 25896.1064453125 | reserved: 27378.0 | max reserved: 27378.0[Rank 10] (after 10 iterations) memory (MB) | allocated: 18915.4521484375 | max allocated: 25896.1064453125 | reserved: 27388.0 | max reserved: 27388.0

(min, max) time across ranks (ms):
    forward-backward ...............................: (36305.07, 36385.67)
    forward-compute ................................: (13262.69, 14756.09)
    backward-compute ...............................: (20183.96, 20656.32)
    batch-generator ................................: (601.04, 666.32)
    forward-recv ...................................: (480.80, 493.92)
    forward-send ...................................: (3.04, 10.76)
    backward-recv ..................................: (50.31, 51.09)
    backward-send ..................................: (1.03, 1.07)
    forward-send-backward-recv .....................: (2541.65, 2774.84)
    backward-send-forward-recv .....................: (501.57, 1348.21)
    layernorm-grads-all-reduce .....................: (0.02, 0.04)
    embedding-grads-all-reduce .....................: (5.95, 6.36)
    grads-reduce-scatter ...........................: (15.29, 223.93)
    params-all-gather ..............................: (8.39, 8.82)
    optimizer-copy-to-main-grad ....................: (0.72, 0.94)
    optimizer-clip-main-grad .......................: (7.46, 7.49)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.44, 9.76)
    optimizer-copy-main-to-model-params ............: (2.96, 3.05)
    optimizer ......................................: (21.91, 22.02)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 35476.0 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 2.271310E+00 | loss scale: 1.0 | grad norm: 39.923 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (35339.48, 35419.48)
    forward-compute ................................: (12780.33, 14372.07)
    backward-compute ...............................: (20061.47, 20571.23)
    batch-generator ................................: (193.04, 275.06)
    forward-recv ...................................: (47.20, 47.28)
    forward-send ...................................: (1.01, 1.03)
    backward-recv ..................................: (50.06, 50.98)
    backward-send ..................................: (1.04, 1.06)
    forward-send-backward-recv .....................: (2160.21, 2414.99)
    backward-send-forward-recv .....................: (430.90, 1258.76)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.84, 6.55)
    grads-reduce-scatter ...........................: (15.39, 15.82)
    params-all-gather ..............................: (8.38, 8.83)
    optimizer-copy-to-main-grad ....................: (0.69, 0.91)
    optimizer-clip-main-grad .......................: (4.50, 4.53)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.17, 9.40)
    optimizer-copy-main-to-model-params ............: (2.95, 3.04)
    optimizer ......................................: (18.26, 18.35)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 36260.5 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.459161E+00 | loss scale: 1.0 | grad norm: 14.065 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (36124.88, 36204.51)
    forward-compute ................................: (12765.78, 14350.75)
    backward-compute ...............................: (20072.49, 20577.77)
    batch-generator ................................: (190.68, 261.10)
    forward-recv ...................................: (47.10, 47.25)
    forward-send ...................................: (1.01, 1.02)
    backward-recv ..................................: (50.36, 50.80)
    backward-send ..................................: (1.04, 1.08)
    forward-send-backward-recv .....................: (3077.97, 3206.08)
    backward-send-forward-recv .....................: (1229.93, 1253.85)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.91, 6.30)
    grads-reduce-scatter ...........................: (15.42, 15.89)
    params-all-gather ..............................: (8.37, 8.82)
    optimizer-copy-to-main-grad ....................: (0.71, 0.88)
    optimizer-clip-main-grad .......................: (4.48, 4.51)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.18, 9.38)
    optimizer-copy-main-to-model-params ............: (2.95, 3.04)
    optimizer ......................................: (18.16, 18.25)
Tue Feb 13 01:55:31 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   39C    P0             543W / 700W |  31214MiB / 81559MiB |     95%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   45C    P0             526W / 700W |  31298MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   46C    P0             512W / 700W |  31506MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0             499W / 700W |  31286MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             461W / 700W |  31066MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   46C    P0             522W / 700W |  31580MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   45C    P0             487W / 700W |  31516MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             499W / 700W |  31190MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 36351.9 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.168637E+00 | loss scale: 1.0 | grad norm: 2.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (36125.05, 36204.59)
    forward-compute ................................: (12737.11, 14369.12)
    backward-compute ...............................: (20064.48, 20569.17)
    batch-generator ................................: (191.44, 257.50)
    forward-recv ...................................: (47.20, 47.37)
    forward-send ...................................: (1.01, 1.02)
    backward-recv ..................................: (50.07, 50.99)
    backward-send ..................................: (1.03, 1.06)
    forward-send-backward-recv .....................: (3079.73, 3251.85)
    backward-send-forward-recv .....................: (1234.12, 1251.37)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.87, 6.54)
    grads-reduce-scatter ...........................: (15.27, 16.03)
    params-all-gather ..............................: (8.29, 8.82)
    optimizer-copy-to-main-grad ....................: (0.71, 0.87)
    optimizer-clip-main-grad .......................: (4.48, 4.52)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.18, 9.32)
    optimizer-copy-main-to-model-params ............: (2.95, 3.04)
    optimizer ......................................: (18.20, 18.29)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 35732.2 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.297852E+00 | loss scale: 1.0 | grad norm: 1.292 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (35596.56, 35676.22)
    forward-compute ................................: (12747.65, 13832.75)
    backward-compute ...............................: (20068.53, 20569.16)
    batch-generator ................................: (191.07, 255.77)
    forward-recv ...................................: (47.02, 47.21)
    forward-send ...................................: (1.02, 1.04)
    backward-recv ..................................: (50.42, 50.90)
    backward-send ..................................: (1.04, 1.07)
    forward-send-backward-recv .....................: (2533.87, 2706.95)
    backward-send-forward-recv .....................: (1232.35, 1251.35)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.87, 6.26)
    grads-reduce-scatter ...........................: (15.39, 15.87)
    params-all-gather ..............................: (8.33, 8.83)
    optimizer-copy-to-main-grad ....................: (0.70, 0.89)
    optimizer-clip-main-grad .......................: (4.48, 4.51)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.18, 9.34)
    optimizer-copy-main-to-model-params ............: (2.95, 3.15)
    optimizer ......................................: (18.15, 18.34)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 36204.7 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.171769E+00 | loss scale: 1.0 | grad norm: 3.339 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (36068.71, 36148.27)
    forward-compute ................................: (12748.04, 14343.71)
    backward-compute ...............................: (20067.19, 20548.24)
    batch-generator ................................: (193.59, 244.96)
    forward-recv ...................................: (46.97, 47.23)
    forward-send ...................................: (1.01, 1.02)
    backward-recv ..................................: (50.37, 50.74)
    backward-send ..................................: (1.03, 1.07)
    forward-send-backward-recv .....................: (3019.33, 3181.68)
    backward-send-forward-recv .....................: (1222.66, 1252.66)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.85, 6.40)
    grads-reduce-scatter ...........................: (15.27, 15.90)
    params-all-gather ..............................: (8.35, 8.84)
    optimizer-copy-to-main-grad ....................: (0.71, 0.87)
    optimizer-clip-main-grad .......................: (4.51, 4.54)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.18, 9.35)
    optimizer-copy-main-to-model-params ............: (2.95, 3.04)
    optimizer ......................................: (18.18, 18.27)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 36343.0 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.161569E+00 | loss scale: 1.0 | grad norm: 4.155 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (36207.00, 36286.96)
    forward-compute ................................: (12802.77, 14479.85)
    backward-compute ...............................: (20079.42, 20562.45)
    batch-generator ................................: (192.12, 270.50)
    forward-recv ...................................: (47.08, 47.13)
    forward-send ...................................: (1.01, 1.03)
    backward-recv ..................................: (50.39, 50.76)
    backward-send ..................................: (1.04, 1.11)
    forward-send-backward-recv .....................: (3106.35, 3175.64)
    backward-send-forward-recv .....................: (1223.80, 1362.02)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.90, 6.27)
    grads-reduce-scatter ...........................: (15.32, 15.86)
    params-all-gather ..............................: (8.38, 8.81)
    optimizer-copy-to-main-grad ....................: (0.70, 0.91)
    optimizer-clip-main-grad .......................: (4.48, 4.52)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.18, 9.34)
    optimizer-copy-main-to-model-params ............: (2.95, 3.05)
    optimizer ......................................: (18.20, 18.30)
Tue Feb 13 02:19:31 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   42C    P0             546W / 700W |  31214MiB / 81559MiB |     93%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   48C    P0             534W / 700W |  31496MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   49C    P0             482W / 700W |  31506MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0             472W / 700W |  31286MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   41C    P0             467W / 700W |  31066MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             463W / 700W |  31580MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             471W / 700W |  31516MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   41C    P0             441W / 700W |  31190MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 35724.1 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.185464E+00 | loss scale: 1.0 | grad norm: 3.955 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (35494.62, 35574.93)
    forward-compute ................................: (12794.49, 13998.10)
    backward-compute ...............................: (20098.48, 20587.05)
    batch-generator ................................: (200.14, 306.34)
    forward-recv ...................................: (47.14, 47.25)
    forward-send ...................................: (1.01, 1.05)
    backward-recv ..................................: (49.41, 50.67)
    backward-send ..................................: (1.03, 1.08)
    forward-send-backward-recv .....................: (2201.07, 2424.51)
    backward-send-forward-recv .....................: (958.84, 1423.02)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.78, 6.61)
    grads-reduce-scatter ...........................: (15.36, 15.93)
    params-all-gather ..............................: (8.41, 8.81)
    optimizer-copy-to-main-grad ....................: (0.71, 0.95)
    optimizer-clip-main-grad .......................: (4.47, 4.51)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.22, 9.61)
    optimizer-copy-main-to-model-params ............: (2.95, 3.05)
    optimizer ......................................: (18.59, 18.69)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 35914.4 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.139546E+00 | loss scale: 1.0 | grad norm: 1.327 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (35777.01, 35857.52)
    forward-compute ................................: (12805.73, 14524.76)
    backward-compute ...............................: (20094.39, 20595.28)
    batch-generator ................................: (206.04, 302.54)
    forward-recv ...................................: (47.09, 47.19)
    forward-send ...................................: (1.01, 1.04)
    backward-recv ..................................: (50.09, 51.04)
    backward-send ..................................: (1.04, 1.09)
    forward-send-backward-recv .....................: (2481.08, 2687.53)
    backward-send-forward-recv .....................: (700.74, 1432.30)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.84, 6.29)
    grads-reduce-scatter ...........................: (15.36, 15.90)
    params-all-gather ..............................: (8.42, 8.80)
    optimizer-copy-to-main-grad ....................: (0.71, 0.96)
    optimizer-clip-main-grad .......................: (4.26, 5.23)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.19, 9.34)
    optimizer-copy-main-to-model-params ............: (2.95, 3.05)
    optimizer ......................................: (18.93, 19.03)
[2024-02-13 02:30:45,347] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-13 02:30:45,348] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 425219 closing signal SIGTERM
[2024-02-13 02:30:45,349] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 425220 closing signal SIGTERM
[2024-02-13 02:30:45,349] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 425221 closing signal SIGTERM
[2024-02-13 02:30:45,349] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 425222 closing signal SIGTERM
[2024-02-13 02:30:45,349] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 425223 closing signal SIGTERM
[2024-02-13 02:30:45,349] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 425224 closing signal SIGTERM
[2024-02-13 02:30:45,349] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 425225 closing signal SIGTERM
[2024-02-13 02:30:45,349] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 425226 closing signal SIGTERM
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (372290): No such process
kill: (372296): No such process
kill: (372302): No such process
kill: (372308): No such process
10.64.24.49 kill done.
13b, 4k, gbs=512: dp=2, tp=2, pp=4, mbs=2
LOCAL_IP = 10.64.24.52
DP=2, MP=2, PP=4
[2024-02-13 02:31:21,765] torch.distributed.run: [WARNING] 
[2024-02-13 02:31:21,765] torch.distributed.run: [WARNING] *****************************************
[2024-02-13 02:31:21,765] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-13 02:31:21,765] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1573350400
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 1573350400
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 1702466560
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1702466560
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.571 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.689 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.736 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.773 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.088 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  4.972 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.116 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.179 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (47.45, 638.18)
    train/valid/test-data-iterators-setup ..........: (0.02, 10313.45)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 28593.8 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 9.984671E+00 | loss scale: 1.0 | grad norm: 683.929 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 18250.712890625 | max allocated: 37305.958984375 | reserved: 39768.0 | max reserved: 39768.0
[Rank 9] (after 10 iterations) memory (MB) | allocated: 18250.712890625 | max allocated: 37305.958984375 | reserved: 39614.0 | max reserved: 39614.0
[Rank 13] (after 10 iterations) memory (MB) | allocated: 19727.3291015625 | max allocated: 31065.5419921875 | reserved: 33264.0 | max reserved: 33264.0[Rank 12] (after 10 iterations) memory (MB) | allocated: 19727.3291015625 | max allocated: 31065.5419921875 | reserved: 33932.0 | max reserved: 33932.0

(min, max) time across ranks (ms):
    forward-backward ...............................: (27959.38, 28326.31)
    forward-compute ................................: (8775.09, 10124.16)
    backward-compute ...............................: (15281.82, 16994.94)
    batch-generator ................................: (195.30, 240.88)
    forward-recv ...................................: (229.35, 641.64)
    forward-send ...................................: (7.81, 251.22)
    backward-recv ..................................: (86.71, 266.18)
    backward-send ..................................: (1.72, 5.04)
    forward-send-backward-recv .....................: (2766.58, 3392.15)
    backward-send-forward-recv .....................: (332.55, 423.42)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 11.84)
    grads-reduce-scatter ...........................: (16.11, 218.56)
    params-all-gather ..............................: (7.57, 8.79)
    optimizer-copy-to-main-grad ....................: (0.36, 0.49)
    optimizer-clip-main-grad .......................: (7.11, 7.37)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.95, 9.96)
    optimizer-copy-main-to-model-params ............: (2.60, 2.89)
    optimizer ......................................: (21.18, 21.49)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 27505.3 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 2.099213E+00 | loss scale: 1.0 | grad norm: 18.187 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (27080.00, 27444.48)
    forward-compute ................................: (8627.65, 9771.02)
    backward-compute ...............................: (15205.02, 16924.55)
    batch-generator ................................: (89.00, 118.72)
    forward-recv ...................................: (71.11, 203.82)
    forward-send ...................................: (1.56, 6.39)
    backward-recv ..................................: (87.09, 266.83)
    backward-send ..................................: (1.70, 5.05)
    forward-send-backward-recv .....................: (2470.93, 3031.16)
    backward-send-forward-recv .....................: (322.80, 393.46)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 12.03)
    grads-reduce-scatter ...........................: (14.84, 16.56)
    params-all-gather ..............................: (7.67, 8.75)
    optimizer-copy-to-main-grad ....................: (0.35, 0.51)
    optimizer-clip-main-grad .......................: (4.18, 4.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.70, 9.65)
    optimizer-copy-main-to-model-params ............: (2.60, 2.88)
    optimizer ......................................: (17.61, 17.89)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 27776.0 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.432051E+00 | loss scale: 1.0 | grad norm: 4.841 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (27351.28, 27715.77)
    forward-compute ................................: (8619.29, 10069.48)
    backward-compute ...............................: (15198.89, 16924.21)
    batch-generator ................................: (85.75, 124.62)
    forward-recv ...................................: (71.45, 203.98)
    forward-send ...................................: (1.58, 6.17)
    backward-recv ..................................: (84.98, 263.93)
    backward-send ..................................: (1.71, 5.06)
    forward-send-backward-recv .....................: (2684.35, 3248.19)
    backward-send-forward-recv .....................: (322.93, 668.71)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 11.73)
    grads-reduce-scatter ...........................: (14.97, 16.61)
    params-all-gather ..............................: (7.58, 8.74)
    optimizer-copy-to-main-grad ....................: (0.36, 0.47)
    optimizer-clip-main-grad .......................: (4.18, 4.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 9.66)
    optimizer-copy-main-to-model-params ............: (2.60, 2.88)
    optimizer ......................................: (17.57, 17.85)
Tue Feb 13 02:50:30 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             423W / 700W |  43108MiB / 81559MiB |     23%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   47C    P0             465W / 700W |  43248MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   48C    P0             352W / 700W |  42928MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0             404W / 700W |  42914MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   41C    P0             372W / 700W |  37358MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             363W / 700W |  36690MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             337W / 700W |  37440MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   41C    P0             303W / 700W |  37640MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 27873.6 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.156690E+00 | loss scale: 1.0 | grad norm: 2.997 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (27359.33, 27724.10)
    forward-compute ................................: (8630.06, 9859.07)
    backward-compute ...............................: (15224.43, 16921.78)
    batch-generator ................................: (88.42, 122.41)
    forward-recv ...................................: (71.19, 203.70)
    forward-send ...................................: (1.55, 6.19)
    backward-recv ..................................: (86.43, 266.45)
    backward-send ..................................: (1.72, 5.07)
    forward-send-backward-recv .....................: (2502.12, 3003.56)
    backward-send-forward-recv .....................: (324.18, 675.30)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 12.08)
    grads-reduce-scatter ...........................: (14.56, 16.52)
    params-all-gather ..............................: (7.60, 8.79)
    optimizer-copy-to-main-grad ....................: (0.36, 0.48)
    optimizer-clip-main-grad .......................: (4.18, 4.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 9.65)
    optimizer-copy-main-to-model-params ............: (2.60, 2.88)
    optimizer ......................................: (17.57, 17.85)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 27747.5 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.288926E+00 | loss scale: 1.0 | grad norm: 2.558 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (27323.88, 27686.99)
    forward-compute ................................: (8635.14, 10025.09)
    backward-compute ...............................: (15246.00, 16901.65)
    batch-generator ................................: (87.71, 118.48)
    forward-recv ...................................: (70.99, 203.78)
    forward-send ...................................: (1.57, 6.24)
    backward-recv ..................................: (86.57, 264.03)
    backward-send ..................................: (1.71, 5.08)
    forward-send-backward-recv .....................: (2747.61, 3248.83)
    backward-send-forward-recv .....................: (322.93, 392.40)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 11.80)
    grads-reduce-scatter ...........................: (14.73, 16.64)
    params-all-gather ..............................: (7.60, 8.76)
    optimizer-copy-to-main-grad ....................: (0.37, 0.48)
    optimizer-clip-main-grad .......................: (4.19, 4.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 9.65)
    optimizer-copy-main-to-model-params ............: (2.60, 2.88)
    optimizer ......................................: (17.56, 17.84)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 27510.9 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.166569E+00 | loss scale: 1.0 | grad norm: 2.128 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (27085.76, 27449.76)
    forward-compute ................................: (8634.71, 9792.64)
    backward-compute ...............................: (15250.01, 16924.74)
    batch-generator ................................: (86.61, 117.46)
    forward-recv ...................................: (71.01, 203.49)
    forward-send ...................................: (1.56, 6.94)
    backward-recv ..................................: (86.80, 265.76)
    backward-send ..................................: (1.70, 5.08)
    forward-send-backward-recv .....................: (2510.53, 2993.04)
    backward-send-forward-recv .....................: (322.82, 392.00)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 12.00)
    grads-reduce-scatter ...........................: (14.82, 16.60)
    params-all-gather ..............................: (7.58, 8.79)
    optimizer-copy-to-main-grad ....................: (0.36, 0.47)
    optimizer-clip-main-grad .......................: (4.18, 4.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.70, 9.65)
    optimizer-copy-main-to-model-params ............: (2.60, 2.88)
    optimizer ......................................: (17.59, 17.88)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 27477.6 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.149401E+00 | loss scale: 1.0 | grad norm: 1.937 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (27052.92, 27416.96)
    forward-compute ................................: (8634.63, 9762.41)
    backward-compute ...............................: (15241.97, 16904.22)
    batch-generator ................................: (83.05, 115.95)
    forward-recv ...................................: (71.08, 203.54)
    forward-send ...................................: (1.56, 6.10)
    backward-recv ..................................: (86.67, 265.05)
    backward-send ..................................: (1.72, 5.12)
    forward-send-backward-recv .....................: (2478.22, 2978.08)
    backward-send-forward-recv .....................: (322.80, 391.76)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 11.74)
    grads-reduce-scatter ...........................: (14.61, 16.49)
    params-all-gather ..............................: (7.64, 8.75)
    optimizer-copy-to-main-grad ....................: (0.36, 0.47)
    optimizer-clip-main-grad .......................: (4.18, 4.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 9.65)
    optimizer-copy-main-to-model-params ............: (2.60, 2.88)
    optimizer ......................................: (17.61, 17.89)
Tue Feb 13 03:08:59 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             395W / 700W |  43108MiB / 81559MiB |     69%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   47C    P0             428W / 700W |  43248MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   48C    P0             356W / 700W |  42928MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0             433W / 700W |  42914MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   41C    P0             396W / 700W |  37358MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             316W / 700W |  37478MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             342W / 700W |  37440MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   41C    P0             302W / 700W |  37640MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 28094.8 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.169449E+00 | loss scale: 1.0 | grad norm: 1.471 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (27581.32, 27945.45)
    forward-compute ................................: (8622.39, 10042.02)
    backward-compute ...............................: (15204.98, 16894.02)
    batch-generator ................................: (83.91, 118.47)
    forward-recv ...................................: (71.04, 203.48)
    forward-send ...................................: (1.55, 6.59)
    backward-recv ..................................: (87.05, 265.76)
    backward-send ..................................: (1.73, 5.18)
    forward-send-backward-recv .....................: (2752.47, 3503.01)
    backward-send-forward-recv .....................: (322.94, 666.33)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 11.94)
    grads-reduce-scatter ...........................: (14.60, 16.57)
    params-all-gather ..............................: (7.63, 9.24)
    optimizer-copy-to-main-grad ....................: (0.36, 0.46)
    optimizer-clip-main-grad .......................: (4.18, 4.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 9.66)
    optimizer-copy-main-to-model-params ............: (2.60, 2.88)
    optimizer ......................................: (17.56, 17.84)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 27818.0 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.129185E+00 | loss scale: 1.0 | grad norm: 3.082 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (27392.77, 27757.44)
    forward-compute ................................: (8632.78, 10104.26)
    backward-compute ...............................: (15235.83, 16901.53)
    batch-generator ................................: (85.77, 125.83)
    forward-recv ...................................: (71.10, 203.95)
    forward-send ...................................: (1.57, 6.26)
    backward-recv ..................................: (87.32, 265.08)
    backward-send ..................................: (1.70, 5.14)
    forward-send-backward-recv .....................: (2630.55, 3327.03)
    backward-send-forward-recv .....................: (322.56, 681.01)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 11.81)
    grads-reduce-scatter ...........................: (14.77, 16.64)
    params-all-gather ..............................: (7.58, 8.78)
    optimizer-copy-to-main-grad ....................: (0.36, 0.47)
    optimizer-clip-main-grad .......................: (3.97, 4.20)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.72, 9.71)
    optimizer-copy-main-to-model-params ............: (2.60, 2.89)
    optimizer ......................................: (17.45, 17.75)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 27462.6 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.092830E+00 | loss scale: 1.0 | grad norm: 3.404 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (27038.47, 27402.28)
    forward-compute ................................: (8614.83, 9760.17)
    backward-compute ...............................: (15198.37, 16890.73)
    batch-generator ................................: (83.98, 135.67)
    forward-recv ...................................: (71.37, 203.96)
    forward-send ...................................: (1.59, 6.32)
    backward-recv ..................................: (86.66, 265.16)
    backward-send ..................................: (1.72, 5.17)
    forward-send-backward-recv .....................: (2471.24, 2936.30)
    backward-send-forward-recv .....................: (322.64, 395.67)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 12.02)
    grads-reduce-scatter ...........................: (14.66, 16.46)
    params-all-gather ..............................: (7.60, 8.77)
    optimizer-copy-to-main-grad ....................: (0.36, 0.51)
    optimizer-clip-main-grad .......................: (3.74, 3.95)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.72, 9.72)
    optimizer-copy-main-to-model-params ............: (2.60, 2.89)
    optimizer ......................................: (17.18, 17.48)
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (373782): No such process
kill: (373788): No such process
kill: (373794): No such process
kill: (373800): No such process
10.64.24.49 kill done.
13b, 4k, gbs=512: dp=2, tp=2, pp=4, mbs=1
LOCAL_IP = 10.64.24.52
DP=2, MP=2, PP=4
[2024-02-13 03:20:26,445] torch.distributed.run: [WARNING] 
[2024-02-13 03:20:26,445] torch.distributed.run: [WARNING] *****************************************
[2024-02-13 03:20:26,445] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-13 03:20:26,445] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1573350400
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 1573350400
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1702466560
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 1702466560
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.797 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.799 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.805 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.864 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.181 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.190 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.265 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.534 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (44.57, 652.69)
    train/valid/test-data-iterators-setup ..........: (0.02, 11113.49)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 29966.3 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 9.981441E+00 | loss scale: 1.0 | grad norm: 683.751 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 18170.712890625 | max allocated: 27298.31494140625 | reserved: 28518.0 | max reserved: 28518.0[Rank 9] (after 10 iterations) memory (MB) | allocated: 18170.712890625 | max allocated: 27298.31494140625 | reserved: 28672.0 | max reserved: 28672.0

[Rank 12] (after 10 iterations) memory (MB) | allocated: 19647.3291015625 | max allocated: 25116.47216796875 | reserved: 26018.0 | max reserved: 26018.0
[Rank 13] (after 10 iterations) memory (MB) | allocated: 19647.3291015625 | max allocated: 25116.47216796875 | reserved: 26412.0 | max reserved: 26412.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (29492.45, 29688.23)
    forward-compute ................................: (9163.40, 10709.40)
    backward-compute ...............................: (16404.33, 18042.49)
    batch-generator ................................: (270.07, 340.88)
    forward-recv ...................................: (196.00, 546.81)
    forward-send ...................................: (5.79, 249.42)
    backward-recv ..................................: (44.94, 137.33)
    backward-send ..................................: (0.85, 2.42)
    forward-send-backward-recv .....................: (2861.79, 3297.10)
    backward-send-forward-recv .....................: (353.33, 453.70)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 11.79)
    grads-reduce-scatter ...........................: (16.26, 227.79)
    params-all-gather ..............................: (7.67, 8.76)
    optimizer-copy-to-main-grad ....................: (0.36, 0.49)
    optimizer-clip-main-grad .......................: (7.02, 7.27)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.94, 9.95)
    optimizer-copy-main-to-model-params ............: (2.60, 2.87)
    optimizer ......................................: (21.07, 21.37)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 28847.9 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 2.099320E+00 | loss scale: 1.0 | grad norm: 17.975 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (28590.79, 28786.39)
    forward-compute ................................: (8970.40, 10369.94)
    backward-compute ...............................: (16326.20, 17997.03)
    batch-generator ................................: (168.93, 213.75)
    forward-recv ...................................: (36.82, 105.87)
    forward-send ...................................: (0.86, 3.38)
    backward-recv ..................................: (44.68, 136.10)
    backward-send ..................................: (0.85, 2.43)
    forward-send-backward-recv .....................: (2588.42, 2902.78)
    backward-send-forward-recv .....................: (334.85, 408.86)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 11.99)
    grads-reduce-scatter ...........................: (14.61, 16.46)
    params-all-gather ..............................: (7.63, 8.73)
    optimizer-copy-to-main-grad ....................: (0.36, 0.47)
    optimizer-clip-main-grad .......................: (4.19, 4.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 9.63)
    optimizer-copy-main-to-model-params ............: (2.60, 2.86)
    optimizer ......................................: (17.55, 17.82)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 29781.2 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.431986E+00 | loss scale: 1.0 | grad norm: 4.819 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (29524.70, 29719.62)
    forward-compute ................................: (8967.03, 11017.31)
    backward-compute ...............................: (16373.81, 18254.33)
    batch-generator ................................: (176.09, 209.10)
    forward-recv ...................................: (71.54, 449.79)
    forward-send ...................................: (0.86, 3.41)
    backward-recv ..................................: (44.81, 136.25)
    backward-send ..................................: (0.84, 2.40)
    forward-send-backward-recv .....................: (3131.27, 3579.18)
    backward-send-forward-recv .....................: (334.47, 800.71)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 11.72)
    grads-reduce-scatter ...........................: (14.74, 16.51)
    params-all-gather ..............................: (7.62, 8.73)
    optimizer-copy-to-main-grad ....................: (0.35, 0.47)
    optimizer-clip-main-grad .......................: (4.20, 4.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.72, 9.64)
    optimizer-copy-main-to-model-params ............: (2.60, 2.87)
    optimizer ......................................: (17.82, 18.10)
Tue Feb 13 03:40:41 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             520W / 700W |  31858MiB / 81559MiB |     91%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   48C    P0             487W / 700W |  32012MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   49C    P0             454W / 700W |  32022MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0             411W / 700W |  31842MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   42C    P0             441W / 700W |  29444MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   50C    P0             444W / 700W |  29838MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   48C    P0             439W / 700W |  29412MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             420W / 700W |  29402MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 29548.4 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.156609E+00 | loss scale: 1.0 | grad norm: 2.833 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (29202.09, 29397.59)
    forward-compute ................................: (9030.17, 10406.97)
    backward-compute ...............................: (16355.25, 17973.29)
    batch-generator ................................: (175.72, 209.99)
    forward-recv ...................................: (36.64, 347.40)
    forward-send ...................................: (0.86, 279.27)
    backward-recv ..................................: (45.20, 137.55)
    backward-send ..................................: (0.85, 2.38)
    forward-send-backward-recv .....................: (2654.82, 3275.33)
    backward-send-forward-recv .....................: (333.98, 994.71)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 11.96)
    grads-reduce-scatter ...........................: (14.89, 16.44)
    params-all-gather ..............................: (7.63, 8.79)
    optimizer-copy-to-main-grad ....................: (0.36, 0.47)
    optimizer-clip-main-grad .......................: (4.21, 4.46)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.72, 9.65)
    optimizer-copy-main-to-model-params ............: (2.60, 2.87)
    optimizer ......................................: (17.64, 17.91)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 28796.3 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.288355E+00 | loss scale: 1.0 | grad norm: 3.040 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (28539.65, 28734.83)
    forward-compute ................................: (8981.01, 10330.33)
    backward-compute ...............................: (16372.73, 17961.79)
    batch-generator ................................: (177.49, 207.31)
    forward-recv ...................................: (37.05, 105.56)
    forward-send ...................................: (0.86, 2.88)
    backward-recv ..................................: (45.81, 137.74)
    backward-send ..................................: (0.84, 2.38)
    forward-send-backward-recv .....................: (2566.17, 2856.74)
    backward-send-forward-recv .....................: (334.50, 406.56)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 11.70)
    grads-reduce-scatter ...........................: (14.77, 16.46)
    params-all-gather ..............................: (7.58, 8.78)
    optimizer-copy-to-main-grad ....................: (0.36, 0.48)
    optimizer-clip-main-grad .......................: (4.19, 4.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 9.62)
    optimizer-copy-main-to-model-params ............: (2.60, 2.86)
    optimizer ......................................: (17.58, 17.84)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 28817.3 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.162077E+00 | loss scale: 1.0 | grad norm: 1.866 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (28560.13, 28755.49)
    forward-compute ................................: (8965.04, 10351.73)
    backward-compute ...............................: (16388.85, 17977.98)
    batch-generator ................................: (177.70, 205.10)
    forward-recv ...................................: (36.78, 105.68)
    forward-send ...................................: (0.85, 3.46)
    backward-recv ..................................: (45.56, 136.52)
    backward-send ..................................: (0.84, 2.40)
    forward-send-backward-recv .....................: (2562.48, 2866.86)
    backward-send-forward-recv .....................: (334.78, 408.76)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 11.85)
    grads-reduce-scatter ...........................: (14.85, 16.47)
    params-all-gather ..............................: (7.56, 8.77)
    optimizer-copy-to-main-grad ....................: (0.34, 0.47)
    optimizer-clip-main-grad .......................: (4.20, 4.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 9.63)
    optimizer-copy-main-to-model-params ............: (2.60, 2.87)
    optimizer ......................................: (17.57, 17.99)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 28818.6 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.148545E+00 | loss scale: 1.0 | grad norm: 1.452 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (28561.71, 28757.07)
    forward-compute ................................: (8965.72, 10345.18)
    backward-compute ...............................: (16394.57, 17977.41)
    batch-generator ................................: (177.55, 206.46)
    forward-recv ...................................: (36.85, 105.71)
    forward-send ...................................: (0.86, 3.64)
    backward-recv ..................................: (44.69, 136.79)
    backward-send ..................................: (0.85, 2.40)
    forward-send-backward-recv .....................: (2557.26, 2888.01)
    backward-send-forward-recv .....................: (335.02, 406.47)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 11.68)
    grads-reduce-scatter ...........................: (14.69, 16.44)
    params-all-gather ..............................: (7.60, 8.77)
    optimizer-copy-to-main-grad ....................: (0.36, 0.48)
    optimizer-clip-main-grad .......................: (4.20, 4.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 9.62)
    optimizer-copy-main-to-model-params ............: (2.60, 2.87)
    optimizer ......................................: (17.57, 17.96)
Tue Feb 13 03:59:55 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             569W / 700W |  31858MiB / 81559MiB |     53%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   48C    P0             401W / 700W |  32012MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   49C    P0             487W / 700W |  32022MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0             381W / 700W |  31842MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   42C    P0             471W / 700W |  29444MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   50C    P0             404W / 700W |  29838MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   48C    P0             423W / 700W |  29412MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   41C    P0             379W / 700W |  29796MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 28901.8 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.161419E+00 | loss scale: 1.0 | grad norm: 1.245 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (28556.25, 28751.12)
    forward-compute ................................: (8964.29, 10324.83)
    backward-compute ...............................: (16385.53, 17969.89)
    batch-generator ................................: (176.06, 217.18)
    forward-recv ...................................: (36.80, 105.83)
    forward-send ...................................: (0.86, 3.43)
    backward-recv ..................................: (45.10, 136.77)
    backward-send ..................................: (0.85, 2.43)
    forward-send-backward-recv .....................: (2552.29, 2857.82)
    backward-send-forward-recv .....................: (334.67, 410.71)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 11.97)
    grads-reduce-scatter ...........................: (14.71, 16.52)
    params-all-gather ..............................: (7.69, 8.73)
    optimizer-copy-to-main-grad ....................: (0.34, 0.46)
    optimizer-clip-main-grad .......................: (4.20, 4.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.72, 9.63)
    optimizer-copy-main-to-model-params ............: (2.60, 2.87)
    optimizer ......................................: (17.58, 17.85)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 28801.0 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.125571E+00 | loss scale: 1.0 | grad norm: 1.237 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (28544.08, 28739.50)
    forward-compute ................................: (8979.71, 10287.30)
    backward-compute ...............................: (16357.06, 17951.91)
    batch-generator ................................: (177.43, 212.00)
    forward-recv ...................................: (36.89, 105.56)
    forward-send ...................................: (0.86, 3.11)
    backward-recv ..................................: (44.78, 135.71)
    backward-send ..................................: (0.85, 2.44)
    forward-send-backward-recv .....................: (2535.59, 2859.11)
    backward-send-forward-recv .....................: (334.70, 404.63)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 11.74)
    grads-reduce-scatter ...........................: (14.83, 16.49)
    params-all-gather ..............................: (7.58, 8.75)
    optimizer-copy-to-main-grad ....................: (0.37, 0.48)
    optimizer-clip-main-grad .......................: (3.96, 4.18)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 9.64)
    optimizer-copy-main-to-model-params ............: (2.60, 2.87)
    optimizer ......................................: (17.33, 17.60)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 28788.3 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.096683E+00 | loss scale: 1.0 | grad norm: 1.218 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (28530.86, 28726.51)
    forward-compute ................................: (8987.67, 10292.35)
    backward-compute ...............................: (16367.45, 17948.55)
    batch-generator ................................: (174.93, 213.33)
    forward-recv ...................................: (36.95, 106.70)
    forward-send ...................................: (0.86, 3.36)
    backward-recv ..................................: (45.04, 135.41)
    backward-send ..................................: (0.84, 2.45)
    forward-send-backward-recv .....................: (2522.12, 2832.95)
    backward-send-forward-recv .....................: (334.94, 408.25)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 11.98)
    grads-reduce-scatter ...........................: (14.90, 16.43)
    params-all-gather ..............................: (7.62, 8.77)
    optimizer-copy-to-main-grad ....................: (0.35, 0.46)
    optimizer-clip-main-grad .......................: (4.21, 4.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 9.63)
    optimizer-copy-main-to-model-params ............: (2.60, 2.87)
    optimizer ......................................: (17.67, 17.94)
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (375274): No such process
kill: (375280): No such process
kill: (375286): No such process
kill: (375292): No such process
10.64.24.49 kill done.
13b, 4k, gbs=512: dp=2, tp=8, pp=1, mbs=2
LOCAL_IP = 10.64.24.52
DP=2, MP=8, PP=1
[2024-02-13 04:11:47,349] torch.distributed.run: [WARNING] 
[2024-02-13 04:11:47,349] torch.distributed.run: [WARNING] *****************************************
[2024-02-13 04:11:47,349] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-13 04:11:47,349] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.169 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.985 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (448.11, 532.78)
    train/valid/test-data-iterators-setup ..........: (0.02, 12457.29)
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 43016.9 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 8.233837E+00 | loss scale: 1.0 | grad norm: 328.806 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (42865.91, 42868.04)
    forward-compute ................................: (18459.38, 18499.89)
    backward-compute ...............................: (24178.53, 24323.85)
    batch-generator ................................: (762.94, 814.90)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.04)
    grads-reduce-scatter ...........................: (72.77, 73.99)
    params-all-gather ..............................: (38.73, 39.19)
    optimizer-copy-to-main-grad ....................: (1.42, 2.15)
    optimizer-clip-main-grad .......................: (8.21, 8.29)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.57, 9.80)
    optimizer-copy-main-to-model-params ............: (3.48, 4.81)
    optimizer ......................................: (26.95, 28.70)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 42581.2 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 2.289425E+00 | loss scale: 1.0 | grad norm: 51.301 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (42435.91, 42437.72)
    forward-compute ................................: (18094.76, 18126.74)
    backward-compute ...............................: (24120.48, 24259.11)
    batch-generator ................................: (133.14, 190.79)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.04)
    grads-reduce-scatter ...........................: (72.82, 73.67)
    params-all-gather ..............................: (38.79, 39.26)
    optimizer-copy-to-main-grad ....................: (1.39, 1.80)
    optimizer-clip-main-grad .......................: (5.39, 5.72)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.27, 9.39)
    optimizer-copy-main-to-model-params ............: (3.48, 3.60)
    optimizer ......................................: (23.53, 23.90)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 42074.2 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.552052E+00 | loss scale: 1.0 | grad norm: 5.656 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (41927.02, 41929.28)
    forward-compute ................................: (17562.93, 17615.53)
    backward-compute ...............................: (24123.53, 24282.23)
    batch-generator ................................: (130.99, 185.27)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.04)
    grads-reduce-scatter ...........................: (72.93, 75.30)
    params-all-gather ..............................: (38.74, 39.08)
    optimizer-copy-to-main-grad ....................: (1.39, 1.79)
    optimizer-clip-main-grad .......................: (5.31, 5.47)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.27, 9.38)
    optimizer-copy-main-to-model-params ............: (3.47, 3.60)
    optimizer ......................................: (23.13, 23.72)
Tue Feb 13 04:40:46 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             522W / 700W |  49148MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   47C    P0             496W / 700W |  49576MiB / 81559MiB |     97%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   48C    P0             483W / 700W |  49516MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0             459W / 700W |  49576MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   40C    P0             446W / 700W |  49476MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   48C    P0             446W / 700W |  49576MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   46C    P0             456W / 700W |  49416MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   40C    P0             437W / 700W |  49276MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 42672.1 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.220859E+00 | loss scale: 1.0 | grad norm: 5.261 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (42437.72, 42439.86)
    forward-compute ................................: (18086.06, 18136.97)
    backward-compute ...............................: (24118.58, 24269.05)
    batch-generator ................................: (131.92, 181.72)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.04)
    grads-reduce-scatter ...........................: (73.03, 73.54)
    params-all-gather ..............................: (38.62, 39.16)
    optimizer-copy-to-main-grad ....................: (1.38, 1.76)
    optimizer-clip-main-grad .......................: (5.47, 5.69)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.25, 9.38)
    optimizer-copy-main-to-model-params ............: (3.47, 3.60)
    optimizer ......................................: (23.24, 23.65)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 42314.9 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.320668E+00 | loss scale: 1.0 | grad norm: 2.032 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (42168.76, 42170.83)
    forward-compute ................................: (17825.43, 17857.39)
    backward-compute ...............................: (24126.44, 24261.22)
    batch-generator ................................: (132.69, 190.46)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.03, 0.04)
    grads-reduce-scatter ...........................: (72.72, 73.63)
    params-all-gather ..............................: (38.79, 39.23)
    optimizer-copy-to-main-grad ....................: (1.38, 1.94)
    optimizer-clip-main-grad .......................: (5.43, 5.51)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.27, 9.39)
    optimizer-copy-main-to-model-params ............: (3.47, 3.60)
    optimizer ......................................: (23.76, 24.42)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 42327.8 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.168908E+00 | loss scale: 1.0 | grad norm: 1.783 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (42182.56, 42184.46)
    forward-compute ................................: (17822.79, 17865.28)
    backward-compute ...............................: (24126.77, 24277.38)
    batch-generator ................................: (129.76, 187.23)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.04)
    grads-reduce-scatter ...........................: (72.80, 73.91)
    params-all-gather ..............................: (38.79, 39.27)
    optimizer-copy-to-main-grad ....................: (1.38, 1.78)
    optimizer-clip-main-grad .......................: (5.64, 5.74)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.25, 9.41)
    optimizer-copy-main-to-model-params ............: (3.48, 3.60)
    optimizer ......................................: (23.45, 23.87)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 42594.8 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.155447E+00 | loss scale: 1.0 | grad norm: 1.733 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (42449.22, 42451.47)
    forward-compute ................................: (18086.27, 18131.20)
    backward-compute ...............................: (24134.05, 24281.22)
    batch-generator ................................: (130.54, 186.11)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.03, 0.04)
    grads-reduce-scatter ...........................: (72.89, 74.09)
    params-all-gather ..............................: (38.66, 39.27)
    optimizer-copy-to-main-grad ....................: (1.38, 1.73)
    optimizer-clip-main-grad .......................: (5.24, 5.36)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.25, 9.37)
    optimizer-copy-main-to-model-params ............: (3.48, 3.60)
    optimizer ......................................: (23.23, 23.74)
Tue Feb 13 05:09:00 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             511W / 700W |  49148MiB / 81559MiB |     26%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   47C    P0             456W / 700W |  49576MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   48C    P0             468W / 700W |  49516MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0             500W / 700W |  49576MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   40C    P0             435W / 700W |  49476MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   48C    P0             450W / 700W |  49576MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   46C    P0             432W / 700W |  49416MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   40C    P0             431W / 700W |  49276MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 42149.7 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.171133E+00 | loss scale: 1.0 | grad norm: 1.796 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (41915.52, 41917.42)
    forward-compute ................................: (17553.18, 17593.12)
    backward-compute ...............................: (24135.37, 24279.46)
    batch-generator ................................: (129.54, 186.39)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.04)
    grads-reduce-scatter ...........................: (72.87, 73.52)
    params-all-gather ..............................: (38.73, 39.13)
    optimizer-copy-to-main-grad ....................: (1.39, 1.77)
    optimizer-clip-main-grad .......................: (5.33, 5.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.26, 9.40)
    optimizer-copy-main-to-model-params ............: (3.48, 3.61)
    optimizer ......................................: (23.25, 23.69)
[2024-02-13 05:11:45,196] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-13 05:11:45,198] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 429058 closing signal SIGTERM
[2024-02-13 05:11:45,198] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 429059 closing signal SIGTERM
[2024-02-13 05:11:45,199] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 429060 closing signal SIGTERM
[2024-02-13 05:11:45,199] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 429061 closing signal SIGTERM
[2024-02-13 05:11:45,199] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 429062 closing signal SIGTERM
[2024-02-13 05:11:45,199] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 429063 closing signal SIGTERM
[2024-02-13 05:11:45,199] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 429064 closing signal SIGTERM
[2024-02-13 05:11:45,199] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 429065 closing signal SIGTERM
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (376146): No such process
kill: (376152): No such process
kill: (376158): No such process
kill: (376164): No such process
10.64.24.49 kill done.
13b, 4k, gbs=512: dp=2, tp=8, pp=1, mbs=1
LOCAL_IP = 10.64.24.52
DP=2, MP=8, PP=1
[2024-02-13 05:12:21,601] torch.distributed.run: [WARNING] 
[2024-02-13 05:12:21,601] torch.distributed.run: [WARNING] *****************************************
[2024-02-13 05:12:21,601] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-13 05:12:21,601] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.118 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.019 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (453.36, 564.05)
    train/valid/test-data-iterators-setup ..........: (0.02, 10483.46)
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 49778.4 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 8.232994E+00 | loss scale: 1.0 | grad norm: 325.932 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (49627.46, 49628.74)
    forward-compute ................................: (22205.83, 22314.74)
    backward-compute ...............................: (27036.66, 27255.98)
    batch-generator ................................: (873.28, 945.50)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (72.98, 74.50)
    params-all-gather ..............................: (38.78, 39.29)
    optimizer-copy-to-main-grad ....................: (1.40, 1.80)
    optimizer-clip-main-grad .......................: (8.27, 8.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.59, 9.72)
    optimizer-copy-main-to-model-params ............: (3.47, 3.61)
    optimizer ......................................: (26.59, 27.14)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 49176.2 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 2.288751E+00 | loss scale: 1.0 | grad norm: 51.199 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (49030.38, 49031.57)
    forward-compute ................................: (21669.37, 21831.65)
    backward-compute ...............................: (26979.04, 27195.46)
    batch-generator ................................: (239.59, 328.38)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (72.76, 73.64)
    params-all-gather ..............................: (38.69, 39.30)
    optimizer-copy-to-main-grad ....................: (1.41, 1.75)
    optimizer-clip-main-grad .......................: (5.38, 5.69)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.26, 9.38)
    optimizer-copy-main-to-model-params ............: (3.47, 3.60)
    optimizer ......................................: (23.49, 23.84)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 49190.1 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.550831E+00 | loss scale: 1.0 | grad norm: 5.570 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (49043.39, 49045.28)
    forward-compute ................................: (21667.63, 21836.19)
    backward-compute ...............................: (26993.59, 27211.37)
    batch-generator ................................: (248.41, 333.99)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (72.76, 73.82)
    params-all-gather ..............................: (38.83, 39.08)
    optimizer-copy-to-main-grad ....................: (1.38, 1.74)
    optimizer-clip-main-grad .......................: (5.34, 5.51)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.27, 9.41)
    optimizer-copy-main-to-model-params ............: (3.47, 3.61)
    optimizer ......................................: (23.22, 23.81)
Tue Feb 13 05:45:47 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   39C    P0             460W / 700W |  34966MiB / 81559MiB |     86%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   46C    P0             480W / 700W |  35054MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   47C    P0             429W / 700W |  35218MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   40C    P0             446W / 700W |  35198MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             406W / 700W |  35172MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   46C    P0             422W / 700W |  35138MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   45C    P0             420W / 700W |  35222MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             408W / 700W |  35098MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 49262.1 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.219680E+00 | loss scale: 1.0 | grad norm: 5.260 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (49028.40, 49030.18)
    forward-compute ................................: (21654.34, 21825.09)
    backward-compute ...............................: (26984.58, 27216.56)
    batch-generator ................................: (247.16, 324.18)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (72.74, 73.85)
    params-all-gather ..............................: (38.73, 39.15)
    optimizer-copy-to-main-grad ....................: (1.38, 1.74)
    optimizer-clip-main-grad .......................: (5.51, 5.73)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.26, 9.38)
    optimizer-copy-main-to-model-params ............: (3.47, 3.60)
    optimizer ......................................: (23.30, 23.72)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 49175.5 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.322322E+00 | loss scale: 1.0 | grad norm: 2.407 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (49029.15, 49031.20)
    forward-compute ................................: (21656.62, 21813.64)
    backward-compute ...............................: (26991.50, 27215.36)
    batch-generator ................................: (243.83, 322.13)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (72.63, 74.01)
    params-all-gather ..............................: (38.80, 39.17)
    optimizer-copy-to-main-grad ....................: (1.40, 1.75)
    optimizer-clip-main-grad .......................: (5.43, 5.52)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.25, 9.38)
    optimizer-copy-main-to-model-params ............: (3.47, 3.60)
    optimizer ......................................: (23.02, 23.65)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 49161.9 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.167694E+00 | loss scale: 1.0 | grad norm: 2.031 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (49016.20, 49017.79)
    forward-compute ................................: (21649.63, 21808.74)
    backward-compute ...............................: (26981.38, 27208.57)
    batch-generator ................................: (242.01, 325.83)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (72.76, 73.51)
    params-all-gather ..............................: (38.65, 39.15)
    optimizer-copy-to-main-grad ....................: (1.41, 1.72)
    optimizer-clip-main-grad .......................: (5.68, 5.77)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.25, 9.40)
    optimizer-copy-main-to-model-params ............: (3.47, 3.60)
    optimizer ......................................: (23.48, 23.86)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 48909.2 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.150856E+00 | loss scale: 1.0 | grad norm: 1.261 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (48763.61, 48764.94)
    forward-compute ................................: (21393.30, 21551.43)
    backward-compute ...............................: (26986.71, 27211.32)
    batch-generator ................................: (241.79, 328.54)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (72.89, 73.59)
    params-all-gather ..............................: (38.66, 39.19)
    optimizer-copy-to-main-grad ....................: (1.38, 1.72)
    optimizer-clip-main-grad .......................: (5.28, 5.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.26, 9.34)
    optimizer-copy-main-to-model-params ............: (3.47, 3.60)
    optimizer ......................................: (23.30, 23.79)
[2024-02-13 06:12:19,463] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-13 06:12:19,464] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 429847 closing signal SIGTERM
[2024-02-13 06:12:19,465] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 429848 closing signal SIGTERM
[2024-02-13 06:12:19,465] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 429849 closing signal SIGTERM
[2024-02-13 06:12:19,465] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 429850 closing signal SIGTERM
[2024-02-13 06:12:19,465] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 429851 closing signal SIGTERM
[2024-02-13 06:12:19,465] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 429852 closing signal SIGTERM
[2024-02-13 06:12:19,465] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 429853 closing signal SIGTERM
[2024-02-13 06:12:19,465] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 429854 closing signal SIGTERM
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (377009): No such process
kill: (377015): No such process
kill: (377021): No such process
kill: (377027): No such process
10.64.24.49 kill done.
13b, 4k, gbs=512: dp=2, tp=1, pp=8, mbs=1
LOCAL_IP = 10.64.24.52
DP=2, MP=1, PP=8
[2024-02-13 06:12:55,898] torch.distributed.run: [WARNING] 
[2024-02-13 06:12:55,898] torch.distributed.run: [WARNING] *****************************************
[2024-02-13 06:12:55,898] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-13 06:12:55,898] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 1573196800
 > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 1573196800
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 1573196800
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 1830763520
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...

> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.707 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.814 s
Loading exists cache end, time cost:  4.814 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.821 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.869 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.877 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.881 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.990 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.075 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.216 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.127 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.244 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.247 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.198 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.228 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.188 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (43.30, 470.70)
    train/valid/test-data-iterators-setup ..........: (10040.39, 10497.01)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 28227.0 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 9.332465E+00 | loss scale: 1.0 | grad norm: 564.581 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 18148.541015625 | max allocated: 32302.310546875 | reserved: 33134.0 | max reserved: 33134.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 18148.541015625 | max allocated: 25415.42578125 | reserved: 26048.0 | max reserved: 26048.0
[Rank 10] (after 10 iterations) memory (MB) | allocated: 18148.541015625 | max allocated: 28858.8681640625 | reserved: 29690.0 | max reserved: 29690.0
[Rank 14] (after 10 iterations) memory (MB) | allocated: 21096.3701171875 | max allocated: 26545.19580078125 | reserved: 27452.0 | max reserved: 27452.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (27543.82, 27936.75)
    forward-compute ................................: (7290.18, 9560.36)
    backward-compute ...............................: (14005.78, 17086.48)
    batch-generator ................................: (192.42, 214.03)
    forward-recv ...................................: (79.92, 458.74)
    forward-send ...................................: (4.04, 207.10)
    backward-recv ..................................: (48.36, 332.04)
    backward-send ..................................: (0.92, 6.87)
    forward-send-backward-recv .....................: (4932.92, 5616.12)
    backward-send-forward-recv .....................: (348.01, 495.12)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 22.60)
    grads-reduce-scatter ...........................: (17.57, 226.31)
    params-all-gather ..............................: (7.32, 9.12)
    optimizer-copy-to-main-grad ....................: (0.22, 0.33)
    optimizer-clip-main-grad .......................: (7.30, 7.75)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.92, 10.52)
    optimizer-copy-main-to-model-params ............: (2.45, 2.91)
    optimizer ......................................: (21.81, 22.29)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 27495.4 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 2.175781E+00 | loss scale: 1.0 | grad norm: 27.935 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (27029.46, 27419.77)
    forward-compute ................................: (7228.32, 9364.02)
    backward-compute ...............................: (13959.14, 17060.08)
    batch-generator ................................: (171.79, 193.44)
    forward-recv ...................................: (33.05, 197.18)
    forward-send ...................................: (0.89, 9.23)
    backward-recv ..................................: (48.23, 332.06)
    backward-send ..................................: (0.91, 6.95)
    forward-send-backward-recv .....................: (4836.91, 5448.55)
    backward-send-forward-recv .....................: (330.96, 433.70)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 22.72)
    grads-reduce-scatter ...........................: (14.57, 17.78)
    params-all-gather ..............................: (7.34, 9.16)
    optimizer-copy-to-main-grad ....................: (0.21, 0.26)
    optimizer-clip-main-grad .......................: (4.20, 4.65)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 10.27)
    optimizer-copy-main-to-model-params ............: (2.45, 2.92)
    optimizer ......................................: (18.03, 18.50)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 27485.5 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.458241E+00 | loss scale: 1.0 | grad norm: 10.041 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (27019.76, 27410.92)
    forward-compute ................................: (7238.14, 9383.08)
    backward-compute ...............................: (13979.28, 17059.34)
    batch-generator ................................: (173.55, 188.32)
    forward-recv ...................................: (33.03, 197.17)
    forward-send ...................................: (0.89, 10.07)
    backward-recv ..................................: (47.90, 330.27)
    backward-send ..................................: (0.91, 7.02)
    forward-send-backward-recv .....................: (4806.30, 5408.06)
    backward-send-forward-recv .....................: (331.92, 432.79)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.37)
    grads-reduce-scatter ...........................: (14.78, 17.81)
    params-all-gather ..............................: (7.39, 9.17)
    optimizer-copy-to-main-grad ....................: (0.21, 0.27)
    optimizer-clip-main-grad .......................: (4.20, 4.64)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 10.26)
    optimizer-copy-main-to-model-params ............: (2.45, 2.91)
    optimizer ......................................: (18.10, 18.56)
Tue Feb 13 06:31:56 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   40C    P0             441W / 700W |  35994MiB / 81559MiB |     11%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   47C    P0             399W / 700W |  36000MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   48C    P0             430W / 700W |  32542MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0             209W / 700W |  32550MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   40C    P0             322W / 700W |  28900MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   48C    P0             211W / 700W |  28906MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   48C    P0             374W / 700W |  30398MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   41C    P0             261W / 700W |  30340MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 27590.0 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.159259E+00 | loss scale: 1.0 | grad norm: 2.322 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (27033.90, 27424.80)
    forward-compute ................................: (7234.44, 9372.72)
    backward-compute ...............................: (13976.07, 17063.33)
    batch-generator ................................: (173.59, 188.12)
    forward-recv ...................................: (32.96, 197.03)
    forward-send ...................................: (0.88, 9.44)
    backward-recv ..................................: (49.00, 331.47)
    backward-send ..................................: (0.91, 6.97)
    forward-send-backward-recv .....................: (4823.09, 5427.86)
    backward-send-forward-recv .....................: (332.35, 431.82)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.89)
    grads-reduce-scatter ...........................: (14.88, 17.94)
    params-all-gather ..............................: (7.44, 9.16)
    optimizer-copy-to-main-grad ....................: (0.21, 0.26)
    optimizer-clip-main-grad .......................: (4.20, 4.77)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.70, 10.26)
    optimizer-copy-main-to-model-params ............: (2.45, 2.91)
    optimizer ......................................: (18.54, 19.00)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 27448.3 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.292863E+00 | loss scale: 1.0 | grad norm: 3.216 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (26983.21, 27373.56)
    forward-compute ................................: (7234.63, 9353.23)
    backward-compute ...............................: (13973.18, 17033.81)
    batch-generator ................................: (172.92, 191.93)
    forward-recv ...................................: (32.95, 197.06)
    forward-send ...................................: (0.89, 9.76)
    backward-recv ..................................: (48.19, 329.65)
    backward-send ..................................: (0.90, 6.86)
    forward-send-backward-recv .....................: (4796.29, 5374.82)
    backward-send-forward-recv .....................: (332.65, 431.71)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.47)
    grads-reduce-scatter ...........................: (14.81, 17.90)
    params-all-gather ..............................: (7.43, 9.11)
    optimizer-copy-to-main-grad ....................: (0.21, 0.26)
    optimizer-clip-main-grad .......................: (4.19, 4.63)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 10.26)
    optimizer-copy-main-to-model-params ............: (2.45, 2.91)
    optimizer ......................................: (18.01, 18.47)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 27696.2 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.156054E+00 | loss scale: 1.0 | grad norm: 2.408 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (27230.02, 27621.14)
    forward-compute ................................: (7233.73, 9351.01)
    backward-compute ...............................: (13973.26, 17041.74)
    batch-generator ................................: (171.59, 195.02)
    forward-recv ...................................: (33.07, 197.13)
    forward-send ...................................: (0.89, 10.80)
    backward-recv ..................................: (48.25, 330.58)
    backward-send ..................................: (0.90, 6.79)
    forward-send-backward-recv .....................: (4803.11, 5377.90)
    backward-send-forward-recv .....................: (574.68, 680.59)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.58)
    grads-reduce-scatter ...........................: (14.61, 17.57)
    params-all-gather ..............................: (7.46, 9.10)
    optimizer-copy-to-main-grad ....................: (0.21, 0.27)
    optimizer-clip-main-grad .......................: (4.20, 4.64)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 10.26)
    optimizer-copy-main-to-model-params ............: (2.45, 2.91)
    optimizer ......................................: (18.03, 18.50)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 27454.8 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.147287E+00 | loss scale: 1.0 | grad norm: 2.511 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (26989.38, 27380.40)
    forward-compute ................................: (7232.93, 9342.99)
    backward-compute ...............................: (13984.78, 17025.99)
    batch-generator ................................: (172.35, 203.16)
    forward-recv ...................................: (33.26, 197.99)
    forward-send ...................................: (0.88, 9.80)
    backward-recv ..................................: (48.32, 329.65)
    backward-send ..................................: (0.90, 6.86)
    forward-send-backward-recv .....................: (4797.15, 5366.80)
    backward-send-forward-recv .....................: (333.72, 432.14)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 22.42)
    grads-reduce-scatter ...........................: (14.85, 17.57)
    params-all-gather ..............................: (7.38, 9.17)
    optimizer-copy-to-main-grad ....................: (0.21, 0.27)
    optimizer-clip-main-grad .......................: (4.32, 4.76)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 10.26)
    optimizer-copy-main-to-model-params ............: (2.45, 2.91)
    optimizer ......................................: (18.15, 18.62)
Tue Feb 13 06:50:18 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   40C    P0             273W / 700W |  35994MiB / 81559MiB |     67%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   47C    P0             530W / 700W |  36000MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   48C    P0             462W / 700W |  32542MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   40C    P0             234W / 700W |  32550MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   40C    P0             191W / 700W |  28900MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   48C    P0             227W / 700W |  28906MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             270W / 700W |  30398MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   41C    P0             263W / 700W |  30340MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 27572.3 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.169934E+00 | loss scale: 1.0 | grad norm: 3.855 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (27018.89, 27409.81)
    forward-compute ................................: (7241.82, 9376.98)
    backward-compute ...............................: (13996.77, 17060.11)
    batch-generator ................................: (171.24, 201.76)
    forward-recv ...................................: (32.89, 197.32)
    forward-send ...................................: (0.89, 10.07)
    backward-recv ..................................: (48.89, 331.46)
    backward-send ..................................: (0.91, 6.84)
    forward-send-backward-recv .....................: (4823.73, 5380.84)
    backward-send-forward-recv .....................: (333.22, 432.78)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 22.72)
    grads-reduce-scatter ...........................: (14.78, 17.49)
    params-all-gather ..............................: (7.41, 9.17)
    optimizer-copy-to-main-grad ....................: (0.21, 0.26)
    optimizer-clip-main-grad .......................: (4.20, 4.65)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 10.29)
    optimizer-copy-main-to-model-params ............: (2.45, 2.91)
    optimizer ......................................: (18.06, 18.53)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 27446.5 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.134862E+00 | loss scale: 1.0 | grad norm: 1.879 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (26979.79, 27370.57)
    forward-compute ................................: (7238.54, 9362.55)
    backward-compute ...............................: (13989.73, 17027.99)
    batch-generator ................................: (171.62, 194.43)
    forward-recv ...................................: (32.90, 197.02)
    forward-send ...................................: (0.88, 9.85)
    backward-recv ..................................: (48.41, 330.64)
    backward-send ..................................: (0.91, 6.90)
    forward-send-backward-recv .....................: (4820.80, 5357.18)
    backward-send-forward-recv .....................: (333.56, 433.79)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.66)
    grads-reduce-scatter ...........................: (14.85, 17.67)
    params-all-gather ..............................: (7.40, 9.14)
    optimizer-copy-to-main-grad ....................: (0.21, 0.26)
    optimizer-clip-main-grad .......................: (4.19, 4.63)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.70, 10.26)
    optimizer-copy-main-to-model-params ............: (2.45, 2.91)
    optimizer ......................................: (18.65, 19.11)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 27466.5 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.096221E+00 | loss scale: 1.0 | grad norm: 1.531 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (26999.90, 27390.69)
    forward-compute ................................: (7237.12, 9345.32)
    backward-compute ...............................: (13973.32, 17051.39)
    batch-generator ................................: (174.82, 199.96)
    forward-recv ...................................: (33.18, 197.08)
    forward-send ...................................: (0.89, 9.68)
    backward-recv ..................................: (47.91, 329.02)
    backward-send ..................................: (0.91, 6.87)
    forward-send-backward-recv .....................: (4818.05, 5389.30)
    backward-send-forward-recv .....................: (336.00, 433.63)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.73)
    grads-reduce-scatter ...........................: (14.86, 17.93)
    params-all-gather ..............................: (7.38, 9.15)
    optimizer-copy-to-main-grad ....................: (0.21, 0.26)
    optimizer-clip-main-grad .......................: (3.99, 4.96)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 10.26)
    optimizer-copy-main-to-model-params ............: (2.45, 2.91)
    optimizer ......................................: (18.49, 18.95)
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (379281): No such process
kill: (379287): No such process
kill: (379293): No such process
kill: (379299): No such process
10.64.24.49 kill done.

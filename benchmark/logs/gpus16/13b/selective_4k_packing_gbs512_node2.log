13b, 4k, gbs=512: dp=4, tp=2, pp=2, mbs=2
LOCAL_IP = 10.64.24.52
DP=4, MP=2, PP=2
[2024-02-12 12:24:18,341] torch.distributed.run: [WARNING] 
[2024-02-12 12:24:18,341] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 12:24:18,341] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 12:24:18,341] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 3275816960
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 3275816960
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.345 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.389 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.466 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.829 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.019 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.172 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.147 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.140 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (920.09, 959.41)
    train/valid/test-data-iterators-setup ..........: (0.02, 11507.02)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 11001.7 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.095017E+01 | loss scale: 1.0 | grad norm: 8.128 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 28241.7431640625 | max allocated: 45602.92138671875 | reserved: 49542.0 | max reserved: 49542.0[Rank 9] (after 10 iterations) memory (MB) | allocated: 28241.7177734375 | max allocated: 45603.24853515625 | reserved: 49440.0 | max reserved: 49440.0

(min, max) time across ranks (ms):
    forward-backward ...............................: (10363.58, 10452.42)
    forward-compute ................................: (1788.99, 5862.39)
    backward-compute ...............................: (3258.29, 3861.29)
    batch-generator ................................: (235.63, 278.15)
    forward-recv ...................................: (273.81, 291.20)
    forward-send ...................................: (2.81, 9.55)
    backward-recv ..................................: (94.67, 123.99)
    backward-send ..................................: (0.66, 0.92)
    forward-send-backward-recv .....................: (4920.04, 5209.68)
    backward-send-forward-recv .....................: (575.92, 725.41)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (11.21, 11.46)
    grads-reduce-scatter ...........................: (36.30, 432.34)
    params-all-gather ..............................: (19.57, 19.76)
    optimizer-copy-to-main-grad ....................: (0.36, 0.49)
    optimizer-clip-main-grad .......................: (7.31, 7.33)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.30, 9.60)
    optimizer-copy-main-to-model-params ............: (2.67, 2.74)
    optimizer ......................................: (20.97, 21.08)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 9236.8 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.082487E+01 | loss scale: 1.0 | grad norm: 5.740 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9047.52, 9100.21)
    forward-compute ................................: (1680.40, 4949.82)
    backward-compute ...............................: (3021.08, 3445.28)
    batch-generator ................................: (50.69, 71.13)
    forward-recv ...................................: (17.18, 21.94)
    forward-send ...................................: (0.40, 0.50)
    backward-recv ..................................: (54.73, 93.75)
    backward-send ..................................: (0.55, 2.18)
    forward-send-backward-recv .....................: (3996.52, 4281.36)
    backward-send-forward-recv .....................: (686.06, 843.14)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (11.01, 11.60)
    grads-reduce-scatter ...........................: (36.30, 36.81)
    params-all-gather ..............................: (19.53, 19.81)
    optimizer-copy-to-main-grad ....................: (0.34, 0.66)
    optimizer-clip-main-grad .......................: (4.28, 4.31)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.07, 9.24)
    optimizer-copy-main-to-model-params ............: (2.67, 2.74)
    optimizer ......................................: (17.41, 17.49)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 9973.6 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.059041E+01 | loss scale: 1.0 | grad norm: 2.230 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9783.58, 9856.72)
    forward-compute ................................: (1842.35, 5291.86)
    backward-compute ...............................: (3318.22, 3712.60)
    batch-generator ................................: (51.02, 64.86)
    forward-recv ...................................: (19.96, 29.37)
    forward-send ...................................: (0.44, 0.67)
    backward-recv ..................................: (65.93, 81.15)
    backward-send ..................................: (0.51, 18.36)
    forward-send-backward-recv .....................: (4495.78, 4557.73)
    backward-send-forward-recv .....................: (813.58, 882.90)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (11.06, 11.39)
    grads-reduce-scatter ...........................: (36.20, 36.77)
    params-all-gather ..............................: (19.55, 19.74)
    optimizer-copy-to-main-grad ....................: (0.34, 0.43)
    optimizer-clip-main-grad .......................: (4.25, 4.27)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.07, 9.23)
    optimizer-copy-main-to-model-params ............: (2.67, 2.73)
    optimizer ......................................: (17.13, 17.21)
Mon Feb 12 12:31:27 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   36C    P0             444W / 700W |  56066MiB / 81559MiB |     24%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   41C    P0             462W / 700W |  55966MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   42C    P0             328W / 700W |  55866MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0             359W / 700W |  55948MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   35C    P0             227W / 700W |  56994MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   42C    P0             215W / 700W |  56988MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   40C    P0             242W / 700W |  56672MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   35C    P0             270W / 700W |  56846MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 9193.1 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.057102E+01 | loss scale: 1.0 | grad norm: 1.557 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8901.74, 8982.96)
    forward-compute ................................: (1676.19, 4756.07)
    backward-compute ...............................: (3056.37, 3489.82)
    batch-generator ................................: (49.45, 62.71)
    forward-recv ...................................: (20.41, 32.24)
    forward-send ...................................: (0.47, 0.76)
    backward-recv ..................................: (86.77, 98.79)
    backward-send ..................................: (0.49, 0.70)
    forward-send-backward-recv .....................: (3848.60, 4096.83)
    backward-send-forward-recv .....................: (717.21, 781.03)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (11.05, 12.33)
    grads-reduce-scatter ...........................: (36.38, 36.76)
    params-all-gather ..............................: (19.55, 19.75)
    optimizer-copy-to-main-grad ....................: (0.34, 0.42)
    optimizer-clip-main-grad .......................: (4.01, 4.03)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.07, 9.18)
    optimizer-copy-main-to-model-params ............: (2.67, 2.73)
    optimizer ......................................: (16.76, 16.83)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 9800.7 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.047834E+01 | loss scale: 1.0 | grad norm: 3.289 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9601.03, 9681.04)
    forward-compute ................................: (1578.74, 5324.51)
    backward-compute ...............................: (3347.65, 3784.23)
    batch-generator ................................: (52.64, 62.84)
    forward-recv ...................................: (23.81, 28.20)
    forward-send ...................................: (0.55, 0.82)
    backward-recv ..................................: (63.66, 121.43)
    backward-send ..................................: (0.54, 20.07)
    forward-send-backward-recv .....................: (4411.33, 4624.38)
    backward-send-forward-recv .....................: (532.61, 623.36)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (11.08, 11.47)
    grads-reduce-scatter ...........................: (36.24, 36.80)
    params-all-gather ..............................: (19.57, 19.76)
    optimizer-copy-to-main-grad ....................: (0.34, 0.40)
    optimizer-clip-main-grad .......................: (4.23, 4.25)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.07, 9.18)
    optimizer-copy-main-to-model-params ............: (2.66, 2.73)
    optimizer ......................................: (16.96, 17.03)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 9522.4 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049564E+01 | loss scale: 1.0 | grad norm: 1.890 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9324.94, 9388.64)
    forward-compute ................................: (1765.51, 5064.33)
    backward-compute ...............................: (3220.26, 3668.09)
    batch-generator ................................: (50.53, 66.02)
    forward-recv ...................................: (17.60, 24.91)
    forward-send ...................................: (0.41, 0.53)
    backward-recv ..................................: (71.02, 97.20)
    backward-send ..................................: (0.68, 17.44)
    forward-send-backward-recv .....................: (4026.81, 4273.83)
    backward-send-forward-recv .....................: (683.08, 903.64)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (11.03, 11.36)
    grads-reduce-scatter ...........................: (36.27, 36.86)
    params-all-gather ..............................: (19.56, 19.75)
    optimizer-copy-to-main-grad ....................: (0.34, 0.41)
    optimizer-clip-main-grad .......................: (4.23, 4.26)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.07, 9.18)
    optimizer-copy-main-to-model-params ............: (2.66, 2.73)
    optimizer ......................................: (16.96, 17.03)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 9631.3 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.048131E+01 | loss scale: 1.0 | grad norm: 2.701 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9432.33, 9498.49)
    forward-compute ................................: (1755.08, 5171.21)
    backward-compute ...............................: (3212.71, 3593.06)
    batch-generator ................................: (51.42, 65.61)
    forward-recv ...................................: (18.52, 28.19)
    forward-send ...................................: (0.42, 0.59)
    backward-recv ..................................: (50.47, 93.89)
    backward-send ..................................: (0.60, 7.83)
    forward-send-backward-recv .....................: (4224.45, 4414.96)
    backward-send-forward-recv .....................: (663.06, 855.24)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (11.06, 11.36)
    grads-reduce-scatter ...........................: (36.30, 36.75)
    params-all-gather ..............................: (19.57, 19.74)
    optimizer-copy-to-main-grad ....................: (0.34, 0.40)
    optimizer-clip-main-grad .......................: (4.25, 4.27)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.07, 9.18)
    optimizer-copy-main-to-model-params ............: (2.67, 2.73)
    optimizer ......................................: (17.09, 17.16)
Mon Feb 12 12:37:47 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   38C    P0             347W / 700W |  56068MiB / 81559MiB |     82%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   44C    P0             527W / 700W |  55966MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   44C    P0             427W / 700W |  55866MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             494W / 700W |  55948MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             269W / 700W |  56994MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             416W / 700W |  56988MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   42C    P0             435W / 700W |  56672MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             380W / 700W |  56846MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 9054.7 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044758E+01 | loss scale: 1.0 | grad norm: 0.869 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8768.46, 8846.88)
    forward-compute ................................: (1548.16, 4660.75)
    backward-compute ...............................: (3263.94, 3706.54)
    batch-generator ................................: (52.99, 62.78)
    forward-recv ...................................: (19.27, 25.16)
    forward-send ...................................: (0.42, 0.58)
    backward-recv ..................................: (82.79, 98.27)
    backward-send ..................................: (0.59, 6.38)
    forward-send-backward-recv .....................: (3754.66, 3880.37)
    backward-send-forward-recv .....................: (490.86, 571.87)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (11.00, 11.56)
    grads-reduce-scatter ...........................: (36.25, 36.83)
    params-all-gather ..............................: (19.56, 19.78)
    optimizer-copy-to-main-grad ....................: (0.34, 0.43)
    optimizer-clip-main-grad .......................: (4.00, 4.02)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.07, 9.19)
    optimizer-copy-main-to-model-params ............: (2.67, 2.73)
    optimizer ......................................: (16.78, 16.84)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 9731.4 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.042806E+01 | loss scale: 1.0 | grad norm: 1.453 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9508.15, 9589.51)
    forward-compute ................................: (1751.61, 5327.79)
    backward-compute ...............................: (3225.61, 3746.32)
    batch-generator ................................: (51.47, 61.98)
    forward-recv ...................................: (20.95, 32.17)
    forward-send ...................................: (0.48, 0.69)
    backward-recv ..................................: (59.07, 86.76)
    backward-send ..................................: (0.57, 11.04)
    forward-send-backward-recv .....................: (4226.41, 4521.75)
    backward-send-forward-recv .....................: (630.60, 872.78)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (11.00, 11.32)
    grads-reduce-scatter ...........................: (36.27, 36.78)
    params-all-gather ..............................: (19.54, 19.89)
    optimizer-copy-to-main-grad ....................: (0.34, 0.42)
    optimizer-clip-main-grad .......................: (3.74, 3.89)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.07, 9.19)
    optimizer-copy-main-to-model-params ............: (2.67, 2.73)
    optimizer ......................................: (16.67, 16.74)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 9413.4 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041386E+01 | loss scale: 1.0 | grad norm: 1.763 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9192.75, 9271.78)
    forward-compute ................................: (1727.50, 4990.79)
    backward-compute ...............................: (3166.49, 3759.51)
    batch-generator ................................: (51.56, 64.35)
    forward-recv ...................................: (17.88, 26.27)
    forward-send ...................................: (0.43, 0.59)
    backward-recv ..................................: (66.45, 98.02)
    backward-send ..................................: (0.63, 13.32)
    forward-send-backward-recv .....................: (3927.57, 4256.50)
    backward-send-forward-recv .....................: (703.07, 793.22)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (11.04, 11.29)
    grads-reduce-scatter ...........................: (36.37, 36.86)
    params-all-gather ..............................: (19.53, 19.76)
    optimizer-copy-to-main-grad ....................: (0.34, 0.41)
    optimizer-clip-main-grad .......................: (3.49, 3.50)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.06, 9.17)
    optimizer-copy-main-to-model-params ............: (2.67, 2.73)
    optimizer ......................................: (16.23, 16.29)
[SYM206-GPU-A0206-P2-Node52:394433:0:394597] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x558d1fafe824)
malloc_consolidate(): invalid chunk size
[2024-02-12 12:41:05,379] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 3 (pid: 394433) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
pretrain_gpt.py FAILED
-------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_12:41:05
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : -6 (pid: 394433)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 394433
=======================================================
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (340430): No such process
kill: (340453): No such process
kill: (340459): No such process
kill: (340465): No such process
10.64.24.49 kill done.
13b, 4k, gbs=512: dp=4, tp=4, pp=1, mbs=2
LOCAL_IP = 10.64.24.52
DP=4, MP=4, PP=1
[2024-02-12 12:43:22,248] torch.distributed.run: [WARNING] 
[2024-02-12 12:43:22,248] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 12:43:22,248] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 12:43:22,248] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.366 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.416 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.144 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.931 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (457.20, 542.57)
    train/valid/test-data-iterators-setup ..........: (0.02, 11967.15)
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 11431.1 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.094169E+01 | loss scale: 1.0 | grad norm: 7.903 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (11183.36, 11205.21)
    forward-compute ................................: (6659.62, 6922.59)
    backward-compute ...............................: (4230.25, 4505.85)
    batch-generator ................................: (455.17, 479.72)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (107.64, 107.88)
    params-all-gather ..............................: (55.98, 56.12)
    optimizer-copy-to-main-grad ....................: (0.77, 0.98)
    optimizer-clip-main-grad .......................: (7.34, 7.36)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.28, 9.42)
    optimizer-copy-main-to-model-params ............: (2.91, 2.99)
    optimizer ......................................: (21.33, 21.44)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 9632.7 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.082710E+01 | loss scale: 1.0 | grad norm: 4.066 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9413.50, 9417.70)
    forward-compute ................................: (5341.40, 5453.34)
    backward-compute ...............................: (3924.35, 4037.99)
    batch-generator ................................: (65.07, 75.80)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (107.29, 107.35)
    params-all-gather ..............................: (56.00, 57.38)
    optimizer-copy-to-main-grad ....................: (0.72, 0.90)
    optimizer-clip-main-grad .......................: (4.49, 4.51)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.02, 9.08)
    optimizer-copy-main-to-model-params ............: (2.91, 2.99)
    optimizer ......................................: (18.02, 18.09)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 10565.9 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.059242E+01 | loss scale: 1.0 | grad norm: 1.720 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10351.67, 10362.25)
    forward-compute ................................: (5982.78, 6081.13)
    backward-compute ...............................: (4240.34, 4334.75)
    batch-generator ................................: (66.39, 76.58)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (107.33, 107.52)
    params-all-gather ..............................: (55.97, 56.07)
    optimizer-copy-to-main-grad ....................: (0.70, 0.91)
    optimizer-clip-main-grad .......................: (4.47, 4.48)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.00, 9.07)
    optimizer-copy-main-to-model-params ............: (2.91, 2.99)
    optimizer ......................................: (17.95, 18.04)
Mon Feb 12 12:51:00 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   35C    P0             217W / 700W |  65562MiB / 81559MiB |     79%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   41C    P0             250W / 700W |  65630MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   42C    P0             256W / 700W |  65978MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   35C    P0             239W / 700W |  65560MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   34C    P0             216W / 700W |  71920MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   40C    P0             243W / 700W |  71632MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   39C    P0             252W / 700W |  71554MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   34C    P0             214W / 700W |  71348MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 10696.4 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.057413E+01 | loss scale: 1.0 | grad norm: 1.669 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10377.39, 10403.77)
    forward-compute ................................: (6276.80, 6436.31)
    backward-compute ...............................: (3896.78, 4089.06)
    batch-generator ................................: (65.75, 76.30)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (107.31, 107.35)
    params-all-gather ..............................: (56.00, 56.11)
    optimizer-copy-to-main-grad ....................: (0.68, 0.88)
    optimizer-clip-main-grad .......................: (4.52, 4.54)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.00, 9.06)
    optimizer-copy-main-to-model-params ............: (2.91, 2.98)
    optimizer ......................................: (18.03, 18.12)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 10823.9 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.048101E+01 | loss scale: 1.0 | grad norm: 4.456 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10601.87, 10616.75)
    forward-compute ................................: (6141.88, 6247.95)
    backward-compute ...............................: (4315.21, 4430.43)
    batch-generator ................................: (65.81, 77.93)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (107.31, 107.55)
    params-all-gather ..............................: (55.96, 56.08)
    optimizer-copy-to-main-grad ....................: (0.69, 0.89)
    optimizer-clip-main-grad .......................: (4.46, 4.48)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.01, 9.08)
    optimizer-copy-main-to-model-params ............: (2.91, 2.98)
    optimizer ......................................: (17.99, 18.06)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 10211.6 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.050215E+01 | loss scale: 1.0 | grad norm: 2.333 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9985.26, 10006.06)
    forward-compute ................................: (5678.94, 5848.00)
    backward-compute ...............................: (4118.90, 4282.70)
    batch-generator ................................: (66.49, 76.53)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (107.29, 108.51)
    params-all-gather ..............................: (55.97, 56.08)
    optimizer-copy-to-main-grad ....................: (0.69, 0.87)
    optimizer-clip-main-grad .......................: (4.50, 4.68)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.02, 9.08)
    optimizer-copy-main-to-model-params ............: (2.91, 2.99)
    optimizer ......................................: (18.25, 18.34)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 10108.3 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047998E+01 | loss scale: 1.0 | grad norm: 1.517 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9882.20, 9895.88)
    forward-compute ................................: (5608.62, 5703.19)
    backward-compute ...............................: (4145.26, 4235.65)
    batch-generator ................................: (65.42, 75.95)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (107.29, 107.76)
    params-all-gather ..............................: (55.97, 56.09)
    optimizer-copy-to-main-grad ....................: (0.67, 0.88)
    optimizer-clip-main-grad .......................: (4.49, 4.51)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.01, 9.08)
    optimizer-copy-main-to-model-params ............: (2.91, 2.99)
    optimizer ......................................: (17.95, 18.05)
Mon Feb 12 12:58:02 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   35C    P0             336W / 700W |  65562MiB / 81559MiB |     83%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   40C    P0             355W / 700W |  65630MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   41C    P0             380W / 700W |  65978MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   35C    P0             347W / 700W |  65560MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   34C    P0             330W / 700W |  71930MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   41C    P0             361W / 700W |  71644MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   39C    P0             337W / 700W |  71554MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   34C    P0             314W / 700W |  71354MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 11139.2 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.045226E+01 | loss scale: 1.0 | grad norm: 2.385 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10823.28, 10839.60)
    forward-compute ................................: (6473.37, 6604.53)
    backward-compute ...............................: (4182.34, 4317.35)
    batch-generator ................................: (65.73, 77.39)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (107.31, 107.41)
    params-all-gather ..............................: (55.97, 56.21)
    optimizer-copy-to-main-grad ....................: (0.68, 0.85)
    optimizer-clip-main-grad .......................: (4.25, 4.26)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.01, 9.08)
    optimizer-copy-main-to-model-params ............: (2.91, 2.99)
    optimizer ......................................: (17.69, 17.77)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 10250.2 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.043711E+01 | loss scale: 1.0 | grad norm: 1.123 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10015.91, 10038.38)
    forward-compute ................................: (5626.41, 5852.07)
    backward-compute ...............................: (4138.89, 4349.46)
    batch-generator ................................: (65.29, 76.19)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (107.28, 107.40)
    params-all-gather ..............................: (56.01, 56.12)
    optimizer-copy-to-main-grad ....................: (0.69, 0.88)
    optimizer-clip-main-grad .......................: (4.56, 4.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.01, 9.06)
    optimizer-copy-main-to-model-params ............: (2.91, 2.99)
    optimizer ......................................: (18.08, 18.16)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 10155.5 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.042170E+01 | loss scale: 1.0 | grad norm: 0.866 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9928.90, 9945.98)
    forward-compute ................................: (5535.63, 5826.53)
    backward-compute ...............................: (4064.59, 4366.25)
    batch-generator ................................: (65.77, 76.97)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (107.30, 107.49)
    params-all-gather ..............................: (55.99, 56.19)
    optimizer-copy-to-main-grad ....................: (0.68, 0.86)
    optimizer-clip-main-grad .......................: (3.28, 3.29)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.00, 9.09)
    optimizer-copy-main-to-model-params ............: (2.91, 2.99)
    optimizer ......................................: (16.74, 16.82)
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (341513): No such process
kill: (341519): No such process
kill: (341525): No such process
kill: (341531): No such process
10.64.24.49 kill done.
13b, 4k, gbs=512: dp=4, tp=1, pp=4, mbs=2
LOCAL_IP = 10.64.24.52
DP=4, MP=1, PP=4
[2024-02-12 13:03:41,258] torch.distributed.run: [WARNING] 
[2024-02-12 13:03:41,258] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 13:03:41,258] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 13:03:41,258] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 3146393600
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 3403960320
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...

> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...

> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.904 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.904 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.908 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.910 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.931 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.954 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.968 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.109 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.245 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.274 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.312 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.330 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.290 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.321 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.358 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  6.337 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (42.22, 631.30)
    train/valid/test-data-iterators-setup ..........: (10153.39, 11604.59)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 11062.0 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.095132E+01 | loss scale: 1.0 | grad norm: 8.065 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 27090.8671875 | max allocated: 43428.1337890625 | reserved: 46620.0 | max reserved: 46620.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 29304.5380859375 | max allocated: 45163.19384765625 | reserved: 46088.0 | max reserved: 46088.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (10226.76, 10461.66)
    forward-compute ................................: (1133.03, 5750.22)
    backward-compute ...............................: (2673.60, 3803.53)
    batch-generator ................................: (75.28, 95.90)
    forward-recv ...................................: (93.10, 254.06)
    forward-send ...................................: (2.93, 144.04)
    backward-recv ..................................: (78.14, 402.23)
    backward-send ..................................: (0.63, 44.16)
    forward-send-backward-recv .....................: (4726.80, 5930.95)
    backward-send-forward-recv .....................: (629.52, 1151.03)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 22.35)
    grads-reduce-scatter ...........................: (37.78, 444.84)
    params-all-gather ..............................: (18.42, 20.09)
    optimizer-copy-to-main-grad ....................: (0.20, 0.33)
    optimizer-clip-main-grad .......................: (7.11, 7.91)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.90, 9.73)
    optimizer-copy-main-to-model-params ............: (2.45, 2.69)
    optimizer ......................................: (21.16, 21.41)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 9024.2 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.084524E+01 | loss scale: 1.0 | grad norm: 5.279 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8681.42, 8856.14)
    forward-compute ................................: (966.84, 4626.42)
    backward-compute ...............................: (2479.36, 3428.41)
    batch-generator ................................: (47.73, 59.82)
    forward-recv ...................................: (28.12, 362.00)
    forward-send ...................................: (0.42, 16.76)
    backward-recv ..................................: (40.48, 344.54)
    backward-send ..................................: (0.55, 54.39)
    forward-send-backward-recv .....................: (3870.82, 4630.28)
    backward-send-forward-recv .....................: (479.46, 1135.31)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.28)
    grads-reduce-scatter ...........................: (35.00, 38.19)
    params-all-gather ..............................: (18.44, 20.06)
    optimizer-copy-to-main-grad ....................: (0.17, 0.29)
    optimizer-clip-main-grad .......................: (4.08, 4.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.68, 9.96)
    optimizer-copy-main-to-model-params ............: (2.45, 2.69)
    optimizer ......................................: (17.48, 17.73)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 10555.8 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.059602E+01 | loss scale: 1.0 | grad norm: 2.672 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10175.40, 10358.51)
    forward-compute ................................: (1106.17, 5753.05)
    backward-compute ...............................: (2726.30, 3641.08)
    batch-generator ................................: (47.88, 61.30)
    forward-recv ...................................: (20.30, 391.32)
    forward-send ...................................: (0.46, 348.92)
    backward-recv ..................................: (52.08, 375.96)
    backward-send ..................................: (0.51, 50.26)
    forward-send-backward-recv .....................: (4796.43, 6047.27)
    backward-send-forward-recv .....................: (596.34, 1288.71)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 22.24)
    grads-reduce-scatter ...........................: (34.94, 38.19)
    params-all-gather ..............................: (18.42, 20.04)
    optimizer-copy-to-main-grad ....................: (0.18, 0.54)
    optimizer-clip-main-grad .......................: (4.08, 4.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.68, 9.45)
    optimizer-copy-main-to-model-params ............: (2.45, 2.69)
    optimizer ......................................: (17.18, 17.41)
Mon Feb 12 13:11:02 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   33C    P0             331W / 700W |  51562MiB / 81559MiB |      8%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   37C    P0             265W / 700W |  56010MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   37C    P0             146W / 700W |  52672MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   32C    P0             217W / 700W |  51320MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   35C    P0             269W / 700W |  50846MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   41C    P0             324W / 700W |  52238MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   40C    P0             226W / 700W |  52032MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   35C    P0             223W / 700W |  50768MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 10146.9 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.057399E+01 | loss scale: 1.0 | grad norm: 1.824 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9717.38, 9901.58)
    forward-compute ................................: (975.38, 5406.71)
    backward-compute ...............................: (2466.89, 3456.24)
    batch-generator ................................: (47.51, 55.32)
    forward-recv ...................................: (22.74, 71.06)
    forward-send ...................................: (0.47, 25.14)
    backward-recv ..................................: (76.61, 244.94)
    backward-send ..................................: (0.45, 23.49)
    forward-send-backward-recv .....................: (4663.93, 6016.85)
    backward-send-forward-recv .....................: (451.20, 1227.61)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 22.48)
    grads-reduce-scatter ...........................: (34.93, 38.25)
    params-all-gather ..............................: (18.44, 20.10)
    optimizer-copy-to-main-grad ....................: (0.18, 0.38)
    optimizer-clip-main-grad .......................: (4.08, 4.34)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.68, 9.47)
    optimizer-copy-main-to-model-params ............: (2.45, 2.69)
    optimizer ......................................: (17.32, 17.56)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 11362.6 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.047306E+01 | loss scale: 1.0 | grad norm: 0.881 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (11007.92, 11214.99)
    forward-compute ................................: (1126.16, 6669.71)
    backward-compute ...............................: (2779.37, 3725.88)
    batch-generator ................................: (47.16, 58.30)
    forward-recv ...................................: (25.07, 68.16)
    forward-send ...................................: (0.55, 21.24)
    backward-recv ..................................: (68.23, 282.92)
    backward-send ..................................: (0.52, 51.37)
    forward-send-backward-recv .....................: (5717.67, 6869.78)
    backward-send-forward-recv .....................: (609.37, 1149.90)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.31)
    grads-reduce-scatter ...........................: (34.91, 38.18)
    params-all-gather ..............................: (18.43, 20.06)
    optimizer-copy-to-main-grad ....................: (0.18, 0.26)
    optimizer-clip-main-grad .......................: (3.83, 4.03)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.68, 9.43)
    optimizer-copy-main-to-model-params ............: (2.45, 2.69)
    optimizer ......................................: (16.58, 16.81)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 9269.4 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049057E+01 | loss scale: 1.0 | grad norm: 1.759 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8889.16, 9051.62)
    forward-compute ................................: (1061.71, 4740.05)
    backward-compute ...............................: (2637.72, 3635.33)
    batch-generator ................................: (46.70, 58.24)
    forward-recv ...................................: (18.22, 58.24)
    forward-send ...................................: (0.41, 19.31)
    backward-recv ..................................: (66.13, 286.04)
    backward-send ..................................: (0.63, 41.84)
    forward-send-backward-recv .....................: (3861.83, 4971.44)
    backward-send-forward-recv .....................: (544.82, 1029.69)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.18)
    grads-reduce-scatter ...........................: (34.92, 38.15)
    params-all-gather ..............................: (18.41, 20.03)
    optimizer-copy-to-main-grad ....................: (0.17, 0.25)
    optimizer-clip-main-grad .......................: (4.06, 4.27)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.68, 9.42)
    optimizer-copy-main-to-model-params ............: (2.45, 2.68)
    optimizer ......................................: (16.81, 17.04)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 9110.0 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047575E+01 | loss scale: 1.0 | grad norm: 1.588 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8761.16, 8928.46)
    forward-compute ................................: (1060.49, 4717.68)
    backward-compute ...............................: (2647.80, 3564.12)
    batch-generator ................................: (46.68, 57.97)
    forward-recv ...................................: (21.60, 61.02)
    forward-send ...................................: (0.43, 23.99)
    backward-recv ..................................: (59.31, 306.88)
    backward-send ..................................: (0.72, 26.19)
    forward-send-backward-recv .....................: (3979.50, 4839.13)
    backward-send-forward-recv .....................: (490.69, 856.20)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.18)
    grads-reduce-scatter ...........................: (34.93, 38.15)
    params-all-gather ..............................: (18.42, 20.08)
    optimizer-copy-to-main-grad ....................: (0.18, 0.25)
    optimizer-clip-main-grad .......................: (4.06, 4.28)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.67, 9.42)
    optimizer-copy-main-to-model-params ............: (2.45, 2.69)
    optimizer ......................................: (16.80, 17.03)
Mon Feb 12 13:17:32 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   35C    P0             397W / 700W |  51608MiB / 81559MiB |     65%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   40C    P0             315W / 700W |  56010MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   40C    P0             328W / 700W |  52674MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   35C    P0             337W / 700W |  51640MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             285W / 700W |  50846MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   44C    P0             370W / 700W |  52238MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             393W / 700W |  52032MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             342W / 700W |  50768MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 9289.2 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044147E+01 | loss scale: 1.0 | grad norm: 1.285 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8846.42, 9054.27)
    forward-compute ................................: (1083.08, 4652.68)
    backward-compute ...............................: (2683.19, 3624.64)
    batch-generator ................................: (46.73, 58.25)
    forward-recv ...................................: (25.20, 56.36)
    forward-send ...................................: (0.41, 17.26)
    backward-recv ..................................: (68.07, 304.77)
    backward-send ..................................: (0.62, 40.47)
    forward-send-backward-recv .....................: (4003.71, 4877.28)
    backward-send-forward-recv .....................: (556.78, 845.53)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.40)
    grads-reduce-scatter ...........................: (34.93, 38.15)
    params-all-gather ..............................: (18.43, 20.08)
    optimizer-copy-to-main-grad ....................: (0.18, 0.24)
    optimizer-clip-main-grad .......................: (3.81, 4.06)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.67, 9.43)
    optimizer-copy-main-to-model-params ............: (2.45, 2.69)
    optimizer ......................................: (16.60, 16.83)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 9263.9 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.043287E+01 | loss scale: 1.0 | grad norm: 0.983 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8875.03, 9068.55)
    forward-compute ................................: (1056.37, 4865.03)
    backward-compute ...............................: (2640.62, 3658.06)
    batch-generator ................................: (47.01, 57.68)
    forward-recv ...................................: (24.58, 73.37)
    forward-send ...................................: (0.50, 21.71)
    backward-recv ..................................: (66.31, 303.37)
    backward-send ..................................: (0.65, 23.98)
    forward-send-backward-recv .....................: (3756.46, 4947.97)
    backward-send-forward-recv .....................: (455.56, 1049.16)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.17)
    grads-reduce-scatter ...........................: (34.94, 38.11)
    params-all-gather ..............................: (18.42, 20.11)
    optimizer-copy-to-main-grad ....................: (0.18, 0.24)
    optimizer-clip-main-grad .......................: (3.58, 3.75)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.67, 9.42)
    optimizer-copy-main-to-model-params ............: (2.45, 2.69)
    optimizer ......................................: (16.28, 16.52)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 9085.7 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041847E+01 | loss scale: 1.0 | grad norm: 0.939 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8686.33, 8868.79)
    forward-compute ................................: (1030.26, 4670.77)
    backward-compute ...............................: (2587.39, 3669.67)
    batch-generator ................................: (46.77, 56.55)
    forward-recv ...................................: (20.94, 58.30)
    forward-send ...................................: (0.42, 15.48)
    backward-recv ..................................: (83.28, 321.23)
    backward-send ..................................: (0.62, 30.71)
    forward-send-backward-recv .....................: (3706.57, 4787.80)
    backward-send-forward-recv .....................: (485.84, 907.40)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.50)
    grads-reduce-scatter ...........................: (34.94, 38.10)
    params-all-gather ..............................: (18.44, 20.05)
    optimizer-copy-to-main-grad ....................: (0.18, 0.24)
    optimizer-clip-main-grad .......................: (3.57, 3.75)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.67, 9.79)
    optimizer-copy-main-to-model-params ............: (2.45, 2.68)
    optimizer ......................................: (16.64, 16.88)
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (343791): No such process
kill: (343797): No such process
kill: (343803): No such process
kill: (343809): No such process
10.64.24.49 kill done.
13b, 4k, gbs=512: dp=2, tp=4, pp=2, mbs=8
LOCAL_IP = 10.64.24.52
DP=2, MP=4, PP=2
[2024-02-12 13:22:49,118] torch.distributed.run: [WARNING] 
[2024-02-12 13:22:49,118] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 13:22:49,118] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 13:22:49,118] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 1638548480
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1638548480
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1638548480
 > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 1638548480
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.277 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.273 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.105 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.178 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (927.83, 991.30)
    train/valid/test-data-iterators-setup ..........: (0.02, 10872.29)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 8284.1 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.092942E+01 | loss scale: 1.0 | grad norm: 8.031 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 10] (after 10 iterations) memory (MB) | allocated: 18883.7724609375 | max allocated: 45549.89892578125 | reserved: 55660.0 | max reserved: 55660.0
[Rank 11] (after 10 iterations) memory (MB) | allocated: 18882.9130859375 | max allocated: 45549.03955078125 | reserved: 54618.0 | max reserved: 54618.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 18883.5146484375 | max allocated: 45549.64111328125 | reserved: 56328.0 | max reserved: 56328.0
[Rank 9] (after 10 iterations) memory (MB) | allocated: 18882.9130859375 | max allocated: 45549.03955078125 | reserved: 54546.0 | max reserved: 54546.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (7881.05, 7993.62)
    forward-compute ................................: (2016.26, 3842.64)
    backward-compute ...............................: (2585.18, 3265.24)
    batch-generator ................................: (420.96, 440.76)
    forward-recv ...................................: (496.92, 511.34)
    forward-send ...................................: (3.35, 13.25)
    backward-recv ..................................: (126.90, 183.75)
    backward-send ..................................: (1.73, 10.31)
    forward-send-backward-recv .....................: (2804.57, 3157.57)
    backward-send-forward-recv .....................: (465.51, 561.56)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.82, 6.15)
    grads-reduce-scatter ...........................: (15.40, 228.78)
    params-all-gather ..............................: (8.22, 8.83)
    optimizer-copy-to-main-grad ....................: (0.69, 0.90)
    optimizer-clip-main-grad .......................: (7.36, 7.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.41, 9.63)
    optimizer-copy-main-to-model-params ............: (2.94, 3.04)
    optimizer ......................................: (21.68, 21.82)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 6010.1 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.085284E+01 | loss scale: 1.0 | grad norm: 5.189 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5848.58, 5933.68)
    forward-compute ................................: (1350.71, 2764.41)
    backward-compute ...............................: (2290.79, 2850.16)
    batch-generator ................................: (28.27, 33.06)
    forward-recv ...................................: (36.28, 58.63)
    forward-send ...................................: (1.04, 1.61)
    backward-recv ..................................: (81.95, 107.72)
    backward-send ..................................: (1.18, 11.18)
    forward-send-backward-recv .....................: (1993.72, 2151.11)
    backward-send-forward-recv .....................: (282.25, 501.05)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.71, 6.26)
    grads-reduce-scatter ...........................: (15.38, 16.03)
    params-all-gather ..............................: (8.38, 8.84)
    optimizer-copy-to-main-grad ....................: (0.65, 0.83)
    optimizer-clip-main-grad .......................: (4.46, 4.53)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.15, 9.30)
    optimizer-copy-main-to-model-params ............: (2.94, 3.04)
    optimizer ......................................: (18.08, 18.18)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 6985.9 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.060152E+01 | loss scale: 1.0 | grad norm: 2.453 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6787.22, 6905.10)
    forward-compute ................................: (1841.37, 2970.13)
    backward-compute ...............................: (2636.10, 3109.92)
    batch-generator ................................: (28.27, 33.15)
    forward-recv ...................................: (48.06, 50.22)
    forward-send ...................................: (1.33, 1.39)
    backward-recv ..................................: (97.98, 132.68)
    backward-send ..................................: (7.79, 17.39)
    forward-send-backward-recv .....................: (2251.35, 2263.08)
    backward-send-forward-recv .....................: (662.04, 782.39)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.71, 5.90)
    grads-reduce-scatter ...........................: (15.33, 16.09)
    params-all-gather ..............................: (8.30, 8.82)
    optimizer-copy-to-main-grad ....................: (0.65, 0.79)
    optimizer-clip-main-grad .......................: (4.50, 4.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.16, 9.30)
    optimizer-copy-main-to-model-params ............: (2.94, 3.05)
    optimizer ......................................: (18.06, 18.17)
Mon Feb 12 13:28:01 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   37C    P0             313W / 700W |  65546MiB / 81559MiB |     62%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   43C    P0             328W / 700W |  65716MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   44C    P0             290W / 700W |  65922MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             306W / 700W |  64908MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             435W / 700W |  58634MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             396W / 700W |  58844MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             443W / 700W |  58844MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             418W / 700W |  57796MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 6613.3 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.057884E+01 | loss scale: 1.0 | grad norm: 1.451 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6360.90, 6461.03)
    forward-compute ................................: (1392.62, 3110.49)
    backward-compute ...............................: (2376.66, 2824.45)
    batch-generator ................................: (28.21, 33.45)
    forward-recv ...................................: (47.40, 53.23)
    forward-send ...................................: (1.31, 1.48)
    backward-recv ..................................: (66.62, 75.20)
    backward-send ..................................: (1.48, 10.51)
    forward-send-backward-recv .....................: (2508.85, 2563.17)
    backward-send-forward-recv .....................: (373.35, 396.71)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.69, 6.24)
    grads-reduce-scatter ...........................: (15.47, 15.98)
    params-all-gather ..............................: (8.24, 8.82)
    optimizer-copy-to-main-grad ....................: (0.65, 0.80)
    optimizer-clip-main-grad .......................: (4.51, 4.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.15, 9.31)
    optimizer-copy-main-to-model-params ............: (2.94, 3.04)
    optimizer ......................................: (18.05, 18.15)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 7095.7 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.047933E+01 | loss scale: 1.0 | grad norm: 1.770 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6938.33, 7017.91)
    forward-compute ................................: (1875.88, 3015.29)
    backward-compute ...............................: (2708.40, 3221.54)
    batch-generator ................................: (29.01, 34.65)
    forward-recv ...................................: (52.39, 64.26)
    forward-send ...................................: (1.43, 1.74)
    backward-recv ..................................: (69.68, 96.88)
    backward-send ..................................: (5.49, 7.50)
    forward-send-backward-recv .....................: (2157.06, 2321.77)
    backward-send-forward-recv .....................: (689.12, 712.53)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.67, 6.02)
    grads-reduce-scatter ...........................: (15.32, 15.96)
    params-all-gather ..............................: (8.47, 8.81)
    optimizer-copy-to-main-grad ....................: (0.64, 0.82)
    optimizer-clip-main-grad .......................: (4.75, 4.80)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.15, 9.34)
    optimizer-copy-main-to-model-params ............: (2.94, 3.05)
    optimizer ......................................: (18.33, 18.44)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 6378.4 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.051056E+01 | loss scale: 1.0 | grad norm: 5.561 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6182.74, 6296.91)
    forward-compute ................................: (1491.24, 2817.56)
    backward-compute ...............................: (2532.14, 2992.75)
    batch-generator ................................: (28.60, 33.55)
    forward-recv ...................................: (39.73, 59.89)
    forward-send ...................................: (1.12, 1.62)
    backward-recv ..................................: (110.49, 120.95)
    backward-send ..................................: (1.34, 1.67)
    forward-send-backward-recv .....................: (2003.97, 2138.75)
    backward-send-forward-recv .....................: (372.77, 390.75)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.67, 6.07)
    grads-reduce-scatter ...........................: (15.29, 15.97)
    params-all-gather ..............................: (8.32, 8.81)
    optimizer-copy-to-main-grad ....................: (0.64, 0.79)
    optimizer-clip-main-grad .......................: (4.52, 4.67)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.16, 9.31)
    optimizer-copy-main-to-model-params ............: (2.94, 3.05)
    optimizer ......................................: (18.17, 18.28)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 6243.8 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.048468E+01 | loss scale: 1.0 | grad norm: 0.927 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6061.04, 6165.03)
    forward-compute ................................: (1467.91, 2837.16)
    backward-compute ...............................: (2492.73, 3082.10)
    batch-generator ................................: (28.78, 33.63)
    forward-recv ...................................: (41.91, 49.35)
    forward-send ...................................: (1.17, 1.37)
    backward-recv ..................................: (103.22, 117.93)
    backward-send ..................................: (1.38, 2.81)
    forward-send-backward-recv .....................: (1827.07, 2047.75)
    backward-send-forward-recv .....................: (280.23, 438.43)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.68, 5.98)
    grads-reduce-scatter ...........................: (15.38, 15.97)
    params-all-gather ..............................: (8.37, 8.84)
    optimizer-copy-to-main-grad ....................: (0.62, 0.79)
    optimizer-clip-main-grad .......................: (4.31, 4.34)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.15, 9.33)
    optimizer-copy-main-to-model-params ............: (2.94, 3.05)
    optimizer ......................................: (17.87, 17.99)
Mon Feb 12 13:32:25 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   39C    P0             474W / 700W |  68714MiB / 81559MiB |     81%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   45C    P0             488W / 700W |  69676MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   46C    P0             483W / 700W |  69882MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0             485W / 700W |  68868MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             467W / 700W |  68592MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             496W / 700W |  68448MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   44C    P0             473W / 700W |  68348MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             507W / 700W |  68058MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 6688.0 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044844E+01 | loss scale: 1.0 | grad norm: 3.812 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6430.02, 6524.04)
    forward-compute ................................: (1804.80, 2676.62)
    backward-compute ...............................: (2621.56, 3070.46)
    batch-generator ................................: (27.80, 33.84)
    forward-recv ...................................: (41.60, 52.85)
    forward-send ...................................: (1.17, 1.49)
    backward-recv ..................................: (73.85, 99.85)
    backward-send ..................................: (1.23, 4.26)
    forward-send-backward-recv .....................: (1915.81, 1959.55)
    backward-send-forward-recv .....................: (651.08, 654.14)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.65, 6.20)
    grads-reduce-scatter ...........................: (15.41, 15.92)
    params-all-gather ..............................: (8.45, 8.83)
    optimizer-copy-to-main-grad ....................: (0.62, 0.77)
    optimizer-clip-main-grad .......................: (4.27, 4.31)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.16, 9.30)
    optimizer-copy-main-to-model-params ............: (2.94, 3.04)
    optimizer ......................................: (17.79, 17.89)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 6813.0 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.044189E+01 | loss scale: 1.0 | grad norm: 1.191 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6634.45, 6735.39)
    forward-compute ................................: (1525.00, 3245.41)
    backward-compute ...............................: (2579.62, 3057.73)
    batch-generator ................................: (28.59, 33.43)
    forward-recv ...................................: (52.91, 54.22)
    forward-send ...................................: (1.43, 1.50)
    backward-recv ..................................: (109.27, 121.77)
    backward-send ..................................: (1.48, 1.65)
    forward-send-backward-recv .....................: (2357.18, 2494.71)
    backward-send-forward-recv .....................: (321.93, 398.67)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.70, 6.03)
    grads-reduce-scatter ...........................: (15.47, 15.98)
    params-all-gather ..............................: (8.31, 8.84)
    optimizer-copy-to-main-grad ....................: (0.63, 0.77)
    optimizer-clip-main-grad .......................: (4.53, 4.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.15, 9.29)
    optimizer-copy-main-to-model-params ............: (2.94, 3.05)
    optimizer ......................................: (18.02, 18.13)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 6214.5 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.042855E+01 | loss scale: 1.0 | grad norm: 0.860 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6020.57, 6136.09)
    forward-compute ................................: (1511.19, 2656.01)
    backward-compute ...............................: (2570.78, 3033.61)
    batch-generator ................................: (28.19, 33.73)
    forward-recv ...................................: (42.74, 44.67)
    forward-send ...................................: (1.20, 1.38)
    backward-recv ..................................: (108.11, 147.62)
    backward-send ..................................: (1.46, 5.35)
    forward-send-backward-recv .....................: (1859.93, 1877.60)
    backward-send-forward-recv .....................: (322.77, 340.61)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.70, 6.15)
    grads-reduce-scatter ...........................: (15.48, 15.89)
    params-all-gather ..............................: (8.42, 8.80)
    optimizer-copy-to-main-grad ....................: (0.63, 0.79)
    optimizer-clip-main-grad .......................: (4.02, 4.05)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.16, 9.36)
    optimizer-copy-main-to-model-params ............: (2.94, 3.05)
    optimizer ......................................: (17.62, 17.73)
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (344905): No such process
kill: (344911): No such process
kill: (344917): No such process
kill: (344923): No such process
10.64.24.49 kill done.
13b, 4k, gbs=512: dp=2, tp=4, pp=2, mbs=4
LOCAL_IP = 10.64.24.52
DP=2, MP=4, PP=2
[2024-02-12 13:36:52,694] torch.distributed.run: [WARNING] 
[2024-02-12 13:36:52,694] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 13:36:52,694] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 13:36:52,694] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 1638548480
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1638548480
 > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 1638548480
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1638548480
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  6.659 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  6.699 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.268 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.516 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (921.79, 987.83)
    train/valid/test-data-iterators-setup ..........: (0.02, 12624.91)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 9505.2 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.092937E+01 | loss scale: 1.0 | grad norm: 8.029 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 9] (after 10 iterations) memory (MB) | allocated: 18901.9013671875 | max allocated: 39004.53564453125 | reserved: 45156.0 | max reserved: 45156.0[Rank 11] (after 10 iterations) memory (MB) | allocated: 18898.9013671875 | max allocated: 39001.244140625 | reserved: 44722.0 | max reserved: 44722.0
[Rank 10] (after 10 iterations) memory (MB) | allocated: 18900.9013671875 | max allocated: 39003.244140625 | reserved: 44618.0 | max reserved: 44618.0

[Rank 8] (after 10 iterations) memory (MB) | allocated: 18898.9013671875 | max allocated: 39001.4384765625 | reserved: 44572.0 | max reserved: 44572.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (9147.72, 9211.43)
    forward-compute ................................: (2274.86, 4329.39)
    backward-compute ...............................: (3175.85, 3666.21)
    batch-generator ................................: (447.83, 481.04)
    forward-recv ...................................: (474.77, 490.16)
    forward-send ...................................: (2.86, 7.03)
    backward-recv ..................................: (70.81, 97.42)
    backward-send ..................................: (0.99, 1.15)
    forward-send-backward-recv .....................: (3478.71, 3627.95)
    backward-send-forward-recv .....................: (729.20, 785.79)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.78, 6.10)
    grads-reduce-scatter ...........................: (15.48, 225.68)
    params-all-gather ..............................: (8.32, 8.78)
    optimizer-copy-to-main-grad ....................: (0.68, 0.88)
    optimizer-clip-main-grad .......................: (7.38, 7.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.41, 9.63)
    optimizer-copy-main-to-model-params ............: (2.94, 3.04)
    optimizer ......................................: (21.83, 21.96)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 8178.8 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.085260E+01 | loss scale: 1.0 | grad norm: 5.213 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8065.16, 8111.94)
    forward-compute ................................: (1900.71, 3973.09)
    backward-compute ...............................: (2866.48, 3286.44)
    batch-generator ................................: (53.83, 70.65)
    forward-recv ...................................: (20.50, 24.11)
    forward-send ...................................: (0.60, 0.72)
    backward-recv ..................................: (45.84, 75.97)
    backward-send ..................................: (0.77, 0.84)
    forward-send-backward-recv .....................: (3131.35, 3239.16)
    backward-send-forward-recv .....................: (802.35, 878.39)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.73, 6.03)
    grads-reduce-scatter ...........................: (15.38, 15.88)
    params-all-gather ..............................: (8.29, 8.76)
    optimizer-copy-to-main-grad ....................: (0.66, 0.99)
    optimizer-clip-main-grad .......................: (4.52, 4.56)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.13, 9.38)
    optimizer-copy-main-to-model-params ............: (2.93, 3.04)
    optimizer ......................................: (18.25, 18.35)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 8320.5 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.060168E+01 | loss scale: 1.0 | grad norm: 2.476 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8210.40, 8258.04)
    forward-compute ................................: (2110.76, 3611.77)
    backward-compute ...............................: (3222.84, 3569.61)
    batch-generator ................................: (54.42, 69.88)
    forward-recv ...................................: (27.85, 28.02)
    forward-send ...................................: (0.78, 0.81)
    backward-recv ..................................: (42.60, 44.52)
    backward-send ..................................: (8.32, 9.70)
    forward-send-backward-recv .....................: (2816.93, 2838.24)
    backward-send-forward-recv .....................: (986.17, 1027.56)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.75, 5.92)
    grads-reduce-scatter ...........................: (15.36, 15.86)
    params-all-gather ..............................: (8.30, 8.74)
    optimizer-copy-to-main-grad ....................: (0.65, 0.92)
    optimizer-clip-main-grad .......................: (4.53, 4.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.13, 9.34)
    optimizer-copy-main-to-model-params ............: (2.94, 3.04)
    optimizer ......................................: (18.18, 18.28)
Mon Feb 12 13:43:07 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   37C    P0             433W / 700W |  55484MiB / 81559MiB |     35%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   43C    P0             408W / 700W |  56584MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   44C    P0             406W / 700W |  55118MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             425W / 700W |  55328MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   37C    P0             257W / 700W |  51310MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   43C    P0             226W / 700W |  51898MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   42C    P0             208W / 700W |  51766MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   36C    P0             235W / 700W |  51420MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 7545.6 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.057883E+01 | loss scale: 1.0 | grad norm: 1.173 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7329.72, 7387.13)
    forward-compute ................................: (1922.10, 3210.84)
    backward-compute ...............................: (2948.56, 3315.47)
    batch-generator ................................: (51.67, 70.00)
    forward-recv ...................................: (24.52, 31.04)
    forward-send ...................................: (0.71, 0.86)
    backward-recv ..................................: (74.17, 76.58)
    backward-send ..................................: (0.80, 0.92)
    forward-send-backward-recv .....................: (2319.81, 2403.80)
    backward-send-forward-recv .....................: (816.20, 842.69)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.66, 6.06)
    grads-reduce-scatter ...........................: (15.08, 15.84)
    params-all-gather ..............................: (8.35, 8.80)
    optimizer-copy-to-main-grad ....................: (0.65, 0.91)
    optimizer-clip-main-grad .......................: (4.49, 4.53)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.14, 9.30)
    optimizer-copy-main-to-model-params ............: (2.94, 3.03)
    optimizer ......................................: (18.08, 18.18)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 8643.2 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.048797E+01 | loss scale: 1.0 | grad norm: 2.425 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8513.36, 8569.51)
    forward-compute ................................: (1891.56, 4136.02)
    backward-compute ...............................: (3307.20, 3650.86)
    batch-generator ................................: (51.49, 72.88)
    forward-recv ...................................: (30.15, 30.40)
    forward-send ...................................: (0.83, 0.87)
    backward-recv ..................................: (39.25, 90.50)
    backward-send ..................................: (0.84, 9.08)
    forward-send-backward-recv .....................: (3197.99, 3298.79)
    backward-send-forward-recv .....................: (671.25, 736.48)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.72, 5.93)
    grads-reduce-scatter ...........................: (14.99, 15.95)
    params-all-gather ..............................: (8.25, 8.77)
    optimizer-copy-to-main-grad ....................: (0.66, 0.91)
    optimizer-clip-main-grad .......................: (4.50, 4.54)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.13, 9.31)
    optimizer-copy-main-to-model-params ............: (2.94, 3.04)
    optimizer ......................................: (18.09, 18.19)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 7862.2 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.050290E+01 | loss scale: 1.0 | grad norm: 2.050 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7736.91, 7791.59)
    forward-compute ................................: (2014.50, 3405.50)
    backward-compute ...............................: (3075.17, 3473.74)
    batch-generator ................................: (51.40, 74.03)
    forward-recv ...................................: (21.90, 25.82)
    forward-send ...................................: (0.63, 0.74)
    backward-recv ..................................: (50.87, 53.80)
    backward-send ..................................: (2.04, 8.65)
    forward-send-backward-recv .....................: (2474.48, 2619.78)
    backward-send-forward-recv .....................: (871.55, 906.16)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.67, 5.99)
    grads-reduce-scatter ...........................: (15.33, 15.91)
    params-all-gather ..............................: (8.27, 8.77)
    optimizer-copy-to-main-grad ....................: (0.65, 0.91)
    optimizer-clip-main-grad .......................: (4.50, 4.53)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.14, 9.30)
    optimizer-copy-main-to-model-params ............: (2.93, 3.04)
    optimizer ......................................: (18.08, 18.18)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 8183.7 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.048777E+01 | loss scale: 1.0 | grad norm: 2.079 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8061.86, 8115.15)
    forward-compute ................................: (2024.46, 3732.53)
    backward-compute ...............................: (3107.20, 3453.35)
    batch-generator ................................: (49.49, 73.61)
    forward-recv ...................................: (26.19, 31.72)
    forward-send ...................................: (0.73, 0.88)
    backward-recv ..................................: (36.37, 69.00)
    backward-send ..................................: (0.91, 4.06)
    forward-send-backward-recv .....................: (2853.56, 2882.58)
    backward-send-forward-recv .....................: (831.87, 848.93)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.68, 5.92)
    grads-reduce-scatter ...........................: (15.45, 15.85)
    params-all-gather ..............................: (8.31, 8.76)
    optimizer-copy-to-main-grad ....................: (0.65, 0.91)
    optimizer-clip-main-grad .......................: (4.56, 4.60)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.14, 9.31)
    optimizer-copy-main-to-model-params ............: (2.94, 3.03)
    optimizer ......................................: (18.18, 18.28)
Mon Feb 12 13:48:30 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   39C    P0             549W / 700W |  55484MiB / 81559MiB |     93%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   45C    P0             454W / 700W |  56584MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   46C    P0             387W / 700W |  55118MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0             431W / 700W |  55328MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             493W / 700W |  60770MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   46C    P0             436W / 700W |  61518MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   44C    P0             493W / 700W |  61466MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             422W / 700W |  61080MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 7623.0 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.045368E+01 | loss scale: 1.0 | grad norm: 1.207 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7397.53, 7463.01)
    forward-compute ................................: (1805.45, 3289.59)
    backward-compute ...............................: (3173.73, 3536.77)
    batch-generator ................................: (52.34, 72.60)
    forward-recv ...................................: (21.65, 27.19)
    forward-send ...................................: (0.64, 0.80)
    backward-recv ..................................: (58.88, 73.14)
    backward-send ..................................: (0.78, 2.41)
    forward-send-backward-recv .....................: (2322.95, 2360.11)
    backward-send-forward-recv .....................: (541.37, 633.88)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.69, 6.15)
    grads-reduce-scatter ...........................: (15.30, 15.83)
    params-all-gather ..............................: (8.36, 8.78)
    optimizer-copy-to-main-grad ....................: (0.67, 0.86)
    optimizer-clip-main-grad .......................: (4.51, 4.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.14, 9.30)
    optimizer-copy-main-to-model-params ............: (2.93, 3.03)
    optimizer ......................................: (18.10, 18.20)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 8407.5 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.043546E+01 | loss scale: 1.0 | grad norm: 1.080 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8266.18, 8345.32)
    forward-compute ................................: (2048.41, 3963.47)
    backward-compute ...............................: (3169.83, 3530.99)
    batch-generator ................................: (51.27, 71.93)
    forward-recv ...................................: (29.53, 31.69)
    forward-send ...................................: (0.83, 0.90)
    backward-recv ..................................: (57.07, 63.55)
    backward-send ..................................: (2.30, 5.58)
    forward-send-backward-recv .....................: (2932.48, 3043.35)
    backward-send-forward-recv .....................: (787.93, 927.06)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.72, 5.98)
    grads-reduce-scatter ...........................: (15.26, 15.90)
    params-all-gather ..............................: (8.28, 8.79)
    optimizer-copy-to-main-grad ....................: (0.66, 0.85)
    optimizer-clip-main-grad .......................: (4.03, 4.05)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.14, 9.32)
    optimizer-copy-main-to-model-params ............: (2.94, 3.03)
    optimizer ......................................: (17.60, 17.70)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 7987.4 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041844E+01 | loss scale: 1.0 | grad norm: 1.666 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7858.06, 7917.60)
    forward-compute ................................: (2010.06, 3571.59)
    backward-compute ...............................: (3107.26, 3525.40)
    batch-generator ................................: (50.72, 72.50)
    forward-recv ...................................: (24.10, 26.43)
    forward-send ...................................: (0.70, 0.79)
    backward-recv ..................................: (46.83, 56.93)
    backward-send ..................................: (1.16, 5.24)
    forward-send-backward-recv .....................: (2530.96, 2717.12)
    backward-send-forward-recv .....................: (833.66, 868.03)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.72, 6.04)
    grads-reduce-scatter ...........................: (15.38, 15.92)
    params-all-gather ..............................: (8.37, 8.75)
    optimizer-copy-to-main-grad ....................: (0.66, 0.89)
    optimizer-clip-main-grad .......................: (3.78, 3.81)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.14, 9.30)
    optimizer-copy-main-to-model-params ............: (2.93, 3.04)
    optimizer ......................................: (17.38, 17.48)
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (346019): No such process
kill: (346025): No such process
kill: (346031): No such process
kill: (346037): No such process
10.64.24.49 kill done.
13b, 4k, gbs=512: dp=2, tp=4, pp=2, mbs=2
LOCAL_IP = 10.64.24.52
DP=2, MP=4, PP=2
[2024-02-12 13:53:31,447] torch.distributed.run: [WARNING] 
[2024-02-12 13:53:31,447] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 13:53:31,447] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 13:53:31,447] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 1638548480
 > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 1638548480
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1638548480
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1638548480
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.257 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.286 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.954 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.041 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (880.55, 977.47)
    train/valid/test-data-iterators-setup ..........: (0.02, 10746.19)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 12592.7 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.092948E+01 | loss scale: 1.0 | grad norm: 8.036 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 18854.0458984375 | max allocated: 32823.03662109375 | reserved: 37786.0 | max reserved: 37786.0
[Rank 11] (after 10 iterations) memory (MB) | allocated: 18854.0458984375 | max allocated: 32823.03662109375 | reserved: 37364.0 | max reserved: 37364.0
[Rank 9] (after 10 iterations) memory (MB) | allocated: 18854.0458984375 | max allocated: 32823.06787109375 | reserved: 37902.0 | max reserved: 37902.0[Rank 10] (after 10 iterations) memory (MB) | allocated: 18854.0458984375 | max allocated: 32823.03662109375 | reserved: 37462.0 | max reserved: 37462.0

(min, max) time across ranks (ms):
    forward-backward ...............................: (12254.43, 12309.43)
    forward-compute ................................: (3260.65, 5871.15)
    backward-compute ...............................: (4363.81, 4637.05)
    batch-generator ................................: (494.30, 524.06)
    forward-recv ...................................: (449.94, 451.98)
    forward-send ...................................: (2.31, 3.05)
    backward-recv ..................................: (47.51, 56.74)
    backward-send ..................................: (3.68, 4.21)
    forward-send-backward-recv .....................: (4511.08, 4554.13)
    backward-send-forward-recv .....................: (1286.19, 1311.35)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.82, 6.10)
    grads-reduce-scatter ...........................: (15.41, 234.73)
    params-all-gather ..............................: (8.26, 8.83)
    optimizer-copy-to-main-grad ....................: (0.71, 0.84)
    optimizer-clip-main-grad .......................: (7.45, 7.48)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.40, 9.66)
    optimizer-copy-main-to-model-params ............: (2.94, 3.04)
    optimizer ......................................: (21.73, 21.85)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 10135.0 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.085292E+01 | loss scale: 1.0 | grad norm: 5.183 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10035.40, 10070.61)
    forward-compute ................................: (2925.14, 4407.81)
    backward-compute ...............................: (4112.92, 4275.77)
    batch-generator ................................: (110.33, 132.76)
    forward-recv ...................................: (15.73, 17.55)
    forward-send ...................................: (0.39, 0.46)
    backward-recv ..................................: (31.21, 34.09)
    backward-send ..................................: (3.02, 3.06)
    forward-send-backward-recv .....................: (2869.52, 2945.02)
    backward-send-forward-recv .....................: (1315.62, 1408.81)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.74, 5.97)
    grads-reduce-scatter ...........................: (15.34, 15.98)
    params-all-gather ..............................: (8.30, 8.78)
    optimizer-copy-to-main-grad ....................: (0.65, 0.77)
    optimizer-clip-main-grad .......................: (4.54, 4.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.15, 9.36)
    optimizer-copy-main-to-model-params ............: (2.94, 3.03)
    optimizer ......................................: (18.16, 18.26)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 11373.2 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.060163E+01 | loss scale: 1.0 | grad norm: 2.501 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (11278.21, 11316.12)
    forward-compute ................................: (2809.11, 5425.71)
    backward-compute ...............................: (4388.27, 4587.30)
    batch-generator ................................: (104.76, 151.22)
    forward-recv ...................................: (17.53, 23.26)
    forward-send ...................................: (0.44, 0.64)
    backward-recv ..................................: (23.77, 31.35)
    backward-send ..................................: (0.49, 0.59)
    forward-send-backward-recv .....................: (3883.28, 3997.37)
    backward-send-forward-recv .....................: (1250.55, 1272.73)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.77, 5.99)
    grads-reduce-scatter ...........................: (15.44, 15.83)
    params-all-gather ..............................: (8.31, 8.79)
    optimizer-copy-to-main-grad ....................: (0.67, 0.87)
    optimizer-clip-main-grad .......................: (4.55, 4.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.16, 9.37)
    optimizer-copy-main-to-model-params ............: (2.94, 3.03)
    optimizer ......................................: (18.28, 18.38)
Mon Feb 12 14:01:34 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   38C    P0             380W / 700W |  41462MiB / 81559MiB |     13%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   44C    P0             394W / 700W |  41626MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   44C    P0             411W / 700W |  41582MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             372W / 700W |  41246MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   37C    P0             329W / 700W |  41044MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   44C    P0             339W / 700W |  41328MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   42C    P0             339W / 700W |  41434MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             340W / 700W |  40822MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 10710.7 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.057962E+01 | loss scale: 1.0 | grad norm: 1.495 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10512.69, 10566.73)
    forward-compute ................................: (2735.41, 5052.94)
    backward-compute ...............................: (4117.39, 4360.42)
    batch-generator ................................: (102.10, 157.39)
    forward-recv ...................................: (17.68, 24.64)
    forward-send ...................................: (0.46, 0.67)
    backward-recv ..................................: (22.45, 36.61)
    backward-send ..................................: (3.26, 4.35)
    forward-send-backward-recv .....................: (3495.32, 3556.21)
    backward-send-forward-recv .....................: (1116.58, 1154.77)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.73, 6.08)
    grads-reduce-scatter ...........................: (15.34, 15.82)
    params-all-gather ..............................: (8.30, 8.80)
    optimizer-copy-to-main-grad ....................: (0.65, 0.85)
    optimizer-clip-main-grad .......................: (4.63, 4.68)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.15, 9.42)
    optimizer-copy-main-to-model-params ............: (2.94, 3.04)
    optimizer ......................................: (18.43, 18.54)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 12138.5 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.047875E+01 | loss scale: 1.0 | grad norm: 2.841 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (12036.52, 12083.55)
    forward-compute ................................: (3149.66, 5823.44)
    backward-compute ...............................: (4473.98, 4679.60)
    batch-generator ................................: (100.26, 151.78)
    forward-recv ...................................: (20.26, 20.48)
    forward-send ...................................: (0.53, 0.56)
    backward-recv ..................................: (31.84, 34.56)
    backward-send ..................................: (0.54, 2.22)
    forward-send-backward-recv .....................: (4169.70, 4356.68)
    backward-send-forward-recv .....................: (1496.97, 1655.03)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.71, 5.93)
    grads-reduce-scatter ...........................: (15.33, 15.91)
    params-all-gather ..............................: (8.34, 8.79)
    optimizer-copy-to-main-grad ....................: (0.67, 0.88)
    optimizer-clip-main-grad .......................: (4.60, 4.64)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.16, 9.40)
    optimizer-copy-main-to-model-params ............: (2.94, 3.03)
    optimizer ......................................: (18.39, 18.49)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 11105.1 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049339E+01 | loss scale: 1.0 | grad norm: 1.168 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (11000.28, 11043.79)
    forward-compute ................................: (2797.67, 5310.20)
    backward-compute ...............................: (4291.97, 4465.91)
    batch-generator ................................: (106.85, 152.98)
    forward-recv ...................................: (16.24, 18.75)
    forward-send ...................................: (0.40, 0.49)
    backward-recv ..................................: (33.86, 40.40)
    backward-send ..................................: (0.51, 1.96)
    forward-send-backward-recv .....................: (3742.16, 3848.35)
    backward-send-forward-recv .....................: (1157.31, 1188.35)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.74, 6.06)
    grads-reduce-scatter ...........................: (15.38, 15.93)
    params-all-gather ..............................: (8.34, 8.78)
    optimizer-copy-to-main-grad ....................: (0.65, 0.85)
    optimizer-clip-main-grad .......................: (4.57, 4.72)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.16, 9.39)
    optimizer-copy-main-to-model-params ............: (2.94, 3.03)
    optimizer ......................................: (18.41, 18.50)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 11514.6 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047552E+01 | loss scale: 1.0 | grad norm: 1.546 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (11413.94, 11456.81)
    forward-compute ................................: (3071.88, 5480.98)
    backward-compute ...............................: (4299.73, 4511.12)
    batch-generator ................................: (103.22, 150.85)
    forward-recv ...................................: (16.29, 22.15)
    forward-send ...................................: (0.41, 0.58)
    backward-recv ..................................: (33.77, 37.75)
    backward-send ..................................: (2.18, 4.61)
    forward-send-backward-recv .....................: (3887.67, 3985.50)
    backward-send-forward-recv .....................: (1380.07, 1445.72)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.75, 5.95)
    grads-reduce-scatter ...........................: (15.52, 15.87)
    params-all-gather ..............................: (8.34, 8.76)
    optimizer-copy-to-main-grad ....................: (0.65, 0.84)
    optimizer-clip-main-grad .......................: (4.30, 4.33)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.15, 9.37)
    optimizer-copy-main-to-model-params ............: (2.94, 3.03)
    optimizer ......................................: (17.95, 18.05)
Mon Feb 12 14:09:13 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   38C    P0             374W / 700W |  41468MiB / 81559MiB |     19%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   44C    P0             389W / 700W |  41636MiB / 81559MiB |     97%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   44C    P0             337W / 700W |  41582MiB / 81559MiB |     97%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             400W / 700W |  41248MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   37C    P0             436W / 700W |  41446MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   43C    P0             429W / 700W |  41334MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   42C    P0             409W / 700W |  41442MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             421W / 700W |  40822MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 11205.9 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.043903E+01 | loss scale: 1.0 | grad norm: 1.089 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (11008.41, 11055.50)
    forward-compute ................................: (2817.80, 5370.30)
    backward-compute ...............................: (4328.33, 4627.90)
    batch-generator ................................: (102.08, 149.33)
    forward-recv ...................................: (16.85, 18.67)
    forward-send ...................................: (0.41, 1.55)
    backward-recv ..................................: (39.72, 45.10)
    backward-send ..................................: (1.36, 2.76)
    forward-send-backward-recv .....................: (3582.85, 3796.00)
    backward-send-forward-recv .....................: (1111.50, 1185.98)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.73, 6.11)
    grads-reduce-scatter ...........................: (15.36, 15.86)
    params-all-gather ..............................: (8.35, 8.77)
    optimizer-copy-to-main-grad ....................: (0.68, 0.87)
    optimizer-clip-main-grad .......................: (4.08, 4.12)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.17, 9.41)
    optimizer-copy-main-to-model-params ............: (2.94, 3.03)
    optimizer ......................................: (17.90, 18.00)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 11718.1 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.042506E+01 | loss scale: 1.0 | grad norm: 1.350 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (11602.19, 11647.81)
    forward-compute ................................: (3091.57, 5661.64)
    backward-compute ...............................: (4345.34, 4571.10)
    batch-generator ................................: (103.63, 151.33)
    forward-recv ...................................: (20.39, 20.67)
    forward-send ...................................: (0.54, 0.56)
    backward-recv ..................................: (38.65, 57.43)
    backward-send ..................................: (0.65, 0.74)
    forward-send-backward-recv .....................: (3997.61, 4084.95)
    backward-send-forward-recv .....................: (1389.68, 1448.85)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.77, 6.05)
    grads-reduce-scatter ...........................: (15.30, 15.81)
    params-all-gather ..............................: (8.34, 8.80)
    optimizer-copy-to-main-grad ....................: (0.66, 0.88)
    optimizer-clip-main-grad .......................: (3.07, 3.09)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.18, 9.41)
    optimizer-copy-main-to-model-params ............: (2.94, 3.04)
    optimizer ......................................: (16.77, 16.87)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 11073.4 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041806E+01 | loss scale: 1.0 | grad norm: 1.960 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10969.93, 11011.87)
    forward-compute ................................: (2800.72, 5395.41)
    backward-compute ...............................: (4293.90, 4549.45)
    batch-generator ................................: (102.88, 152.59)
    forward-recv ...................................: (18.05, 18.13)
    forward-send ...................................: (0.45, 0.50)
    backward-recv ..................................: (35.62, 57.33)
    backward-send ..................................: (0.67, 6.05)
    forward-send-backward-recv .....................: (3690.29, 3791.15)
    backward-send-forward-recv .....................: (1075.15, 1193.28)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.72, 6.10)
    grads-reduce-scatter ...........................: (15.38, 15.76)
    params-all-gather ..............................: (8.25, 8.78)
    optimizer-copy-to-main-grad ....................: (0.66, 0.85)
    optimizer-clip-main-grad .......................: (4.57, 4.61)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.17, 9.40)
    optimizer-copy-main-to-model-params ............: (2.94, 3.03)
    optimizer ......................................: (18.37, 18.47)
[2024-02-12 14:14:48,903] torch.distributed.elastic.agent.server.api: [ERROR] Error waiting on exit barrier. Elapsed: 100.246102809906 seconds
benchmark/test_packing.sh.two.bak-222: line 139: 400999 Killed                  torchrun $DISTRIBUTED_ARGS pretrain_gpt.py $GPT_ARGS $DATA_ARGS $OUTPUT_ARGS --distributed-backend nccl
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (347133): No such process
kill: (347139): No such process
kill: (347145): No such process
kill: (347151): No such process
10.64.24.49 kill done.
13b, 4k, gbs=512: dp=2, tp=2, pp=4, mbs=8
LOCAL_IP = 10.64.24.52
DP=2, MP=2, PP=4
[2024-02-12 14:15:25,854] torch.distributed.run: [WARNING] 
[2024-02-12 14:15:25,854] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 14:15:25,854] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 14:15:25,854] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1573350400
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 1573350400
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1702466560
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 1702466560
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...

> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...

 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.861 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.858 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.873 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.876 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.059 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.215 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.215 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.227 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (46.06, 632.65)
    train/valid/test-data-iterators-setup ..........: (0.02, 10398.81)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 7886.3 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.095054E+01 | loss scale: 1.0 | grad norm: 8.193 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 18137.978515625 | max allocated: 42289.5732421875 | reserved: 55760.0 | max reserved: 55760.0[Rank 9] (after 10 iterations) memory (MB) | allocated: 18137.978515625 | max allocated: 42291.2412109375 | reserved: 56178.0 | max reserved: 56178.0

[Rank 12] (after 10 iterations) memory (MB) | allocated: 19614.5947265625 | max allocated: 43269.32470703125 | reserved: 53048.0 | max reserved: 53048.0
[Rank 13] (after 10 iterations) memory (MB) | allocated: 19614.5947265625 | max allocated: 43269.30224609375 | reserved: 53048.0 | max reserved: 53048.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (7285.17, 7572.00)
    forward-compute ................................: (1286.61, 3285.32)
    backward-compute ...............................: (2098.82, 3200.09)
    batch-generator ................................: (134.34, 159.50)
    forward-recv ...................................: (213.81, 578.42)
    forward-send ...................................: (4.75, 310.87)
    backward-recv ..................................: (120.28, 464.20)
    backward-send ..................................: (1.46, 36.83)
    forward-send-backward-recv .....................: (2559.07, 3263.37)
    backward-send-forward-recv .....................: (420.96, 636.15)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 11.63)
    grads-reduce-scatter ...........................: (16.22, 220.91)
    params-all-gather ..............................: (7.55, 8.76)
    optimizer-copy-to-main-grad ....................: (0.35, 0.47)
    optimizer-clip-main-grad .......................: (8.09, 8.35)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.92, 9.97)
    optimizer-copy-main-to-model-params ............: (2.60, 2.95)
    optimizer ......................................: (22.13, 22.48)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 6122.5 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.080104E+01 | loss scale: 1.0 | grad norm: 3.855 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5809.54, 6035.36)
    forward-compute ................................: (991.89, 2790.12)
    backward-compute ...............................: (1843.88, 2844.24)
    batch-generator ................................: (23.75, 31.62)
    forward-recv ...................................: (45.08, 130.97)
    forward-send ...................................: (0.90, 41.17)
    backward-recv ..................................: (83.61, 346.47)
    backward-send ..................................: (1.05, 50.96)
    forward-send-backward-recv .....................: (2060.90, 2719.52)
    backward-send-forward-recv .....................: (199.33, 641.98)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 11.71)
    grads-reduce-scatter ...........................: (14.80, 16.56)
    params-all-gather ..............................: (7.58, 8.73)
    optimizer-copy-to-main-grad ....................: (0.31, 0.44)
    optimizer-clip-main-grad .......................: (4.16, 4.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.73, 9.72)
    optimizer-copy-main-to-model-params ............: (2.59, 2.84)
    optimizer ......................................: (17.67, 17.92)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 7014.9 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.058194E+01 | loss scale: 1.0 | grad norm: 1.893 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6591.16, 6897.65)
    forward-compute ................................: (1156.86, 2876.17)
    backward-compute ...............................: (2136.28, 3042.63)
    batch-generator ................................: (23.19, 31.75)
    forward-recv ...................................: (43.45, 112.78)
    forward-send ...................................: (1.13, 31.37)
    backward-recv ..................................: (95.07, 469.64)
    backward-send ..................................: (4.01, 44.22)
    forward-send-backward-recv .....................: (2209.50, 2751.21)
    backward-send-forward-recv .....................: (575.23, 930.20)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 11.44)
    grads-reduce-scatter ...........................: (14.42, 16.53)
    params-all-gather ..............................: (7.61, 8.73)
    optimizer-copy-to-main-grad ....................: (0.31, 0.43)
    optimizer-clip-main-grad .......................: (4.15, 4.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.70, 9.63)
    optimizer-copy-main-to-model-params ............: (2.59, 2.85)
    optimizer ......................................: (17.45, 17.71)
Mon Feb 12 14:20:32 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   35C    P0             236W / 700W |  70450MiB / 81559MiB |     10%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   40C    P0             219W / 700W |  70690MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   41C    P0             310W / 700W |  65946MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0             298W / 700W |  66080MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             226W / 700W |  59626MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   46C    P0             196W / 700W |  59626MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   45C    P0             309W / 700W |  54500MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             352W / 700W |  54500MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 6416.9 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.056755E+01 | loss scale: 1.0 | grad norm: 1.067 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6005.95, 6249.34)
    forward-compute ................................: (1035.76, 2553.56)
    backward-compute ...............................: (1918.70, 2819.35)
    batch-generator ................................: (23.17, 31.08)
    forward-recv ...................................: (48.59, 401.09)
    forward-send ...................................: (1.12, 340.49)
    backward-recv ..................................: (64.88, 374.32)
    backward-send ..................................: (1.21, 47.83)
    forward-send-backward-recv .....................: (2086.43, 2634.77)
    backward-send-forward-recv .....................: (376.18, 611.37)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 11.69)
    grads-reduce-scatter ...........................: (14.88, 16.45)
    params-all-gather ..............................: (7.64, 8.70)
    optimizer-copy-to-main-grad ....................: (0.31, 0.43)
    optimizer-clip-main-grad .......................: (4.27, 4.52)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.69, 9.66)
    optimizer-copy-main-to-model-params ............: (2.59, 2.85)
    optimizer ......................................: (17.59, 17.85)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 7003.8 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.047212E+01 | loss scale: 1.0 | grad norm: 4.037 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6638.90, 6892.56)
    forward-compute ................................: (1191.94, 2989.74)
    backward-compute ...............................: (2199.54, 3166.04)
    batch-generator ................................: (23.21, 30.57)
    forward-recv ...................................: (49.77, 143.67)
    forward-send ...................................: (1.22, 36.36)
    backward-recv ..................................: (79.80, 396.47)
    backward-send ..................................: (4.10, 47.38)
    forward-send-backward-recv .....................: (2305.08, 3005.39)
    backward-send-forward-recv .....................: (439.15, 688.42)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 11.46)
    grads-reduce-scatter ...........................: (14.72, 16.54)
    params-all-gather ..............................: (7.65, 8.71)
    optimizer-copy-to-main-grad ....................: (0.32, 0.42)
    optimizer-clip-main-grad .......................: (4.15, 4.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.69, 9.61)
    optimizer-copy-main-to-model-params ............: (2.59, 2.85)
    optimizer ......................................: (17.52, 17.77)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 6762.2 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049168E+01 | loss scale: 1.0 | grad norm: 3.440 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6396.13, 6647.61)
    forward-compute ................................: (1104.26, 2940.47)
    backward-compute ...............................: (2046.11, 2965.42)
    batch-generator ................................: (23.12, 30.44)
    forward-recv ...................................: (49.00, 133.47)
    forward-send ...................................: (0.97, 32.93)
    backward-recv ..................................: (114.49, 380.22)
    backward-send ..................................: (1.16, 28.96)
    forward-send-backward-recv .....................: (2271.68, 3049.46)
    backward-send-forward-recv .....................: (347.38, 644.57)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 11.53)
    grads-reduce-scatter ...........................: (14.81, 16.59)
    params-all-gather ..............................: (7.60, 8.72)
    optimizer-copy-to-main-grad ....................: (0.31, 0.42)
    optimizer-clip-main-grad .......................: (4.14, 4.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.69, 9.59)
    optimizer-copy-main-to-model-params ............: (2.59, 2.86)
    optimizer ......................................: (17.40, 17.67)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 7183.2 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047434E+01 | loss scale: 1.0 | grad norm: 1.373 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6835.14, 7087.99)
    forward-compute ................................: (1079.07, 3360.64)
    backward-compute ...............................: (1987.99, 3019.25)
    batch-generator ................................: (23.68, 36.20)
    forward-recv ...................................: (42.54, 112.60)
    forward-send ...................................: (1.01, 30.18)
    backward-recv ..................................: (120.89, 596.36)
    backward-send ..................................: (1.23, 23.99)
    forward-send-backward-recv .....................: (2435.52, 3255.46)
    backward-send-forward-recv .....................: (340.27, 687.37)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 12.13)
    grads-reduce-scatter ...........................: (14.83, 16.61)
    params-all-gather ..............................: (7.52, 8.70)
    optimizer-copy-to-main-grad ....................: (0.31, 0.51)
    optimizer-clip-main-grad .......................: (4.21, 4.46)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 9.64)
    optimizer-copy-main-to-model-params ............: (2.60, 2.87)
    optimizer ......................................: (17.65, 17.93)
Mon Feb 12 14:25:06 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   36C    P0             390W / 700W |  76470MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   41C    P0             439W / 700W |  76806MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   42C    P0             369W / 700W |  78984MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0             374W / 700W |  78816MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             428W / 700W |  62778MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   47C    P0             409W / 700W |  62778MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   45C    P0             444W / 700W |  60804MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             423W / 700W |  60804MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 6370.0 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044377E+01 | loss scale: 1.0 | grad norm: 3.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5961.61, 6185.71)
    forward-compute ................................: (1149.77, 2594.49)
    backward-compute ...............................: (2117.92, 3035.68)
    batch-generator ................................: (23.88, 35.05)
    forward-recv ...................................: (52.31, 118.08)
    forward-send ...................................: (1.02, 22.51)
    backward-recv ..................................: (87.19, 297.20)
    backward-send ..................................: (1.09, 28.08)
    forward-send-backward-recv .....................: (2018.67, 2475.35)
    backward-send-forward-recv .....................: (246.97, 514.90)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 11.67)
    grads-reduce-scatter ...........................: (14.62, 16.53)
    params-all-gather ..............................: (7.61, 8.75)
    optimizer-copy-to-main-grad ....................: (0.31, 0.48)
    optimizer-clip-main-grad .......................: (3.93, 4.15)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.71, 9.61)
    optimizer-copy-main-to-model-params ............: (2.60, 2.84)
    optimizer ......................................: (17.23, 17.47)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 6353.0 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.042698E+01 | loss scale: 1.0 | grad norm: 0.987 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6016.32, 6256.18)
    forward-compute ................................: (1130.24, 2643.54)
    backward-compute ...............................: (2084.88, 3024.41)
    batch-generator ................................: (23.32, 35.05)
    forward-recv ...................................: (53.06, 121.35)
    forward-send ...................................: (1.24, 35.67)
    backward-recv ..................................: (109.06, 323.05)
    backward-send ..................................: (1.28, 17.29)
    forward-send-backward-recv .....................: (2019.68, 2630.18)
    backward-send-forward-recv .....................: (296.72, 554.65)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 11.49)
    grads-reduce-scatter ...........................: (14.53, 16.53)
    params-all-gather ..............................: (7.61, 8.73)
    optimizer-copy-to-main-grad ....................: (0.31, 0.47)
    optimizer-clip-main-grad .......................: (3.68, 3.88)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.70, 9.61)
    optimizer-copy-main-to-model-params ............: (2.59, 2.86)
    optimizer ......................................: (16.93, 17.20)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 6268.9 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041186E+01 | loss scale: 1.0 | grad norm: 1.193 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5889.39, 6165.72)
    forward-compute ................................: (1115.92, 2592.59)
    backward-compute ...............................: (2054.91, 2974.06)
    batch-generator ................................: (23.61, 35.53)
    forward-recv ...................................: (35.28, 98.70)
    forward-send ...................................: (1.04, 21.84)
    backward-recv ..................................: (125.26, 419.35)
    backward-send ..................................: (1.26, 20.10)
    forward-send-backward-recv .....................: (2029.10, 2483.19)
    backward-send-forward-recv .....................: (246.45, 519.61)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 11.59)
    grads-reduce-scatter ...........................: (14.70, 16.51)
    params-all-gather ..............................: (7.59, 8.73)
    optimizer-copy-to-main-grad ....................: (0.31, 0.46)
    optimizer-clip-main-grad .......................: (3.44, 3.62)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.69, 9.60)
    optimizer-copy-main-to-model-params ............: (2.59, 2.85)
    optimizer ......................................: (16.84, 17.10)
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (348625): No such process
kill: (348631): No such process
kill: (348637): No such process
kill: (348643): No such process
10.64.24.49 kill done.
13b, 4k, gbs=512: dp=2, tp=2, pp=4, mbs=4
LOCAL_IP = 10.64.24.52
DP=2, MP=2, PP=4
[2024-02-12 14:29:28,443] torch.distributed.run: [WARNING] 
[2024-02-12 14:29:28,443] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 14:29:28,443] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 14:29:28,443] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 1573350400
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1573350400
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 1702466560
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1702466560
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.736 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.738 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.793 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.802 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.181 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.200 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.152 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.266 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (51.62, 656.44)
    train/valid/test-data-iterators-setup ..........: (0.02, 11399.42)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 8996.8 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.095044E+01 | loss scale: 1.0 | grad norm: 8.185 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 18151.162109375 | max allocated: 33233.2001953125 | reserved: 45032.0 | max reserved: 45032.0[Rank 9] (after 10 iterations) memory (MB) | allocated: 18151.162109375 | max allocated: 33236.9345703125 | reserved: 45228.0 | max reserved: 45228.0

[Rank 12] (after 10 iterations) memory (MB) | allocated: 19627.7783203125 | max allocated: 36097.58544921875 | reserved: 39022.0 | max reserved: 39022.0
[Rank 13] (after 10 iterations) memory (MB) | allocated: 19627.7783203125 | max allocated: 36097.36181640625 | reserved: 39022.0 | max reserved: 39022.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (8520.54, 8687.99)
    forward-compute ................................: (1428.70, 3792.85)
    backward-compute ...............................: (2507.38, 3487.13)
    batch-generator ................................: (153.84, 183.20)
    forward-recv ...................................: (188.82, 515.45)
    forward-send ...................................: (4.86, 308.22)
    backward-recv ..................................: (55.96, 298.39)
    backward-send ..................................: (0.87, 25.73)
    forward-send-backward-recv .....................: (3039.84, 4002.89)
    backward-send-forward-recv .....................: (732.83, 1045.60)
    layernorm-grads-all-reduce .....................: (0.02, 0.04)
    embedding-grads-all-reduce .....................: (0.02, 11.66)
    grads-reduce-scatter ...........................: (16.09, 218.30)
    params-all-gather ..............................: (7.63, 8.74)
    optimizer-copy-to-main-grad ....................: (0.33, 0.52)
    optimizer-clip-main-grad .......................: (7.95, 8.21)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.95, 9.98)
    optimizer-copy-main-to-model-params ............: (2.60, 2.86)
    optimizer ......................................: (22.12, 22.42)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 7556.6 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.080130E+01 | loss scale: 1.0 | grad norm: 3.882 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7339.03, 7471.13)
    forward-compute ................................: (1157.38, 3435.92)
    backward-compute ...............................: (2286.09, 3164.44)
    batch-generator ................................: (45.57, 57.02)
    forward-recv ...................................: (28.71, 60.91)
    forward-send ...................................: (0.52, 19.27)
    backward-recv ..................................: (42.28, 255.27)
    backward-send ..................................: (0.68, 31.36)
    forward-send-backward-recv .....................: (2492.75, 3330.76)
    backward-send-forward-recv .....................: (669.15, 1289.07)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 11.71)
    grads-reduce-scatter ...........................: (14.66, 16.47)
    params-all-gather ..............................: (7.59, 8.75)
    optimizer-copy-to-main-grad ....................: (0.32, 0.44)
    optimizer-clip-main-grad .......................: (4.17, 4.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.69, 9.70)
    optimizer-copy-main-to-model-params ............: (2.59, 2.85)
    optimizer ......................................: (17.53, 17.78)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 8250.4 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.058179E+01 | loss scale: 1.0 | grad norm: 1.909 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8018.46, 8163.06)
    forward-compute ................................: (1303.81, 3431.27)
    backward-compute ...............................: (2554.48, 3389.73)
    batch-generator ................................: (45.53, 57.20)
    forward-recv ...................................: (26.74, 357.75)
    forward-send ...................................: (0.64, 319.61)
    backward-recv ..................................: (41.67, 227.22)
    backward-send ..................................: (10.69, 36.75)
    forward-send-backward-recv .....................: (2732.20, 3664.08)
    backward-send-forward-recv .....................: (657.07, 1176.84)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 11.52)
    grads-reduce-scatter ...........................: (14.69, 16.42)
    params-all-gather ..............................: (7.58, 8.73)
    optimizer-copy-to-main-grad ....................: (0.32, 0.43)
    optimizer-clip-main-grad .......................: (4.16, 4.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.70, 9.69)
    optimizer-copy-main-to-model-params ............: (2.60, 2.85)
    optimizer ......................................: (17.52, 17.77)
Mon Feb 12 14:35:24 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   35C    P0             340W / 700W |  57616MiB / 81559MiB |     32%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   40C    P0             294W / 700W |  57812MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   40C    P0             151W / 700W |  51334MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   35C    P0             174W / 700W |  51592MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             324W / 700W |  44024MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             333W / 700W |  44024MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             305W / 700W |  44166MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             241W / 700W |  44240MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 7465.6 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.056751E+01 | loss scale: 1.0 | grad norm: 1.046 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7152.17, 7294.51)
    forward-compute ................................: (1172.69, 3265.31)
    backward-compute ...............................: (2314.44, 3152.54)
    batch-generator ................................: (47.11, 58.04)
    forward-recv ...................................: (26.18, 68.35)
    forward-send ...................................: (0.60, 20.74)
    backward-recv ..................................: (65.83, 199.79)
    backward-send ..................................: (0.68, 17.86)
    forward-send-backward-recv .....................: (2370.19, 3460.88)
    backward-send-forward-recv .....................: (543.34, 884.15)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 11.70)
    grads-reduce-scatter ...........................: (14.85, 16.45)
    params-all-gather ..............................: (7.62, 8.79)
    optimizer-copy-to-main-grad ....................: (0.33, 0.44)
    optimizer-clip-main-grad .......................: (4.18, 4.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.70, 9.66)
    optimizer-copy-main-to-model-params ............: (2.59, 2.85)
    optimizer ......................................: (17.53, 17.79)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 8305.0 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.047233E+01 | loss scale: 1.0 | grad norm: 4.180 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8074.16, 8224.91)
    forward-compute ................................: (1349.74, 3818.16)
    backward-compute ...............................: (2631.46, 3742.42)
    batch-generator ................................: (47.12, 58.67)
    forward-recv ...................................: (33.47, 66.74)
    forward-send ...................................: (0.73, 21.99)
    backward-recv ..................................: (46.56, 203.06)
    backward-send ..................................: (0.76, 47.19)
    forward-send-backward-recv .....................: (2873.74, 3940.14)
    backward-send-forward-recv .....................: (623.62, 1093.04)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 11.62)
    grads-reduce-scatter ...........................: (14.77, 16.40)
    params-all-gather ..............................: (7.64, 8.77)
    optimizer-copy-to-main-grad ....................: (0.32, 0.43)
    optimizer-clip-main-grad .......................: (4.19, 4.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.69, 9.67)
    optimizer-copy-main-to-model-params ............: (2.60, 2.85)
    optimizer ......................................: (17.56, 17.81)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 7555.9 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049291E+01 | loss scale: 1.0 | grad norm: 3.382 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7313.47, 7458.00)
    forward-compute ................................: (1243.17, 3337.04)
    backward-compute ...............................: (2438.66, 3321.52)
    batch-generator ................................: (47.00, 58.49)
    forward-recv ...................................: (27.44, 63.13)
    forward-send ...................................: (0.51, 14.15)
    backward-recv ..................................: (58.02, 226.43)
    backward-send ..................................: (1.00, 21.41)
    forward-send-backward-recv .....................: (2501.37, 3441.12)
    backward-send-forward-recv .....................: (556.40, 945.93)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 11.61)
    grads-reduce-scatter ...........................: (14.88, 16.59)
    params-all-gather ..............................: (7.66, 8.74)
    optimizer-copy-to-main-grad ....................: (0.32, 0.44)
    optimizer-clip-main-grad .......................: (4.18, 4.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.70, 9.63)
    optimizer-copy-main-to-model-params ............: (2.60, 2.85)
    optimizer ......................................: (17.57, 17.83)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 7261.3 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047733E+01 | loss scale: 1.0 | grad norm: 3.844 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7037.73, 7167.99)
    forward-compute ................................: (1257.41, 3139.68)
    backward-compute ...............................: (2461.60, 3306.11)
    batch-generator ................................: (47.08, 57.66)
    forward-recv ...................................: (24.23, 69.02)
    forward-send ...................................: (0.62, 25.84)
    backward-recv ..................................: (43.55, 201.90)
    backward-send ..................................: (0.76, 20.20)
    forward-send-backward-recv .....................: (2353.72, 3130.12)
    backward-send-forward-recv .....................: (529.09, 863.93)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 11.75)
    grads-reduce-scatter ...........................: (14.70, 16.54)
    params-all-gather ..............................: (7.58, 8.77)
    optimizer-copy-to-main-grad ....................: (0.32, 0.43)
    optimizer-clip-main-grad .......................: (3.96, 4.18)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.70, 9.68)
    optimizer-copy-main-to-model-params ............: (2.60, 2.87)
    optimizer ......................................: (17.27, 17.54)
Mon Feb 12 14:40:30 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   36C    P0             324W / 700W |  57616MiB / 81559MiB |     77%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   41C    P0             358W / 700W |  57812MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   41C    P0             294W / 700W |  59978MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0             322W / 700W |  60156MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             384W / 700W |  44024MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   46C    P0             387W / 700W |  44024MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   45C    P0             458W / 700W |  49386MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             440W / 700W |  49700MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 7449.9 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044409E+01 | loss scale: 1.0 | grad norm: 1.621 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7122.81, 7291.64)
    forward-compute ................................: (1288.08, 3176.22)
    backward-compute ...............................: (2514.67, 3373.09)
    batch-generator ................................: (48.00, 58.79)
    forward-recv ...................................: (30.62, 59.25)
    forward-send ...................................: (0.55, 11.23)
    backward-recv ..................................: (50.90, 220.51)
    backward-send ..................................: (0.67, 30.45)
    forward-send-backward-recv .....................: (2357.16, 3135.87)
    backward-send-forward-recv .....................: (557.86, 858.79)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 11.75)
    grads-reduce-scatter ...........................: (14.39, 16.55)
    params-all-gather ..............................: (7.65, 8.77)
    optimizer-copy-to-main-grad ....................: (0.33, 0.42)
    optimizer-clip-main-grad .......................: (3.96, 4.73)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.69, 9.68)
    optimizer-copy-main-to-model-params ............: (2.60, 2.86)
    optimizer ......................................: (17.83, 18.10)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 7404.7 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.042982E+01 | loss scale: 1.0 | grad norm: 0.863 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7146.95, 7326.21)
    forward-compute ................................: (1266.44, 3213.22)
    backward-compute ...............................: (2490.09, 3330.41)
    batch-generator ................................: (45.94, 57.09)
    forward-recv ...................................: (30.09, 70.39)
    forward-send ...................................: (0.73, 14.02)
    backward-recv ..................................: (59.08, 232.31)
    backward-send ..................................: (0.67, 14.42)
    forward-send-backward-recv .....................: (2367.99, 3160.19)
    backward-send-forward-recv .....................: (541.87, 845.00)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 11.63)
    grads-reduce-scatter ...........................: (14.67, 16.51)
    params-all-gather ..............................: (7.56, 8.78)
    optimizer-copy-to-main-grad ....................: (0.32, 0.43)
    optimizer-clip-main-grad .......................: (3.95, 4.17)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.70, 9.68)
    optimizer-copy-main-to-model-params ............: (2.60, 2.85)
    optimizer ......................................: (17.26, 17.52)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 7265.9 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041617E+01 | loss scale: 1.0 | grad norm: 3.628 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7020.24, 7173.51)
    forward-compute ................................: (1238.16, 3160.66)
    backward-compute ...............................: (2435.27, 3328.96)
    batch-generator ................................: (47.33, 57.28)
    forward-recv ...................................: (22.92, 56.70)
    forward-send ...................................: (0.60, 17.50)
    backward-recv ..................................: (52.22, 219.37)
    backward-send ..................................: (0.77, 20.22)
    forward-send-backward-recv .....................: (2335.48, 3108.88)
    backward-send-forward-recv .....................: (526.96, 778.62)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 11.73)
    grads-reduce-scatter ...........................: (14.62, 16.51)
    params-all-gather ..............................: (7.62, 8.77)
    optimizer-copy-to-main-grad ....................: (0.32, 0.43)
    optimizer-clip-main-grad .......................: (3.71, 3.91)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.69, 9.67)
    optimizer-copy-main-to-model-params ............: (2.60, 2.86)
    optimizer ......................................: (17.01, 17.27)
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (350117): No such process
kill: (350123): No such process
kill: (350129): No such process
kill: (350135): No such process
10.64.24.49 kill done.
13b, 4k, gbs=512: dp=2, tp=2, pp=4, mbs=2
LOCAL_IP = 10.64.24.52
DP=2, MP=2, PP=4
[2024-02-12 14:45:11,171] torch.distributed.run: [WARNING] 
[2024-02-12 14:45:11,171] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 14:45:11,171] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 14:45:11,171] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 1573350400
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1573350400
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1702466560
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 1702466560
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.667 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.735 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.149 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.684 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.194 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.141 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.303 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.777 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (43.45, 634.91)
    train/valid/test-data-iterators-setup ..........: (0.02, 11925.91)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 11684.4 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.095060E+01 | loss scale: 1.0 | grad norm: 8.192 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 18109.3564453125 | max allocated: 30405.75 | reserved: 36184.0 | max reserved: 36184.0
[Rank 9] (after 10 iterations) memory (MB) | allocated: 18109.517578125 | max allocated: 30402.349609375 | reserved: 36438.0 | max reserved: 36438.0
[Rank 13] (after 10 iterations) memory (MB) | allocated: 19586.6259765625 | max allocated: 30533.65869140625 | reserved: 33156.0 | max reserved: 33156.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 19586.6259765625 | max allocated: 30533.65869140625 | reserved: 33088.0 | max reserved: 33088.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (11265.36, 11393.74)
    forward-compute ................................: (1803.88, 5058.37)
    backward-compute ...............................: (3374.45, 4278.27)
    batch-generator ................................: (206.21, 238.18)
    forward-recv ...................................: (185.92, 484.15)
    forward-send ...................................: (5.74, 297.22)
    backward-recv ..................................: (40.92, 184.21)
    backward-send ..................................: (2.39, 19.10)
    forward-send-backward-recv .....................: (3735.72, 5456.74)
    backward-send-forward-recv .....................: (925.37, 1892.15)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 11.67)
    grads-reduce-scatter ...........................: (16.11, 223.61)
    params-all-gather ..............................: (7.61, 8.79)
    optimizer-copy-to-main-grad ....................: (0.34, 1.11)
    optimizer-clip-main-grad .......................: (7.42, 7.68)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.97, 10.03)
    optimizer-copy-main-to-model-params ............: (2.59, 2.85)
    optimizer ......................................: (22.99, 23.30)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 9755.6 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.080141E+01 | loss scale: 1.0 | grad norm: 3.835 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9588.11, 9676.60)
    forward-compute ................................: (1579.51, 4511.94)
    backward-compute ...............................: (3171.80, 3973.24)
    batch-generator ................................: (91.11, 117.54)
    forward-recv ...................................: (19.16, 293.94)
    forward-send ...................................: (0.32, 269.20)
    backward-recv ..................................: (29.40, 113.65)
    backward-send ..................................: (0.47, 7.23)
    forward-send-backward-recv .....................: (3121.76, 4519.44)
    backward-send-forward-recv .....................: (752.87, 1519.65)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 11.59)
    grads-reduce-scatter ...........................: (14.78, 16.48)
    params-all-gather ..............................: (7.63, 8.71)
    optimizer-copy-to-main-grad ....................: (0.34, 0.51)
    optimizer-clip-main-grad .......................: (4.19, 4.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.73, 9.65)
    optimizer-copy-main-to-model-params ............: (2.59, 2.85)
    optimizer ......................................: (17.61, 17.87)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 10956.0 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.058194E+01 | loss scale: 1.0 | grad norm: 1.921 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10799.40, 10883.68)
    forward-compute ................................: (1694.55, 4936.69)
    backward-compute ...............................: (3393.06, 4172.56)
    batch-generator ................................: (92.64, 113.34)
    forward-recv ...................................: (20.77, 46.83)
    forward-send ...................................: (0.37, 16.48)
    backward-recv ..................................: (29.12, 125.21)
    backward-send ..................................: (1.64, 17.43)
    forward-send-backward-recv .....................: (3621.21, 5426.42)
    backward-send-forward-recv .....................: (856.54, 1932.83)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 11.45)
    grads-reduce-scatter ...........................: (14.86, 16.39)
    params-all-gather ..............................: (7.64, 8.67)
    optimizer-copy-to-main-grad ....................: (0.33, 0.46)
    optimizer-clip-main-grad .......................: (4.17, 4.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.73, 9.64)
    optimizer-copy-main-to-model-params ............: (2.59, 2.85)
    optimizer ......................................: (17.68, 17.94)
Mon Feb 12 14:52:40 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   35C    P0             304W / 700W |  39542MiB / 81559MiB |     29%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   40C    P0             293W / 700W |  39796MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   40C    P0             330W / 700W |  42532MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   35C    P0             363W / 700W |  42742MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             356W / 700W |  36514MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             360W / 700W |  36582MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             380W / 700W |  36902MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             351W / 700W |  36906MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 9179.4 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.056779E+01 | loss scale: 1.0 | grad norm: 1.215 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8912.67, 9026.71)
    forward-compute ................................: (1578.50, 3827.21)
    backward-compute ...............................: (3176.29, 3954.82)
    batch-generator ................................: (91.16, 112.47)
    forward-recv ...................................: (19.64, 52.05)
    forward-send ...................................: (0.38, 14.33)
    backward-recv ..................................: (32.21, 123.18)
    backward-send ..................................: (1.83, 8.41)
    forward-send-backward-recv .....................: (2893.26, 3875.54)
    backward-send-forward-recv .....................: (736.38, 1177.95)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 11.60)
    grads-reduce-scatter ...........................: (14.70, 16.40)
    params-all-gather ..............................: (7.62, 9.46)
    optimizer-copy-to-main-grad ....................: (0.32, 0.43)
    optimizer-clip-main-grad .......................: (4.17, 4.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.73, 9.62)
    optimizer-copy-main-to-model-params ............: (2.59, 2.85)
    optimizer ......................................: (17.45, 17.71)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 10103.8 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.047196E+01 | loss scale: 1.0 | grad norm: 2.974 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9932.05, 10041.63)
    forward-compute ................................: (1710.67, 4443.13)
    backward-compute ...............................: (3447.21, 4283.51)
    batch-generator ................................: (92.85, 114.04)
    forward-recv ...................................: (19.94, 43.16)
    forward-send ...................................: (0.46, 11.32)
    backward-recv ..................................: (32.08, 123.04)
    backward-send ..................................: (0.44, 14.34)
    forward-send-backward-recv .....................: (3124.48, 4489.27)
    backward-send-forward-recv .....................: (890.13, 1481.04)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 11.57)
    grads-reduce-scatter ...........................: (14.72, 16.54)
    params-all-gather ..............................: (7.64, 8.73)
    optimizer-copy-to-main-grad ....................: (0.33, 0.45)
    optimizer-clip-main-grad .......................: (4.17, 4.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.73, 9.62)
    optimizer-copy-main-to-model-params ............: (2.59, 2.84)
    optimizer ......................................: (17.50, 17.75)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 9558.3 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049335E+01 | loss scale: 1.0 | grad norm: 1.664 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9388.62, 9486.98)
    forward-compute ................................: (1636.80, 4191.43)
    backward-compute ...............................: (3302.29, 4099.36)
    batch-generator ................................: (91.03, 112.86)
    forward-recv ...................................: (13.99, 37.61)
    forward-send ...................................: (0.31, 9.85)
    backward-recv ..................................: (34.79, 136.02)
    backward-send ..................................: (0.56, 20.74)
    forward-send-backward-recv .....................: (2978.96, 4179.14)
    backward-send-forward-recv .....................: (778.79, 1303.20)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 11.44)
    grads-reduce-scatter ...........................: (14.65, 16.61)
    params-all-gather ..............................: (7.64, 8.69)
    optimizer-copy-to-main-grad ....................: (0.33, 0.45)
    optimizer-clip-main-grad .......................: (4.18, 4.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.72, 9.62)
    optimizer-copy-main-to-model-params ............: (2.59, 2.85)
    optimizer ......................................: (17.49, 17.75)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 9390.6 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047616E+01 | loss scale: 1.0 | grad norm: 1.548 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9212.63, 9321.93)
    forward-compute ................................: (1632.95, 4086.87)
    backward-compute ...............................: (3332.31, 4078.58)
    batch-generator ................................: (91.47, 113.23)
    forward-recv ...................................: (16.90, 46.28)
    forward-send ...................................: (0.34, 10.45)
    backward-recv ..................................: (29.80, 129.55)
    backward-send ..................................: (1.37, 18.87)
    forward-send-backward-recv .....................: (2973.21, 3963.62)
    backward-send-forward-recv .....................: (747.38, 1180.89)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 11.37)
    grads-reduce-scatter ...........................: (14.64, 16.43)
    params-all-gather ..............................: (7.69, 8.72)
    optimizer-copy-to-main-grad ....................: (0.34, 0.43)
    optimizer-clip-main-grad .......................: (4.18, 4.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.72, 9.60)
    optimizer-copy-main-to-model-params ............: (2.59, 2.84)
    optimizer ......................................: (17.47, 17.72)
Mon Feb 12 14:59:07 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   36C    P0             388W / 700W |  39552MiB / 81559MiB |     56%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   41C    P0             383W / 700W |  39806MiB / 81559MiB |     96%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   41C    P0             403W / 700W |  42532MiB / 81559MiB |     92%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   35C    P0             382W / 700W |  42742MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             432W / 700W |  36516MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             454W / 700W |  36582MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             465W / 700W |  36902MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             456W / 700W |  36906MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 9651.2 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.043828E+01 | loss scale: 1.0 | grad norm: 1.085 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9384.56, 9496.63)
    forward-compute ................................: (1660.14, 4174.21)
    backward-compute ...............................: (3346.32, 4167.19)
    batch-generator ................................: (91.22, 112.69)
    forward-recv ...................................: (17.17, 35.37)
    forward-send ...................................: (0.35, 6.23)
    backward-recv ..................................: (40.43, 139.53)
    backward-send ..................................: (0.50, 10.79)
    forward-send-backward-recv .....................: (2903.15, 4094.27)
    backward-send-forward-recv .....................: (788.88, 1246.92)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 11.57)
    grads-reduce-scatter ...........................: (14.58, 16.44)
    params-all-gather ..............................: (7.58, 8.73)
    optimizer-copy-to-main-grad ....................: (0.33, 0.43)
    optimizer-clip-main-grad .......................: (3.92, 4.18)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.72, 9.61)
    optimizer-copy-main-to-model-params ............: (2.59, 2.85)
    optimizer ......................................: (17.28, 17.54)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 9518.8 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.042194E+01 | loss scale: 1.0 | grad norm: 1.043 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9321.79, 9424.18)
    forward-compute ................................: (1655.42, 4052.12)
    backward-compute ...............................: (3368.40, 4113.94)
    batch-generator ................................: (90.49, 113.00)
    forward-recv ...................................: (18.63, 42.70)
    forward-send ...................................: (0.45, 9.52)
    backward-recv ..................................: (35.39, 150.04)
    backward-send ..................................: (0.61, 19.59)
    forward-send-backward-recv .....................: (2940.40, 4024.30)
    backward-send-forward-recv .....................: (798.35, 1234.36)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 11.36)
    grads-reduce-scatter ...........................: (14.81, 16.40)
    params-all-gather ..............................: (7.55, 8.68)
    optimizer-copy-to-main-grad ....................: (0.33, 0.43)
    optimizer-clip-main-grad .......................: (3.69, 3.89)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.72, 9.61)
    optimizer-copy-main-to-model-params ............: (2.59, 2.85)
    optimizer ......................................: (16.95, 17.20)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 9415.6 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.040907E+01 | loss scale: 1.0 | grad norm: 2.259 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9224.67, 9325.51)
    forward-compute ................................: (1640.29, 4072.09)
    backward-compute ...............................: (3325.57, 4090.51)
    batch-generator ................................: (91.65, 111.35)
    forward-recv ...................................: (17.14, 37.92)
    forward-send ...................................: (0.37, 7.41)
    backward-recv ..................................: (30.18, 163.71)
    backward-send ..................................: (0.75, 25.56)
    forward-send-backward-recv .....................: (2905.10, 3963.77)
    backward-send-forward-recv .....................: (764.33, 1198.21)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 11.59)
    grads-reduce-scatter ...........................: (14.78, 16.55)
    params-all-gather ..............................: (7.64, 8.73)
    optimizer-copy-to-main-grad ....................: (0.33, 0.42)
    optimizer-clip-main-grad .......................: (3.68, 3.89)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.72, 9.62)
    optimizer-copy-main-to-model-params ............: (2.59, 2.85)
    optimizer ......................................: (16.92, 17.18)
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (351609): No such process
kill: (351615): No such process
kill: (351621): No such process
kill: (351627): No such process
10.64.24.49 kill done.
13b, 4k, gbs=512: dp=2, tp=8, pp=1, mbs=4
LOCAL_IP = 10.64.24.52
DP=2, MP=8, PP=1
[2024-02-12 15:04:34,133] torch.distributed.run: [WARNING] 
[2024-02-12 15:04:34,133] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 15:04:34,133] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 15:04:34,133] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.153 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.974 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (444.02, 531.17)
    train/valid/test-data-iterators-setup ..........: (0.02, 11185.11)
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 10568.2 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.095541E+01 | loss scale: 1.0 | grad norm: 8.029 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10397.43, 10403.68)
    forward-compute ................................: (5567.47, 5695.68)
    backward-compute ...............................: (4622.63, 4794.54)
    batch-generator ................................: (697.20, 721.98)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.04)
    grads-reduce-scatter ...........................: (73.29, 74.37)
    params-all-gather ..............................: (38.81, 39.28)
    optimizer-copy-to-main-grad ....................: (1.42, 1.94)
    optimizer-clip-main-grad .......................: (8.41, 8.50)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.53, 9.78)
    optimizer-copy-main-to-model-params ............: (3.47, 3.61)
    optimizer ......................................: (27.15, 27.66)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 8852.2 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.083639E+01 | loss scale: 1.0 | grad norm: 6.507 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8695.75, 8698.07)
    forward-compute ................................: (4328.33, 4418.67)
    backward-compute ...............................: (4187.34, 4327.88)
    batch-generator ................................: (66.98, 98.74)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (72.91, 73.60)
    params-all-gather ..............................: (38.82, 39.24)
    optimizer-copy-to-main-grad ....................: (1.39, 1.96)
    optimizer-clip-main-grad .......................: (5.50, 5.83)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.21, 9.40)
    optimizer-copy-main-to-model-params ............: (3.47, 3.60)
    optimizer ......................................: (23.83, 24.19)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 9647.7 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.060936E+01 | loss scale: 1.0 | grad norm: 2.224 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9491.19, 9495.64)
    forward-compute ................................: (4804.04, 4808.13)
    backward-compute ...............................: (4595.50, 4650.41)
    batch-generator ................................: (69.21, 97.77)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (72.62, 73.84)
    params-all-gather ..............................: (38.75, 39.14)
    optimizer-copy-to-main-grad ....................: (1.37, 1.82)
    optimizer-clip-main-grad .......................: (5.42, 5.60)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.21, 9.36)
    optimizer-copy-main-to-model-params ............: (3.47, 3.60)
    optimizer ......................................: (23.52, 24.06)
Mon Feb 12 15:11:29 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   36C    P0             235W / 700W |  71094MiB / 81559MiB |     24%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   42C    P0             238W / 700W |  71460MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   43C    P0             254W / 700W |  71478MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             226W / 700W |  71446MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   37C    P0             241W / 700W |  71184MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   43C    P0             239W / 700W |  71384MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   41C    P0             249W / 700W |  71622MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   36C    P0             223W / 700W |  71108MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 9230.5 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.058769E+01 | loss scale: 1.0 | grad norm: 2.268 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8975.98, 8985.18)
    forward-compute ................................: (4617.90, 4694.94)
    backward-compute ...............................: (4239.60, 4327.39)
    batch-generator ................................: (67.94, 100.16)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (72.99, 73.68)
    params-all-gather ..............................: (38.74, 39.19)
    optimizer-copy-to-main-grad ....................: (1.38, 1.82)
    optimizer-clip-main-grad .......................: (5.37, 5.60)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.21, 9.33)
    optimizer-copy-main-to-model-params ............: (3.47, 3.61)
    optimizer ......................................: (23.41, 23.81)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 9896.9 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.048672E+01 | loss scale: 1.0 | grad norm: 1.667 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9734.87, 9736.16)
    forward-compute ................................: (4915.51, 4947.45)
    backward-compute ...............................: (4724.62, 4781.21)
    batch-generator ................................: (70.24, 98.00)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (72.88, 73.59)
    params-all-gather ..............................: (38.66, 39.14)
    optimizer-copy-to-main-grad ....................: (1.39, 1.84)
    optimizer-clip-main-grad .......................: (5.47, 5.56)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.22, 9.43)
    optimizer-copy-main-to-model-params ............: (3.47, 3.60)
    optimizer ......................................: (23.58, 24.17)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 9306.9 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.050777E+01 | loss scale: 1.0 | grad norm: 2.225 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9151.94, 9153.92)
    forward-compute ................................: (4578.84, 4662.42)
    backward-compute ...............................: (4397.95, 4536.01)
    batch-generator ................................: (68.96, 100.19)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (72.96, 73.51)
    params-all-gather ..............................: (38.61, 39.27)
    optimizer-copy-to-main-grad ....................: (1.38, 1.76)
    optimizer-clip-main-grad .......................: (5.72, 5.82)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.20, 9.30)
    optimizer-copy-main-to-model-params ............: (3.47, 3.60)
    optimizer ......................................: (23.52, 23.88)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 9145.2 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.049270E+01 | loss scale: 1.0 | grad norm: 1.870 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8986.34, 8993.80)
    forward-compute ................................: (4440.86, 4452.55)
    backward-compute ...............................: (4449.53, 4508.52)
    batch-generator ................................: (67.95, 94.36)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (72.65, 73.88)
    params-all-gather ..............................: (38.65, 38.98)
    optimizer-copy-to-main-grad ....................: (1.33, 1.74)
    optimizer-clip-main-grad .......................: (5.30, 5.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.21, 9.30)
    optimizer-copy-main-to-model-params ............: (3.47, 3.60)
    optimizer ......................................: (23.39, 23.84)
Mon Feb 12 15:17:50 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   37C    P0             386W / 700W |  77482MiB / 81559MiB |     61%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   44C    P0             338W / 700W |  77340MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   45C    P0             340W / 700W |  77388MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             378W / 700W |  77378MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             383W / 700W |  77734MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   44C    P0             368W / 700W |  77382MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             399W / 700W |  77678MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             371W / 700W |  77050MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 9696.4 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.046050E+01 | loss scale: 1.0 | grad norm: 2.347 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9438.22, 9450.61)
    forward-compute ................................: (4794.21, 4796.79)
    backward-compute ...............................: (4551.08, 4618.98)
    batch-generator ................................: (68.33, 92.12)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (73.09, 73.66)
    params-all-gather ..............................: (38.76, 39.13)
    optimizer-copy-to-main-grad ....................: (1.36, 1.74)
    optimizer-clip-main-grad .......................: (5.36, 5.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.20, 9.30)
    optimizer-copy-main-to-model-params ............: (3.47, 3.60)
    optimizer ......................................: (23.28, 23.70)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 9338.2 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.044463E+01 | loss scale: 1.0 | grad norm: 1.202 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9169.49, 9189.38)
    forward-compute ................................: (4524.55, 4618.03)
    backward-compute ...............................: (4528.83, 4606.89)
    batch-generator ................................: (68.88, 97.79)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (72.69, 73.84)
    params-all-gather ..............................: (38.76, 39.25)
    optimizer-copy-to-main-grad ....................: (1.41, 1.77)
    optimizer-clip-main-grad .......................: (4.66, 4.74)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.21, 9.30)
    optimizer-copy-main-to-model-params ............: (3.47, 3.60)
    optimizer ......................................: (22.57, 23.10)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 9242.0 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.043119E+01 | loss scale: 1.0 | grad norm: 2.686 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9083.82, 9087.60)
    forward-compute ................................: (4437.34, 4597.52)
    backward-compute ...............................: (4448.15, 4608.81)
    batch-generator ................................: (68.13, 97.62)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (72.73, 74.09)
    params-all-gather ..............................: (38.59, 39.10)
    optimizer-copy-to-main-grad ....................: (1.39, 1.72)
    optimizer-clip-main-grad .......................: (4.69, 5.00)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.21, 9.28)
    optimizer-copy-main-to-model-params ............: (3.47, 3.60)
    optimizer ......................................: (22.67, 23.03)
[2024-02-12 15:22:41,348] torch.distributed.elastic.agent.server.api: [ERROR] Error waiting on exit barrier. Elapsed: 96.19062662124634 seconds
benchmark/test_packing.sh.two.bak-222: line 139: 406249 Killed                  torchrun $DISTRIBUTED_ARGS pretrain_gpt.py $GPT_ARGS $DATA_ARGS $OUTPUT_ARGS --distributed-backend nccl
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (352482): No such process
kill: (352488): No such process
kill: (352494): No such process
kill: (352500): No such process
10.64.24.49 kill done.
13b, 4k, gbs=512: dp=2, tp=8, pp=1, mbs=2
LOCAL_IP = 10.64.24.52
DP=2, MP=8, PP=1
[2024-02-12 15:23:18,275] torch.distributed.run: [WARNING] 
[2024-02-12 15:23:18,275] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 15:23:18,275] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 15:23:18,275] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.247 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.898 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (466.40, 538.11)
    train/valid/test-data-iterators-setup ..........: (0.02, 10646.58)
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 14964.3 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.095518E+01 | loss scale: 1.0 | grad norm: 8.033 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14798.69, 14812.91)
    forward-compute ................................: (8081.16, 8248.33)
    backward-compute ...............................: (6472.09, 6645.69)
    batch-generator ................................: (779.38, 855.82)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.04)
    grads-reduce-scatter ...........................: (73.35, 74.22)
    params-all-gather ..............................: (38.75, 39.16)
    optimizer-copy-to-main-grad ....................: (1.43, 1.81)
    optimizer-clip-main-grad .......................: (8.06, 8.15)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.53, 9.76)
    optimizer-copy-main-to-model-params ............: (3.48, 3.60)
    optimizer ......................................: (26.16, 26.67)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 13840.6 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.083624E+01 | loss scale: 1.0 | grad norm: 6.488 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13685.22, 13688.08)
    forward-compute ................................: (7301.92, 7502.16)
    backward-compute ...............................: (6105.94, 6296.77)
    batch-generator ................................: (158.98, 245.52)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.04)
    grads-reduce-scatter ...........................: (72.96, 73.92)
    params-all-gather ..............................: (38.67, 39.23)
    optimizer-copy-to-main-grad ....................: (1.40, 1.83)
    optimizer-clip-main-grad .......................: (5.40, 5.72)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.22, 9.38)
    optimizer-copy-main-to-model-params ............: (3.47, 3.60)
    optimizer ......................................: (23.31, 23.64)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 14069.7 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.060973E+01 | loss scale: 1.0 | grad norm: 2.185 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13917.20, 13923.42)
    forward-compute ................................: (7229.51, 7318.41)
    backward-compute ...............................: (6493.17, 6602.82)
    batch-generator ................................: (157.02, 240.03)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.04)
    grads-reduce-scatter ...........................: (72.88, 73.74)
    params-all-gather ..............................: (38.69, 39.14)
    optimizer-copy-to-main-grad ....................: (1.38, 1.82)
    optimizer-clip-main-grad .......................: (5.41, 5.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.22, 9.38)
    optimizer-copy-main-to-model-params ............: (3.47, 3.60)
    optimizer ......................................: (23.12, 23.67)
Mon Feb 12 15:33:13 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   37C    P0             385W / 700W |  52228MiB / 81559MiB |     18%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   42C    P0             366W / 700W |  52208MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   43C    P0             325W / 700W |  52508MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0             375W / 700W |  52462MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   36C    P0             311W / 700W |  52268MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   42C    P0             331W / 700W |  52394MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   41C    P0             319W / 700W |  52854MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   36C    P0             281W / 700W |  52258MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 13419.1 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.058761E+01 | loss scale: 1.0 | grad norm: 1.501 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13170.02, 13184.79)
    forward-compute ................................: (6840.18, 6933.94)
    backward-compute ...............................: (6145.37, 6248.07)
    batch-generator ................................: (157.81, 242.14)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.04)
    grads-reduce-scatter ...........................: (72.74, 73.87)
    params-all-gather ..............................: (38.76, 39.09)
    optimizer-copy-to-main-grad ....................: (1.40, 1.87)
    optimizer-clip-main-grad .......................: (5.28, 5.50)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.21, 9.36)
    optimizer-copy-main-to-model-params ............: (3.47, 3.61)
    optimizer ......................................: (22.98, 23.36)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 14735.7 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.048625E+01 | loss scale: 1.0 | grad norm: 2.280 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14581.12, 14591.01)
    forward-compute ................................: (7854.19, 7951.11)
    backward-compute ...............................: (6550.04, 6659.93)
    batch-generator ................................: (155.93, 195.32)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (72.81, 73.74)
    params-all-gather ..............................: (38.75, 39.09)
    optimizer-copy-to-main-grad ....................: (1.41, 1.59)
    optimizer-clip-main-grad .......................: (5.48, 5.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.21, 9.31)
    optimizer-copy-main-to-model-params ............: (3.47, 3.60)
    optimizer ......................................: (23.08, 23.65)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 13600.9 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.050558E+01 | loss scale: 1.0 | grad norm: 2.438 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13445.08, 13454.24)
    forward-compute ................................: (6981.23, 7047.60)
    backward-compute ...............................: (6317.10, 6396.04)
    batch-generator ................................: (156.83, 192.62)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.04)
    grads-reduce-scatter ...........................: (72.99, 73.53)
    params-all-gather ..............................: (38.40, 39.11)
    optimizer-copy-to-main-grad ....................: (1.43, 1.56)
    optimizer-clip-main-grad .......................: (5.67, 5.77)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.22, 9.28)
    optimizer-copy-main-to-model-params ............: (3.47, 3.60)
    optimizer ......................................: (23.18, 23.56)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 13067.7 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.048850E+01 | loss scale: 1.0 | grad norm: 1.528 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (12911.88, 12920.69)
    forward-compute ................................: (6472.17, 6507.38)
    backward-compute ...............................: (6331.01, 6364.71)
    batch-generator ................................: (154.58, 184.77)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (73.05, 74.28)
    params-all-gather ..............................: (38.87, 39.18)
    optimizer-copy-to-main-grad ....................: (1.38, 1.56)
    optimizer-clip-main-grad .......................: (5.03, 5.17)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.22, 9.26)
    optimizer-copy-main-to-model-params ............: (3.47, 3.60)
    optimizer ......................................: (22.84, 23.32)
Mon Feb 12 15:42:26 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   37C    P0             335W / 700W |  52232MiB / 81559MiB |     62%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   42C    P0             349W / 700W |  52210MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   42C    P0             335W / 700W |  52510MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0             340W / 700W |  52464MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   36C    P0             278W / 700W |  52274MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   42C    P0             313W / 700W |  52398MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   41C    P0             307W / 700W |  52858MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   36C    P0             280W / 700W |  52264MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 13865.4 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.045313E+01 | loss scale: 1.0 | grad norm: 1.228 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13620.69, 13629.61)
    forward-compute ................................: (7036.36, 7183.35)
    backward-compute ...............................: (6362.95, 6509.32)
    batch-generator ................................: (151.22, 183.92)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (72.85, 73.68)
    params-all-gather ..............................: (38.57, 39.13)
    optimizer-copy-to-main-grad ....................: (1.36, 1.62)
    optimizer-clip-main-grad .......................: (5.41, 5.62)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.21, 9.28)
    optimizer-copy-main-to-model-params ............: (3.46, 3.60)
    optimizer ......................................: (23.01, 23.42)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 14263.6 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.043868E+01 | loss scale: 1.0 | grad norm: 1.085 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14105.36, 14111.90)
    forward-compute ................................: (7598.34, 7659.85)
    backward-compute ...............................: (6368.72, 6431.90)
    batch-generator ................................: (155.73, 197.05)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (72.57, 73.82)
    params-all-gather ..............................: (38.48, 39.11)
    optimizer-copy-to-main-grad ....................: (1.40, 1.63)
    optimizer-clip-main-grad .......................: (4.85, 4.94)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.22, 9.27)
    optimizer-copy-main-to-model-params ............: (3.47, 3.60)
    optimizer ......................................: (22.44, 22.97)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 14171.3 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.042771E+01 | loss scale: 1.0 | grad norm: 1.135 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14016.55, 14027.40)
    forward-compute ................................: (7532.32, 7618.97)
    backward-compute ...............................: (6325.79, 6408.39)
    batch-generator ................................: (157.33, 193.06)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (72.94, 74.02)
    params-all-gather ..............................: (38.56, 39.23)
    optimizer-copy-to-main-grad ....................: (1.39, 1.55)
    optimizer-clip-main-grad .......................: (4.92, 5.23)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.22, 9.27)
    optimizer-copy-main-to-model-params ............: (3.47, 3.61)
    optimizer ......................................: (22.72, 23.07)
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (353355): No such process
kill: (353361): No such process
kill: (353367): No such process
kill: (353373): No such process
10.64.24.49 kill done.
13b, 4k, gbs=512: dp=2, tp=1, pp=8, mbs=4
LOCAL_IP = 10.64.24.52
DP=2, MP=1, PP=8
[2024-02-12 15:49:26,612] torch.distributed.run: [WARNING] 
[2024-02-12 15:49:26,612] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 15:49:26,612] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 15:49:26,612] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 1573196800
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 1573196800
 > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 1573196800
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 1830763520
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...

Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.622 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.689 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.838 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.832 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.842 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.842 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.844 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.946 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.155 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.208 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.092 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.120 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.310 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.319 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.337 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.304 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (35.41, 470.08)
    train/valid/test-data-iterators-setup ..........: (10087.87, 11746.21)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 9486.0 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.094000E+01 | loss scale: 1.0 | grad norm: 7.917 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 10] (after 10 iterations) memory (MB) | allocated: 18128.990234375 | max allocated: 31334.3671875 | reserved: 44340.0 | max reserved: 44340.0[Rank 12] (after 10 iterations) memory (MB) | allocated: 18128.990234375 | max allocated: 30095.318359375 | reserved: 42120.0 | max reserved: 42120.0

[Rank 14] (after 10 iterations) memory (MB) | allocated: 21076.8193359375 | max allocated: 38673.66796875 | reserved: 43870.0 | max reserved: 43870.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 18129.078125 | max allocated: 37870.51611328125 | reserved: 46884.0 | max reserved: 46884.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (8749.01, 9144.43)
    forward-compute ................................: (1034.29, 3899.20)
    backward-compute ...............................: (2122.30, 4014.82)
    batch-generator ................................: (60.91, 74.92)
    forward-recv ...................................: (87.32, 413.45)
    forward-send ...................................: (4.08, 306.71)
    backward-recv ..................................: (72.72, 681.13)
    backward-send ..................................: (0.91, 91.77)
    forward-send-backward-recv .....................: (3514.89, 4769.78)
    backward-send-forward-recv .....................: (489.42, 1384.10)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 22.49)
    grads-reduce-scatter ...........................: (17.45, 218.91)
    params-all-gather ..............................: (7.42, 9.16)
    optimizer-copy-to-main-grad ....................: (0.20, 0.31)
    optimizer-clip-main-grad .......................: (7.28, 7.73)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.88, 10.51)
    optimizer-copy-main-to-model-params ............: (2.45, 2.91)
    optimizer ......................................: (21.77, 22.32)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 8025.3 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.082189E+01 | loss scale: 1.0 | grad norm: 4.498 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7606.75, 7902.47)
    forward-compute ................................: (887.43, 3420.01)
    backward-compute ...............................: (1913.00, 3728.46)
    batch-generator ................................: (46.72, 56.24)
    forward-recv ...................................: (32.36, 145.18)
    forward-send ...................................: (0.72, 84.92)
    backward-recv ..................................: (67.43, 686.42)
    backward-send ..................................: (0.71, 95.13)
    forward-send-backward-recv .....................: (3344.96, 4252.54)
    backward-send-forward-recv .....................: (367.53, 1073.06)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.41)
    grads-reduce-scatter ...........................: (14.57, 17.65)
    params-all-gather ..............................: (7.48, 9.15)
    optimizer-copy-to-main-grad ....................: (0.20, 0.25)
    optimizer-clip-main-grad .......................: (4.19, 4.71)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.67, 10.29)
    optimizer-copy-main-to-model-params ............: (2.45, 2.89)
    optimizer ......................................: (18.22, 18.69)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 9027.2 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.060354E+01 | loss scale: 1.0 | grad norm: 4.357 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8538.75, 8884.86)
    forward-compute ................................: (1013.68, 4015.15)
    backward-compute ...............................: (2154.17, 3929.92)
    batch-generator ................................: (46.26, 53.14)
    forward-recv ...................................: (28.75, 456.20)
    forward-send ...................................: (0.80, 71.03)
    backward-recv ..................................: (50.64, 696.66)
    backward-send ..................................: (2.17, 88.41)
    forward-send-backward-recv .....................: (3426.58, 4906.86)
    backward-send-forward-recv .....................: (484.59, 1359.78)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.38)
    grads-reduce-scatter ...........................: (14.55, 17.60)
    params-all-gather ..............................: (7.50, 9.14)
    optimizer-copy-to-main-grad ....................: (0.19, 0.24)
    optimizer-clip-main-grad .......................: (4.16, 4.60)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.67, 10.23)
    optimizer-copy-main-to-model-params ............: (2.45, 2.89)
    optimizer ......................................: (17.92, 18.36)
Mon Feb 12 15:55:55 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   33C    P0             251W / 700W |  58990MiB / 81559MiB |     94%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   37C    P0             139W / 700W |  47814MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   38C    P0             189W / 700W |  56436MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   33C    P0             133W / 700W |  43874MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   33C    P0             182W / 700W |  54214MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   38C    P0             142W / 700W |  39300MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   44C    P0             133W / 700W |  46816MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             128W / 700W |  46170MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 8997.0 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.058194E+01 | loss scale: 1.0 | grad norm: 1.869 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8488.54, 8781.01)
    forward-compute ................................: (899.65, 4078.27)
    backward-compute ...............................: (1937.34, 3716.18)
    batch-generator ................................: (45.57, 51.15)
    forward-recv ...................................: (31.56, 509.55)
    forward-send ...................................: (0.79, 537.84)
    backward-recv ..................................: (80.25, 836.44)
    backward-send ..................................: (0.73, 60.46)
    forward-send-backward-recv .....................: (3256.13, 5283.96)
    backward-send-forward-recv .....................: (489.98, 1757.62)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.52)
    grads-reduce-scatter ...........................: (14.88, 17.58)
    params-all-gather ..............................: (7.41, 9.14)
    optimizer-copy-to-main-grad ....................: (0.19, 0.24)
    optimizer-clip-main-grad .......................: (4.15, 4.60)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.67, 10.23)
    optimizer-copy-main-to-model-params ............: (2.45, 2.90)
    optimizer ......................................: (17.90, 18.36)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 9470.1 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.047770E+01 | loss scale: 1.0 | grad norm: 1.678 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8990.81, 9362.02)
    forward-compute ................................: (1047.37, 3904.36)
    backward-compute ...............................: (2213.90, 4005.95)
    batch-generator ................................: (46.26, 51.80)
    forward-recv ...................................: (32.17, 806.86)
    forward-send ...................................: (0.84, 744.98)
    backward-recv ..................................: (63.38, 664.88)
    backward-send ..................................: (0.74, 106.10)
    forward-send-backward-recv .....................: (3420.78, 4584.55)
    backward-send-forward-recv .....................: (601.45, 1729.69)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.26)
    grads-reduce-scatter ...........................: (14.62, 17.47)
    params-all-gather ..............................: (7.51, 9.14)
    optimizer-copy-to-main-grad ....................: (0.19, 0.24)
    optimizer-clip-main-grad .......................: (3.91, 4.31)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.68, 10.23)
    optimizer-copy-main-to-model-params ............: (2.45, 2.90)
    optimizer ......................................: (17.63, 18.08)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 8398.5 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049167E+01 | loss scale: 1.0 | grad norm: 1.761 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7950.24, 8248.73)
    forward-compute ................................: (955.48, 3672.29)
    backward-compute ...............................: (2039.70, 3872.20)
    batch-generator ................................: (45.67, 51.61)
    forward-recv ...................................: (32.26, 152.26)
    forward-send ...................................: (0.65, 72.40)
    backward-recv ..................................: (76.16, 590.51)
    backward-send ..................................: (0.77, 60.71)
    forward-send-backward-recv .....................: (3381.17, 4481.76)
    backward-send-forward-recv .....................: (378.62, 1221.70)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.40)
    grads-reduce-scatter ...........................: (14.69, 17.36)
    params-all-gather ..............................: (7.46, 9.17)
    optimizer-copy-to-main-grad ....................: (0.19, 0.24)
    optimizer-clip-main-grad .......................: (4.16, 4.60)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.67, 10.22)
    optimizer-copy-main-to-model-params ............: (2.45, 2.89)
    optimizer ......................................: (18.12, 18.56)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 8482.5 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047457E+01 | loss scale: 1.0 | grad norm: 1.391 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8076.64, 8349.91)
    forward-compute ................................: (970.13, 3651.11)
    backward-compute ...............................: (2065.29, 3855.47)
    batch-generator ................................: (46.74, 51.19)
    forward-recv ...................................: (28.06, 409.51)
    forward-send ...................................: (0.74, 361.27)
    backward-recv ..................................: (61.14, 527.80)
    backward-send ..................................: (0.78, 48.46)
    forward-send-backward-recv .....................: (3176.79, 4568.14)
    backward-send-forward-recv .....................: (488.22, 1285.95)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.31)
    grads-reduce-scatter ...........................: (14.96, 17.46)
    params-all-gather ..............................: (7.35, 9.13)
    optimizer-copy-to-main-grad ....................: (0.20, 0.24)
    optimizer-clip-main-grad .......................: (4.15, 4.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.67, 10.23)
    optimizer-copy-main-to-model-params ............: (2.45, 2.89)
    optimizer ......................................: (18.04, 18.48)
Mon Feb 12 16:01:51 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   34C    P0             342W / 700W |  58990MiB / 81559MiB |     70%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   39C    P0             277W / 700W |  51016MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   39C    P0             167W / 700W |  56438MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   34C    P0             284W / 700W |  47554MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   34C    P0             401W / 700W |  54216MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   39C    P0             233W / 700W |  42984MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   46C    P0             382W / 700W |  46816MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   40C    P0             386W / 700W |  49314MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 9266.7 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.043727E+01 | loss scale: 1.0 | grad norm: 1.930 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8732.20, 9090.60)
    forward-compute ................................: (995.88, 4144.83)
    backward-compute ...............................: (2110.44, 3917.68)
    batch-generator ................................: (46.83, 51.88)
    forward-recv ...................................: (30.99, 407.41)
    forward-send ...................................: (0.67, 341.64)
    backward-recv ..................................: (75.06, 612.38)
    backward-send ..................................: (0.70, 78.73)
    forward-send-backward-recv .....................: (4159.47, 5090.50)
    backward-send-forward-recv .....................: (505.82, 955.78)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.39)
    grads-reduce-scatter ...........................: (14.86, 17.77)
    params-all-gather ..............................: (7.49, 9.14)
    optimizer-copy-to-main-grad ....................: (0.20, 0.24)
    optimizer-clip-main-grad .......................: (4.15, 4.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.67, 10.23)
    optimizer-copy-main-to-model-params ............: (2.45, 2.89)
    optimizer ......................................: (17.90, 18.34)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 8180.1 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.042518E+01 | loss scale: 1.0 | grad norm: 1.744 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7723.56, 8076.37)
    forward-compute ................................: (984.53, 3456.12)
    backward-compute ...............................: (2093.13, 3882.82)
    batch-generator ................................: (46.64, 51.40)
    forward-recv ...................................: (30.36, 133.86)
    forward-send ...................................: (0.87, 73.99)
    backward-recv ..................................: (80.17, 600.20)
    backward-send ..................................: (0.82, 68.49)
    forward-send-backward-recv .....................: (3167.20, 4150.53)
    backward-send-forward-recv .....................: (265.84, 1077.72)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.19)
    grads-reduce-scatter ...........................: (14.71, 17.77)
    params-all-gather ..............................: (7.42, 9.11)
    optimizer-copy-to-main-grad ....................: (0.20, 0.24)
    optimizer-clip-main-grad .......................: (3.91, 4.31)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.67, 10.22)
    optimizer-copy-main-to-model-params ............: (2.45, 2.88)
    optimizer ......................................: (17.60, 18.03)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 8068.1 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041366E+01 | loss scale: 1.0 | grad norm: 1.033 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7589.32, 7931.59)
    forward-compute ................................: (958.61, 3492.70)
    backward-compute ...............................: (2043.63, 3877.42)
    batch-generator ................................: (46.28, 51.81)
    forward-recv ...................................: (25.84, 118.95)
    forward-send ...................................: (0.73, 63.15)
    backward-recv ..................................: (74.10, 632.65)
    backward-send ..................................: (0.80, 60.99)
    forward-send-backward-recv .....................: (3196.33, 4057.04)
    backward-send-forward-recv .....................: (167.82, 989.99)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 22.44)
    grads-reduce-scatter ...........................: (14.99, 17.45)
    params-all-gather ..............................: (7.47, 9.11)
    optimizer-copy-to-main-grad ....................: (0.19, 0.24)
    optimizer-clip-main-grad .......................: (4.16, 4.60)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.67, 10.24)
    optimizer-copy-main-to-model-params ............: (2.45, 2.89)
    optimizer ......................................: (17.95, 18.94)
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (355627): No such process
kill: (355633): No such process
kill: (355639): No such process
kill: (355645): No such process
10.64.24.49 kill done.
13b, 4k, gbs=512: dp=2, tp=1, pp=8, mbs=2
LOCAL_IP = 10.64.24.52
DP=2, MP=1, PP=8
[2024-02-12 16:06:49,440] torch.distributed.run: [WARNING] 
[2024-02-12 16:06:49,440] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 16:06:49,440] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 16:06:49,440] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 1573196800
 > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 1573196800
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 1573196800
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 1830763520
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304) > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)

Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...

Loading exists cache end, time cost:  4.845 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.852 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.854 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.883 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.915 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.023 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.112 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.383 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.080 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.074 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.087 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.054 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.114 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.166 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.315 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.492 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (34.68, 1923.09)
    train/valid/test-data-iterators-setup ..........: (10209.74, 11320.86)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 11529.8 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.093997E+01 | loss scale: 1.0 | grad norm: 7.916 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 18088.1171875 | max allocated: 33688.19775390625 | reserved: 37798.0 | max reserved: 37798.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 18087.26171875 | max allocated: 27918.98046875 | reserved: 32622.0 | max reserved: 32622.0
[Rank 10] (after 10 iterations) memory (MB) | allocated: 18087.134765625 | max allocated: 30944.5283203125 | reserved: 35632.0 | max reserved: 35632.0
[Rank 14] (after 10 iterations) memory (MB) | allocated: 21034.9638671875 | max allocated: 31942.322265625 | reserved: 32894.0 | max reserved: 32894.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (10934.90, 11208.92)
    forward-compute ................................: (1240.67, 4677.73)
    backward-compute ...............................: (2762.99, 4640.81)
    batch-generator ................................: (102.35, 120.11)
    forward-recv ...................................: (71.95, 343.98)
    forward-send ...................................: (2.88, 276.99)
    backward-recv ..................................: (47.84, 494.35)
    backward-send ..................................: (0.93, 63.94)
    forward-send-backward-recv .....................: (4329.66, 6156.39)
    backward-send-forward-recv .....................: (816.79, 2143.28)
    layernorm-grads-all-reduce .....................: (0.02, 0.05)
    embedding-grads-all-reduce .....................: (0.02, 22.41)
    grads-reduce-scatter ...........................: (17.39, 221.56)
    params-all-gather ..............................: (7.51, 9.14)
    optimizer-copy-to-main-grad ....................: (0.20, 0.30)
    optimizer-clip-main-grad .......................: (8.43, 8.88)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.89, 10.49)
    optimizer-copy-main-to-model-params ............: (2.45, 2.89)
    optimizer ......................................: (23.04, 23.52)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 9472.3 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.082173E+01 | loss scale: 1.0 | grad norm: 4.540 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9163.48, 9380.71)
    forward-compute ................................: (1109.38, 4010.81)
    backward-compute ...............................: (2578.24, 4371.67)
    batch-generator ................................: (89.21, 102.61)
    forward-recv ...................................: (20.07, 77.38)
    forward-send ...................................: (0.43, 42.93)
    backward-recv ..................................: (32.55, 361.56)
    backward-send ..................................: (0.46, 67.90)
    forward-send-backward-recv .....................: (3576.29, 5047.55)
    backward-send-forward-recv .....................: (683.65, 1673.37)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.42)
    grads-reduce-scatter ...........................: (14.80, 17.76)
    params-all-gather ..............................: (7.41, 9.09)
    optimizer-copy-to-main-grad ....................: (0.19, 0.27)
    optimizer-clip-main-grad .......................: (4.15, 4.60)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.66, 10.24)
    optimizer-copy-main-to-model-params ............: (2.45, 2.89)
    optimizer ......................................: (18.51, 18.95)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 11170.2 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.060409E+01 | loss scale: 1.0 | grad norm: 4.822 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10835.41, 11072.21)
    forward-compute ................................: (1217.78, 4668.96)
    backward-compute ...............................: (2743.90, 4561.24)
    batch-generator ................................: (87.59, 103.70)
    forward-recv ...................................: (58.03, 659.20)
    forward-send ...................................: (0.49, 346.37)
    backward-recv ..................................: (35.22, 410.14)
    backward-send ..................................: (0.66, 92.51)
    forward-send-backward-recv .....................: (4079.13, 5972.86)
    backward-send-forward-recv .....................: (805.75, 2499.68)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 22.27)
    grads-reduce-scatter ...........................: (14.76, 17.53)
    params-all-gather ..............................: (7.37, 9.10)
    optimizer-copy-to-main-grad ....................: (0.19, 0.28)
    optimizer-clip-main-grad .......................: (4.15, 4.61)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.66, 10.25)
    optimizer-copy-main-to-model-params ............: (2.45, 2.89)
    optimizer ......................................: (17.95, 18.39)
Mon Feb 12 16:14:36 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   33C    P0             269W / 700W |  40660MiB / 81559MiB |     13%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   37C    P0             213W / 700W |  40958MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   37C    P0             269W / 700W |  38494MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   32C    P0             230W / 700W |  36902MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   33C    P0             218W / 700W |  35482MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   37C    P0             282W / 700W |  37918MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             347W / 700W |  35840MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             251W / 700W |  36764MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 11040.0 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.058204E+01 | loss scale: 1.0 | grad norm: 1.873 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10642.67, 10869.64)
    forward-compute ................................: (1110.24, 4669.39)
    backward-compute ...............................: (2536.52, 4360.32)
    batch-generator ................................: (89.74, 105.36)
    forward-recv ...................................: (21.67, 694.44)
    forward-send ...................................: (0.48, 926.26)
    backward-recv ..................................: (41.27, 334.57)
    backward-send ..................................: (0.40, 44.51)
    forward-send-backward-recv .....................: (4206.34, 5911.47)
    backward-send-forward-recv .....................: (688.90, 2423.67)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.39)
    grads-reduce-scatter ...........................: (14.87, 17.51)
    params-all-gather ..............................: (7.36, 9.12)
    optimizer-copy-to-main-grad ....................: (0.19, 0.27)
    optimizer-clip-main-grad .......................: (4.15, 4.61)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.67, 10.25)
    optimizer-copy-main-to-model-params ............: (2.45, 2.89)
    optimizer ......................................: (17.95, 18.39)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 11605.2 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.047868E+01 | loss scale: 1.0 | grad norm: 1.593 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (11289.62, 11514.13)
    forward-compute ................................: (1245.54, 5624.16)
    backward-compute ...............................: (2835.44, 4672.13)
    batch-generator ................................: (89.64, 103.36)
    forward-recv ...................................: (22.11, 90.26)
    forward-send ...................................: (0.53, 50.86)
    backward-recv ..................................: (37.12, 315.46)
    backward-send ..................................: (0.52, 61.82)
    forward-send-backward-recv .....................: (4405.81, 6742.56)
    backward-send-forward-recv .....................: (808.88, 2568.65)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.22)
    grads-reduce-scatter ...........................: (14.78, 17.68)
    params-all-gather ..............................: (7.38, 9.14)
    optimizer-copy-to-main-grad ....................: (0.20, 0.27)
    optimizer-clip-main-grad .......................: (3.92, 4.32)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.66, 10.26)
    optimizer-copy-main-to-model-params ............: (2.45, 2.89)
    optimizer ......................................: (17.68, 18.13)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 10877.9 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049707E+01 | loss scale: 1.0 | grad norm: 1.997 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10536.97, 10744.67)
    forward-compute ................................: (1178.27, 4533.16)
    backward-compute ...............................: (2704.03, 4509.61)
    batch-generator ................................: (89.36, 106.66)
    forward-recv ...................................: (21.97, 77.26)
    forward-send ...................................: (0.43, 47.41)
    backward-recv ..................................: (38.89, 357.53)
    backward-send ..................................: (0.98, 63.17)
    forward-send-backward-recv .....................: (4117.10, 6201.43)
    backward-send-forward-recv .....................: (724.81, 2090.94)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.32)
    grads-reduce-scatter ...........................: (14.57, 17.53)
    params-all-gather ..............................: (7.34, 9.15)
    optimizer-copy-to-main-grad ....................: (0.19, 0.27)
    optimizer-clip-main-grad .......................: (4.17, 4.62)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.66, 10.28)
    optimizer-copy-main-to-model-params ............: (2.45, 3.17)
    optimizer ......................................: (18.02, 18.73)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 9808.0 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.048234E+01 | loss scale: 1.0 | grad norm: 1.376 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9487.04, 9712.12)
    forward-compute ................................: (1181.25, 4185.06)
    backward-compute ...............................: (2708.32, 4487.59)
    batch-generator ................................: (87.94, 106.22)
    forward-recv ...................................: (17.32, 80.50)
    forward-send ...................................: (0.45, 37.81)
    backward-recv ..................................: (37.33, 386.85)
    backward-send ..................................: (0.43, 37.49)
    forward-send-backward-recv .....................: (3730.94, 5146.83)
    backward-send-forward-recv .....................: (725.10, 1717.73)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.21)
    grads-reduce-scatter ...........................: (14.89, 17.60)
    params-all-gather ..............................: (7.31, 9.12)
    optimizer-copy-to-main-grad ....................: (0.19, 0.26)
    optimizer-clip-main-grad .......................: (4.16, 4.61)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.66, 10.27)
    optimizer-copy-main-to-model-params ............: (2.45, 2.89)
    optimizer ......................................: (17.97, 18.41)
Mon Feb 12 16:21:40 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   35C    P0             252W / 700W |  40678MiB / 81559MiB |     59%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   39C    P0             360W / 700W |  40958MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   39C    P0             332W / 700W |  38512MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   34C    P0             391W / 700W |  37600MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   34C    P0             228W / 700W |  35482MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   39C    P0             353W / 700W |  37918MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   45C    P0             405W / 700W |  35840MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             405W / 700W |  36764MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 10059.8 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044472E+01 | loss scale: 1.0 | grad norm: 1.157 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9646.10, 9888.80)
    forward-compute ................................: (1196.12, 4322.92)
    backward-compute ...............................: (2731.88, 4558.18)
    batch-generator ................................: (89.53, 107.14)
    forward-recv ...................................: (18.88, 87.90)
    forward-send ...................................: (0.44, 44.50)
    backward-recv ..................................: (45.27, 317.19)
    backward-send ..................................: (0.49, 44.50)
    forward-send-backward-recv .....................: (3722.33, 5274.68)
    backward-send-forward-recv .....................: (740.49, 1690.32)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.40)
    grads-reduce-scatter ...........................: (14.72, 17.34)
    params-all-gather ..............................: (7.33, 9.13)
    optimizer-copy-to-main-grad ....................: (0.19, 0.27)
    optimizer-clip-main-grad .......................: (3.92, 4.33)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.67, 10.26)
    optimizer-copy-main-to-model-params ............: (2.45, 3.51)
    optimizer ......................................: (17.71, 18.77)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 9973.6 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.042797E+01 | loss scale: 1.0 | grad norm: 1.070 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9616.07, 9840.79)
    forward-compute ................................: (1197.57, 4170.96)
    backward-compute ...............................: (2746.23, 4523.59)
    batch-generator ................................: (89.94, 106.40)
    forward-recv ...................................: (18.84, 93.61)
    forward-send ...................................: (0.56, 50.95)
    backward-recv ..................................: (37.37, 370.57)
    backward-send ..................................: (0.60, 43.73)
    forward-send-backward-recv .....................: (3803.05, 5231.89)
    backward-send-forward-recv .....................: (756.39, 1704.70)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.32)
    grads-reduce-scatter ...........................: (14.62, 17.69)
    params-all-gather ..............................: (7.41, 9.08)
    optimizer-copy-to-main-grad ....................: (0.20, 0.27)
    optimizer-clip-main-grad .......................: (3.68, 4.04)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.65, 10.25)
    optimizer-copy-main-to-model-params ............: (2.44, 2.89)
    optimizer ......................................: (17.38, 17.83)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 9828.7 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041347E+01 | loss scale: 1.0 | grad norm: 1.282 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9468.52, 9695.78)
    forward-compute ................................: (1179.14, 4099.39)
    backward-compute ...............................: (2669.91, 4487.90)
    batch-generator ................................: (89.45, 104.99)
    forward-recv ...................................: (16.51, 73.07)
    forward-send ...................................: (0.47, 31.66)
    backward-recv ..................................: (38.26, 405.78)
    backward-send ..................................: (0.59, 57.44)
    forward-send-backward-recv .....................: (3773.15, 5082.86)
    backward-send-forward-recv .....................: (741.25, 1676.64)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.53)
    grads-reduce-scatter ...........................: (14.49, 17.64)
    params-all-gather ..............................: (7.36, 9.11)
    optimizer-copy-to-main-grad ....................: (0.19, 0.27)
    optimizer-clip-main-grad .......................: (3.67, 4.03)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.66, 10.31)
    optimizer-copy-main-to-model-params ............: (2.45, 2.89)
    optimizer ......................................: (17.47, 17.91)
Kill on 10.64.24.49 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.49
kill: (357899): No such process
kill: (357905): No such process
kill: (357911): No such process
kill: (357917): No such process
10.64.24.49 kill done.

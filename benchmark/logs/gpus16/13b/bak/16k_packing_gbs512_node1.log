13b, 16k, gbs=512: dp=16, tp=1, pp=1, mbs=32
LOCAL_IP = 10.64.24.52
DP=16, MP=1, PP=1
[2024-02-09 09:55:53,545] torch.distributed.run: [WARNING] 
[2024-02-09 09:55:53,545] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 09:55:53,545] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 09:55:53,545] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "<string>", line 3, in bias_gelu

def mul(a : float, b : Tensor) -> Tensor:
  return b * a
         ~~~~~ <--- HERE
def add(a : float, b : Tensor) -> Tensor:
  return b + a
RuntimeError: CUDA out of memory. Tried to allocate 20.00 GiB. GPU 5 has a total capacty of 79.11 GiB of which 17.48 GiB is free. Process 567124 has 61.62 GiB memory in use. Of the allocated memory 60.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "<string>", line 3, in bias_gelu

def mul(a : float, b : Tensor) -> Tensor:
  return b * a
         ~~~~~ <--- HERE
def add(a : float, b : Tensor) -> Tensor:
  return b + a
RuntimeError: CUDA out of memory. Tried to allocate 20.00 GiB. GPU 7 has a total capacty of 79.11 GiB of which 17.48 GiB is free. Process 567126 has 61.62 GiB memory in use. Of the allocated memory 60.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "<string>", line 3, in bias_gelu

def mul(a : float, b : Tensor) -> Tensor:
  return b * a
         ~~~~~ <--- HERE
def add(a : float, b : Tensor) -> Tensor:
  return b + a
RuntimeError: CUDA out of memory. Tried to allocate 20.00 GiB. GPU 2 has a total capacty of 79.11 GiB of which 17.48 GiB is free. Process 567121 has 61.62 GiB memory in use. Of the allocated memory 60.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "<string>", line 3, in bias_gelu

def mul(a : float, b : Tensor) -> Tensor:
  return b * a
         ~~~~~ <--- HERE
def add(a : float, b : Tensor) -> Tensor:
  return b + a
RuntimeError: CUDA out of memory. Tried to allocate 20.00 GiB. GPU 1 has a total capacty of 79.11 GiB of which 17.48 GiB is free. Process 567120 has 61.62 GiB memory in use. Of the allocated memory 60.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "<string>", line 3, in bias_gelu

def mul(a : float, b : Tensor) -> Tensor:
  return b * a
         ~~~~~ <--- HERE
def add(a : float, b : Tensor) -> Tensor:
  return b + a
RuntimeError: CUDA out of memory. Tried to allocate 20.00 GiB. GPU 3 has a total capacty of 79.11 GiB of which 17.48 GiB is free. Process 567122 has 61.62 GiB memory in use. Of the allocated memory 60.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "<string>", line 3, in bias_gelu

def mul(a : float, b : Tensor) -> Tensor:
  return b * a
         ~~~~~ <--- HERE
def add(a : float, b : Tensor) -> Tensor:
  return b + a
RuntimeError: CUDA out of memory. Tried to allocate 20.00 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.48 GiB is free. Process 567119 has 61.62 GiB memory in use. Of the allocated memory 60.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "<string>", line 3, in bias_gelu

def mul(a : float, b : Tensor) -> Tensor:
  return b * a
         ~~~~~ <--- HERE
def add(a : float, b : Tensor) -> Tensor:
  return b + a
RuntimeError: CUDA out of memory. Tried to allocate 20.00 GiB. GPU 4 has a total capacty of 79.11 GiB of which 17.48 GiB is free. Process 567123 has 61.62 GiB memory in use. Of the allocated memory 60.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "<string>", line 3, in bias_gelu

def mul(a : float, b : Tensor) -> Tensor:
  return b * a
         ~~~~~ <--- HERE
def add(a : float, b : Tensor) -> Tensor:
  return b + a
RuntimeError: CUDA out of memory. Tried to allocate 20.00 GiB. GPU 6 has a total capacty of 79.11 GiB of which 17.48 GiB is free. Process 567125 has 61.62 GiB memory in use. Of the allocated memory 60.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

[2024-02-09 09:56:20,592] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 214937) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_09:56:20
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 214938)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_09:56:20
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 214939)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_09:56:20
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 214940)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_09:56:20
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 214941)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_09:56:20
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 214942)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_09:56:20
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 214943)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_09:56:20
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 214944)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_09:56:20
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 214937)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=16, tp=1, pp=1, mbs=16
LOCAL_IP = 10.64.24.52
DP=16, MP=1, PP=1
[2024-02-09 09:59:43,335] torch.distributed.run: [WARNING] 
[2024-02-09 09:59:43,335] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 09:59:43,335] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 09:59:43,335] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 5 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 571336 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 6 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 571337 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
Traceback (most recent call last):
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 4 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 571335 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 7 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 571338 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 3 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 571334 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 2 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 571333 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 571331 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 1 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 571332 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

[2024-02-09 10:00:08,379] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 215325) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_10:00:08
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 215326)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_10:00:08
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 215327)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_10:00:08
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 215328)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_10:00:08
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 215329)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_10:00:08
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 215330)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_10:00:08
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 215331)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_10:00:08
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 215332)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_10:00:08
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 215325)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=16, tp=1, pp=1, mbs=8
LOCAL_IP = 10.64.24.52
DP=16, MP=1, PP=1
[2024-02-09 10:03:31,105] torch.distributed.run: [WARNING] 
[2024-02-09 10:03:31,105] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 10:03:31,105] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 10:03:31,105] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 5 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 575322 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
    output = bias_gelu(bias, input)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 6 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 575323 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 7 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 575324 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 4 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 575321 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 0 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 575317 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    output = bias_gelu(bias, input)
RuntimeError    : pretrain(train_dataset_provider,The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 2 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 575319 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    
set_jit_fusion_options()

  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 1 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 575318 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 3 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 575320 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


[2024-02-09 10:03:57,144] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 215713) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_10:03:57
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 215714)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_10:03:57
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 215715)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_10:03:57
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 215716)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_10:03:57
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 215717)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_10:03:57
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 215718)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_10:03:57
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 215719)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_10:03:57
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 215720)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_10:03:57
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 215713)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=16, tp=1, pp=1, mbs=4
LOCAL_IP = 10.64.24.52
DP=16, MP=1, PP=1
[2024-02-09 10:07:19,871] torch.distributed.run: [WARNING] 
[2024-02-09 10:07:19,871] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 10:07:19,871] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 10:07:19,871] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 6 has a total capacty of 79.11 GiB of which 2.17 GiB is free. Process 578881 has 76.93 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 64.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 4 has a total capacty of 79.11 GiB of which 1.97 GiB is free. Process 578879 has 77.12 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 265.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 1 has a total capacty of 79.11 GiB of which 2.13 GiB is free. Process 578876 has 76.97 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 109.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 7 has a total capacty of 79.11 GiB of which 2.04 GiB is free. Process 578882 has 77.06 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 197.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 0 has a total capacty of 79.11 GiB of which 2.00 GiB is free. Process 578875 has 77.10 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 242.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 5 has a total capacty of 79.11 GiB of which 2.02 GiB is free. Process 578880 has 77.08 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 223.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 2 has a total capacty of 79.11 GiB of which 2.00 GiB is free. Process 578877 has 77.10 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 237.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 3 has a total capacty of 79.11 GiB of which 2.00 GiB is free. Process 578878 has 77.10 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 242.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-02-09 10:07:45,911] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 216104 closing signal SIGTERM
[2024-02-09 10:07:45,911] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 216105 closing signal SIGTERM
[2024-02-09 10:07:46,375] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 216101) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_10:07:45
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 216102)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_10:07:45
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 216103)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_10:07:45
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 216106)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_10:07:45
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 216107)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_10:07:45
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 216108)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_10:07:45
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 216101)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=16, tp=1, pp=1, mbs=2
LOCAL_IP = 10.64.24.52
DP=16, MP=1, PP=1
[2024-02-09 10:10:49,068] torch.distributed.run: [WARNING] 
[2024-02-09 10:10:49,068] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 10:10:49,068] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 10:10:49,068] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 3 has a total capacty of 79.11 GiB of which 2.00 GiB is free. Process 583022 has 77.10 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 242.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 2 has a total capacty of 79.11 GiB of which 2.00 GiB is free. Process 583021 has 77.10 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 237.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 0 has a total capacty of 79.11 GiB of which 2.00 GiB is free. Process 583019 has 77.10 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 242.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 7 has a total capacty of 79.11 GiB of which 2.04 GiB is free. Process 583026 has 77.06 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 197.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 1 has a total capacty of 79.11 GiB of which 2.13 GiB is free. Process 583020 has 76.97 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 109.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 4 has a total capacty of 79.11 GiB of which 1.97 GiB is free. Process 583023 has 77.12 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 265.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 5 has a total capacty of 79.11 GiB of which 2.02 GiB is free. Process 583024 has 77.08 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 223.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 6 has a total capacty of 79.11 GiB of which 2.17 GiB is free. Process 583025 has 76.93 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 64.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-02-09 10:11:14,103] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 216489) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_10:11:14
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 216490)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_10:11:14
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 216491)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_10:11:14
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 216492)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_10:11:14
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 216493)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_10:11:14
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 216494)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_10:11:14
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 216495)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_10:11:14
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 216496)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_10:11:14
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 216489)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=16, tp=1, pp=1, mbs=1
LOCAL_IP = 10.64.24.52
DP=16, MP=1, PP=1
[2024-02-09 10:14:36,863] torch.distributed.run: [WARNING] 
[2024-02-09 10:14:36,863] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 10:14:36,863] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 10:14:36,863] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 2 has a total capacty of 79.11 GiB of which 2.00 GiB is free. Process 587231 has 77.10 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 237.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 5 has a total capacty of 79.11 GiB of which 2.02 GiB is free. Process 587234 has 77.08 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 223.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 0 has a total capacty of 79.11 GiB of which 2.00 GiB is free. Process 587229 has 77.10 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 242.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 3 has a total capacty of 79.11 GiB of which 2.00 GiB is free. Process 587232 has 77.10 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 242.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 4 has a total capacty of 79.11 GiB of which 1.97 GiB is free. Process 587233 has 77.12 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 265.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 6 has a total capacty of 79.11 GiB of which 2.17 GiB is free. Process 587235 has 76.93 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 64.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 7 has a total capacty of 79.11 GiB of which 2.04 GiB is free. Process 587236 has 77.06 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 197.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 370, in setup_model_and_optimizer
    optimizer = get_megatron_optimizer(model, no_wd_decay_cond,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/__init__.py", line 128, in get_megatron_optimizer
    return opt_ty(optimizer,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 417, in __init__
    param_buffer = torch.tensor(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.16 GiB. GPU 1 has a total capacty of 79.11 GiB of which 2.13 GiB is free. Process 587230 has 76.97 GiB memory in use. Of the allocated memory 75.25 GiB is allocated by PyTorch, and 109.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-02-09 10:15:01,895] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 216877) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_10:15:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 216878)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_10:15:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 216879)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_10:15:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 216880)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_10:15:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 216881)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_10:15:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 216882)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_10:15:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 216883)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_10:15:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 216884)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_10:15:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 216877)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=8, tp=2, pp=1, mbs=32
LOCAL_IP = 10.64.24.52
DP=8, MP=2, PP=1
[2024-02-09 10:18:24,668] torch.distributed.run: [WARNING] 
[2024-02-09 10:18:24,668] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 10:18:24,668] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 10:18:24,668] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 2 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 590792 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 5 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 590795 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 7 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 590797 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 1 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 590791 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 3 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 590793 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 4 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 590794 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 590790 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 6 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 590796 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

[2024-02-09 10:18:50,709] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 217265) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_10:18:50
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 217266)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_10:18:50
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 217267)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_10:18:50
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 217268)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_10:18:50
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 217269)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_10:18:50
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 217270)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_10:18:50
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 217271)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_10:18:50
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 217272)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_10:18:50
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 217265)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=8, tp=2, pp=1, mbs=16
LOCAL_IP = 10.64.24.52
DP=8, MP=2, PP=1
[2024-02-09 10:22:13,558] torch.distributed.run: [WARNING] 
[2024-02-09 10:22:13,558] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 10:22:13,558] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 10:22:13,558] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 5 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 594787 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 6 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 594788 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    output = bias_gelu(bias, input)
    pretrain(train_dataset_provider,
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 0 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 594782 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 7 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 594789 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 2 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 594784 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 3 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 594785 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 4 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 594786 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 1 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 594783 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


[2024-02-09 10:22:38,591] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 217653) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_10:22:38
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 217654)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_10:22:38
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 217655)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_10:22:38
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 217656)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_10:22:38
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 217657)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_10:22:38
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 217658)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_10:22:38
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 217659)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_10:22:38
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 217660)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_10:22:38
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 217653)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=8, tp=2, pp=1, mbs=8
LOCAL_IP = 10.64.24.52
DP=8, MP=2, PP=1
[2024-02-09 10:26:01,313] torch.distributed.run: [WARNING] 
[2024-02-09 10:26:01,313] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 10:26:01,313] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 10:26:01,313] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...

 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.169 s
Cutting or padding data to max_seq_len + 1 = 16385 begin ...
Loading exists cache end, time cost:  5.399 s
Cutting or padding data to max_seq_len + 1 = 16385 begin ...
Loading exists cache end, time cost:  5.529 s
Cutting or padding data to max_seq_len + 1 = 16385 begin ...
Loading exists cache end, time cost:  5.685 s
Cutting or padding data to max_seq_len + 1 = 16385 begin ...
Cutting or padding data end, time cost:  18.576 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  18.715 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  18.808 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  18.916 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (1128.41, 1246.88)
    train/valid/test-data-iterators-setup ..........: (0.02, 27109.17)
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 165, in pretrain
    iteration = train(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 699, in train
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    train_step(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 416, in train_step
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 165, in pretrain
    losses_reduced = forward_backward_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 372, in forward_backward_no_pipelining
    iteration = train(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 699, in train
    train_step(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 416, in train_step
    losses_reduced = forward_backward_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 372, in forward_backward_no_pipelining
    output_tensor = forward_step(forward_step_func, data_iterator,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 228, in forward_step
    output_tensor = forward_step(forward_step_func, data_iterator,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 228, in forward_step
    output_tensor, loss_func = forward_step_func(data_iterator, model)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 153, in forward_step
    output_tensor, loss_func = forward_step_func(data_iterator, model)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 153, in forward_step
    output_tensor = model(tokens, position_ids, attention_mask,
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    output_tensor = model(tokens, position_ids, attention_mask,
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/distributed.py", line 58, in forward
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/distributed.py", line 58, in forward
    return self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 183, in forward
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 183, in forward
    outputs = self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    outputs = self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 134, in forward
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 134, in forward
    return post_language_model_processing(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 43, in post_language_model_processing
    return post_language_model_processing(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 43, in post_language_model_processing
    loss = tensor_parallel.vocab_parallel_cross_entropy(output.float(), labels)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.31 GiB. GPU 2 has a total capacty of 79.11 GiB of which 4.98 GiB is free. Process 598984 has 74.12 GiB memory in use. Of the allocated memory 69.42 GiB is allocated by PyTorch, and 2.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    loss = tensor_parallel.vocab_parallel_cross_entropy(output.float(), labels)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.31 GiB. GPU 3 has a total capacty of 79.11 GiB of which 4.43 GiB is free. Process 598985 has 74.67 GiB memory in use. Of the allocated memory 69.42 GiB is allocated by PyTorch, and 2.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 165, in pretrain
    iteration = train(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 699, in train
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    train_step(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 416, in train_step
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 165, in pretrain
    losses_reduced = forward_backward_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 372, in forward_backward_no_pipelining
    iteration = train(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 699, in train
    output_tensor = forward_step(forward_step_func, data_iterator,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 228, in forward_step
    train_step(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 416, in train_step
    output_tensor, loss_func = forward_step_func(data_iterator, model)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 153, in forward_step
    losses_reduced = forward_backward_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 372, in forward_backward_no_pipelining
    output_tensor = model(tokens, position_ids, attention_mask,
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    output_tensor = forward_step(forward_step_func, data_iterator,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 228, in forward_step
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/distributed.py", line 58, in forward
    output_tensor, loss_func = forward_step_func(data_iterator, model)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 153, in forward_step
    return self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    output_tensor = model(tokens, position_ids, attention_mask,
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 183, in forward
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/distributed.py", line 58, in forward
    outputs = self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 134, in forward
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 183, in forward
    return post_language_model_processing(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 43, in post_language_model_processing
    outputs = self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 134, in forward
    loss = tensor_parallel.vocab_parallel_cross_entropy(output.float(), labels)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/tensor_parallel/cross_entropy.py", line 143, in vocab_parallel_cross_entropy
    return post_language_model_processing(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 43, in post_language_model_processing
    loss = tensor_parallel.vocab_parallel_cross_entropy(output.float(), labels)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/tensor_parallel/cross_entropy.py", line 143, in vocab_parallel_cross_entropy
    return _VocabParallelCrossEntropy.apply(vocab_parallel_logits, target, label_smoothing)
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/tensor_parallel/cross_entropy.py", line 25, in forward
    return _VocabParallelCrossEntropy.apply(vocab_parallel_logits, target, label_smoothing)
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/tensor_parallel/cross_entropy.py", line 25, in forward
    vocab_parallel_logits = vocab_parallel_logits - logits_max.unsqueeze(dim=-1)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.31 GiB. GPU 0 has a total capacty of 79.11 GiB of which 477.50 MiB is free. Process 598982 has 78.63 GiB memory in use. Of the allocated memory 71.64 GiB is allocated by PyTorch, and 4.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    vocab_parallel_logits = vocab_parallel_logits - logits_max.unsqueeze(dim=-1)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.31 GiB. GPU 1 has a total capacty of 79.11 GiB of which 477.50 MiB is free. Process 598983 has 78.63 GiB memory in use. Of the allocated memory 71.64 GiB is allocated by PyTorch, and 4.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 165, in pretrain
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    iteration = train(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 699, in train
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 165, in pretrain
    train_step(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 416, in train_step
    iteration = train(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 699, in train
    losses_reduced = forward_backward_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 372, in forward_backward_no_pipelining
    train_step(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 416, in train_step
    output_tensor = forward_step(forward_step_func, data_iterator,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 228, in forward_step
    losses_reduced = forward_backward_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 372, in forward_backward_no_pipelining
    output_tensor, loss_func = forward_step_func(data_iterator, model)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 153, in forward_step
    output_tensor = forward_step(forward_step_func, data_iterator,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 228, in forward_step
    output_tensor = model(tokens, position_ids, attention_mask,
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    output_tensor, loss_func = forward_step_func(data_iterator, model)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 153, in forward_step
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/distributed.py", line 58, in forward
    output_tensor = model(tokens, position_ids, attention_mask,
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/distributed.py", line 58, in forward
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 183, in forward
    return self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    outputs = self.module(*inputs, **kwargs)
      File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 183, in forward
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 134, in forward
    outputs = self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return post_language_model_processing(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 43, in post_language_model_processing
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 134, in forward
    loss = tensor_parallel.vocab_parallel_cross_entropy(output.float(), labels)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.31 GiB. GPU 4 has a total capacty of 79.11 GiB of which 6.67 GiB is free. Process 598986 has 72.43 GiB memory in use. Of the allocated memory 63.25 GiB is allocated by PyTorch, and 6.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    return post_language_model_processing(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 43, in post_language_model_processing
    loss = tensor_parallel.vocab_parallel_cross_entropy(output.float(), labels)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.31 GiB. GPU 5 has a total capacty of 79.11 GiB of which 6.67 GiB is free. Process 598987 has 72.43 GiB memory in use. Of the allocated memory 63.25 GiB is allocated by PyTorch, and 6.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 165, in pretrain
    iteration = train(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 699, in train
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    train_step(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 416, in train_step
    losses_reduced = forward_backward_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 372, in forward_backward_no_pipelining
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 165, in pretrain
    output_tensor = forward_step(forward_step_func, data_iterator,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 228, in forward_step
    iteration = train(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 699, in train
    output_tensor, loss_func = forward_step_func(data_iterator, model)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 153, in forward_step
    train_step(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 416, in train_step
    output_tensor = model(tokens, position_ids, attention_mask,
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/distributed.py", line 58, in forward
    losses_reduced = forward_backward_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 372, in forward_backward_no_pipelining
    return self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    output_tensor = forward_step(forward_step_func, data_iterator,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 228, in forward_step
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 183, in forward
    output_tensor, loss_func = forward_step_func(data_iterator, model)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 153, in forward_step
    outputs = self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 134, in forward
    output_tensor = model(tokens, position_ids, attention_mask,
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/distributed.py", line 58, in forward
    return post_language_model_processing(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 43, in post_language_model_processing
    return self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    loss = tensor_parallel.vocab_parallel_cross_entropy(output.float(), labels)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.31 GiB. GPU 7 has a total capacty of 79.11 GiB of which 2.01 GiB is free. Process 598989 has 77.08 GiB memory in use. Of the allocated memory 71.26 GiB is allocated by PyTorch, and 3.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 183, in forward
    outputs = self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 134, in forward
    return post_language_model_processing(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 43, in post_language_model_processing
    loss = tensor_parallel.vocab_parallel_cross_entropy(output.float(), labels)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.31 GiB. GPU 6 has a total capacty of 79.11 GiB of which 3.61 GiB is free. Process 598988 has 75.48 GiB memory in use. Of the allocated memory 71.26 GiB is allocated by PyTorch, and 1.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-02-09 10:26:57,389] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 218041 closing signal SIGTERM
[2024-02-09 10:26:57,390] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 218043 closing signal SIGTERM
[2024-02-09 10:26:57,390] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 218045 closing signal SIGTERM
[2024-02-09 10:26:57,391] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 218047 closing signal SIGTERM
[2024-02-09 10:26:59,334] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 218042) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_10:26:57
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 218044)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_10:26:57
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 218046)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_10:26:57
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 218048)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_10:26:57
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 218042)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=8, tp=2, pp=1, mbs=4
LOCAL_IP = 10.64.24.52
DP=8, MP=2, PP=1
[2024-02-09 10:29:42,000] torch.distributed.run: [WARNING] 
[2024-02-09 10:29:42,000] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 10:29:42,000] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 10:29:42,000] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...

> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.354 s
Cutting or padding data to max_seq_len + 1 = 16385 begin ...
Loading exists cache end, time cost:  5.364 s
Cutting or padding data to max_seq_len + 1 = 16385 begin ...
Loading exists cache end, time cost:  5.447 s
Cutting or padding data to max_seq_len + 1 = 16385 begin ...
Loading exists cache end, time cost:  5.554 s
Cutting or padding data to max_seq_len + 1 = 16385 begin ...
Cutting or padding data end, time cost:  18.736 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  18.655 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  18.804 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  18.653 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (1123.07, 1274.59)
    train/valid/test-data-iterators-setup ..........: (0.02, 24994.94)
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 165, in pretrain
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    iteration = train(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 699, in train
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 165, in pretrain
    train_step(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 416, in train_step
    iteration = train(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 699, in train
    losses_reduced = forward_backward_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 372, in forward_backward_no_pipelining
    train_step(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 416, in train_step
    output_tensor = forward_step(forward_step_func, data_iterator,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 228, in forward_step
    losses_reduced = forward_backward_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 372, in forward_backward_no_pipelining
    output_tensor, loss_func = forward_step_func(data_iterator, model)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 153, in forward_step
    output_tensor = forward_step(forward_step_func, data_iterator,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 228, in forward_step
    output_tensor = model(tokens, position_ids, attention_mask,
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/distributed.py", line 58, in forward
    output_tensor, loss_func = forward_step_func(data_iterator, model)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 153, in forward_step
    return self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    output_tensor = model(tokens, position_ids, attention_mask,
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 183, in forward
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/distributed.py", line 58, in forward
    outputs = self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 134, in forward
    return self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 183, in forward
    return post_language_model_processing(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 43, in post_language_model_processing
    outputs = self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    loss = tensor_parallel.vocab_parallel_cross_entropy(output.float(), labels)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/tensor_parallel/cross_entropy.py", line 143, in vocab_parallel_cross_entropy
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 134, in forward
    return _VocabParallelCrossEntropy.apply(vocab_parallel_logits, target, label_smoothing)
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/tensor_parallel/cross_entropy.py", line 25, in forward
    return post_language_model_processing(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 43, in post_language_model_processing
    vocab_parallel_logits = vocab_parallel_logits - logits_max.unsqueeze(dim=-1)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.16 GiB. GPU 5 has a total capacty of 79.11 GiB of which 1.89 GiB is free. Process 603974 has 77.21 GiB memory in use. Of the allocated memory 70.95 GiB is allocated by PyTorch, and 3.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    loss = tensor_parallel.vocab_parallel_cross_entropy(output.float(), labels)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/tensor_parallel/cross_entropy.py", line 143, in vocab_parallel_cross_entropy
    return _VocabParallelCrossEntropy.apply(vocab_parallel_logits, target, label_smoothing)
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/tensor_parallel/cross_entropy.py", line 25, in forward
    vocab_parallel_logits = vocab_parallel_logits - logits_max.unsqueeze(dim=-1)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.16 GiB. GPU 4 has a total capacty of 79.11 GiB of which 1.87 GiB is free. Process 603973 has 77.23 GiB memory in use. Of the allocated memory 70.95 GiB is allocated by PyTorch, and 3.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-02-09 10:30:43,075] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 219237 closing signal SIGTERM
[2024-02-09 10:30:43,076] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 219238 closing signal SIGTERM
[2024-02-09 10:30:43,076] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 219239 closing signal SIGTERM
[2024-02-09 10:30:43,078] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 219240 closing signal SIGTERM
[2024-02-09 10:30:43,078] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 219241 closing signal SIGTERM
[2024-02-09 10:30:43,079] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 219243 closing signal SIGTERM
[2024-02-09 10:30:43,079] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 219244 closing signal SIGTERM
[2024-02-09 10:30:44,102] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 5 (pid: 219242) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_10:30:43
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 219242)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=8, tp=2, pp=1, mbs=2
LOCAL_IP = 10.64.24.52
DP=8, MP=2, PP=1
[2024-02-09 10:32:56,720] torch.distributed.run: [WARNING] 
[2024-02-09 10:32:56,720] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 10:32:56,720] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 10:32:56,720] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.562 s
Cutting or padding data to max_seq_len + 1 = 16385 begin ...
Loading exists cache end, time cost:  5.601 s
Cutting or padding data to max_seq_len + 1 = 16385 begin ...
Loading exists cache end, time cost:  5.605 s
Cutting or padding data to max_seq_len + 1 = 16385 begin ...
Loading exists cache end, time cost:  5.676 s
Cutting or padding data to max_seq_len + 1 = 16385 begin ...
Cutting or padding data end, time cost:  18.753 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  18.794 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  18.822 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  19.042 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (1117.75, 1264.27)
    train/valid/test-data-iterators-setup ..........: (0.02, 25208.79)
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 165, in pretrain
    iteration = train(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 699, in train
    train_step(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 416, in train_step
    losses_reduced = forward_backward_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 372, in forward_backward_no_pipelining
    output_tensor = forward_step(forward_step_func, data_iterator,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 228, in forward_step
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    output_tensor, loss_func = forward_step_func(data_iterator, model)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 153, in forward_step
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 165, in pretrain
    output_tensor = model(tokens, position_ids, attention_mask,
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    iteration = train(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 699, in train
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/distributed.py", line 58, in forward
    train_step(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 416, in train_step
    return self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 183, in forward
    losses_reduced = forward_backward_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 372, in forward_backward_no_pipelining
    outputs = self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    output_tensor = forward_step(forward_step_func, data_iterator,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 228, in forward_step
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 134, in forward
    output_tensor, loss_func = forward_step_func(data_iterator, model)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 153, in forward_step
    return post_language_model_processing(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 43, in post_language_model_processing
    output_tensor = model(tokens, position_ids, attention_mask,
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    loss = tensor_parallel.vocab_parallel_cross_entropy(output.float(), labels)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 763.50 MiB is free. Process 608188 has 78.35 GiB memory in use. Of the allocated memory 70.70 GiB is allocated by PyTorch, and 4.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/distributed.py", line 58, in forward
    return self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 183, in forward
    outputs = self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 134, in forward
    return post_language_model_processing(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 43, in post_language_model_processing
    loss = tensor_parallel.vocab_parallel_cross_entropy(output.float(), labels)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.08 GiB. GPU 1 has a total capacty of 79.11 GiB of which 831.50 MiB is free. Process 608189 has 78.29 GiB memory in use. Of the allocated memory 70.70 GiB is allocated by PyTorch, and 4.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-02-09 10:34:16,796] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 220521 closing signal SIGTERM
[2024-02-09 10:34:16,797] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 220523 closing signal SIGTERM
[2024-02-09 10:34:16,797] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 220524 closing signal SIGTERM
[2024-02-09 10:34:16,798] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 220525 closing signal SIGTERM
[2024-02-09 10:34:16,799] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 220526 closing signal SIGTERM
[2024-02-09 10:34:16,799] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 220527 closing signal SIGTERM
[2024-02-09 10:34:16,800] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 220528 closing signal SIGTERM
[2024-02-09 10:34:17,866] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 220522) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_10:34:16
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 220522)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=8, tp=2, pp=1, mbs=1
LOCAL_IP = 10.64.24.52
DP=8, MP=2, PP=1
[2024-02-09 10:36:30,443] torch.distributed.run: [WARNING] 
[2024-02-09 10:36:30,443] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 10:36:30,443] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 10:36:30,443] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 10:36:45,467] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 221806 closing signal SIGTERM
[2024-02-09 10:36:45,467] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 221807 closing signal SIGTERM
[2024-02-09 10:36:45,468] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 221808 closing signal SIGTERM
[2024-02-09 10:36:45,469] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 221810 closing signal SIGTERM
[2024-02-09 10:36:45,469] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 221811 closing signal SIGTERM
[2024-02-09 10:36:46,061] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 221805) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_10:36:45
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 221809)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_10:36:45
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 221812)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_10:36:45
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 221805)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=8, tp=1, pp=2, mbs=32
LOCAL_IP = 10.64.24.52
DP=8, MP=1, PP=2
[2024-02-09 10:39:09,899] torch.distributed.run: [WARNING] 
[2024-02-09 10:39:09,899] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 10:39:09,899] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 10:39:09,899] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error

SYM206-GPU-A0206-P2-Node52:222040:222182 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<33521> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<33521> failed : Software caused connection abort
[2024-02-09 10:39:24,922] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 222044 closing signal SIGTERM
[2024-02-09 10:39:24,923] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 222045 closing signal SIGTERM
[2024-02-09 10:39:24,923] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 222046 closing signal SIGTERM
[2024-02-09 10:39:24,924] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 222047 closing signal SIGTERM
[2024-02-09 10:39:25,452] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 222040) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_10:39:24
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 222041)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_10:39:24
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 222042)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_10:39:24
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 222043)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_10:39:24
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 222040)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=8, tp=1, pp=2, mbs=16
LOCAL_IP = 10.64.24.52
DP=8, MP=1, PP=2
[2024-02-09 10:42:00,102] torch.distributed.run: [WARNING] 
[2024-02-09 10:42:00,102] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 10:42:00,102] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 10:42:00,102] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 10:42:15,126] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 222291 closing signal SIGTERM
[2024-02-09 10:42:15,127] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 222292 closing signal SIGTERM
[2024-02-09 10:42:15,127] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 222293 closing signal SIGTERM
[2024-02-09 10:42:15,128] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 222294 closing signal SIGTERM
[2024-02-09 10:42:15,128] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 222295 closing signal SIGTERM
[2024-02-09 10:42:15,128] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 222296 closing signal SIGTERM
[2024-02-09 10:42:15,128] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 222298 closing signal SIGTERM
[2024-02-09 10:42:15,771] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 6 (pid: 222297) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_10:42:15
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 222297)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=8, tp=1, pp=2, mbs=8
LOCAL_IP = 10.64.24.52
DP=8, MP=1, PP=2
[2024-02-09 10:44:20,183] torch.distributed.run: [WARNING] 
[2024-02-09 10:44:20,183] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 10:44:20,183] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 10:44:20,183] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 10:44:35,204] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 222526) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_10:44:35
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 222527)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_10:44:35
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 222528)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_10:44:35
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 222529)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_10:44:35
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 222530)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_10:44:35
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 222531)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_10:44:35
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 222532)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_10:44:35
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 222533)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_10:44:35
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 222526)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=8, tp=1, pp=2, mbs=4
LOCAL_IP = 10.64.24.52
DP=8, MP=1, PP=2
[2024-02-09 10:47:57,957] torch.distributed.run: [WARNING] 
[2024-02-09 10:47:57,957] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 10:47:57,957] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 10:47:57,957] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 10:48:12,981] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 222761 closing signal SIGTERM
[2024-02-09 10:48:12,982] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 222762 closing signal SIGTERM
[2024-02-09 10:48:12,982] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 222763 closing signal SIGTERM
[2024-02-09 10:48:12,983] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 222766 closing signal SIGTERM
[2024-02-09 10:48:12,983] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 222767 closing signal SIGTERM
[2024-02-09 10:48:13,575] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 3 (pid: 222764) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_10:48:12
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 222765)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_10:48:12
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 222768)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_10:48:12
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 222764)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=8, tp=1, pp=2, mbs=2
LOCAL_IP = 10.64.24.52
DP=8, MP=1, PP=2
[2024-02-09 10:50:46,215] torch.distributed.run: [WARNING] 
[2024-02-09 10:50:46,215] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 10:50:46,215] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 10:50:46,215] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 10:51:01,239] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 222996 closing signal SIGTERM
[2024-02-09 10:51:01,239] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 222998 closing signal SIGTERM
[2024-02-09 10:51:01,240] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 222999 closing signal SIGTERM
[2024-02-09 10:51:01,240] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 223000 closing signal SIGTERM
[2024-02-09 10:51:01,241] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 223003 closing signal SIGTERM
[2024-02-09 10:51:01,846] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 222997) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_10:51:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 223001)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_10:51:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 223002)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_10:51:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 222997)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=8, tp=1, pp=2, mbs=1
LOCAL_IP = 10.64.24.52
DP=8, MP=1, PP=2
[2024-02-09 10:53:30,798] torch.distributed.run: [WARNING] 
[2024-02-09 10:53:30,798] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 10:53:30,798] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 10:53:30,798] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 10:53:45,825] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 223233 closing signal SIGTERM
[2024-02-09 10:53:45,825] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 223235 closing signal SIGTERM
[2024-02-09 10:53:45,825] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 223238 closing signal SIGTERM
[2024-02-09 10:53:46,354] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 223231) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_10:53:45
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 223232)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_10:53:45
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 223234)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_10:53:45
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 223236)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_10:53:45
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 223237)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_10:53:45
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 223231)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=4, tp=4, pp=1, mbs=32
LOCAL_IP = 10.64.24.52
DP=4, MP=4, PP=1
[2024-02-09 10:56:35,974] torch.distributed.run: [WARNING] 
[2024-02-09 10:56:35,974] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 10:56:35,974] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 10:56:35,974] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 317
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 317
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 317
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 317
[2024-02-09 10:56:50,998] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 223468 closing signal SIGTERM
[2024-02-09 10:56:50,998] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 223469 closing signal SIGTERM
[2024-02-09 10:56:50,999] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 223472 closing signal SIGTERM
[2024-02-09 10:56:50,999] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 223473 closing signal SIGTERM
[2024-02-09 10:56:51,541] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 223466) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_10:56:50
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 223467)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_10:56:50
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 223470)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_10:56:50
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 223471)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_10:56:50
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 223466)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=4, tp=4, pp=1, mbs=16
LOCAL_IP = 10.64.24.52
DP=4, MP=4, PP=1
[2024-02-09 10:59:26,131] torch.distributed.run: [WARNING] 
[2024-02-09 10:59:26,131] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 10:59:26,131] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 10:59:26,131] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 317
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 317
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 317
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 317
[2024-02-09 10:59:41,155] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 224062 closing signal SIGTERM
[2024-02-09 10:59:41,156] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 224063 closing signal SIGTERM
[2024-02-09 10:59:41,156] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 224066 closing signal SIGTERM
[2024-02-09 10:59:41,157] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 224067 closing signal SIGTERM
[2024-02-09 10:59:41,749] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 224060) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_10:59:41
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 224061)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_10:59:41
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 224064)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_10:59:41
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 224065)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_10:59:41
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 224060)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=4, tp=4, pp=1, mbs=8
LOCAL_IP = 10.64.24.52
DP=4, MP=4, PP=1
[2024-02-09 11:02:21,299] torch.distributed.run: [WARNING] 
[2024-02-09 11:02:21,299] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 11:02:21,299] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 11:02:21,299] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 317
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 317
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 317
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 317
[2024-02-09 11:02:36,323] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 224297 closing signal SIGTERM
[2024-02-09 11:02:36,323] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 224298 closing signal SIGTERM
[2024-02-09 11:02:36,324] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 224301 closing signal SIGTERM
[2024-02-09 11:02:36,324] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 224302 closing signal SIGTERM
[2024-02-09 11:02:36,853] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 224295) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_11:02:36
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 224296)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_11:02:36
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 224299)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_11:02:36
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 224300)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_11:02:36
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 224295)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=4, tp=4, pp=1, mbs=4
LOCAL_IP = 10.64.24.52
DP=4, MP=4, PP=1
[2024-02-09 11:05:16,471] torch.distributed.run: [WARNING] 
[2024-02-09 11:05:16,471] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 11:05:16,471] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 11:05:16,471] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 317
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 317
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 317
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 317
[2024-02-09 11:05:31,494] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 224532 closing signal SIGTERM
[2024-02-09 11:05:31,495] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 224533 closing signal SIGTERM
[2024-02-09 11:05:31,495] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 224536 closing signal SIGTERM
[2024-02-09 11:05:31,495] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 224537 closing signal SIGTERM
[2024-02-09 11:05:32,074] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 224530) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_11:05:31
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 224531)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_11:05:31
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 224534)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_11:05:31
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 224535)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_11:05:31
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 224530)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=4, tp=4, pp=1, mbs=2
LOCAL_IP = 10.64.24.52
DP=4, MP=4, PP=1
[2024-02-09 11:08:11,623] torch.distributed.run: [WARNING] 
[2024-02-09 11:08:11,623] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 11:08:11,623] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 11:08:11,623] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 317
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 317
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 317
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 317
[2024-02-09 11:08:47,653] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 224767 closing signal SIGTERM
[2024-02-09 11:08:47,654] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 224768 closing signal SIGTERM
[2024-02-09 11:08:47,654] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 224771 closing signal SIGTERM
[2024-02-09 11:08:47,654] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 224772 closing signal SIGTERM
[2024-02-09 11:08:48,196] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 224765) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_11:08:47
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 224766)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_11:08:47
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 224769)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_11:08:47
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 224770)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_11:08:47
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 224765)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=4, tp=4, pp=1, mbs=1
LOCAL_IP = 10.64.24.52
DP=4, MP=4, PP=1
[2024-02-09 11:11:26,814] torch.distributed.run: [WARNING] 
[2024-02-09 11:11:26,814] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 11:11:26,814] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 11:11:26,814] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 317
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 317
[2024-02-09 11:11:41,848] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 225001 closing signal SIGTERM
[2024-02-09 11:11:41,848] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 225002 closing signal SIGTERM
[2024-02-09 11:11:41,849] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 225003 closing signal SIGTERM
[2024-02-09 11:11:41,850] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 225005 closing signal SIGTERM
[2024-02-09 11:11:41,850] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 225006 closing signal SIGTERM
[2024-02-09 11:11:41,851] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 225007 closing signal SIGTERM
[2024-02-09 11:11:42,492] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 225000) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_11:11:41
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 225004)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_11:11:41
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 225000)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=4, tp=1, pp=4, mbs=32
LOCAL_IP = 10.64.24.52
DP=4, MP=1, PP=4
[2024-02-09 11:14:05,137] torch.distributed.run: [WARNING] 
[2024-02-09 11:14:05,137] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 11:14:05,137] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 11:14:05,137] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "<string>", line 3, in bias_gelu

def mul(a : float, b : Tensor) -> Tensor:
  return b * a
         ~~~~~ <--- HERE
def add(a : float, b : Tensor) -> Tensor:
  return b + a
RuntimeError: CUDA out of memory. Tried to allocate 20.00 GiB. GPU 5 has a total capacty of 79.11 GiB of which 17.48 GiB is free. Process 652473 has 61.62 GiB memory in use. Of the allocated memory 60.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "<string>", line 3, in bias_gelu

def mul(a : float, b : Tensor) -> Tensor:
  return b * a
         ~~~~~ <--- HERE
def add(a : float, b : Tensor) -> Tensor:
  return b + a
RuntimeError: CUDA out of memory. Tried to allocate 20.00 GiB. GPU 6 has a total capacty of 79.11 GiB of which 17.48 GiB is free. Process 652474 has 61.62 GiB memory in use. Of the allocated memory 60.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "<string>", line 3, in bias_gelu

def mul(a : float, b : Tensor) -> Tensor:
  return b * a
         ~~~~~ <--- HERE
def add(a : float, b : Tensor) -> Tensor:
  return b + a
RuntimeError: CUDA out of memory. Tried to allocate 20.00 GiB. GPU 1 has a total capacty of 79.11 GiB of which 17.48 GiB is free. Process 652469 has 61.62 GiB memory in use. Of the allocated memory 60.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "<string>", line 3, in bias_gelu

def mul(a : float, b : Tensor) -> Tensor:
  return b * a
         ~~~~~ <--- HERE
def add(a : float, b : Tensor) -> Tensor:
  return b + a
RuntimeError: CUDA out of memory. Tried to allocate 20.00 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.48 GiB is free. Process 652468 has 61.62 GiB memory in use. Of the allocated memory 60.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "<string>", line 3, in bias_gelu

def mul(a : float, b : Tensor) -> Tensor:
  return b * a
         ~~~~~ <--- HERE
def add(a : float, b : Tensor) -> Tensor:
  return b + a
RuntimeError: CUDA out of memory. Tried to allocate 20.00 GiB. GPU 2 has a total capacty of 79.11 GiB of which 17.48 GiB is free. Process 652470 has 61.62 GiB memory in use. Of the allocated memory 60.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "<string>", line 3, in bias_gelu

def mul(a : float, b : Tensor) -> Tensor:
  return b * a
         ~~~~~ <--- HERE
def add(a : float, b : Tensor) -> Tensor:
  return b + a
RuntimeError: CUDA out of memory. Tried to allocate 20.00 GiB. GPU 3 has a total capacty of 79.11 GiB of which 17.48 GiB is free. Process 652471 has 61.62 GiB memory in use. Of the allocated memory 60.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    _warmup_jit_function()
      File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "<string>", line 3, in bias_gelu

def mul(a : float, b : Tensor) -> Tensor:
  return b * a
         ~~~~~ <--- HERE
def add(a : float, b : Tensor) -> Tensor:
  return b + a
RuntimeError: CUDA out of memory. Tried to allocate 20.00 GiB. GPU 4 has a total capacty of 79.11 GiB of which 17.48 GiB is free. Process 652472 has 61.62 GiB memory in use. Of the allocated memory 60.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "<string>", line 3, in bias_gelu

def mul(a : float, b : Tensor) -> Tensor:
  return b * a
         ~~~~~ <--- HERE
def add(a : float, b : Tensor) -> Tensor:
  return b + a
RuntimeError: CUDA out of memory. Tried to allocate 20.00 GiB. GPU 7 has a total capacty of 79.11 GiB of which 17.48 GiB is free. Process 652475 has 61.62 GiB memory in use. Of the allocated memory 60.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

[2024-02-09 11:14:30,174] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 225235) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_11:14:30
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 225236)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_11:14:30
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 225237)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_11:14:30
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 225238)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_11:14:30
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 225239)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_11:14:30
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 225240)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_11:14:30
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 225241)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_11:14:30
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 225242)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_11:14:30
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 225235)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=4, tp=1, pp=4, mbs=16
LOCAL_IP = 10.64.24.52
DP=4, MP=1, PP=4
[2024-02-09 11:17:52,914] torch.distributed.run: [WARNING] 
[2024-02-09 11:17:52,914] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 11:17:52,914] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 11:17:52,914] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 5 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 656643 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 656638 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 3 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 656641 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
        _warmup_jit_function()pretrain(train_dataset_provider,

  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 2 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 656640 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 7 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 656645 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 1 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 656639 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 6 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 656644 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 4 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 656642 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

[2024-02-09 11:18:17,956] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 225611) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_11:18:17
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 225612)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_11:18:17
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 225613)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_11:18:17
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 225614)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_11:18:17
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 225615)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_11:18:17
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 225616)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_11:18:17
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 225617)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_11:18:17
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 225618)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_11:18:17
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 225611)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=4, tp=1, pp=4, mbs=8
LOCAL_IP = 10.64.24.52
DP=4, MP=1, PP=4
[2024-02-09 11:21:40,721] torch.distributed.run: [WARNING] 
[2024-02-09 11:21:40,721] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 11:21:40,721] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 11:21:40,721] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 4 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 660814 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 1 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 660811 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 0 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 660810 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


    output = bias_gelu(bias, input)
RuntimeError    _warmup_jit_function(): 
The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 7 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 660817 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 2 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 660812 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 3 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 660813 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 6 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 660816 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in <forward op>
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 5 has a total capacty of 79.11 GiB of which 2.48 GiB is free. Process 660815 has 76.62 GiB memory in use. Of the allocated memory 75.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


[2024-02-09 11:22:05,759] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 225987) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_11:22:05
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 225988)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_11:22:05
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 225989)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_11:22:05
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 225990)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_11:22:05
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 225991)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_11:22:05
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 225992)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_11:22:05
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 225993)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_11:22:05
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 225994)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_11:22:05
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 225987)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=4, tp=1, pp=4, mbs=4
LOCAL_IP = 10.64.24.52
DP=4, MP=1, PP=4
[2024-02-09 11:25:28,510] torch.distributed.run: [WARNING] 
[2024-02-09 11:25:28,510] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 11:25:28,510] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 11:25:28,510] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 3146393600
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 3403960320
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
[E ProcessGroupNCCL.cpp:467] [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600341 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600496 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600590 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600605 milliseconds before timing out.
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
[E ProcessGroupNCCL.cpp:467] [Rank 12] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600162 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 9] NCCL watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600341 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 9] NCCL watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600341 milliseconds before timing out.
Loading exists cache end, time cost:  4.616 s
Cutting or padding data to max_seq_len + 1 = 16385 begin ...
Loading exists cache end, time cost:  4.793 s
Cutting or padding data to max_seq_len + 1 = 16385 begin ...
Loading exists cache end, time cost:  4.809 sLoading exists cache end, time cost:  4.809 sLoading exists cache end, time cost:  4.809 s


Cutting or padding data to max_seq_len + 1 = 16385 begin ...Cutting or padding data to max_seq_len + 1 = 16385 begin ...
Cutting or padding data to max_seq_len + 1 = 16385 begin ...

[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 10] NCCL watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600605 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 10] NCCL watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600605 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:852] [Rank 12] NCCL watchdog thread terminated with exception: [Rank 12] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600162 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 12] NCCL watchdog thread terminated with exception: [Rank 12] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600162 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 8] NCCL watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600496 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 8] NCCL watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600496 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 11] NCCL watchdog thread terminated with exception: [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600590 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 11] NCCL watchdog thread terminated with exception: [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600590 milliseconds before timing out.
Cutting or padding data end, time cost:  18.556 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  18.593 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  18.580 s
consumed_train_samples = 0, dataloader_type = single
[2024-02-09 11:36:29,187] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 226364 closing signal SIGTERM
[2024-02-09 11:36:29,187] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 226368 closing signal SIGTERM
[2024-02-09 11:36:29,188] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 226369 closing signal SIGTERM
[2024-02-09 11:36:29,188] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 226370 closing signal SIGTERM
[2024-02-09 11:36:59,189] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 226364 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-09 11:36:59,683] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 0 (pid: 226363) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
pretrain_gpt.py FAILED
-------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_11:36:29
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : -6 (pid: 226365)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 226365
[2]:
  time      : 2024-02-09_11:36:29
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : -6 (pid: 226366)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 226366
[3]:
  time      : 2024-02-09_11:36:29
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : -6 (pid: 226367)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 226367
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_11:36:29
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : -6 (pid: 226363)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 226363
=======================================================
13b, 16k, gbs=512: dp=4, tp=1, pp=4, mbs=2
LOCAL_IP = 10.64.24.52
DP=4, MP=1, PP=4
[2024-02-09 11:39:42,342] torch.distributed.run: [WARNING] 
[2024-02-09 11:39:42,342] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 11:39:42,342] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 11:39:42,342] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 3146393600
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 3403960320
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
[E ProcessGroupNCCL.cpp:467] [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600490 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600846 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600875 milliseconds before timing out.
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...

[E ProcessGroupNCCL.cpp:467] [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600939 milliseconds before timing out.
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
[E ProcessGroupNCCL.cpp:467] [Rank 14] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600318 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 8] NCCL watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600490 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 8] NCCL watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600490 milliseconds before timing out.
Loading exists cache end, time cost:  4.768 s
Cutting or padding data to max_seq_len + 1 = 16385 begin ...
Loading exists cache end, time cost:  4.806 s
Cutting or padding data to max_seq_len + 1 = 16385 begin ...
Loading exists cache end, time cost:  4.807 s
Cutting or padding data to max_seq_len + 1 = 16385 begin ...
Loading exists cache end, time cost:  4.807 s
Cutting or padding data to max_seq_len + 1 = 16385 begin ...
Loading exists cache end, time cost:  4.820 s
Cutting or padding data to max_seq_len + 1 = 16385 begin ...
Loading exists cache end, time cost:  4.824 s
Cutting or padding data to max_seq_len + 1 = 16385 begin ...
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 10] NCCL watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600939 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 10] NCCL watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600939 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 9] NCCL watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600875 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 9] NCCL watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600875 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 14] NCCL watchdog thread terminated with exception: [Rank 14] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600318 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 14] NCCL watchdog thread terminated with exception: [Rank 14] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600318 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 11] NCCL watchdog thread terminated with exception: [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600846 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 11] NCCL watchdog thread terminated with exception: [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600846 milliseconds before timing out.
Loading exists cache end, time cost:  4.884 s
Cutting or padding data to max_seq_len + 1 = 16385 begin ...
[2024-02-09 11:50:12,997] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 227337 closing signal SIGTERM
[2024-02-09 11:50:12,997] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 227338 closing signal SIGTERM
[2024-02-09 11:50:12,997] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 227339 closing signal SIGTERM
[2024-02-09 11:50:12,997] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 227340 closing signal SIGTERM
[2024-02-09 11:50:12,997] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 227341 closing signal SIGTERM
[2024-02-09 11:50:12,998] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 227342 closing signal SIGTERM
[2024-02-09 11:50:12,999] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 227343 closing signal SIGTERM
[2024-02-09 11:50:30,927] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 0 (pid: 227336) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
pretrain_gpt.py FAILED
-------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_11:50:12
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : -6 (pid: 227336)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 227336
=======================================================
13b, 16k, gbs=512: dp=4, tp=1, pp=4, mbs=1
LOCAL_IP = 10.64.24.52
DP=4, MP=1, PP=4
[2024-02-09 11:52:43,522] torch.distributed.run: [WARNING] 
[2024-02-09 11:52:43,522] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 11:52:43,522] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 11:52:43,522] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 3146393600
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 3403960320
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
[E ProcessGroupNCCL.cpp:467] [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600368 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600396 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600410 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600728 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 14] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600004 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600102 milliseconds before timing out.
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
[E ProcessGroupNCCL.cpp:467] [Rank 15] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600195 milliseconds before timing out.
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
[E ProcessGroupNCCL.cpp:467] [Rank 12] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600207 milliseconds before timing out.
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 8] NCCL watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600728 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 8] NCCL watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600728 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 10] NCCL watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600368 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 10] NCCL watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600368 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 11] NCCL watchdog thread terminated with exception: [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600410 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 11] NCCL watchdog thread terminated with exception: [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600410 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 13] NCCL watchdog thread terminated with exception: [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600102 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 13] NCCL watchdog thread terminated with exception: [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600102 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 15] NCCL watchdog thread terminated with exception: [Rank 15] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600195 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 15] NCCL watchdog thread terminated with exception: [Rank 15] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600195 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 14] NCCL watchdog thread terminated with exception: [Rank 14] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600004 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 14] NCCL watchdog thread terminated with exception: [Rank 14] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600004 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 9] NCCL watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600396 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 9] NCCL watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600396 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 12] NCCL watchdog thread terminated with exception: [Rank 12] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600207 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 12] NCCL watchdog thread terminated with exception: [Rank 12] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600207 milliseconds before timing out.
[2024-02-09 12:03:09,155] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 227725 closing signal SIGTERM
[2024-02-09 12:03:09,156] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 227726 closing signal SIGTERM
[2024-02-09 12:03:09,156] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 227727 closing signal SIGTERM
[2024-02-09 12:03:09,156] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 227728 closing signal SIGTERM
[2024-02-09 12:03:09,156] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 227729 closing signal SIGTERM
[2024-02-09 12:03:09,156] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 227730 closing signal SIGTERM
[2024-02-09 12:03:09,156] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 227731 closing signal SIGTERM
[2024-02-09 12:03:17,695] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 0 (pid: 227724) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
pretrain_gpt.py FAILED
-------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_12:03:09
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : -6 (pid: 227724)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 227724
=======================================================
13b, 16k, gbs=512: dp=4, tp=2, pp=2, mbs=32
LOCAL_IP = 10.64.24.52
DP=4, MP=2, PP=2
[2024-02-09 12:05:30,315] torch.distributed.run: [WARNING] 
[2024-02-09 12:05:30,315] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 12:05:30,315] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 12:05:30,315] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [14] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f2f7fdb0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f2f7fd6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f2f3279123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f2f3279150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f2f32791816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f2f3274a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f2f3274a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f2f3274a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f2f3274a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f2ee5ad73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f2ee5ada908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f2ee5ae6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f2ee5ae82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f2ee5aea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f2ee5af9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f2f32737c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f2f3274775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f2f32754b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f2f327555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f2f38f1c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f2f38732407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x56194c838e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x56194c82f5eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x56194c8477bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x56194c8230d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x56194c83970c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x56194c8242c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x56194c83970c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x56194c8278a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x56194c83970c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x56194c821e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x56194c83970c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x56194c8230d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x56194c83970c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x56194c8230d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x56194c912e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x56194c912cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x56194c93d7d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x56194c9370bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x56194c93d525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x56194c93ca08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x56194c93c653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x56194c92f41e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x56194c905cad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f2f81337d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f2f81337e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x56194c905ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [12] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7ff5277b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7ff52776b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7ff4e3f9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7ff4e3f9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7ff4e3f91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7ff4e3f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7ff4e3f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7ff4e3f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7ff4e3f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7ff4972d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7ff4972da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7ff4972e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7ff4972e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7ff4972ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7ff4972f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7ff4e3f37c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7ff4e3f4775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7ff4e3f54b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7ff4e3f555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7ff4ea71c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7ff4e9f32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x56285e4ede0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x56285e4e45eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x56285e4fc7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x56285e4d80d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x56285e4ee70c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x56285e4d92c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x56285e4ee70c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x56285e4dc8a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x56285e4ee70c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x56285e4d6e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x56285e4ee70c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x56285e4d80d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x56285e4ee70c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x56285e4d80d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x56285e5c7e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x56285e5c7cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x56285e5f27d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x56285e5ec0bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x56285e5f2525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x56285e5f1a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x56285e5f1653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x56285e5e441e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x56285e5bacad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7ff532a1ed90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7ff532a1ee40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x56285e5baba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [15] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fc826fb0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7fc826f6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7fc7e379123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7fc7e379150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7fc7e3791816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fc7e374a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fc7e374a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fc7e374a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fc7e374a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7fc796ad73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7fc796ada908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7fc796ae6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7fc796ae82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7fc796aea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7fc796af9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7fc7e3737c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7fc7e374775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7fc7e3754b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7fc7e37555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7fc7e9f1c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7fc7e9732407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x560da57d2e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x560da57c95eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x560da57e17bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x560da57bd0d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x560da57d370c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x560da57be2c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x560da57d370c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x560da57c18a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x560da57d370c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x560da57bbe0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x560da57d370c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x560da57bd0d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x560da57d370c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x560da57bd0d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x560da58ace56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x560da58accf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x560da58d77d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x560da58d10bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x560da58d7525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x560da58d6a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x560da58d6653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x560da58c941e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x560da589fcad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7fc83225dd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7fc83225de40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x560da589fba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [10] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f6dc31b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f6dc316b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f6d7f99123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f6d7f99150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f6d7f991816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f6d7f94a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f6d7f94a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f6d7f94a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f6d7f94a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f6d32cd73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f6d32cda908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f6d32ce6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f6d32ce82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f6d32cea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f6d32cf9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f6d7f937c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f6d7f94775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f6d7f954b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f6d7f9555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f6d8611c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f6d85932407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55a3395ace0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55a3395a35eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55a3395bb7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x55a3395970d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55a3395ad70c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x55a3395982c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x55a3395ad70c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x55a33959b8a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x55a3395ad70c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x55a339595e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x55a3395ad70c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x55a3395970d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x55a3395ad70c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x55a3395970d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x55a339686e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x55a339686cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x55a3396b17d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x55a3396ab0bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x55a3396b1525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x55a3396b0a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x55a3396b0653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x55a3396a341e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x55a339679cad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f6dce479d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f6dce479e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x55a339679ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [9] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fc92abb0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7fc92ab6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7fc8dd59123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7fc8dd59150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7fc8dd591816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fc8dd54a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fc8dd54a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fc8dd54a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fc8dd54a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7fc8908d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7fc8908da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7fc8908e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7fc8908e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7fc8908ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7fc8908f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7fc8dd537c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7fc8dd54775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7fc8dd554b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7fc8dd5555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7fc8e3d1c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7fc8e3532407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x5625ec192e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x5625ec1895eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x5625ec1a17bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x5625ec17d0d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x5625ec19370c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x5625ec17e2c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x5625ec19370c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x5625ec1818a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x5625ec19370c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x5625ec17be0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x5625ec19370c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x5625ec17d0d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x5625ec19370c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x5625ec17d0d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x5625ec26ce56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x5625ec26ccf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x5625ec2977d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x5625ec2910bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x5625ec297525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x5625ec296a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x5625ec296653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x5625ec28941e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x5625ec25fcad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7fc92c107d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7fc92c107e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x5625ec25fba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [8] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7ff5423b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7ff54236b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7ff4feb9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7ff4feb9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7ff4feb91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7ff4feb4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7ff4feb4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7ff4feb4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7ff4feb4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7ff4b1ed73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7ff4b1eda908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7ff4b1ee6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7ff4b1ee82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7ff4b1eea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7ff4b1ef9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7ff4feb37c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7ff4feb4775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7ff4feb54b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7ff4feb555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7ff50531c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7ff504b32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55c78f57ce0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55c78f5735eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55c78f58b7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x55c78f5670d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55c78f57d70c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x55c78f5682c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x55c78f57d70c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x55c78f56b8a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x55c78f57d70c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x55c78f565e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x55c78f57d70c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x55c78f5670d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x55c78f57d70c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x55c78f5670d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x55c78f656e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x55c78f656cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x55c78f6817d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x55c78f67b0bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x55c78f681525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x55c78f680a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x55c78f680653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x55c78f67341e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x55c78f649cad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7ff54d644d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7ff54d644e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x55c78f649ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
[2024-02-09 12:15:48,956] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 228115 closing signal SIGTERM
[2024-02-09 12:15:48,956] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 228117 closing signal SIGTERM
[2024-02-09 12:15:49,435] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 228112) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_12:15:48
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 228113)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_12:15:48
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 228114)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_12:15:48
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 228116)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_12:15:48
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 228118)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_12:15:48
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 228119)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_12:15:48
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 228112)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=4, tp=2, pp=2, mbs=16
LOCAL_IP = 10.64.24.52
DP=4, MP=2, PP=2
[2024-02-09 12:18:52,128] torch.distributed.run: [WARNING] 
[2024-02-09 12:18:52,128] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 12:18:52,128] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 12:18:52,128] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [15] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fccf91b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7fccf916b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7fccabb9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7fccabb9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7fccabb91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fccabb4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fccabb4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fccabb4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fccabb4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7fcc5eed73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7fcc5eeda908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7fcc5eee6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7fcc5eee82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7fcc5eeea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7fcc5eef9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7fccabb37c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7fccabb4775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7fccabb54b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7fccabb555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7fccb231c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7fccb1b32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x562e05652e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x562e056495eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x562e056617bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x562e0563d0d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x562e0565370c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x562e0563e2c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x562e0565370c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x562e056418a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x562e0565370c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x562e0563be0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x562e0565370c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x562e0563d0d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x562e0565370c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x562e0563d0d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x562e0572ce56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x562e0572ccf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x562e057577d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x562e057510bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x562e05757525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x562e05756a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x562e05756653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x562e0574941e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x562e0571fcad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7fccfa711d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7fccfa711e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x562e0571fba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [11] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f35035b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f350356b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f34b5f9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f34b5f9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f34b5f91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f34b5f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f34b5f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f34b5f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f34b5f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f34692d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f34692da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f34692e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f34692e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f34692ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f34692f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f34b5f37c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f34b5f4775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f34b5f54b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f34b5f555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f34bc71c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f34bbf32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55b199af8e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55b199aef5eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55b199b077bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x55b199ae30d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55b199af970c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x55b199ae42c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x55b199af970c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x55b199ae78a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x55b199af970c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x55b199ae1e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x55b199af970c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x55b199ae30d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x55b199af970c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x55b199ae30d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x55b199bd2e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x55b199bd2cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x55b199bfd7d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x55b199bf70bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x55b199bfd525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x55b199bfca08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x55b199bfc653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x55b199bef41e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x55b199bc5cad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f3504b01d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f3504b01e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x55b199bc5ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [12] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fb9b09b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7fb9b096b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7fb96d19123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7fb96d19150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7fb96d191816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fb96d14a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fb96d14a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fb96d14a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fb96d14a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7fb9204d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7fb9204da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7fb9204e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7fb9204e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7fb9204ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7fb9204f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7fb96d137c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7fb96d14775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7fb96d154b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7fb96d1555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7fb97391c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7fb973132407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55e492197e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55e49218e5eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55e4921a67bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x55e4921820d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55e49219870c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x55e4921832c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x55e49219870c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x55e4921868a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x55e49219870c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x55e492180e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x55e49219870c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x55e4921820d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x55e49219870c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x55e4921820d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x55e492271e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x55e492271cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x55e49229c7d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x55e4922960bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x55e49229c525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x55e49229ba08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x55e49229b653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x55e49228e41e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x55e492264cad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7fb9bbc6fd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7fb9bbc6fe40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x55e492264ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [14] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fcc171b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7fcc1716b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7fcbde79123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7fcbde79150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7fcbde791816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fcbde74a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fcbde74a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fcbde74a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fcbde74a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7fcb91ad73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7fcb91ada908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7fcb91ae6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7fcb91ae82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7fcb91aea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7fcb91af9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7fcbde737c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7fcbde74775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7fcbde754b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7fcbde7555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7fcbe4f1c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7fcbe4732407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x5626cdd81e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x5626cdd785eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x5626cdd907bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x5626cdd6c0d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x5626cdd8270c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x5626cdd6d2c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x5626cdd8270c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x5626cdd708a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x5626cdd8270c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x5626cdd6ae0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x5626cdd8270c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x5626cdd6c0d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x5626cdd8270c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x5626cdd6c0d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x5626cde5be56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x5626cde5bcf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x5626cde867d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x5626cde800bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x5626cde86525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x5626cde85a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x5626cde85653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x5626cde7841e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x5626cde4ecad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7fcc2d1d2d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7fcc2d1d2e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x5626cde4eba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [9] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7efd0bbb0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7efd0bb6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7efcc839123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7efcc839150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7efcc8391816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7efcc834a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7efcc834a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7efcc834a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7efcc834a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7efc7b6d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7efc7b6da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7efc7b6e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7efc7b6e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7efc7b6ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7efc7b6f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7efcc8337c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7efcc834775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7efcc8354b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7efcc83555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7efcceb1c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7efcce332407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x558bd38abe0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x558bd38a25eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x558bd38ba7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x558bd38960d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x558bd38ac70c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x558bd38972c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x558bd38ac70c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x558bd389a8a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x558bd38ac70c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x558bd3894e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x558bd38ac70c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x558bd38960d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x558bd38ac70c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x558bd38960d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x558bd3985e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x558bd3985cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x558bd39b07d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x558bd39aa0bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x558bd39b0525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x558bd39afa08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x558bd39af653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x558bd39a241e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x558bd3978cad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7efd16e9fd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7efd16e9fe40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x558bd3978ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [13] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f55f4db0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f55f4d6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f55a779123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f55a779150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f55a7791816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f55a774a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f55a774a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f55a774a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f55a774a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f555aad73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f555aada908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f555aae6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f555aae82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f555aaea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f555aaf9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f55a7737c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f55a774775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f55a7754b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f55a77555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f55adf1c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f55ad732407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55cd050c6e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55cd050bd5eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55cd050d57bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x55cd050b10d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55cd050c770c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x55cd050b22c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x55cd050c770c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x55cd050b58a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x55cd050c770c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x55cd050afe0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x55cd050c770c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x55cd050b10d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x55cd050c770c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x55cd050b10d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x55cd051a0e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x55cd051a0cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x55cd051cb7d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x55cd051c50bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x55cd051cb525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x55cd051caa08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x55cd051ca653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x55cd051bd41e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x55cd05193cad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f55f631dd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f55f631de40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x55cd05193ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [8] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fea4a9d0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7fea4a98b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7fe9fc59123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7fe9fc59150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7fe9fc591816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fe9fc54a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fe9fc54a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fe9fc54a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fe9fc54a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7fe9af8d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7fe9af8da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7fe9af8e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7fe9af8e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7fe9af8ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7fe9af8f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7fe9fc537c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7fe9fc54775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7fe9fc554b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7fe9fc5555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7fea02d1c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7fea02532407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55bea4cfae0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55bea4cf15eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55bea4d097bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x55bea4ce50d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55bea4cfb70c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x55bea4ce62c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x55bea4cfb70c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x55bea4ce98a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x55bea4cfb70c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x55bea4ce3e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x55bea4cfb70c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x55bea4ce50d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x55bea4cfb70c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x55bea4ce50d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x55bea4dd4e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x55bea4dd4cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x55bea4dff7d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x55bea4df90bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x55bea4dff525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x55bea4dfea08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x55bea4dfe653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x55bea4df141e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x55bea4dc7cad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7fea4b19cd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7fea4b19ce40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x55bea4dc7ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [10] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f65579b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f655796b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f650a39123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f650a39150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f650a391816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f650a34a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f650a34a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f650a34a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f650a34a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f64bd6d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f64bd6da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f64bd6e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f64bd6e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f64bd6ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f64bd6f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f650a337c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f650a34775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f650a354b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f650a3555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f6510b1c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f6510332407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x559da8f30e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x559da8f275eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x559da8f3f7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x559da8f1b0d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x559da8f3170c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x559da8f1c2c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x559da8f3170c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x559da8f1f8a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x559da8f3170c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x559da8f19e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x559da8f3170c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x559da8f1b0d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x559da8f3170c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x559da8f1b0d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x559da900ae56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x559da900acf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x559da90357d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x559da902f0bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x559da9035525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x559da9034a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x559da9034653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x559da902741e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x559da8ffdcad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f6558f1cd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f6558f1ce40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x559da8ffdba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
[2024-02-09 12:29:07,767] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 228384) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_12:29:07
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 228385)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_12:29:07
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 228386)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_12:29:07
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 228387)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_12:29:07
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 228388)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_12:29:07
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 228389)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_12:29:07
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 228390)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_12:29:07
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 228391)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_12:29:07
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 228384)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=4, tp=2, pp=2, mbs=8
LOCAL_IP = 10.64.24.52
DP=4, MP=2, PP=2
[2024-02-09 12:32:30,510] torch.distributed.run: [WARNING] 
[2024-02-09 12:32:30,510] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 12:32:30,510] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 12:32:30,510] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 12:32:45,533] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 228668 closing signal SIGTERM
[2024-02-09 12:32:45,533] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 228669 closing signal SIGTERM
[2024-02-09 12:32:45,534] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 228670 closing signal SIGTERM
[2024-02-09 12:32:45,535] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 228671 closing signal SIGTERM
[2024-02-09 12:32:45,535] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 228672 closing signal SIGTERM
[2024-02-09 12:32:45,536] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 228674 closing signal SIGTERM
[2024-02-09 12:32:46,146] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 5 (pid: 228673) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_12:32:45
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 228675)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_12:32:45
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 228673)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=4, tp=2, pp=2, mbs=4
LOCAL_IP = 10.64.24.52
DP=4, MP=2, PP=2
[2024-02-09 12:35:08,762] torch.distributed.run: [WARNING] 
[2024-02-09 12:35:08,762] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 12:35:08,762] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 12:35:08,762] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 12:35:23,786] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 228904 closing signal SIGTERM
[2024-02-09 12:35:23,786] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 228905 closing signal SIGTERM
[2024-02-09 12:35:23,787] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 228907 closing signal SIGTERM
[2024-02-09 12:35:23,787] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 228908 closing signal SIGTERM
[2024-02-09 12:35:23,787] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 228909 closing signal SIGTERM
[2024-02-09 12:35:23,788] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 228910 closing signal SIGTERM
[2024-02-09 12:35:24,424] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 228903) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_12:35:23
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 228906)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_12:35:23
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 228903)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=4, tp=2, pp=2, mbs=2
LOCAL_IP = 10.64.24.52
DP=4, MP=2, PP=2
[2024-02-09 12:37:42,124] torch.distributed.run: [WARNING] 
[2024-02-09 12:37:42,124] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 12:37:42,124] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 12:37:42,124] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 12:37:57,148] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 229144 closing signal SIGTERM
[2024-02-09 12:37:57,148] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 229145 closing signal SIGTERM
[2024-02-09 12:37:57,149] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 229146 closing signal SIGTERM
[2024-02-09 12:37:57,149] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 229148 closing signal SIGTERM
[2024-02-09 12:37:57,150] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 229150 closing signal SIGTERM
[2024-02-09 12:37:57,150] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 229151 closing signal SIGTERM
[2024-02-09 12:37:57,792] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 3 (pid: 229147) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_12:37:57
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 229149)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_12:37:57
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 229147)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=4, tp=2, pp=2, mbs=1
LOCAL_IP = 10.64.24.52
DP=4, MP=2, PP=2
[2024-02-09 12:40:17,320] torch.distributed.run: [WARNING] 
[2024-02-09 12:40:17,320] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 12:40:17,320] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 12:40:17,320] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 12:40:32,344] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 229379 closing signal SIGTERM
[2024-02-09 12:40:32,344] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 229380 closing signal SIGTERM
[2024-02-09 12:40:32,345] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 229381 closing signal SIGTERM
[2024-02-09 12:40:32,346] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 229383 closing signal SIGTERM
[2024-02-09 12:40:32,346] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 229386 closing signal SIGTERM
[2024-02-09 12:40:32,919] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 3 (pid: 229382) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_12:40:32
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 229384)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_12:40:32
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 229385)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_12:40:32
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 229382)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=8, pp=1, mbs=32
LOCAL_IP = 10.64.24.52
DP=2, MP=8, PP=1
[2024-02-09 12:42:57,461] torch.distributed.run: [WARNING] 
[2024-02-09 12:42:57,461] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 12:42:57,461] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 12:42:57,461] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
[2024-02-09 12:43:12,482] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 229618 closing signal SIGTERM
[2024-02-09 12:43:12,483] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 229619 closing signal SIGTERM
[2024-02-09 12:43:12,483] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 229620 closing signal SIGTERM
[2024-02-09 12:43:12,484] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 229621 closing signal SIGTERM
[2024-02-09 12:43:13,062] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 229614) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_12:43:12
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 229615)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_12:43:12
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 229616)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_12:43:12
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 229617)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_12:43:12
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 229614)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=8, pp=1, mbs=16
LOCAL_IP = 10.64.24.52
DP=2, MP=8, PP=1
[2024-02-09 12:45:52,663] torch.distributed.run: [WARNING] 
[2024-02-09 12:45:52,663] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 12:45:52,663] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 12:45:52,663] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
[2024-02-09 12:46:07,691] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 229853 closing signal SIGTERM
[2024-02-09 12:46:07,691] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 229854 closing signal SIGTERM
[2024-02-09 12:46:07,691] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 229855 closing signal SIGTERM
[2024-02-09 12:46:07,692] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 229856 closing signal SIGTERM
[2024-02-09 12:46:08,247] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 229849) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_12:46:07
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 229850)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_12:46:07
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 229851)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_12:46:07
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 229852)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_12:46:07
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 229849)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=8, pp=1, mbs=8
LOCAL_IP = 10.64.24.52
DP=2, MP=8, PP=1
[2024-02-09 12:48:50,875] torch.distributed.run: [WARNING] 
[2024-02-09 12:48:50,875] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 12:48:50,875] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 12:48:50,875] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
[2024-02-09 12:49:05,908] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 230692 closing signal SIGTERM
[2024-02-09 12:49:05,908] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 230693 closing signal SIGTERM
[2024-02-09 12:49:05,909] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 230694 closing signal SIGTERM
[2024-02-09 12:49:05,909] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 230695 closing signal SIGTERM
[2024-02-09 12:49:06,437] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 230688) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_12:49:05
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 230689)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_12:49:05
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 230690)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_12:49:05
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 230691)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_12:49:05
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 230688)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=8, pp=1, mbs=4
LOCAL_IP = 10.64.24.52
DP=2, MP=8, PP=1
[2024-02-09 12:51:49,135] torch.distributed.run: [WARNING] 
[2024-02-09 12:51:49,135] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 12:51:49,135] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 12:51:49,135] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
[2024-02-09 12:52:04,159] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 230927 closing signal SIGTERM
[2024-02-09 12:52:04,160] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 230928 closing signal SIGTERM
[2024-02-09 12:52:04,160] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 230929 closing signal SIGTERM
[2024-02-09 12:52:04,161] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 230930 closing signal SIGTERM
[2024-02-09 12:52:04,752] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 230923) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_12:52:04
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 230924)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_12:52:04
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 230925)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_12:52:04
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 230926)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_12:52:04
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 230923)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=8, pp=1, mbs=2
LOCAL_IP = 10.64.24.52
DP=2, MP=8, PP=1
[2024-02-09 12:54:38,214] torch.distributed.run: [WARNING] 
[2024-02-09 12:54:38,214] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 12:54:38,214] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 12:54:38,214] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
[2024-02-09 12:54:53,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 231162 closing signal SIGTERM
[2024-02-09 12:54:53,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 231163 closing signal SIGTERM
[2024-02-09 12:54:53,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 231164 closing signal SIGTERM
[2024-02-09 12:54:53,237] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 231165 closing signal SIGTERM
[2024-02-09 12:54:53,816] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 231158) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_12:54:53
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 231159)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_12:54:53
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 231160)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_12:54:53
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 231161)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_12:54:53
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 231158)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=8, pp=1, mbs=1
LOCAL_IP = 10.64.24.52
DP=2, MP=8, PP=1
[2024-02-09 12:57:33,399] torch.distributed.run: [WARNING] 
[2024-02-09 12:57:33,399] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 12:57:33,399] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 12:57:33,399] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 408
[2024-02-09 12:57:48,423] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 231397 closing signal SIGTERM
[2024-02-09 12:57:48,423] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 231398 closing signal SIGTERM
[2024-02-09 12:57:48,424] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 231399 closing signal SIGTERM
[2024-02-09 12:57:48,424] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 231400 closing signal SIGTERM
[2024-02-09 12:57:48,966] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 231393) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_12:57:48
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 231394)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_12:57:48
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 231395)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_12:57:48
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 231396)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_12:57:48
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 231393)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=1, pp=8, mbs=32
LOCAL_IP = 10.64.24.52
DP=2, MP=1, PP=8
[2024-02-09 13:00:28,566] torch.distributed.run: [WARNING] 
[2024-02-09 13:00:28,566] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 13:00:28,566] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 13:00:28,566] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [9] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fd1d15b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7fd1d156b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7fd198b9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7fd198b9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7fd198b91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fd198b4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fd198b4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fd198b4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fd198b4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7fd14bed73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7fd14beda908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7fd14bee6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7fd14bee82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7fd14beea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7fd14bef9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7fd198b37c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7fd198b4775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7fd198b54b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7fd198b555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7fd19f31c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7fd19eb32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x556e811b0e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x556e811a75eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x556e811bf7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x556e8119b0d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x556e811b170c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x556e8119c2c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x556e811b170c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x556e8119f8a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x556e811b170c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x556e81199e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x556e811b170c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x556e8119b0d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x556e811b170c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x556e8119b0d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x556e8128ae56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x556e8128acf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x556e812b57d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x556e812af0bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x556e812b5525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x556e812b4a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x556e812b4653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x556e812a741e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x556e8127dcad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7fd1e75e2d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7fd1e75e2e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x556e8127dba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [14] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7ff4181b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7ff41816b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7ff3d499123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7ff3d499150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7ff3d4991816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7ff3d494a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7ff3d494a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7ff3d494a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7ff3d494a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7ff387cd73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7ff387cda908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7ff387ce6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7ff387ce82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7ff387cea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7ff387cf9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7ff3d4937c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7ff3d494775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7ff3d4954b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7ff3d49555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7ff3db11c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7ff3da932407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x560ba771ae0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x560ba77115eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x560ba77297bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x560ba77050d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x560ba771b70c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x560ba77062c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x560ba771b70c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x560ba77098a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x560ba771b70c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x560ba7703e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x560ba771b70c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x560ba77050d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x560ba771b70c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x560ba77050d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x560ba77f4e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x560ba77f4cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x560ba781f7d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x560ba78190bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x560ba781f525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x560ba781ea08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x560ba781e653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x560ba781141e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x560ba77e7cad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7ff4234aed90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7ff4234aee40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x560ba77e7ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [15] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fa471bb0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7fa471b6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7fa42e39123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7fa42e39150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7fa42e391816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fa42e34a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fa42e34a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fa42e34a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fa42e34a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7fa3e16d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7fa3e16da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7fa3e16e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7fa3e16e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7fa3e16ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7fa3e16f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7fa42e337c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7fa42e34775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7fa42e354b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7fa42e3555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7fa434b1c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7fa434332407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x563ad0670e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x563ad06675eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x563ad067f7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x563ad065b0d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x563ad067170c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x563ad065c2c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x563ad067170c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x563ad065f8a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x563ad067170c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x563ad0659e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x563ad067170c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x563ad065b0d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x563ad067170c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x563ad065b0d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x563ad074ae56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x563ad074acf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x563ad07757d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x563ad076f0bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x563ad0775525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x563ad0774a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x563ad0774653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x563ad076741e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x563ad073dcad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7fa47ceafd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7fa47ceafe40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x563ad073dba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [8] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f08bfbb0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f08bfb6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f087259123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f087259150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f0872591816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f087254a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f087254a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f087254a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f087254a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f08258d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f08258da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f08258e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f08258e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f08258ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f08258f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f0872537c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f087254775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f0872554b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f08725555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f0878d1c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f0878532407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55cdd469fe0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55cdd46965eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55cdd46ae7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x55cdd468a0d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55cdd46a070c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x55cdd468b2c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x55cdd46a070c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x55cdd468e8a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x55cdd46a070c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x55cdd4688e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x55cdd46a070c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x55cdd468a0d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x55cdd46a070c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x55cdd468a0d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x55cdd4779e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x55cdd4779cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x55cdd47a47d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x55cdd479e0bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x55cdd47a4525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x55cdd47a3a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x55cdd47a3653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x55cdd479641e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x55cdd476ccad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f08c10c5d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f08c10c5e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x55cdd476cba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [11] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f46215b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f462156b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f45d3f9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f45d3f9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f45d3f91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f45d3f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f45d3f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f45d3f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f45d3f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f45872d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f45872da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f45872e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f45872e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f45872ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f45872f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f45d3f37c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f45d3f4775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f45d3f54b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f45d3f555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f45da71c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f45d9f32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x556912056e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55691204d5eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x5569120657bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x5569120410d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55691205770c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x5569120422c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x55691205770c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x5569120458a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x55691205770c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x55691203fe0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x55691205770c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x5569120410d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x55691205770c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x5569120410d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x556912130e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x556912130cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x55691215b7d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x5569121550bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x55691215b525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x55691215aa08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x55691215a653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x55691214d41e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x556912123cad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f4622abfd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f4622abfe40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x556912123ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [10] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f016d9b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f016d96b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f012a19123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f012a19150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f012a191816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f012a14a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f012a14a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f012a14a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f012a14a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f00dd4d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f00dd4da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f00dd4e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f00dd4e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f00dd4ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f00dd4f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f012a137c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f012a14775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f012a154b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f012a1555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f013091c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f0130132407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55e1b8e6fe0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55e1b8e665eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55e1b8e7e7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x55e1b8e5a0d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55e1b8e7070c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x55e1b8e5b2c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x55e1b8e7070c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x55e1b8e5e8a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x55e1b8e7070c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x55e1b8e58e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x55e1b8e7070c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x55e1b8e5a0d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x55e1b8e7070c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x55e1b8e5a0d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x55e1b8f49e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x55e1b8f49cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x55e1b8f747d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x55e1b8f6e0bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x55e1b8f74525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x55e1b8f73a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x55e1b8f73653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x55e1b8f6641e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x55e1b8f3ccad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f0178c31d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f0178c31e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x55e1b8f3cba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [12] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f97e0db0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f97e0d6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f979379123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f979379150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f9793791816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f979374a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f979374a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f979374a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f979374a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f9746ad73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f9746ada908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f9746ae6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f9746ae82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f9746aea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f9746af9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f9793737c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f979374775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f9793754b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f97937555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f9799f1c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f9799732407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55d4eb6ade0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55d4eb6a45eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55d4eb6bc7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x55d4eb6980d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55d4eb6ae70c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x55d4eb6992c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x55d4eb6ae70c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x55d4eb69c8a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x55d4eb6ae70c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x55d4eb696e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x55d4eb6ae70c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x55d4eb6980d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x55d4eb6ae70c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x55d4eb6980d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x55d4eb787e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x55d4eb787cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x55d4eb7b27d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x55d4eb7ac0bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x55d4eb7b2525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x55d4eb7b1a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x55d4eb7b1653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x55d4eb7a441e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x55d4eb77acad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f97e22c4d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f97e22c4e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x55d4eb77aba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [13] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f7b975bb449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f7b975761d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f7b4919123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f7b4919150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f7b49191816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f7b4914a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f7b4914a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f7b4914a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f7b4914a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f7afc4d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f7afc4da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f7afc4e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f7afc4e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f7afc4ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f7afc4f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f7b49137c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f7b4914775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f7b49154b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f7b491555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f7b4f91c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f7b4f132407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55e0c57a6e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55e0c579d5eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55e0c57b57bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x55e0c57910d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55e0c57a770c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x55e0c57922c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x55e0c57a770c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x55e0c57958a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x55e0c57a770c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x55e0c578fe0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x55e0c57a770c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x55e0c57910d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x55e0c57a770c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x55e0c57910d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x55e0c5880e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x55e0c5880cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x55e0c58ab7d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x55e0c58a50bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x55e0c58ab525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x55e0c58aaa08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x55e0c58aa653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x55e0c589d41e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x55e0c5873cad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f7b97d87d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f7b97d87e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x55e0c5873ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
[2024-02-09 13:10:44,202] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 231628) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_13:10:44
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 231629)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_13:10:44
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 231630)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_13:10:44
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 231631)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_13:10:44
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 231632)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_13:10:44
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 231633)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_13:10:44
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 231634)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_13:10:44
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 231635)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_13:10:44
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 231628)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=1, pp=8, mbs=16
LOCAL_IP = 10.64.24.52
DP=2, MP=1, PP=8
[2024-02-09 13:14:06,954] torch.distributed.run: [WARNING] 
[2024-02-09 13:14:06,954] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 13:14:06,954] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 13:14:06,954] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 13:14:21,978] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 231906 closing signal SIGTERM
[2024-02-09 13:14:21,978] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 231907 closing signal SIGTERM
[2024-02-09 13:14:21,979] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 231910 closing signal SIGTERM
[2024-02-09 13:14:21,980] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 231913 closing signal SIGTERM
[2024-02-09 13:14:22,536] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 2 (pid: 231908) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_13:14:21
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 231909)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_13:14:21
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 231911)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_13:14:21
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 231912)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_13:14:21
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 231908)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=1, pp=8, mbs=8
LOCAL_IP = 10.64.24.52
DP=2, MP=1, PP=8
[2024-02-09 13:17:05,192] torch.distributed.run: [WARNING] 
[2024-02-09 13:17:05,192] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 13:17:05,192] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 13:17:05,192] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 13:17:20,216] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 232142 closing signal SIGTERM
[2024-02-09 13:17:20,216] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 232144 closing signal SIGTERM
[2024-02-09 13:17:20,217] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 232148 closing signal SIGTERM
[2024-02-09 13:17:20,745] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 232141) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_13:17:20
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 232143)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_13:17:20
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 232145)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_13:17:20
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 232146)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_13:17:20
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 232147)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_13:17:20
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 232141)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=1, pp=8, mbs=4
LOCAL_IP = 10.64.24.52
DP=2, MP=1, PP=8
[2024-02-09 13:20:04,725] torch.distributed.run: [WARNING] 
[2024-02-09 13:20:04,725] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 13:20:04,725] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 13:20:04,725] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 13:20:19,748] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 232376 closing signal SIGTERM
[2024-02-09 13:20:19,748] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 232378 closing signal SIGTERM
[2024-02-09 13:20:19,749] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 232379 closing signal SIGTERM
[2024-02-09 13:20:19,749] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 232381 closing signal SIGTERM
[2024-02-09 13:20:20,291] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 232377) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_13:20:19
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 232380)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_13:20:19
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 232382)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_13:20:19
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 232383)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_13:20:19
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 232377)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=1, pp=8, mbs=2
LOCAL_IP = 10.64.24.52
DP=2, MP=1, PP=8
[2024-02-09 13:22:59,959] torch.distributed.run: [WARNING] 
[2024-02-09 13:22:59,959] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 13:22:59,959] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 13:22:59,959] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [9] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fdbad9b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7fdbad96b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7fdb6039123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7fdb6039150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7fdb60391816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fdb6034a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fdb6034a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fdb6034a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fdb6034a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7fdb136d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7fdb136da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7fdb136e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7fdb136e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7fdb136ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7fdb136f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7fdb60337c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7fdb6034775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7fdb60354b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7fdb603555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7fdb66b1c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7fdb66332407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55d53262de0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55d5326245eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55d53263c7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x55d5326180d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55d53262e70c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x55d5326192c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x55d53262e70c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x55d53261c8a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x55d53262e70c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x55d532616e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x55d53262e70c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x55d5326180d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x55d53262e70c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x55d5326180d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x55d532707e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x55d532707cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x55d5327327d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x55d53272c0bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x55d532732525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x55d532731a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x55d532731653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x55d53272441e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x55d5326facad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7fdbaef24d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7fdbaef24e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x55d5326faba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [12] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f8788fb0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f8788f6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f874579123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f874579150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f8745791816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f874574a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f874574a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f874574a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f874574a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f86f8ad73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f86f8ada908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f86f8ae6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f86f8ae82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f86f8aea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f86f8af9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f8745737c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f874574775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f8745754b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f87457555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f874bf1c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f874b732407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x564760ff1e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x564760fe85eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x5647610007bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x564760fdc0d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x564760ff270c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x564760fdd2c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x564760ff270c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x564760fe08a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x564760ff270c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x564760fdae0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x564760ff270c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x564760fdc0d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x564760ff270c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x564760fdc0d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x5647610cbe56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x5647610cbcf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x5647610f67d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x5647610f00bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x5647610f6525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x5647610f5a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x5647610f5653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x5647610e841e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x5647610becad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f8794280d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f8794280e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x5647610beba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [15] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f4e3f7b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f4e3f76b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f4dfbf9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f4dfbf9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f4dfbf91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f4dfbf4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f4dfbf4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f4dfbf4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f4dfbf4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f4daf2d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f4daf2da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f4daf2e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f4daf2e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f4daf2ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f4daf2f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f4dfbf37c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f4dfbf4775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f4dfbf54b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f4dfbf555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f4e0271c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f4e01f32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55c47cdfde0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55c47cdf45eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55c47ce0c7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x55c47cde80d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55c47cdfe70c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x55c47cde92c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x55c47cdfe70c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x55c47cdec8a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x55c47cdfe70c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x55c47cde6e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x55c47cdfe70c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x55c47cde80d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x55c47cdfe70c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x55c47cde80d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x55c47ced7e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x55c47ced7cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x55c47cf027d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x55c47cefc0bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x55c47cf02525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x55c47cf01a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x55c47cf01653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x55c47cef441e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x55c47cecacad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f4e4aa3ed90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f4e4aa3ee40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x55c47cecaba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [14] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f0f9a1b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f0f9a16b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f0f5699123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f0f5699150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f0f56991816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f0f5694a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f0f5694a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f0f5694a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f0f5694a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f0f09cd73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f0f09cda908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f0f09ce6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f0f09ce82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f0f09cea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f0f09cf9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f0f56937c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f0f5694775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f0f56954b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f0f569555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f0f5d11c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f0f5c932407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x561f7b1a0e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x561f7b1975eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x561f7b1af7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x561f7b18b0d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x561f7b1a170c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x561f7b18c2c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x561f7b1a170c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x561f7b18f8a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x561f7b1a170c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x561f7b189e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x561f7b1a170c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x561f7b18b0d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x561f7b1a170c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x561f7b18b0d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x561f7b27ae56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x561f7b27acf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x561f7b2a57d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x561f7b29f0bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x561f7b2a5525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x561f7b2a4a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x561f7b2a4653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x561f7b29741e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x561f7b26dcad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f0fa545fd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f0fa545fe40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x561f7b26dba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [10] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f06041b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f060416b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f05c099123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f05c099150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f05c0991816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f05c094a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f05c094a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f05c094a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f05c094a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f0573cd73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f0573cda908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f0573ce6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f0573ce82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f0573cea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f0573cf9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f05c0937c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f05c094775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f05c0954b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f05c09555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f05c711c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f05c6932407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x560f9c26ce0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x560f9c2635eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x560f9c27b7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x560f9c2570d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x560f9c26d70c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x560f9c2582c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x560f9c26d70c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x560f9c25b8a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x560f9c26d70c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x560f9c255e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x560f9c26d70c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x560f9c2570d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x560f9c26d70c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x560f9c2570d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x560f9c346e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x560f9c346cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x560f9c3717d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x560f9c36b0bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x560f9c371525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x560f9c370a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x560f9c370653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x560f9c36341e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x560f9c339cad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f060f4aad90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f060f4aae40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x560f9c339ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [8] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f7578bb0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f7578b6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f752b59123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f752b59150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f752b591816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f752b54a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f752b54a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f752b54a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f752b54a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f74de8d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f74de8da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f74de8e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f74de8e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f74de8ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f74de8f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f752b537c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f752b54775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f752b554b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f752b5555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f7531d1c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f7531532407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x556f0d38ce0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x556f0d3835eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x556f0d39b7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x556f0d3770d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x556f0d38d70c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x556f0d3782c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x556f0d38d70c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x556f0d37b8a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x556f0d38d70c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x556f0d375e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x556f0d38d70c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x556f0d3770d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x556f0d38d70c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x556f0d3770d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x556f0d466e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x556f0d466cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x556f0d4917d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x556f0d48b0bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x556f0d491525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x556f0d490a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x556f0d490653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x556f0d48341e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x556f0d459cad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f757a0f6d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f757a0f6e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x556f0d459ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [13] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f3d71fe7449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f3d71fa21d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f3d23b9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f3d23b9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f3d23b91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f3d23b4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f3d23b4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f3d23b4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f3d23b4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f3cd6ed73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f3cd6eda908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f3cd6ee6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f3cd6ee82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f3cd6eea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f3cd6ef9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f3d23b37c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f3d23b4775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f3d23b54b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f3d23b555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f3d2a31c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f3d29b32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55920fec5e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55920febc5eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55920fed47bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x55920feb00d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55920fec670c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x55920feb12c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x55920fec670c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x55920feb48a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x55920fec670c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x55920feaee0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x55920fec670c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x55920feb00d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x55920fec670c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x55920feb00d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x55920ff9fe56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x55920ff9fcf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x55920ffca7d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x55920ffc40bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x55920ffca525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x55920ffc9a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x55920ffc9653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x55920ffbc41e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x55920ff92cad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f3d727b3d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f3d727b3e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x55920ff92ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [11] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f02235b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f022356b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f01dfd9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f01dfd9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f01dfd91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f01dfd4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f01dfd4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f01dfd4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f01dfd4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f01930d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f01930da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f01930e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f01930e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f01930ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f01930f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f01dfd37c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f01dfd4775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f01dfd54b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f01dfd555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f01e651c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f01e5d32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x56542f739e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x56542f7305eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x56542f7487bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x56542f7240d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x56542f73a70c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x56542f7252c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x56542f73a70c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x56542f7288a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x56542f73a70c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x56542f722e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x56542f73a70c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x56542f7240d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x56542f73a70c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x56542f7240d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x56542f813e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x56542f813cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x56542f83e7d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x56542f8380bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x56542f83e525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x56542f83da08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x56542f83d653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x56542f83041e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x56542f806cad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f022e835d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f022e835e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x56542f806ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
[2024-02-09 13:33:15,597] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 232621) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_13:33:15
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 232622)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_13:33:15
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 232623)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_13:33:15
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 232624)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_13:33:15
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 232625)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_13:33:15
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 232626)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_13:33:15
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 232627)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_13:33:15
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 232628)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_13:33:15
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 232621)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=1, pp=8, mbs=1
LOCAL_IP = 10.64.24.52
DP=2, MP=1, PP=8
[2024-02-09 13:36:38,355] torch.distributed.run: [WARNING] 
[2024-02-09 13:36:38,355] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 13:36:38,355] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 13:36:38,355] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 13:36:53,379] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 232901 closing signal SIGTERM
[2024-02-09 13:36:53,379] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 232902 closing signal SIGTERM
[2024-02-09 13:36:53,380] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 232906 closing signal SIGTERM
[2024-02-09 13:36:53,859] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 232899) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_13:36:53
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 232900)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_13:36:53
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 232903)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_13:36:53
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 232904)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_13:36:53
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 232905)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_13:36:53
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 232899)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=4, pp=2, mbs=32
LOCAL_IP = 10.64.24.52
DP=2, MP=4, PP=2
[2024-02-09 13:39:46,548] torch.distributed.run: [WARNING] 
[2024-02-09 13:39:46,548] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 13:39:46,548] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 13:39:46,548] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 13:40:01,572] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 233145 closing signal SIGTERM
[2024-02-09 13:40:01,572] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 233146 closing signal SIGTERM
[2024-02-09 13:40:01,573] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 233147 closing signal SIGTERM
[2024-02-09 13:40:01,574] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 233148 closing signal SIGTERM
[2024-02-09 13:40:01,574] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 233150 closing signal SIGTERM
[2024-02-09 13:40:01,574] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 233151 closing signal SIGTERM
[2024-02-09 13:40:02,202] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 233144) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_13:40:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 233149)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_13:40:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 233144)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=4, pp=2, mbs=16
LOCAL_IP = 10.64.24.52
DP=2, MP=4, PP=2
[2024-02-09 13:42:24,844] torch.distributed.run: [WARNING] 
[2024-02-09 13:42:24,844] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 13:42:24,844] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 13:42:24,844] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [10] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fb8a47b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7fb8a476b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7fb860f9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7fb860f9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7fb860f91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fb860f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fb860f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fb860f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fb860f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7fb8142d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7fb8142da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7fb8142e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7fb8142e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7fb8142ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7fb8142f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7fb860f37c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7fb860f4775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7fb860f54b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7fb860f555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7fb86771c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7fb866f32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55e740522e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55e7405195eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55e7405317bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x55e74050d0d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55e74052370c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x55e74050e2c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x55e74052370c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x55e7405118a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x55e74052370c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x55e74050be0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x55e74052370c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x55e74050d0d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x55e74052370c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x55e74050d0d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x55e7405fce56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x55e7405fccf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x55e7406277d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x55e7406210bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x55e740627525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x55e740626a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x55e740626653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x55e74061941e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x55e7405efcad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7fb8afa8fd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7fb8afa8fe40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x55e7405efba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [14] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f32d5fb0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f32d5f6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f329279123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f329279150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f3292791816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f329274a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f329274a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f329274a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f329274a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f3245ad73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f3245ada908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f3245ae6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f3245ae82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f3245aea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f3245af9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f3292737c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f329274775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f3292754b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f32927555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f3298f1c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f3298732407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x555d9eb24e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x555d9eb1b5eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x555d9eb337bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x555d9eb0f0d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x555d9eb2570c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x555d9eb102c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x555d9eb2570c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x555d9eb138a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x555d9eb2570c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x555d9eb0de0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x555d9eb2570c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x555d9eb0f0d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x555d9eb2570c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x555d9eb0f0d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x555d9ebfee56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x555d9ebfecf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x555d9ec297d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x555d9ec230bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x555d9ec29525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x555d9ec28a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x555d9ec28653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x555d9ec1b41e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x555d9ebf1cad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f32e1273d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f32e1273e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x555d9ebf1ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [12] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fed595b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7fed5956b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7fed0bf9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7fed0bf9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7fed0bf91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fed0bf4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fed0bf4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fed0bf4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fed0bf4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7fecbf2d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7fecbf2da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7fecbf2e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7fecbf2e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7fecbf2ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7fecbf2f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7fed0bf37c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7fed0bf4775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7fed0bf54b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7fed0bf555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7fed1271c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7fed11f32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55a8ecca3e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55a8ecc9a5eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55a8eccb27bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x55a8ecc8e0d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55a8ecca470c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x55a8ecc8f2c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x55a8ecca470c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x55a8ecc928a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x55a8ecca470c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x55a8ecc8ce0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x55a8ecca470c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x55a8ecc8e0d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x55a8ecca470c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x55a8ecc8e0d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x55a8ecd7de56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x55a8ecd7dcf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x55a8ecda87d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x55a8ecda20bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x55a8ecda8525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x55a8ecda7a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x55a8ecda7653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x55a8ecd9a41e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x55a8ecd70cad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7fed5aad6d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7fed5aad6e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x55a8ecd70ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [11] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f7709db0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f7709d6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f76d139123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f76d139150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f76d1391816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f76d134a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f76d134a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f76d134a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f76d134a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f76846d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f76846da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f76846e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f76846e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f76846ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f76846f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f76d1337c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f76d134775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f76d1354b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f76d13555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f76d7b1c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f76d7332407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x5626d1718e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x5626d170f5eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x5626d17277bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x5626d17030d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x5626d171970c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x5626d17042c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x5626d171970c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x5626d17078a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x5626d171970c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x5626d1701e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x5626d171970c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x5626d17030d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x5626d171970c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x5626d17030d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x5626d17f2e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x5626d17f2cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x5626d181d7d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x5626d18170bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x5626d181d525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x5626d181ca08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x5626d181c653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x5626d180f41e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x5626d17e5cad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f771fde7d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f771fde7e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x5626d17e5ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [15] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f7908fb1449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f7908f6c1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f78bab9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f78bab9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f78bab91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f78bab4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f78bab4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f78bab4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f78bab4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f786ded73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f786deda908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f786dee6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f786dee82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f786deea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f786def9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f78bab37c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f78bab4775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f78bab54b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f78bab555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f78c131c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f78c0b32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x559a9e4b9e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x559a9e4b05eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x559a9e4c87bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x559a9e4a40d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x559a9e4ba70c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x559a9e4a52c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x559a9e4ba70c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x559a9e4a88a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x559a9e4ba70c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x559a9e4a2e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x559a9e4ba70c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x559a9e4a40d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x559a9e4ba70c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x559a9e4a40d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x559a9e593e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x559a9e593cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x559a9e5be7d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x559a9e5b80bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x559a9e5be525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x559a9e5bda08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x559a9e5bd653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x559a9e5b041e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x559a9e586cad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f790977dd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f790977de40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x559a9e586ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [8] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f9f3a9b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f9f3a96b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f9eed39123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f9eed39150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f9eed391816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f9eed34a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f9eed34a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f9eed34a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f9eed34a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f9ea06d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f9ea06da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f9ea06e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f9ea06e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f9ea06ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f9ea06f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f9eed337c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f9eed34775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f9eed354b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f9eed3555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f9ef3b1c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f9ef3332407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55c79e52de0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55c79e5245eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55c79e53c7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x55c79e5180d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55c79e52e70c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x55c79e5192c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x55c79e52e70c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x55c79e51c8a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x55c79e52e70c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x55c79e516e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x55c79e52e70c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x55c79e5180d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x55c79e52e70c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x55c79e5180d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x55c79e607e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x55c79e607cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x55c79e6327d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x55c79e62c0bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x55c79e632525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x55c79e631a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x55c79e631653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x55c79e62441e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x55c79e5facad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f9f3bf66d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f9f3bf66e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x55c79e5faba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
[2024-02-09 13:52:40,474] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 233404 closing signal SIGTERM
[2024-02-09 13:52:40,475] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 233408 closing signal SIGTERM
[2024-02-09 13:52:40,953] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 233403) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_13:52:40
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 233405)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_13:52:40
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 233406)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_13:52:40
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 233407)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_13:52:40
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 233409)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_13:52:40
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 233410)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_13:52:40
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 233403)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=4, pp=2, mbs=8
LOCAL_IP = 10.64.24.52
DP=2, MP=4, PP=2
[2024-02-09 13:55:43,642] torch.distributed.run: [WARNING] 
[2024-02-09 13:55:43,642] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 13:55:43,642] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 13:55:43,642] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 13:55:58,666] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 233675 closing signal SIGTERM
[2024-02-09 13:55:58,666] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 233676 closing signal SIGTERM
[2024-02-09 13:55:58,667] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 233677 closing signal SIGTERM
[2024-02-09 13:55:58,668] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 233679 closing signal SIGTERM
[2024-02-09 13:55:58,668] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 233681 closing signal SIGTERM
[2024-02-09 13:55:58,669] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 233682 closing signal SIGTERM
[2024-02-09 13:55:59,342] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 3 (pid: 233678) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_13:55:58
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 233680)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_13:55:58
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 233678)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=4, pp=2, mbs=4
LOCAL_IP = 10.64.24.52
DP=2, MP=4, PP=2
[2024-02-09 13:58:21,951] torch.distributed.run: [WARNING] 
[2024-02-09 13:58:21,951] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 13:58:21,951] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 13:58:21,951] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [14] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f1d573b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f1d5736b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f1d09d9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f1d09d9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f1d09d91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f1d09d4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f1d09d4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f1d09d4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f1d09d4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f1cbd0d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f1cbd0da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f1cbd0e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f1cbd0e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f1cbd0ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f1cbd0f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f1d09d37c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f1d09d4775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f1d09d54b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f1d09d555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f1d1051c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f1d0fd32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55e9c7fbfe0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55e9c7fb65eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55e9c7fce7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x55e9c7faa0d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55e9c7fc070c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x55e9c7fab2c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x55e9c7fc070c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x55e9c7fae8a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x55e9c7fc070c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x55e9c7fa8e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x55e9c7fc070c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x55e9c7faa0d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x55e9c7fc070c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x55e9c7faa0d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x55e9c8099e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x55e9c8099cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x55e9c80c47d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x55e9c80be0bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x55e9c80c4525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x55e9c80c3a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x55e9c80c3653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x55e9c80b641e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x55e9c808ccad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f1d5893fd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f1d5893fe40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x55e9c808cba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [10] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fd0281b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7fd02816b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7fcfdab9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7fcfdab9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7fcfdab91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fcfdab4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fcfdab4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fcfdab4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fcfdab4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7fcf8ded73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7fcf8deda908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7fcf8dee6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7fcf8dee82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7fcf8deea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7fcf8def9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7fcfdab37c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7fcfdab4775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7fcfdab54b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7fcfdab555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7fcfe131c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7fcfe0b32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x561633e87e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x561633e7e5eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x561633e967bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x561633e720d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x561633e8870c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x561633e732c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x561633e8870c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x561633e768a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x561633e8870c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x561633e70e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x561633e8870c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x561633e720d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x561633e8870c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x561633e720d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x561633f61e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x561633f61cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x561633f8c7d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x561633f860bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x561633f8c525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x561633f8ba08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x561633f8b653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x561633f7e41e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x561633f54cad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7fd0296b7d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7fd0296b7e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x561633f54ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [11] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f598fdb0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f598fd6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f594c59123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f594c59150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f594c591816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f594c54a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f594c54a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f594c54a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f594c54a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f58ff8d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f58ff8da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f58ff8e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f58ff8e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f58ff8ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f58ff8f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f594c537c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f594c54775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f594c554b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f594c5555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f5952d1c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f5952532407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55d94ebbce0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55d94ebb35eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55d94ebcb7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x55d94eba70d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55d94ebbd70c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x55d94eba82c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x55d94ebbd70c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x55d94ebab8a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x55d94ebbd70c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x55d94eba5e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x55d94ebbd70c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x55d94eba70d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x55d94ebbd70c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x55d94eba70d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x55d94ec96e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x55d94ec96cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x55d94ecc17d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x55d94ecbb0bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x55d94ecc1525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x55d94ecc0a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x55d94ecc0653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x55d94ecb341e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x55d94ec89cad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f599b097d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f599b097e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x55d94ec89ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [13] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f0fcddb0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f0fcdd6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f0f8079123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f0f8079150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f0f80791816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f0f8074a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f0f8074a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f0f8074a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f0f8074a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f0f33ad73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f0f33ada908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f0f33ae6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f0f33ae82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f0f33aea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f0f33af9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f0f80737c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f0f8074775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f0f80754b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f0f807555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f0f86f1c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f0f86732407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x558e904f2e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x558e904e95eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x558e905017bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x558e904dd0d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x558e904f370c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x558e904de2c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x558e904f370c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x558e904e18a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x558e904f370c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x558e904dbe0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x558e904f370c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x558e904dd0d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x558e904f370c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x558e904dd0d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x558e905cce56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x558e905cccf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x558e905f77d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x558e905f10bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x558e905f7525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x558e905f6a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x558e905f6653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x558e905e941e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x558e905bfcad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f0fcf312d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f0fcf312e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x558e905bfba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [9] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fd3337b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7fd33376b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7fd2eff9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7fd2eff9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7fd2eff91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fd2eff4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fd2eff4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fd2eff4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fd2eff4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7fd2a32d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7fd2a32da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7fd2a32e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7fd2a32e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7fd2a32ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7fd2a32f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7fd2eff37c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7fd2eff4775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7fd2eff54b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7fd2eff555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7fd2f671c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7fd2f5f32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55a9b30b1e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55a9b30a85eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55a9b30c07bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x55a9b309c0d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55a9b30b270c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x55a9b309d2c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x55a9b30b270c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x55a9b30a08a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x55a9b30b270c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x55a9b309ae0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x55a9b30b270c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x55a9b309c0d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x55a9b30b270c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x55a9b309c0d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x55a9b318be56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x55a9b318bcf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x55a9b31b67d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x55a9b31b00bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x55a9b31b6525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x55a9b31b5a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x55a9b31b5653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x55a9b31a841e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x55a9b317ecad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7fd33ea75d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7fd33ea75e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x55a9b317eba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [15] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f3cb7db0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f3cb7d6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f3c6a79123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f3c6a79150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f3c6a791816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f3c6a74a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f3c6a74a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f3c6a74a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f3c6a74a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f3c1dad73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f3c1dada908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f3c1dae6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f3c1dae82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f3c1daea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f3c1daf9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f3c6a737c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f3c6a74775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f3c6a754b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f3c6a7555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f3c70f1c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f3c70732407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55c9e285fe0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55c9e28565eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55c9e286e7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x55c9e284a0d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55c9e286070c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x55c9e284b2c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x55c9e286070c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x55c9e284e8a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x55c9e286070c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x55c9e2848e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x55c9e286070c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x55c9e284a0d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x55c9e286070c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x55c9e284a0d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x55c9e2939e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x55c9e2939cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x55c9e29647d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x55c9e295e0bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x55c9e2964525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x55c9e2963a08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x55c9e2963653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x55c9e295641e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x55c9e292ccad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f3cb9300d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f3cb9300e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x55c9e292cba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [12] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f60f13b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f60f136b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f60adb9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f60adb9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f60adb91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f60adb4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f60adb4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f60adb4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f60adb4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f6060ed73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f6060eda908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f6060ee6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f6060ee82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f6060eea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f6060ef9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f60adb37c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f60adb4775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f60adb54b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f60adb555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f60b431c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f60b3b32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x562c87116e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x562c8710d5eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x562c871257bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x562c871010d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x562c8711770c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x562c871022c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x562c8711770c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x562c871058a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x562c8711770c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x562c870ffe0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x562c8711770c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x562c871010d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x562c8711770c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x562c871010d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x562c871f0e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x562c871f0cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x562c8721b7d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x562c872150bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x562c8721b525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x562c8721aa08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x562c8721a653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x562c8720d41e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x562c871e3cad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f60fc68ed90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f60fc68ee40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x562c871e3ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
RuntimeError: [8] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f9dc37b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f9dc376b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f9d7619123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f9d7619150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f9d76191816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f9d7614a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f9d7614a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f9d7614a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f9d7614a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f9d294d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f9d294da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: <unknown function> + 0xe27d37 (0x7f9d294e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f9d294e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f9d294ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::barrier(c10d::BarrierOptions const&) + 0x8e4 (0x7f9d294f9994 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4935c6f (0x7f9d76137c6f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494575c (0x7f9d7614775c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4952b12 (0x7f9d76154b12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x49535b9 (0x7f9d761555b9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb868a2 (0x7f9d7c91c8a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f9d7c132407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55813b027e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55813b01e5eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55813b0367bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x1981 (0x55813b0120d1 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55813b02870c in /usr/bin/python)
frame #26: _PyEval_EvalFrameDefault + 0x2b71 (0x55813b0132c1 in /usr/bin/python)
frame #27: _PyFunction_Vectorcall + 0x7c (0x55813b02870c in /usr/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6152 (0x55813b0168a2 in /usr/bin/python)
frame #29: _PyFunction_Vectorcall + 0x7c (0x55813b02870c in /usr/bin/python)
frame #30: _PyEval_EvalFrameDefault + 0x6bd (0x55813b010e0d in /usr/bin/python)
frame #31: _PyFunction_Vectorcall + 0x7c (0x55813b02870c in /usr/bin/python)
frame #32: _PyEval_EvalFrameDefault + 0x1981 (0x55813b0120d1 in /usr/bin/python)
frame #33: _PyFunction_Vectorcall + 0x7c (0x55813b02870c in /usr/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x1981 (0x55813b0120d1 in /usr/bin/python)
frame #35: <unknown function> + 0x239e56 (0x55813b101e56 in /usr/bin/python)
frame #36: PyEval_EvalCode + 0x86 (0x55813b101cf6 in /usr/bin/python)
frame #37: <unknown function> + 0x2647d8 (0x55813b12c7d8 in /usr/bin/python)
frame #38: <unknown function> + 0x25e0bb (0x55813b1260bb in /usr/bin/python)
frame #39: <unknown function> + 0x264525 (0x55813b12c525 in /usr/bin/python)
frame #40: _PyRun_SimpleFileObject + 0x1a8 (0x55813b12ba08 in /usr/bin/python)
frame #41: _PyRun_AnyFileObject + 0x43 (0x55813b12b653 in /usr/bin/python)
frame #42: Py_RunMain + 0x2be (0x55813b11e41e in /usr/bin/python)
frame #43: Py_BytesMain + 0x2d (0x55813b0f4cad in /usr/bin/python)
frame #44: <unknown function> + 0x29d90 (0x7f9dc4d34d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f9dc4d34e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #46: _start + 0x25 (0x55813b0f4ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
[2024-02-09 14:08:37,586] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 233935) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_14:08:37
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 233936)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_14:08:37
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 233937)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_14:08:37
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 233938)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_14:08:37
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 233939)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_14:08:37
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 233940)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_14:08:37
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 233941)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_14:08:37
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 233942)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_14:08:37
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 233935)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=4, pp=2, mbs=2
LOCAL_IP = 10.64.24.52
DP=2, MP=4, PP=2
[2024-02-09 14:12:00,341] torch.distributed.run: [WARNING] 
[2024-02-09 14:12:00,341] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 14:12:00,341] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 14:12:00,341] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 14:12:15,362] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234220 closing signal SIGTERM
[2024-02-09 14:12:15,362] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234221 closing signal SIGTERM
[2024-02-09 14:12:15,363] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234222 closing signal SIGTERM
[2024-02-09 14:12:15,364] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234223 closing signal SIGTERM
[2024-02-09 14:12:15,364] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234224 closing signal SIGTERM
[2024-02-09 14:12:15,365] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234225 closing signal SIGTERM
[2024-02-09 14:12:15,365] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234226 closing signal SIGTERM
[2024-02-09 14:12:16,007] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 234219) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_14:12:15
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 234219)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=4, pp=2, mbs=1
LOCAL_IP = 10.64.24.52
DP=2, MP=4, PP=2
[2024-02-09 14:14:28,628] torch.distributed.run: [WARNING] 
[2024-02-09 14:14:28,628] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 14:14:28,628] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 14:14:28,628] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 14:14:43,651] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234478 closing signal SIGTERM
[2024-02-09 14:14:43,652] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234479 closing signal SIGTERM
[2024-02-09 14:14:43,652] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234481 closing signal SIGTERM
[2024-02-09 14:14:43,653] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234482 closing signal SIGTERM
[2024-02-09 14:14:43,654] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234483 closing signal SIGTERM
[2024-02-09 14:14:43,654] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234484 closing signal SIGTERM
[2024-02-09 14:14:43,655] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234485 closing signal SIGTERM
[2024-02-09 14:14:44,347] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 2 (pid: 234480) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_14:14:43
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 234480)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=2, pp=4, mbs=32
LOCAL_IP = 10.64.24.52
DP=2, MP=2, PP=4
[2024-02-09 14:16:48,225] torch.distributed.run: [WARNING] 
[2024-02-09 14:16:48,225] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 14:16:48,225] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 14:16:48,225] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 14:17:03,249] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234739 closing signal SIGTERM
[2024-02-09 14:17:03,249] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234740 closing signal SIGTERM
[2024-02-09 14:17:03,249] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234741 closing signal SIGTERM
[2024-02-09 14:17:03,250] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234742 closing signal SIGTERM
[2024-02-09 14:17:03,251] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234744 closing signal SIGTERM
[2024-02-09 14:17:03,893] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 234738) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_14:17:03
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 234743)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_14:17:03
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 234745)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_14:17:03
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 234738)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=2, pp=4, mbs=16
LOCAL_IP = 10.64.24.52
DP=2, MP=2, PP=4
[2024-02-09 14:19:33,378] torch.distributed.run: [WARNING] 
[2024-02-09 14:19:33,378] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 14:19:33,378] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 14:19:33,378] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 14:19:48,402] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234974 closing signal SIGTERM
[2024-02-09 14:19:48,402] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234976 closing signal SIGTERM
[2024-02-09 14:19:48,403] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234977 closing signal SIGTERM
[2024-02-09 14:19:48,404] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234978 closing signal SIGTERM
[2024-02-09 14:19:48,405] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 234979 closing signal SIGTERM
[2024-02-09 14:19:49,032] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 234973) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_14:19:48
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 234975)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_14:19:48
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 234980)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_14:19:48
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 234973)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=2, pp=4, mbs=8
LOCAL_IP = 10.64.24.52
DP=2, MP=2, PP=4
[2024-02-09 14:22:13,892] torch.distributed.run: [WARNING] 
[2024-02-09 14:22:13,892] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 14:22:13,892] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 14:22:13,892] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 14:22:28,916] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 235234 closing signal SIGTERM
[2024-02-09 14:22:28,917] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 235235 closing signal SIGTERM
[2024-02-09 14:22:28,917] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 235236 closing signal SIGTERM
[2024-02-09 14:22:28,918] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 235237 closing signal SIGTERM
[2024-02-09 14:22:29,511] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 235230) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_14:22:28
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 235231)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_14:22:28
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 235232)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_14:22:28
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 235233)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_14:22:28
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 235230)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=2, pp=4, mbs=4
LOCAL_IP = 10.64.24.52
DP=2, MP=2, PP=4
[2024-02-09 14:25:12,180] torch.distributed.run: [WARNING] 
[2024-02-09 14:25:12,180] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 14:25:12,180] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 14:25:12,180] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 14:25:27,204] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 235491 closing signal SIGTERM
[2024-02-09 14:25:27,204] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 235492 closing signal SIGTERM
[2024-02-09 14:25:27,204] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 235493 closing signal SIGTERM
[2024-02-09 14:25:27,205] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 235494 closing signal SIGTERM
[2024-02-09 14:25:27,205] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 235496 closing signal SIGTERM
[2024-02-09 14:25:27,797] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 235489) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_14:25:27
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 235490)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_14:25:27
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 235495)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_14:25:27
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 235489)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=2, pp=4, mbs=2
LOCAL_IP = 10.64.24.52
DP=2, MP=2, PP=4
[2024-02-09 14:27:58,978] torch.distributed.run: [WARNING] 
[2024-02-09 14:27:58,978] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 14:27:58,978] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 14:27:58,978] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 14:28:13,998] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 235737 closing signal SIGTERM
[2024-02-09 14:28:13,999] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 235738 closing signal SIGTERM
[2024-02-09 14:28:14,000] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 235739 closing signal SIGTERM
[2024-02-09 14:28:14,000] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 235741 closing signal SIGTERM
[2024-02-09 14:28:14,000] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 235742 closing signal SIGTERM
[2024-02-09 14:28:14,000] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 235743 closing signal SIGTERM
[2024-02-09 14:28:14,655] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 235736) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_14:28:13
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 235740)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_14:28:13
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 235736)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=2, tp=2, pp=4, mbs=1
LOCAL_IP = 10.64.24.52
DP=2, MP=2, PP=4
[2024-02-09 14:30:34,083] torch.distributed.run: [WARNING] 
[2024-02-09 14:30:34,083] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 14:30:34,083] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 14:30:34,083] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-09 14:30:49,102] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 235994 closing signal SIGTERM
[2024-02-09 14:30:49,102] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 235995 closing signal SIGTERM
[2024-02-09 14:30:49,103] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 235996 closing signal SIGTERM
[2024-02-09 14:30:49,104] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 235997 closing signal SIGTERM
[2024-02-09 14:30:49,104] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 235999 closing signal SIGTERM
[2024-02-09 14:30:49,105] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 236000 closing signal SIGTERM
[2024-02-09 14:30:49,783] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 235993) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_14:30:49
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 235998)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_14:30:49
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 235993)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=1, tp=16, pp=1, mbs=32
LOCAL_IP = 10.64.24.52
DP=1, MP=16, PP=1
[2024-02-09 14:33:04,167] torch.distributed.run: [WARNING] 
[2024-02-09 14:33:04,167] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 14:33:04,167] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 14:33:04,167] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])

SYM206-GPU-A0206-P2-Node52:236240:236518 [0] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.51<40930> with error 12, opcode 129, len 0, vendor err 129

SYM206-GPU-A0206-P2-Node52:236240:236518 [0] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.51<40930> with error 5, opcode 129, len 0, vendor err 244
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 8] NCCL watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.19.3
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
NET/IB : Got completion from peer 10.64.24.51<40930> with error 5, opcode 129, len 0, vendor err 244
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 8] NCCL watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.19.3
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
NET/IB : Got completion from peer 10.64.24.51<40930> with error 5, opcode 129, len 0, vendor err 244
[2024-02-09 14:37:34,450] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 236241 closing signal SIGTERM
[2024-02-09 14:37:34,451] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 236242 closing signal SIGTERM
[2024-02-09 14:37:34,451] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 236243 closing signal SIGTERM
[2024-02-09 14:37:34,452] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 236244 closing signal SIGTERM
[2024-02-09 14:37:34,452] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 236245 closing signal SIGTERM
[2024-02-09 14:37:34,453] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 236246 closing signal SIGTERM
[2024-02-09 14:37:34,453] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 236247 closing signal SIGTERM
[2024-02-09 14:37:35,395] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 0 (pid: 236240) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
pretrain_gpt.py FAILED
-------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_14:37:34
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : -6 (pid: 236240)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 236240
=======================================================
13b, 16k, gbs=512: dp=1, tp=16, pp=1, mbs=16
LOCAL_IP = 10.64.24.52
DP=1, MP=16, PP=1
[2024-02-09 14:39:48,030] torch.distributed.run: [WARNING] 
[2024-02-09 14:39:48,030] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 14:39:48,030] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 14:39:48,030] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])

SYM206-GPU-A0206-P2-Node52:236628:236905 [0] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.51<34396> with error 12, opcode 129, len 16, vendor err 129

SYM206-GPU-A0206-P2-Node52:236628:236905 [0] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.51<34396> with error 5, opcode 129, len 16, vendor err 244
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 8] NCCL watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.19.3
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
NET/IB : Got completion from peer 10.64.24.51<34396> with error 5, opcode 129, len 16, vendor err 244
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 8] NCCL watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.19.3
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
NET/IB : Got completion from peer 10.64.24.51<34396> with error 5, opcode 129, len 16, vendor err 244
[2024-02-09 14:44:08,258] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 236629 closing signal SIGTERM
[2024-02-09 14:44:08,258] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 236630 closing signal SIGTERM
[2024-02-09 14:44:08,259] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 236631 closing signal SIGTERM
[2024-02-09 14:44:08,259] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 236632 closing signal SIGTERM
[2024-02-09 14:44:08,259] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 236633 closing signal SIGTERM
[2024-02-09 14:44:08,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 236634 closing signal SIGTERM
[2024-02-09 14:44:08,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 236635 closing signal SIGTERM
[2024-02-09 14:44:09,139] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 0 (pid: 236628) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
pretrain_gpt.py FAILED
-------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_14:44:08
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : -6 (pid: 236628)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 236628
=======================================================
13b, 16k, gbs=512: dp=1, tp=16, pp=1, mbs=8
LOCAL_IP = 10.64.24.52
DP=1, MP=16, PP=1
[2024-02-09 14:46:21,763] torch.distributed.run: [WARNING] 
[2024-02-09 14:46:21,763] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 14:46:21,763] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 14:46:21,763] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])

SYM206-GPU-A0206-P2-Node52:237016:237293 [0] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.51<33272> with error 12, opcode 129, len 16, vendor err 129

SYM206-GPU-A0206-P2-Node52:237016:237293 [0] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.51<33272> with error 5, opcode 129, len 16, vendor err 244
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 8] NCCL watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.19.3
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
NET/IB : Got completion from peer 10.64.24.51<33272> with error 5, opcode 129, len 16, vendor err 244
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 8] NCCL watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.19.3
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
NET/IB : Got completion from peer 10.64.24.51<33272> with error 5, opcode 129, len 16, vendor err 244
[2024-02-09 14:50:42,041] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 237017 closing signal SIGTERM
[2024-02-09 14:50:42,042] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 237018 closing signal SIGTERM
[2024-02-09 14:50:42,042] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 237019 closing signal SIGTERM
[2024-02-09 14:50:42,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 237020 closing signal SIGTERM
[2024-02-09 14:50:42,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 237021 closing signal SIGTERM
[2024-02-09 14:50:42,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 237022 closing signal SIGTERM
[2024-02-09 14:50:42,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 237023 closing signal SIGTERM
[2024-02-09 14:50:42,923] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 0 (pid: 237016) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
pretrain_gpt.py FAILED
-------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_14:50:42
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : -6 (pid: 237016)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 237016
=======================================================
13b, 16k, gbs=512: dp=1, tp=16, pp=1, mbs=4
LOCAL_IP = 10.64.24.52
DP=1, MP=16, PP=1
[2024-02-09 14:52:55,496] torch.distributed.run: [WARNING] 
[2024-02-09 14:52:55,496] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 14:52:55,496] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 14:52:55,496] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 69, in __init__
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    self.language_model, self._language_model_key = get_language_model(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 68, in get_language_model
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    language_model = TransformerLanguageModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 400, in __init__
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 69, in __init__
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    self.language_model, self._language_model_key = get_language_model(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 68, in get_language_model
    self.encoder = ParallelTransformer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1468, in __init__
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 69, in __init__
    language_model = TransformerLanguageModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 400, in __init__
    [build_layer(i + 1 + offset) for i in range(self.num_layers)])
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1468, in <listcomp>
    self.language_model, self._language_model_key = get_language_model(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 68, in get_language_model
    self.encoder = ParallelTransformer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1468, in __init__
    [build_layer(i + 1 + offset) for i in range(self.num_layers)])
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1387, in build_layer
    language_model = TransformerLanguageModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 400, in __init__
    [build_layer(i + 1 + offset) for i in range(self.num_layers)])
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1468, in <listcomp>
    return ParallelTransformerLayer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 774, in __init__
    self.encoder = ParallelTransformer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1468, in __init__
    [build_layer(i + 1 + offset) for i in range(self.num_layers)])
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1387, in build_layer
    [build_layer(i + 1 + offset) for i in range(self.num_layers)])
    return ParallelTransformerLayer(  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1468, in <listcomp>

  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 774, in __init__
    self.self_attention = ParallelAttention(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 442, in __init__
    self.self_attention = ParallelAttention(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 442, in __init__
    self.num_attention_heads_per_partition = core.utils.divide(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/utils.py", line 22, in divide
    self.num_attention_heads_per_partition = core.utils.divide(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/utils.py", line 22, in divide
    [build_layer(i + 1 + offset) for i in range(self.num_layers)])
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1387, in build_layer
    ensure_divisibility(numerator, denominator)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/utils.py", line 14, in ensure_divisibility
    return ParallelTransformerLayer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 774, in __init__
    ensure_divisibility(numerator, denominator)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/utils.py", line 14, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(
AssertionError: 40 is not divisible by 16
    self.self_attention = ParallelAttention(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 442, in __init__
    assert numerator % denominator == 0, "{} is not divisible by {}".format(
AssertionError: 40 is not divisible by 16
    self.num_attention_heads_per_partition = core.utils.divide(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/utils.py", line 22, in divide
    ensure_divisibility(numerator, denominator)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/utils.py", line 14, in ensure_divisibility
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    assert numerator % denominator == 0, "{} is not divisible by {}".format(
AssertionError: 40 is not divisible by 16
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 69, in __init__
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    self.language_model, self._language_model_key = get_language_model(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 68, in get_language_model
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    language_model = TransformerLanguageModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 400, in __init__
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    self.encoder = ParallelTransformer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1468, in __init__
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    [build_layer(i + 1 + offset) for i in range(self.num_layers)])
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1468, in <listcomp>
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 69, in __init__
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 69, in __init__
    [build_layer(i + 1 + offset) for i in range(self.num_layers)])
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1387, in build_layer
    self.language_model, self._language_model_key = get_language_model(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 68, in get_language_model
    self.language_model, self._language_model_key = get_language_model(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 68, in get_language_model
    language_model = TransformerLanguageModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 400, in __init__
    return ParallelTransformerLayer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 774, in __init__
    language_model = TransformerLanguageModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 400, in __init__
    self.encoder = ParallelTransformer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1468, in __init__
    self.self_attention = ParallelAttention(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 442, in __init__
    self.encoder = ParallelTransformer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1468, in __init__
    self.num_attention_heads_per_partition = core.utils.divide(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/utils.py", line 22, in divide
    [build_layer(i + 1 + offset) for i in range(self.num_layers)])
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1468, in <listcomp>
    [build_layer(i + 1 + offset) for i in range(self.num_layers)])
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1468, in <listcomp>
    ensure_divisibility(numerator, denominator)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/utils.py", line 14, in ensure_divisibility
    [build_layer(i + 1 + offset) for i in range(self.num_layers)])
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1387, in build_layer
    [build_layer(i + 1 + offset) for i in range(self.num_layers)])
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1387, in build_layer
    return ParallelTransformerLayer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 774, in __init__
    assert numerator % denominator == 0, "{} is not divisible by {}".format(
AssertionError: 40 is not divisible by 16
    self.self_attention = ParallelAttention(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 442, in __init__
    return ParallelTransformerLayer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 774, in __init__
    self.num_attention_heads_per_partition = core.utils.divide(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/utils.py", line 22, in divide
    self.self_attention = ParallelAttention(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 442, in __init__
    ensure_divisibility(numerator, denominator)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/utils.py", line 14, in ensure_divisibility
    self.num_attention_heads_per_partition = core.utils.divide(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/utils.py", line 22, in divide
    assert numerator % denominator == 0, "{} is not divisible by {}".format(
AssertionError: 40 is not divisible by 16
    ensure_divisibility(numerator, denominator)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/utils.py", line 14, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(
AssertionError: 40 is not divisible by 16
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 69, in __init__
    self.language_model, self._language_model_key = get_language_model(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 68, in get_language_model
    language_model = TransformerLanguageModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 400, in __init__
    self.encoder = ParallelTransformer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1468, in __init__
    [build_layer(i + 1 + offset) for i in range(self.num_layers)])
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1468, in <listcomp>
    [build_layer(i + 1 + offset) for i in range(self.num_layers)])
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1387, in build_layer
    return ParallelTransformerLayer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 774, in __init__
    self.self_attention = ParallelAttention(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 442, in __init__
    self.num_attention_heads_per_partition = core.utils.divide(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/utils.py", line 22, in divide
    ensure_divisibility(numerator, denominator)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/utils.py", line 14, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(
AssertionError: 40 is not divisible by 16
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 69, in __init__
    self.language_model, self._language_model_key = get_language_model(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 68, in get_language_model
    language_model = TransformerLanguageModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 400, in __init__
    self.encoder = ParallelTransformer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1468, in __init__
    [build_layer(i + 1 + offset) for i in range(self.num_layers)])
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1468, in <listcomp>
    [build_layer(i + 1 + offset) for i in range(self.num_layers)])
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1387, in build_layer
    return ParallelTransformerLayer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 774, in __init__
    self.self_attention = ParallelAttention(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 442, in __init__
    self.num_attention_heads_per_partition = core.utils.divide(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/utils.py", line 22, in divide
    ensure_divisibility(numerator, denominator)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/utils.py", line 14, in ensure_divisibility
    assert numerator % denominator == 0, "{} is not divisible by {}".format(
AssertionError: 40 is not divisible by 16
[2024-02-09 14:53:20,531] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 237404) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_14:53:20
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 237405)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_14:53:20
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 237406)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_14:53:20
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 237407)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_14:53:20
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 237408)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_14:53:20
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 237409)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_14:53:20
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 237410)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_14:53:20
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 237411)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_14:53:20
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 237404)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=1, tp=16, pp=1, mbs=2
LOCAL_IP = 10.64.24.52
DP=1, MP=16, PP=1
[2024-02-09 14:56:35,689] torch.distributed.run: [WARNING] 
[2024-02-09 14:56:35,689] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 14:56:35,689] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 14:56:35,689] torch.distributed.run: [WARNING] *****************************************

SYM206-GPU-A0206-P2-Node52:237792:237924 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:237793:237925 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:237795:237966 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:237798:237969 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:237796:237968 [4] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:237799:237995 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:237794:237975 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:237797:237993 [5] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
[2024-02-09 14:56:50,709] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 237794 closing signal SIGTERM
[2024-02-09 14:56:50,709] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 237796 closing signal SIGTERM
[2024-02-09 14:56:50,710] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 237797 closing signal SIGTERM
[2024-02-09 14:56:50,711] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 237798 closing signal SIGTERM
[2024-02-09 14:56:50,711] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 237799 closing signal SIGTERM
[2024-02-09 14:56:51,239] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 237792) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_14:56:50
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 237793)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_14:56:50
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 237795)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_14:56:50
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 237792)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=1, tp=16, pp=1, mbs=1
LOCAL_IP = 10.64.24.52
DP=1, MP=16, PP=1
[2024-02-09 14:59:20,840] torch.distributed.run: [WARNING] 
[2024-02-09 14:59:20,840] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 14:59:20,840] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 14:59:20,840] torch.distributed.run: [WARNING] *****************************************

SYM206-GPU-A0206-P2-Node52:238161:238296 [5] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:238158:238297 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:238159:238348 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:238163:238360 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:238157:238334 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:238162:238357 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:238160:238368 [4] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper

SYM206-GPU-A0206-P2-Node52:238156:238335 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
[2024-02-09 14:59:35,860] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 238162 closing signal SIGTERM
[2024-02-09 14:59:36,074] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 238156) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_14:59:35
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 238157)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_14:59:35
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 238158)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_14:59:35
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 238159)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_14:59:35
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 238160)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_14:59:35
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 238161)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_14:59:35
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 238163)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_14:59:35
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 238156)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=1, tp=1, pp=16, mbs=32
LOCAL_IP = 10.64.24.52
DP=1, MP=1, PP=16
[2024-02-09 15:02:46,097] torch.distributed.run: [WARNING] 
[2024-02-09 15:02:46,097] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 15:02:46,097] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 15:02:46,097] torch.distributed.run: [WARNING] *****************************************

SYM206-GPU-A0206-P2-Node52:238527:238657 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:238520:238659 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:238523:238658 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:238526:238684 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:238524:238685 [4] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:238522:238708 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:238525:238707 [5] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:238521:238709 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<51551> failed : Software caused connection abort
[2024-02-09 15:03:01,121] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 238520) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_15:03:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 238521)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_15:03:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 238522)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_15:03:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 238523)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_15:03:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 238524)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_15:03:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 238525)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_15:03:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 238526)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_15:03:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 238527)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_15:03:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 238520)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=1, tp=1, pp=16, mbs=16
LOCAL_IP = 10.64.24.52
DP=1, MP=1, PP=16
[2024-02-09 15:06:16,353] torch.distributed.run: [WARNING] 
[2024-02-09 15:06:16,353] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 15:06:16,353] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 15:06:16,353] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 7 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 888275 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 3 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 888271 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 888268 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 1 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 888269 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 5 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 888273 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 6 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 888274 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 2 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 888270 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 94, in pretrain
    set_jit_fusion_options()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 253, in set_jit_fusion_options
    _warmup_jit_function()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 277, in _warmup_jit_function
    output = bias_gelu(bias, input)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/fused_bias_gelu.py", line 17, in bias_gelu
def bias_gelu(bias, y):
    x = bias + y
    return  x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))
                             ~~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 4 has a total capacty of 79.11 GiB of which 7.48 GiB is free. Process 888272 has 71.62 GiB memory in use. Of the allocated memory 70.00 GiB is allocated by PyTorch, and 1.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

[2024-02-09 15:06:41,392] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 238869) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_15:06:41
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 238870)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_15:06:41
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 238871)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_15:06:41
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 238872)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_15:06:41
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 238873)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_15:06:41
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 238874)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_15:06:41
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 238875)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_15:06:41
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 238876)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_15:06:41
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 238869)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=1, tp=1, pp=16, mbs=8
LOCAL_IP = 10.64.24.52
DP=1, MP=1, PP=16
[2024-02-09 15:10:04,085] torch.distributed.run: [WARNING] 
[2024-02-09 15:10:04,085] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 15:10:04,085] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 15:10:04,085] torch.distributed.run: [WARNING] *****************************************

SYM206-GPU-A0206-P2-Node52:239247:239412 [5] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<46069> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<46069> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:239243:239417 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<46069> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:239246:239397 [4] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<46069> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<46069> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<46069> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:239244:239425 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<46069> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:239249:239413 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<46069> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:239245:239426 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<46069> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:239242:239423 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<46069> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain

SYM206-GPU-A0206-P2-Node52:239248:239424 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<46069> failed : Software caused connection abort
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<46069> failed : Software caused connection abort
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<46069> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<46069> failed : Software caused connection abort
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<46069> failed : Software caused connection abort
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<46069> failed : Software caused connection abort
[2024-02-09 15:10:24,115] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 239242) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_15:10:24
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 239243)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_15:10:24
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 239244)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_15:10:24
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 239245)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_15:10:24
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 239246)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_15:10:24
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 239247)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_15:10:24
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 239248)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_15:10:24
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 239249)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_15:10:24
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 239242)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=1, tp=1, pp=16, mbs=4
LOCAL_IP = 10.64.24.52
DP=1, MP=1, PP=16
[2024-02-09 15:13:46,856] torch.distributed.run: [WARNING] 
[2024-02-09 15:13:46,856] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 15:13:46,856] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 15:13:46,856] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    pretrain(train_dataset_provider,
    pretrain(train_dataset_provider,  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain

    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    pretrain(train_dataset_provider,
    pretrain(train_dataset_provider,  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain

  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 69, in __init__
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer

  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
        model, optimizer, opt_param_scheduler = setup_model_and_optimizer(model, optimizer, opt_param_scheduler = setup_model_and_optimizer(

  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = get_model(model_provider_func, model_type)    
model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
        self.language_model, self._language_model_key = get_language_model(model = model_provider_func(

  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 68, in get_language_model
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 69, in __init__
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 69, in __init__
    language_model = TransformerLanguageModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 400, in __init__
    self.language_model, self._language_model_key = get_language_model(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 68, in get_language_model
    model = GPTModel(
      File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 69, in __init__
model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 69, in __init__
    model = GPTModel(
      File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 69, in __init__
model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 69, in __init__
    self.encoder = ParallelTransformer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1361, in __init__
    self.language_model, self._language_model_key = get_language_model(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 68, in get_language_model
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 69, in __init__
    language_model = TransformerLanguageModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 400, in __init__
    language_model = TransformerLanguageModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 400, in __init__
    self.language_model, self._language_model_key = get_language_model(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 68, in get_language_model
    self.num_layers = _get_num_layers(args, model_type,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1253, in _get_num_layers
    self.encoder = ParallelTransformer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1361, in __init__
    self.encoder = ParallelTransformer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1361, in __init__
    self.language_model, self._language_model_key = get_language_model(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 68, in get_language_model
    self.language_model, self._language_model_key = get_language_model(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 68, in get_language_model
    language_model = TransformerLanguageModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 400, in __init__
    self.language_model, self._language_model_key = get_language_model(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 68, in get_language_model
    assert args.num_layers % args.transformer_pipeline_model_parallel_size == 0, \    
self.num_layers = _get_num_layers(args, model_type,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1253, in _get_num_layers
AssertionError: num_layers must be divisible by transformer_pipeline_model_parallel_size
    assert args.num_layers % args.transformer_pipeline_model_parallel_size == 0, \
AssertionError: num_layers must be divisible by transformer_pipeline_model_parallel_size
    self.language_model, self._language_model_key = get_language_model(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 68, in get_language_model
    self.num_layers = _get_num_layers(args, model_type,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1253, in _get_num_layers
    self.encoder = ParallelTransformer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1361, in __init__
    assert args.num_layers % args.transformer_pipeline_model_parallel_size == 0, \
AssertionError: num_layers must be divisible by transformer_pipeline_model_parallel_size
    language_model = TransformerLanguageModel(
    language_model = TransformerLanguageModel(  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 400, in __init__

  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 400, in __init__
    language_model = TransformerLanguageModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 400, in __init__
    language_model = TransformerLanguageModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 400, in __init__
    self.encoder = ParallelTransformer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1361, in __init__
    self.encoder = ParallelTransformer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1361, in __init__
    self.encoder = ParallelTransformer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1361, in __init__
    self.num_layers = _get_num_layers(args, model_type,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1253, in _get_num_layers
    self.encoder = ParallelTransformer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1361, in __init__
    self.num_layers = _get_num_layers(args, model_type,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1253, in _get_num_layers
    self.num_layers = _get_num_layers(args, model_type,    
self.num_layers = _get_num_layers(args, model_type,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1253, in _get_num_layers
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1253, in _get_num_layers
    assert args.num_layers % args.transformer_pipeline_model_parallel_size == 0, \
AssertionError: num_layers must be divisible by transformer_pipeline_model_parallel_size
    assert args.num_layers % args.transformer_pipeline_model_parallel_size == 0, \
    assert args.num_layers % args.transformer_pipeline_model_parallel_size == 0, \
AssertionError: num_layers must be divisible by transformer_pipeline_model_parallel_size
    assert args.num_layers % args.transformer_pipeline_model_parallel_size == 0, \
AssertionError: num_layers must be divisible by transformer_pipeline_model_parallel_size
AssertionError: num_layers must be divisible by transformer_pipeline_model_parallel_size
    self.num_layers = _get_num_layers(args, model_type,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/transformer.py", line 1253, in _get_num_layers
    assert args.num_layers % args.transformer_pipeline_model_parallel_size == 0, \
AssertionError: num_layers must be divisible by transformer_pipeline_model_parallel_size
[2024-02-09 15:14:11,891] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 239591) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_15:14:11
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 239592)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_15:14:11
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 239593)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_15:14:11
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 239594)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_15:14:11
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 239595)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_15:14:11
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 239596)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_15:14:11
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 239597)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_15:14:11
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 239598)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_15:14:11
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 239591)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=1, tp=1, pp=16, mbs=2
LOCAL_IP = 10.64.24.52
DP=1, MP=1, PP=16
[2024-02-09 15:17:34,620] torch.distributed.run: [WARNING] 
[2024-02-09 15:17:34,620] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 15:17:34,620] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 15:17:34,620] torch.distributed.run: [WARNING] *****************************************

SYM206-GPU-A0206-P2-Node52:239966:240133 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:239968:240135 [4] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:239971:240146 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:239970:240148 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:239964:240142 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>

SYM206-GPU-A0206-P2-Node52:239969:240147 [5] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
        _compile_dependencies()work = default_pg.barrier(opts=opts)

  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:239965:240134 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:239967:240136 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed    work = default_pg.barrier(opts=opts).
DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection aborttorch.distributed
.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
[2024-02-09 15:17:54,648] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 239964) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_15:17:54
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 239965)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_15:17:54
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 239966)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_15:17:54
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 239967)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_15:17:54
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 239968)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_15:17:54
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 239969)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_15:17:54
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 239970)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_15:17:54
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 239971)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_15:17:54
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 239964)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=1, tp=1, pp=16, mbs=1
LOCAL_IP = 10.64.24.52
DP=1, MP=1, PP=16
[2024-02-09 15:21:17,408] torch.distributed.run: [WARNING] 
[2024-02-09 15:21:17,408] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 15:21:17,408] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 15:21:17,408] torch.distributed.run: [WARNING] *****************************************

SYM206-GPU-A0206-P2-Node52:240315:240467 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:240319:240468 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:240314:240466 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:240313:240469 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:240318:240465 [5] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:240320:240495 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:240317:240496 [4] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:240316:240497 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
[2024-02-09 15:21:32,432] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 240314 closing signal SIGTERM
[2024-02-09 15:21:32,433] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 240317 closing signal SIGTERM
[2024-02-09 15:21:32,754] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 240313) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_15:21:32
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 240315)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_15:21:32
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 240316)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_15:21:32
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 240318)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_15:21:32
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 240319)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_15:21:32
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 240320)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_15:21:32
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 240313)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=1, tp=8, pp=2, mbs=32
LOCAL_IP = 10.64.24.52
DP=1, MP=8, PP=2
[2024-02-09 15:24:35,453] torch.distributed.run: [WARNING] 
[2024-02-09 15:24:35,453] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 15:24:35,453] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 15:24:35,453] torch.distributed.run: [WARNING] *****************************************

SYM206-GPU-A0206-P2-Node52:240664:240849 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:240662:240839 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:240668:240828 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:240667:240838 [5] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:240666:240845 [4] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:240665:240853 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron

SYM206-GPU-A0206-P2-Node52:240669:240852 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:240663:240846 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<57629> failed : Software caused connection abort
[2024-02-09 15:24:55,482] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 240662) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_15:24:55
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 240663)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_15:24:55
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 240664)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_15:24:55
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 240665)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_15:24:55
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 240666)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_15:24:55
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 240667)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_15:24:55
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 240668)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_15:24:55
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 240669)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_15:24:55
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 240662)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=1, tp=8, pp=2, mbs=16
LOCAL_IP = 10.64.24.52
DP=1, MP=8, PP=2
[2024-02-09 15:28:18,156] torch.distributed.run: [WARNING] 
[2024-02-09 15:28:18,156] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 15:28:18,156] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 15:28:18,156] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])

SYM206-GPU-A0206-P2-Node52:241018:241288 [0] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.51<36960> with error 12, opcode 129, len 0, vendor err 129

SYM206-GPU-A0206-P2-Node52:241018:241288 [0] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.51<36960> with error 5, opcode 129, len 0, vendor err 244
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 8] NCCL watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.19.3
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
NET/IB : Got completion from peer 10.64.24.51<36960> with error 5, opcode 129, len 0, vendor err 244
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 8] NCCL watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.19.3
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
NET/IB : Got completion from peer 10.64.24.51<36960> with error 5, opcode 129, len 0, vendor err 244
[2024-02-09 15:32:48,440] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 241019 closing signal SIGTERM
[2024-02-09 15:32:48,441] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 241020 closing signal SIGTERM
[2024-02-09 15:32:48,441] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 241021 closing signal SIGTERM
[2024-02-09 15:32:48,442] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 241022 closing signal SIGTERM
[2024-02-09 15:32:48,442] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 241023 closing signal SIGTERM
[2024-02-09 15:32:48,443] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 241024 closing signal SIGTERM
[2024-02-09 15:32:48,443] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 241025 closing signal SIGTERM
[2024-02-09 15:32:49,321] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 0 (pid: 241018) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
pretrain_gpt.py FAILED
-------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_15:32:48
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : -6 (pid: 241018)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 241018
=======================================================
13b, 16k, gbs=512: dp=1, tp=8, pp=2, mbs=8
LOCAL_IP = 10.64.24.52
DP=1, MP=8, PP=2
[2024-02-09 15:35:01,955] torch.distributed.run: [WARNING] 
[2024-02-09 15:35:01,955] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 15:35:01,955] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 15:35:01,955] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])

SYM206-GPU-A0206-P2-Node52:241398:241662 [0] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.51<36328> with error 12, opcode 129, len 16, vendor err 129

SYM206-GPU-A0206-P2-Node52:241398:241662 [0] ib_plugin.c:1007 NCCL WARN NET/IB : Got completion from peer 10.64.24.51<36328> with error 5, opcode 129, len 16, vendor err 244
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 8] NCCL watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.19.3
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
NET/IB : Got completion from peer 10.64.24.51<36328> with error 5, opcode 129, len 16, vendor err 244
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 8] NCCL watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.19.3
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
NET/IB : Got completion from peer 10.64.24.51<36328> with error 5, opcode 129, len 16, vendor err 244
[2024-02-09 15:39:22,228] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 241399 closing signal SIGTERM
[2024-02-09 15:39:22,228] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 241400 closing signal SIGTERM
[2024-02-09 15:39:22,229] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 241401 closing signal SIGTERM
[2024-02-09 15:39:22,229] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 241402 closing signal SIGTERM
[2024-02-09 15:39:22,230] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 241403 closing signal SIGTERM
[2024-02-09 15:39:22,230] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 241404 closing signal SIGTERM
[2024-02-09 15:39:22,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 241405 closing signal SIGTERM
[2024-02-09 15:39:23,109] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 0 (pid: 241398) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
pretrain_gpt.py FAILED
-------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_15:39:22
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : -6 (pid: 241398)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 241398
=======================================================
13b, 16k, gbs=512: dp=1, tp=8, pp=2, mbs=4
LOCAL_IP = 10.64.24.52
DP=1, MP=8, PP=2
[2024-02-09 15:41:35,736] torch.distributed.run: [WARNING] 
[2024-02-09 15:41:35,736] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 15:41:35,736] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 15:41:35,736] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 819914240
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 819914240
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 819914240
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 80, in __init__
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 80, in __init__
    self.initialize_word_embeddings(init_method_normal)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 106, in initialize_word_embeddings
    self.initialize_word_embeddings(init_method_normal)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 106, in initialize_word_embeddings
    torch.distributed.all_reduce(self.word_embeddings_weight().data,
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
    torch.distributed.all_reduce(self.word_embeddings_weight().data,
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
    work = group.allreduce([tensor], opts)
RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f6c8a9b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f6c8a96b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f6c51f9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f6c51f9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f6c51f91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f6c51f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f6c51f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f6c51f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f6c51f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f6c51f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f6c052d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f6c052da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0xe27d37 (0x7f6c052e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f6c052e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f6c052ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4936f0a (0x7f6c51f38f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494766f (0x7f6c51f4966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4950b97 (0x7f6c51f52b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x495f1ee (0x7f6c51f611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb8cc15 (0x7f6c58722c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f6c57f32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x560e83e7ce0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x560e83e735eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x560e83e8b7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x560e83e6b8a2 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x560e83e7d70c in /usr/bin/python)
frame #26: PyObject_Call + 0x122 (0x560e83e8c192 in /usr/bin/python)
frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x560e83e682c1 in /usr/bin/python)
frame #28: _PyFunction_Vectorcall + 0x7c (0x560e83e7d70c in /usr/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x560e83e670d1 in /usr/bin/python)
frame #30: <unknown function> + 0x16e4e1 (0x560e83e8b4e1 in /usr/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x560e83e6b8a2 in /usr/bin/python)
frame #32: _PyFunction_Vectorcall + 0x7c (0x560e83e7d70c in /usr/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x16d (0x560e83e7282d in /usr/bin/python)
frame #34: <unknown function> + 0x16a744 (0x560e83e87744 in /usr/bin/python)
frame #35: _PyObject_MakeTpCall + 0x1fc (0x560e83e7358c in /usr/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x560e83e6c908 in /usr/bin/python)
frame #37: _PyFunction_Vectorcall + 0x7c (0x560e83e7d70c in /usr/bin/python)
frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x560e83e670d1 in /usr/bin/python)
frame #39: _PyFunction_Vectorcall + 0x7c (0x560e83e7d70c in /usr/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x560e83e65e0d in /usr/bin/python)
frame #41: _PyFunction_Vectorcall + 0x7c (0x560e83e7d70c in /usr/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x560e83e65e0d in /usr/bin/python)
frame #43: _PyFunction_Vectorcall + 0x7c (0x560e83e7d70c in /usr/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x560e83e670d1 in /usr/bin/python)
frame #45: <unknown function> + 0x239e56 (0x560e83f56e56 in /usr/bin/python)
frame #46: PyEval_EvalCode + 0x86 (0x560e83f56cf6 in /usr/bin/python)
frame #47: <unknown function> + 0x2647d8 (0x560e83f817d8 in /usr/bin/python)
frame #48: <unknown function> + 0x25e0bb (0x560e83f7b0bb in /usr/bin/python)
frame #49: <unknown function> + 0x264525 (0x560e83f81525 in /usr/bin/python)
frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x560e83f80a08 in /usr/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x560e83f80653 in /usr/bin/python)
frame #52: Py_RunMain + 0x2be (0x560e83f7341e in /usr/bin/python)
frame #53: Py_BytesMain + 0x2d (0x560e83f49cad in /usr/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7f6ca09f5d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7f6ca09f5e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #56: _start + 0x25 (0x560e83f49ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
    work = group.allreduce([tensor], opts)
RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f6dc61ac449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f6dc61671d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f6d77d9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f6d77d9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f6d77d91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f6d77d4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f6d77d4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f6d77d4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f6d77d4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f6d77d4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f6d2b0d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f6d2b0da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0xe27d37 (0x7f6d2b0e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f6d2b0e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f6d2b0ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4936f0a (0x7f6d77d38f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494766f (0x7f6d77d4966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4950b97 (0x7f6d77d52b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x495f1ee (0x7f6d77d611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb8cc15 (0x7f6d7e522c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f6d7dd32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55b7b92bce0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55b7b92b35eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55b7b92cb7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x55b7b92ab8a2 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55b7b92bd70c in /usr/bin/python)
frame #26: PyObject_Call + 0x122 (0x55b7b92cc192 in /usr/bin/python)
frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x55b7b92a82c1 in /usr/bin/python)
frame #28: _PyFunction_Vectorcall + 0x7c (0x55b7b92bd70c in /usr/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x55b7b92a70d1 in /usr/bin/python)
frame #30: <unknown function> + 0x16e4e1 (0x55b7b92cb4e1 in /usr/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x55b7b92ab8a2 in /usr/bin/python)
frame #32: _PyFunction_Vectorcall + 0x7c (0x55b7b92bd70c in /usr/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x16d (0x55b7b92b282d in /usr/bin/python)
frame #34: <unknown function> + 0x16a744 (0x55b7b92c7744 in /usr/bin/python)
frame #35: _PyObject_MakeTpCall + 0x1fc (0x55b7b92b358c in /usr/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x55b7b92ac908 in /usr/bin/python)
frame #37: _PyFunction_Vectorcall + 0x7c (0x55b7b92bd70c in /usr/bin/python)
frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x55b7b92a70d1 in /usr/bin/python)
frame #39: _PyFunction_Vectorcall + 0x7c (0x55b7b92bd70c in /usr/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x55b7b92a5e0d in /usr/bin/python)
frame #41: _PyFunction_Vectorcall + 0x7c (0x55b7b92bd70c in /usr/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x55b7b92a5e0d in /usr/bin/python)
frame #43: _PyFunction_Vectorcall + 0x7c (0x55b7b92bd70c in /usr/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x55b7b92a70d1 in /usr/bin/python)
frame #45: <unknown function> + 0x239e56 (0x55b7b9396e56 in /usr/bin/python)
frame #46: PyEval_EvalCode + 0x86 (0x55b7b9396cf6 in /usr/bin/python)
frame #47: <unknown function> + 0x2647d8 (0x55b7b93c17d8 in /usr/bin/python)
frame #48: <unknown function> + 0x25e0bb (0x55b7b93bb0bb in /usr/bin/python)
frame #49: <unknown function> + 0x264525 (0x55b7b93c1525 in /usr/bin/python)
frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x55b7b93c0a08 in /usr/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x55b7b93c0653 in /usr/bin/python)
frame #52: Py_RunMain + 0x2be (0x55b7b93b341e in /usr/bin/python)
frame #53: Py_BytesMain + 0x2d (0x55b7b9389cad in /usr/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7f6dc6978d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7f6dc6978e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #56: _start + 0x25 (0x55b7b9389ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 80, in __init__
    self.initialize_word_embeddings(init_method_normal)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 106, in initialize_word_embeddings
    torch.distributed.all_reduce(self.word_embeddings_weight().data,
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
    work = group.allreduce([tensor], opts)
RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7ffb5a5b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7ffb5a56b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7ffb0cf9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7ffb0cf9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7ffb0cf91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7ffb0cf4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7ffb0cf4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7ffb0cf4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7ffb0cf4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7ffb0cf4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7ffac02d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7ffac02da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0xe27d37 (0x7ffac02e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7ffac02e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7ffac02ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4936f0a (0x7ffb0cf38f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494766f (0x7ffb0cf4966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4950b97 (0x7ffb0cf52b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x495f1ee (0x7ffb0cf611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb8cc15 (0x7ffb13722c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7ffb12f32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x560481a21e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x560481a185eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x560481a307bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x560481a108a2 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x560481a2270c in /usr/bin/python)
frame #26: PyObject_Call + 0x122 (0x560481a31192 in /usr/bin/python)
frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x560481a0d2c1 in /usr/bin/python)
frame #28: _PyFunction_Vectorcall + 0x7c (0x560481a2270c in /usr/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x560481a0c0d1 in /usr/bin/python)
frame #30: <unknown function> + 0x16e4e1 (0x560481a304e1 in /usr/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x560481a108a2 in /usr/bin/python)
frame #32: _PyFunction_Vectorcall + 0x7c (0x560481a2270c in /usr/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x16d (0x560481a1782d in /usr/bin/python)
frame #34: <unknown function> + 0x16a744 (0x560481a2c744 in /usr/bin/python)
frame #35: _PyObject_MakeTpCall + 0x1fc (0x560481a1858c in /usr/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x560481a11908 in /usr/bin/python)
frame #37: _PyFunction_Vectorcall + 0x7c (0x560481a2270c in /usr/bin/python)
frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x560481a0c0d1 in /usr/bin/python)
frame #39: _PyFunction_Vectorcall + 0x7c (0x560481a2270c in /usr/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x560481a0ae0d in /usr/bin/python)
frame #41: _PyFunction_Vectorcall + 0x7c (0x560481a2270c in /usr/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x560481a0ae0d in /usr/bin/python)
frame #43: _PyFunction_Vectorcall + 0x7c (0x560481a2270c in /usr/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x560481a0c0d1 in /usr/bin/python)
frame #45: <unknown function> + 0x239e56 (0x560481afbe56 in /usr/bin/python)
frame #46: PyEval_EvalCode + 0x86 (0x560481afbcf6 in /usr/bin/python)
frame #47: <unknown function> + 0x2647d8 (0x560481b267d8 in /usr/bin/python)
frame #48: <unknown function> + 0x25e0bb (0x560481b200bb in /usr/bin/python)
frame #49: <unknown function> + 0x264525 (0x560481b26525 in /usr/bin/python)
frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x560481b25a08 in /usr/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x560481b25653 in /usr/bin/python)
frame #52: Py_RunMain + 0x2be (0x560481b1841e in /usr/bin/python)
frame #53: Py_BytesMain + 0x2d (0x560481aeecad in /usr/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7ffb5bab7d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7ffb5bab7e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #56: _start + 0x25 (0x560481aeeba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 80, in __init__
    self.initialize_word_embeddings(init_method_normal)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 106, in initialize_word_embeddings
    torch.distributed.all_reduce(self.word_embeddings_weight().data,
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
    work = group.allreduce([tensor], opts)
RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fa317db0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7fa317d6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7fa2d459123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7fa2d459150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7fa2d4591816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fa2d454a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fa2d454a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fa2d454a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fa2d454a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fa2d454a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7fa2878d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7fa2878da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0xe27d37 (0x7fa2878e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7fa2878e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7fa2878ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4936f0a (0x7fa2d4538f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494766f (0x7fa2d454966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4950b97 (0x7fa2d4552b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x495f1ee (0x7fa2d45611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb8cc15 (0x7fa2dad22c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7fa2da532407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x5573497d3e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x5573497ca5eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x5573497e27bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x5573497c28a2 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x5573497d470c in /usr/bin/python)
frame #26: PyObject_Call + 0x122 (0x5573497e3192 in /usr/bin/python)
frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x5573497bf2c1 in /usr/bin/python)
frame #28: _PyFunction_Vectorcall + 0x7c (0x5573497d470c in /usr/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x5573497be0d1 in /usr/bin/python)
frame #30: <unknown function> + 0x16e4e1 (0x5573497e24e1 in /usr/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x5573497c28a2 in /usr/bin/python)
frame #32: _PyFunction_Vectorcall + 0x7c (0x5573497d470c in /usr/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x16d (0x5573497c982d in /usr/bin/python)
frame #34: <unknown function> + 0x16a744 (0x5573497de744 in /usr/bin/python)
frame #35: _PyObject_MakeTpCall + 0x1fc (0x5573497ca58c in /usr/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x5573497c3908 in /usr/bin/python)
frame #37: _PyFunction_Vectorcall + 0x7c (0x5573497d470c in /usr/bin/python)
frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x5573497be0d1 in /usr/bin/python)
frame #39: _PyFunction_Vectorcall + 0x7c (0x5573497d470c in /usr/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x5573497bce0d in /usr/bin/python)
frame #41: _PyFunction_Vectorcall + 0x7c (0x5573497d470c in /usr/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x5573497bce0d in /usr/bin/python)
frame #43: _PyFunction_Vectorcall + 0x7c (0x5573497d470c in /usr/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x5573497be0d1 in /usr/bin/python)
frame #45: <unknown function> + 0x239e56 (0x5573498ade56 in /usr/bin/python)
frame #46: PyEval_EvalCode + 0x86 (0x5573498adcf6 in /usr/bin/python)
frame #47: <unknown function> + 0x2647d8 (0x5573498d87d8 in /usr/bin/python)
frame #48: <unknown function> + 0x25e0bb (0x5573498d20bb in /usr/bin/python)
frame #49: <unknown function> + 0x264525 (0x5573498d8525 in /usr/bin/python)
frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x5573498d7a08 in /usr/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x5573498d7653 in /usr/bin/python)
frame #52: Py_RunMain + 0x2be (0x5573498ca41e in /usr/bin/python)
frame #53: Py_BytesMain + 0x2d (0x5573498a0cad in /usr/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7fa323089d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7fa323089e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #56: _start + 0x25 (0x5573498a0ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 80, in __init__
    self.initialize_word_embeddings(init_method_normal)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 106, in initialize_word_embeddings
    torch.distributed.all_reduce(self.word_embeddings_weight().data,
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
    work = group.allreduce([tensor], opts)
RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fea9b7b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7fea9b76b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7fea4e19123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7fea4e19150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7fea4e191816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fea4e14a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fea4e14a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fea4e14a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fea4e14a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fea4e14a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7fea014d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7fea014da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0xe27d37 (0x7fea014e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7fea014e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7fea014ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4936f0a (0x7fea4e138f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494766f (0x7fea4e14966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4950b97 (0x7fea4e152b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x495f1ee (0x7fea4e1611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb8cc15 (0x7fea54922c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7fea54132407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55a2cf16ae0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55a2cf1615eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55a2cf1797bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x55a2cf1598a2 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55a2cf16b70c in /usr/bin/python)
frame #26: PyObject_Call + 0x122 (0x55a2cf17a192 in /usr/bin/python)
frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x55a2cf1562c1 in /usr/bin/python)
frame #28: _PyFunction_Vectorcall + 0x7c (0x55a2cf16b70c in /usr/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x55a2cf1550d1 in /usr/bin/python)
frame #30: <unknown function> + 0x16e4e1 (0x55a2cf1794e1 in /usr/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x55a2cf1598a2 in /usr/bin/python)
frame #32: _PyFunction_Vectorcall + 0x7c (0x55a2cf16b70c in /usr/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x16d (0x55a2cf16082d in /usr/bin/python)
frame #34: <unknown function> + 0x16a744 (0x55a2cf175744 in /usr/bin/python)
frame #35: _PyObject_MakeTpCall + 0x1fc (0x55a2cf16158c in /usr/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x55a2cf15a908 in /usr/bin/python)
frame #37: _PyFunction_Vectorcall + 0x7c (0x55a2cf16b70c in /usr/bin/python)
frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x55a2cf1550d1 in /usr/bin/python)
frame #39: _PyFunction_Vectorcall + 0x7c (0x55a2cf16b70c in /usr/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x55a2cf153e0d in /usr/bin/python)
frame #41: _PyFunction_Vectorcall + 0x7c (0x55a2cf16b70c in /usr/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x55a2cf153e0d in /usr/bin/python)
frame #43: _PyFunction_Vectorcall + 0x7c (0x55a2cf16b70c in /usr/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x55a2cf1550d1 in /usr/bin/python)
frame #45: <unknown function> + 0x239e56 (0x55a2cf244e56 in /usr/bin/python)
frame #46: PyEval_EvalCode + 0x86 (0x55a2cf244cf6 in /usr/bin/python)
frame #47: <unknown function> + 0x2647d8 (0x55a2cf26f7d8 in /usr/bin/python)
frame #48: <unknown function> + 0x25e0bb (0x55a2cf2690bb in /usr/bin/python)
frame #49: <unknown function> + 0x264525 (0x55a2cf26f525 in /usr/bin/python)
frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x55a2cf26ea08 in /usr/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x55a2cf26e653 in /usr/bin/python)
frame #52: Py_RunMain + 0x2be (0x55a2cf26141e in /usr/bin/python)
frame #53: Py_BytesMain + 0x2d (0x55a2cf237cad in /usr/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7fea9cd58d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7fea9cd58e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #56: _start + 0x25 (0x55a2cf237ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
[E ProcessGroupNCCL.cpp:467] [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600506 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600771 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600826 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 10] NCCL watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600771 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 10] NCCL watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600771 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 9] NCCL watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600826 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 9] NCCL watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600826 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 8] NCCL watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600506 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 8] NCCL watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600506 milliseconds before timing out.
[2024-02-09 15:52:01,371] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 241778 closing signal SIGTERM
[2024-02-09 15:52:01,372] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 241780 closing signal SIGTERM
[2024-02-09 15:52:03,488] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 1 (pid: 241779) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_15:52:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 241781)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_15:52:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 241782)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_15:52:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 241783)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_15:52:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 241784)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_15:52:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 241785)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_15:52:01
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : -6 (pid: 241779)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 241779
============================================================
13b, 16k, gbs=512: dp=1, tp=8, pp=2, mbs=2
LOCAL_IP = 10.64.24.52
DP=1, MP=8, PP=2
[2024-02-09 15:55:06,205] torch.distributed.run: [WARNING] 
[2024-02-09 15:55:06,205] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 15:55:06,205] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 15:55:06,205] torch.distributed.run: [WARNING] *****************************************

SYM206-GPU-A0206-P2-Node52:242169:242333 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:242172:242334 [5] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:242168:242354 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:242173:242344 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    work = default_pg.barrier(opts=opts)
    return func(*args, **kwargs)
torch.distributed  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:242167:242356 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:242174:242355 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron

SYM206-GPU-A0206-P2-Node52:242171:242357 [4] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>

SYM206-GPU-A0206-P2-Node52:242170:242358 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
[2024-02-09 15:55:21,228] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 242171 closing signal SIGTERM
[2024-02-09 15:55:21,442] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 242167) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_15:55:21
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 242168)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_15:55:21
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 242169)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_15:55:21
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 242170)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_15:55:21
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 242172)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_15:55:21
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 242173)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_15:55:21
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 242174)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_15:55:21
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 242167)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=1, tp=8, pp=2, mbs=1
LOCAL_IP = 10.64.24.52
DP=1, MP=8, PP=2
[2024-02-09 15:58:34,115] torch.distributed.run: [WARNING] 
[2024-02-09 15:58:34,115] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 15:58:34,115] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 15:58:34,115] torch.distributed.run: [WARNING] *****************************************

SYM206-GPU-A0206-P2-Node52:242703:242865 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:242699:242864 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:242701:242866 [4] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:242700:242888 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:242704:242885 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies

SYM206-GPU-A0206-P2-Node52:242698:242886 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:242702:242889 [5] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:242697:242887 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
[2024-02-09 15:58:49,137] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 242697 closing signal SIGTERM
[2024-02-09 15:58:49,137] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 242698 closing signal SIGTERM
[2024-02-09 15:58:49,138] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 242699 closing signal SIGTERM
[2024-02-09 15:58:49,138] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 242700 closing signal SIGTERM
[2024-02-09 15:58:49,139] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 242701 closing signal SIGTERM
[2024-02-09 15:58:49,139] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 242702 closing signal SIGTERM
[2024-02-09 15:58:49,139] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 242704 closing signal SIGTERM
[2024-02-09 15:58:49,844] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 6 (pid: 242703) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_15:58:49
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 242703)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=1, tp=2, pp=8, mbs=32
LOCAL_IP = 10.64.24.52
DP=1, MP=2, PP=8
[2024-02-09 16:01:02,463] torch.distributed.run: [WARNING] 
[2024-02-09 16:01:02,463] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 16:01:02,463] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 16:01:02,463] torch.distributed.run: [WARNING] *****************************************

SYM206-GPU-A0206-P2-Node52:243061:243183 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:243058:243204 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:243060:243243 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:243059:243238 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:243064:243242 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:243065:243232 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:243063:243237 [5] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:243062:243250 [4] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
[2024-02-09 16:01:17,487] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 243058) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_16:01:17
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 243059)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_16:01:17
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 243060)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_16:01:17
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 243061)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_16:01:17
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 243062)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_16:01:17
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 243063)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_16:01:17
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 243064)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_16:01:17
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 243065)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_16:01:17
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 243058)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=1, tp=2, pp=8, mbs=16
LOCAL_IP = 10.64.24.52
DP=1, MP=2, PP=8
[2024-02-09 16:04:34,862] torch.distributed.run: [WARNING] 
[2024-02-09 16:04:34,862] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 16:04:34,862] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 16:04:34,862] torch.distributed.run: [WARNING] *****************************************

SYM206-GPU-A0206-P2-Node52:243409:243543 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:243412:243561 [4] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:243415:243597 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:243410:243582 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:243411:243571 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies

SYM206-GPU-A0206-P2-Node52:243408:243586 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:243413:243599 [5] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:243414:243592 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
[2024-02-09 16:04:49,887] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 243413 closing signal SIGTERM
[2024-02-09 16:04:50,051] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 243408) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_16:04:49
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 243409)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_16:04:49
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 243410)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_16:04:49
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 243411)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_16:04:49
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 243412)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_16:04:49
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 243414)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_16:04:49
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 243415)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_16:04:49
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 243408)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=1, tp=2, pp=8, mbs=8
LOCAL_IP = 10.64.24.52
DP=1, MP=2, PP=8
[2024-02-09 16:07:55,079] torch.distributed.run: [WARNING] 
[2024-02-09 16:07:55,079] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 16:07:55,079] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 16:07:55,079] torch.distributed.run: [WARNING] *****************************************

SYM206-GPU-A0206-P2-Node52:243762:243928 [4] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:243759:243894 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:243763:243932 [5] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:243760:243939 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:243761:243940 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:243765:243941 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:243764:243942 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:243758:243943 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
[2024-02-09 16:08:10,102] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 243758 closing signal SIGTERM
[2024-02-09 16:08:10,103] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 243761 closing signal SIGTERM
[2024-02-09 16:08:10,103] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 243764 closing signal SIGTERM
[2024-02-09 16:08:10,103] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 243765 closing signal SIGTERM
[2024-02-09 16:08:10,568] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 243759) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_16:08:10
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 243760)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_16:08:10
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 243762)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_16:08:10
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 243763)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_16:08:10
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 243759)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=1, tp=2, pp=8, mbs=4
LOCAL_IP = 10.64.24.52
DP=1, MP=2, PP=8
[2024-02-09 16:10:45,216] torch.distributed.run: [WARNING] 
[2024-02-09 16:10:45,216] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 16:10:45,216] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 16:10:45,216] torch.distributed.run: [WARNING] *****************************************

SYM206-GPU-A0206-P2-Node52:244112:244272 [4] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:244115:244271 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:244108:244270 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:244114:244282 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:244111:244279 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:244110:244292 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:244113:244291 [5] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain

SYM206-GPU-A0206-P2-Node52:244109:244293 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
[2024-02-09 16:11:00,239] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 244108 closing signal SIGTERM
[2024-02-09 16:11:00,239] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 244109 closing signal SIGTERM
[2024-02-09 16:11:00,240] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 244110 closing signal SIGTERM
[2024-02-09 16:11:00,241] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 244111 closing signal SIGTERM
[2024-02-09 16:11:00,241] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 244113 closing signal SIGTERM
[2024-02-09 16:11:00,242] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 244114 closing signal SIGTERM
[2024-02-09 16:11:00,242] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 244115 closing signal SIGTERM
[2024-02-09 16:11:00,920] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 4 (pid: 244112) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_16:11:00
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 244112)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=1, tp=2, pp=8, mbs=2
LOCAL_IP = 10.64.24.52
DP=1, MP=2, PP=8
[2024-02-09 16:13:10,354] torch.distributed.run: [WARNING] 
[2024-02-09 16:13:10,354] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 16:13:10,354] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 16:13:10,354] torch.distributed.run: [WARNING] *****************************************

SYM206-GPU-A0206-P2-Node52:244463:244631 [5] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:244459:244632 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>

SYM206-GPU-A0206-P2-Node52:244461:244635 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:244464:244633 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>

SYM206-GPU-A0206-P2-Node52:244458:244611 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
    pretrain(train_dataset_provider,

SYM206-GPU-A0206-P2-Node52:244460:244624 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:244462:244643 [4] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:244465:244627 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
[2024-02-09 16:13:25,377] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 244458 closing signal SIGTERM
[2024-02-09 16:13:25,378] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 244459 closing signal SIGTERM
[2024-02-09 16:13:25,378] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 244460 closing signal SIGTERM
[2024-02-09 16:13:25,379] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 244461 closing signal SIGTERM
[2024-02-09 16:13:25,379] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 244462 closing signal SIGTERM
[2024-02-09 16:13:25,380] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 244464 closing signal SIGTERM
[2024-02-09 16:13:25,380] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 244465 closing signal SIGTERM
[2024-02-09 16:13:26,085] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 5 (pid: 244463) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_16:13:25
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 244463)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=1, tp=2, pp=8, mbs=1
LOCAL_IP = 10.64.24.52
DP=1, MP=2, PP=8
[2024-02-09 16:15:30,409] torch.distributed.run: [WARNING] 
[2024-02-09 16:15:30,409] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 16:15:30,409] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 16:15:30,409] torch.distributed.run: [WARNING] *****************************************

SYM206-GPU-A0206-P2-Node52:244808:244953 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:244815:244954 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:244811:244958 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:244813:244955 [5] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:244814:244991 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:244810:244990 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:244812:244992 [4] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:244809:244993 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
[2024-02-09 16:15:45,429] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 244809 closing signal SIGTERM
[2024-02-09 16:15:45,429] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 244810 closing signal SIGTERM
[2024-02-09 16:15:45,430] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 244812 closing signal SIGTERM
[2024-02-09 16:15:45,430] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 244813 closing signal SIGTERM
[2024-02-09 16:15:45,857] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 244808) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_16:15:45
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 244811)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_16:15:45
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 244814)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_16:15:45
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 244815)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_16:15:45
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 244808)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=1, tp=4, pp=4, mbs=32
LOCAL_IP = 10.64.24.52
DP=1, MP=4, PP=4
[2024-02-09 16:18:28,527] torch.distributed.run: [WARNING] 
[2024-02-09 16:18:28,527] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 16:18:28,527] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 16:18:28,527] torch.distributed.run: [WARNING] *****************************************

SYM206-GPU-A0206-P2-Node52:245158:245313 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:245165:245333 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron

SYM206-GPU-A0206-P2-Node52:245160:245321 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:245164:245323 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:245159:245343 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:245162:245340 [4] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:245163:245344 [5] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:245161:245345 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
[2024-02-09 16:18:43,546] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 245159 closing signal SIGTERM
[2024-02-09 16:18:43,547] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 245160 closing signal SIGTERM
[2024-02-09 16:18:43,547] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 245161 closing signal SIGTERM
[2024-02-09 16:18:43,547] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 245162 closing signal SIGTERM
[2024-02-09 16:18:43,547] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 245164 closing signal SIGTERM
[2024-02-09 16:18:43,993] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 245158) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_16:18:43
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 245163)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_16:18:43
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 245165)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_16:18:43
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 245158)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=1, tp=4, pp=4, mbs=16
LOCAL_IP = 10.64.24.52
DP=1, MP=4, PP=4
[2024-02-09 16:21:16,667] torch.distributed.run: [WARNING] 
[2024-02-09 16:21:16,667] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 16:21:16,667] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 16:21:16,667] torch.distributed.run: [WARNING] *****************************************

SYM206-GPU-A0206-P2-Node52:245516:245639 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed
SYM206-GPU-A0206-P2-Node52:245510:245640 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:245512:245693 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:245517:245698 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:245514:245697 [4] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:245511:245700 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:245515:245702 [5] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:245513:245696 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<41799> failed : Software caused connection abort
[2024-02-09 16:21:31,690] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 245510) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_16:21:31
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 245511)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_16:21:31
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 245512)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_16:21:31
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 245513)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_16:21:31
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 245514)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_16:21:31
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 245515)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_16:21:31
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 245516)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_16:21:31
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 245517)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_16:21:31
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 245510)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=1, tp=4, pp=4, mbs=8
LOCAL_IP = 10.64.24.52
DP=1, MP=4, PP=4
[2024-02-09 16:24:46,121] torch.distributed.run: [WARNING] 
[2024-02-09 16:24:46,121] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 16:24:46,121] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 16:24:46,121] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (2, 2): 786828800
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 786828800
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 786828800
 > number of parameters on (tensor, pipeline) model parallel rank (3, 2): 786828800
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
[E ProcessGroupNCCL.cpp:467] [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600004 milliseconds before timing out.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 80, in __init__
    self.initialize_word_embeddings(init_method_normal)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 106, in initialize_word_embeddings
    torch.distributed.all_reduce(self.word_embeddings_weight().data,
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
    work = group.allreduce([tensor], opts)
RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f4d1c7a4449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f4d1c75f1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f4cce39123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f4cce39150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f4cce391816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f4cce34a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f4cce34a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f4cce34a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f4cce34a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f4cce34a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f4c816d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f4c816da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0xe27d37 (0x7f4c816e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f4c816e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f4c816ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4936f0a (0x7f4cce338f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494766f (0x7f4cce34966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4950b97 (0x7f4cce352b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x495f1ee (0x7f4cce3611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb8cc15 (0x7f4cd4b22c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f4cd4332407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x5614f655ce0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x5614f65535eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x5614f656b7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x5614f654b8a2 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x5614f655d70c in /usr/bin/python)
frame #26: PyObject_Call + 0x122 (0x5614f656c192 in /usr/bin/python)
frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x5614f65482c1 in /usr/bin/python)
frame #28: _PyFunction_Vectorcall + 0x7c (0x5614f655d70c in /usr/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x5614f65470d1 in /usr/bin/python)
frame #30: <unknown function> + 0x16e4e1 (0x5614f656b4e1 in /usr/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x5614f654b8a2 in /usr/bin/python)
frame #32: _PyFunction_Vectorcall + 0x7c (0x5614f655d70c in /usr/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x16d (0x5614f655282d in /usr/bin/python)
frame #34: <unknown function> + 0x16a744 (0x5614f6567744 in /usr/bin/python)
frame #35: _PyObject_MakeTpCall + 0x1fc (0x5614f655358c in /usr/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x5614f654c908 in /usr/bin/python)
frame #37: _PyFunction_Vectorcall + 0x7c (0x5614f655d70c in /usr/bin/python)
frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x5614f65470d1 in /usr/bin/python)
frame #39: _PyFunction_Vectorcall + 0x7c (0x5614f655d70c in /usr/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x5614f6545e0d in /usr/bin/python)
frame #41: _PyFunction_Vectorcall + 0x7c (0x5614f655d70c in /usr/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x5614f6545e0d in /usr/bin/python)
frame #43: _PyFunction_Vectorcall + 0x7c (0x5614f655d70c in /usr/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x5614f65470d1 in /usr/bin/python)
frame #45: <unknown function> + 0x239e56 (0x5614f6636e56 in /usr/bin/python)
frame #46: PyEval_EvalCode + 0x86 (0x5614f6636cf6 in /usr/bin/python)
frame #47: <unknown function> + 0x2647d8 (0x5614f66617d8 in /usr/bin/python)
frame #48: <unknown function> + 0x25e0bb (0x5614f665b0bb in /usr/bin/python)
frame #49: <unknown function> + 0x264525 (0x5614f6661525 in /usr/bin/python)
frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x5614f6660a08 in /usr/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x5614f6660653 in /usr/bin/python)
frame #52: Py_RunMain + 0x2be (0x5614f665341e in /usr/bin/python)
frame #53: Py_BytesMain + 0x2d (0x5614f6629cad in /usr/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7f4d1cf70d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7f4d1cf70e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #56: _start + 0x25 (0x5614f6629ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
[E ProcessGroupNCCL.cpp:467] [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600041 milliseconds before timing out.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 80, in __init__
    self.initialize_word_embeddings(init_method_normal)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 106, in initialize_word_embeddings
    torch.distributed.all_reduce(self.word_embeddings_weight().data,
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
    work = group.allreduce([tensor], opts)
RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fc5c59b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7fc5c596b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7fc58cf9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7fc58cf9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7fc58cf91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fc58cf4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fc58cf4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fc58cf4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fc58cf4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fc58cf4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7fc5402d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7fc5402da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0xe27d37 (0x7fc5402e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7fc5402e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7fc5402ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4936f0a (0x7fc58cf38f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494766f (0x7fc58cf4966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4950b97 (0x7fc58cf52b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x495f1ee (0x7fc58cf611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb8cc15 (0x7fc593722c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7fc592f32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x563abeb49e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x563abeb405eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x563abeb587bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x563abeb388a2 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x563abeb4a70c in /usr/bin/python)
frame #26: PyObject_Call + 0x122 (0x563abeb59192 in /usr/bin/python)
frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x563abeb352c1 in /usr/bin/python)
frame #28: _PyFunction_Vectorcall + 0x7c (0x563abeb4a70c in /usr/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x563abeb340d1 in /usr/bin/python)
frame #30: <unknown function> + 0x16e4e1 (0x563abeb584e1 in /usr/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x563abeb388a2 in /usr/bin/python)
frame #32: _PyFunction_Vectorcall + 0x7c (0x563abeb4a70c in /usr/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x16d (0x563abeb3f82d in /usr/bin/python)
frame #34: <unknown function> + 0x16a744 (0x563abeb54744 in /usr/bin/python)
frame #35: _PyObject_MakeTpCall + 0x1fc (0x563abeb4058c in /usr/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x563abeb39908 in /usr/bin/python)
frame #37: _PyFunction_Vectorcall + 0x7c (0x563abeb4a70c in /usr/bin/python)
frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x563abeb340d1 in /usr/bin/python)
frame #39: _PyFunction_Vectorcall + 0x7c (0x563abeb4a70c in /usr/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x563abeb32e0d in /usr/bin/python)
frame #41: _PyFunction_Vectorcall + 0x7c (0x563abeb4a70c in /usr/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x563abeb32e0d in /usr/bin/python)
frame #43: _PyFunction_Vectorcall + 0x7c (0x563abeb4a70c in /usr/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x563abeb340d1 in /usr/bin/python)
frame #45: <unknown function> + 0x239e56 (0x563abec23e56 in /usr/bin/python)
frame #46: PyEval_EvalCode + 0x86 (0x563abec23cf6 in /usr/bin/python)
frame #47: <unknown function> + 0x2647d8 (0x563abec4e7d8 in /usr/bin/python)
frame #48: <unknown function> + 0x25e0bb (0x563abec480bb in /usr/bin/python)
frame #49: <unknown function> + 0x264525 (0x563abec4e525 in /usr/bin/python)
frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x563abec4da08 in /usr/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x563abec4d653 in /usr/bin/python)
frame #52: Py_RunMain + 0x2be (0x563abec4041e in /usr/bin/python)
frame #53: Py_BytesMain + 0x2d (0x563abec16cad in /usr/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7fc5db9eed90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7fc5db9eee40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #56: _start + 0x25 (0x563abec16ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 80, in __init__
    self.initialize_word_embeddings(init_method_normal)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 106, in initialize_word_embeddings
    torch.distributed.all_reduce(self.word_embeddings_weight().data,
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
    work = group.allreduce([tensor], opts)
RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fb8e95b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7fb8e956b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7fb8b0b9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7fb8b0b9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7fb8b0b91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fb8b0b4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fb8b0b4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fb8b0b4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fb8b0b4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fb8b0b4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7fb863ed73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7fb863eda908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0xe27d37 (0x7fb863ee6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7fb863ee82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7fb863eea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4936f0a (0x7fb8b0b38f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494766f (0x7fb8b0b4966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4950b97 (0x7fb8b0b52b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x495f1ee (0x7fb8b0b611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb8cc15 (0x7fb8b7322c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7fb8b6b32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55b0151dde0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55b0151d45eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55b0151ec7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x55b0151cc8a2 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55b0151de70c in /usr/bin/python)
frame #26: PyObject_Call + 0x122 (0x55b0151ed192 in /usr/bin/python)
frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x55b0151c92c1 in /usr/bin/python)
frame #28: _PyFunction_Vectorcall + 0x7c (0x55b0151de70c in /usr/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x55b0151c80d1 in /usr/bin/python)
frame #30: <unknown function> + 0x16e4e1 (0x55b0151ec4e1 in /usr/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x55b0151cc8a2 in /usr/bin/python)
frame #32: _PyFunction_Vectorcall + 0x7c (0x55b0151de70c in /usr/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x16d (0x55b0151d382d in /usr/bin/python)
frame #34: <unknown function> + 0x16a744 (0x55b0151e8744 in /usr/bin/python)
frame #35: _PyObject_MakeTpCall + 0x1fc (0x55b0151d458c in /usr/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x55b0151cd908 in /usr/bin/python)
frame #37: _PyFunction_Vectorcall + 0x7c (0x55b0151de70c in /usr/bin/python)
frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x55b0151c80d1 in /usr/bin/python)
frame #39: _PyFunction_Vectorcall + 0x7c (0x55b0151de70c in /usr/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x55b0151c6e0d in /usr/bin/python)
frame #41: _PyFunction_Vectorcall + 0x7c (0x55b0151de70c in /usr/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x55b0151c6e0d in /usr/bin/python)
frame #43: _PyFunction_Vectorcall + 0x7c (0x55b0151de70c in /usr/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x55b0151c80d1 in /usr/bin/python)
frame #45: <unknown function> + 0x239e56 (0x55b0152b7e56 in /usr/bin/python)
frame #46: PyEval_EvalCode + 0x86 (0x55b0152b7cf6 in /usr/bin/python)
frame #47: <unknown function> + 0x2647d8 (0x55b0152e27d8 in /usr/bin/python)
frame #48: <unknown function> + 0x25e0bb (0x55b0152dc0bb in /usr/bin/python)
frame #49: <unknown function> + 0x264525 (0x55b0152e2525 in /usr/bin/python)
frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x55b0152e1a08 in /usr/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x55b0152e1653 in /usr/bin/python)
frame #52: Py_RunMain + 0x2be (0x55b0152d441e in /usr/bin/python)
frame #53: Py_BytesMain + 0x2d (0x55b0152aacad in /usr/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7fb8ff5bcd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7fb8ff5bce40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #56: _start + 0x25 (0x55b0152aaba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 80, in __init__
    self.initialize_word_embeddings(init_method_normal)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 106, in initialize_word_embeddings
    torch.distributed.all_reduce(self.word_embeddings_weight().data,
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
    work = group.allreduce([tensor], opts)
RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f41a33ba449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f41a33751d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f4154f9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f4154f9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f4154f91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f4154f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f4154f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f4154f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f4154f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f4154f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f41082d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f41082da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0xe27d37 (0x7f41082e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f41082e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f41082ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4936f0a (0x7f4154f38f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494766f (0x7f4154f4966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4950b97 (0x7f4154f52b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x495f1ee (0x7f4154f611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb8cc15 (0x7f415b722c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f415af32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55d6f7383e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55d6f737a5eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55d6f73927bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x55d6f73728a2 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55d6f738470c in /usr/bin/python)
frame #26: PyObject_Call + 0x122 (0x55d6f7393192 in /usr/bin/python)
frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x55d6f736f2c1 in /usr/bin/python)
frame #28: _PyFunction_Vectorcall + 0x7c (0x55d6f738470c in /usr/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x55d6f736e0d1 in /usr/bin/python)
frame #30: <unknown function> + 0x16e4e1 (0x55d6f73924e1 in /usr/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x55d6f73728a2 in /usr/bin/python)
frame #32: _PyFunction_Vectorcall + 0x7c (0x55d6f738470c in /usr/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x16d (0x55d6f737982d in /usr/bin/python)
frame #34: <unknown function> + 0x16a744 (0x55d6f738e744 in /usr/bin/python)
frame #35: _PyObject_MakeTpCall + 0x1fc (0x55d6f737a58c in /usr/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x55d6f7373908 in /usr/bin/python)
frame #37: _PyFunction_Vectorcall + 0x7c (0x55d6f738470c in /usr/bin/python)
frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x55d6f736e0d1 in /usr/bin/python)
frame #39: _PyFunction_Vectorcall + 0x7c (0x55d6f738470c in /usr/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x55d6f736ce0d in /usr/bin/python)
frame #41: _PyFunction_Vectorcall + 0x7c (0x55d6f738470c in /usr/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x55d6f736ce0d in /usr/bin/python)
frame #43: _PyFunction_Vectorcall + 0x7c (0x55d6f738470c in /usr/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x55d6f736e0d1 in /usr/bin/python)
frame #45: <unknown function> + 0x239e56 (0x55d6f745de56 in /usr/bin/python)
frame #46: PyEval_EvalCode + 0x86 (0x55d6f745dcf6 in /usr/bin/python)
frame #47: <unknown function> + 0x2647d8 (0x55d6f74887d8 in /usr/bin/python)
frame #48: <unknown function> + 0x25e0bb (0x55d6f74820bb in /usr/bin/python)
frame #49: <unknown function> + 0x264525 (0x55d6f7488525 in /usr/bin/python)
frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x55d6f7487a08 in /usr/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x55d6f7487653 in /usr/bin/python)
frame #52: Py_RunMain + 0x2be (0x55d6f747a41e in /usr/bin/python)
frame #53: Py_BytesMain + 0x2d (0x55d6f7450cad in /usr/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7f41a3b86d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7f41a3b86e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #56: _start + 0x25 (0x55d6f7450ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
[E ProcessGroupNCCL.cpp:467] [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600711 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600873 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 8] NCCL watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600711 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 8] NCCL watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600711 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 9] NCCL watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600041 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 9] NCCL watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600041 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 10] NCCL watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600004 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 10] NCCL watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600004 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 11] NCCL watchdog thread terminated with exception: [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600873 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 11] NCCL watchdog thread terminated with exception: [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600873 milliseconds before timing out.
[2024-02-09 16:35:11,711] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 245864 closing signal SIGTERM
[2024-02-09 16:35:11,711] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 245865 closing signal SIGTERM
[2024-02-09 16:35:14,042] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 0 (pid: 245862) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_16:35:11
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : -6 (pid: 245863)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 245863
[2]:
  time      : 2024-02-09_16:35:11
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 245866)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_16:35:11
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 245867)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_16:35:11
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 245868)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_16:35:11
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 245869)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_16:35:11
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : -6 (pid: 245862)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 245862
============================================================
13b, 16k, gbs=512: dp=1, tp=4, pp=4, mbs=4
LOCAL_IP = 10.64.24.52
DP=1, MP=4, PP=4
[2024-02-09 16:38:16,776] torch.distributed.run: [WARNING] 
[2024-02-09 16:38:16,776] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 16:38:16,776] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 16:38:16,776] torch.distributed.run: [WARNING] *****************************************

SYM206-GPU-A0206-P2-Node52:246243:246385 [5] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:246242:246394 [4] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:246245:246399 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:246238:246395 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:246240:246424 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:246244:246423 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain

SYM206-GPU-A0206-P2-Node52:246241:246422 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron

SYM206-GPU-A0206-P2-Node52:246239:246425 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
[2024-02-09 16:38:31,799] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 246238 closing signal SIGTERM
[2024-02-09 16:38:31,800] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 246239 closing signal SIGTERM
[2024-02-09 16:38:31,800] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 246240 closing signal SIGTERM
[2024-02-09 16:38:31,801] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 246241 closing signal SIGTERM
[2024-02-09 16:38:31,801] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 246242 closing signal SIGTERM
[2024-02-09 16:38:31,802] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 246244 closing signal SIGTERM
[2024-02-09 16:38:31,802] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 246245 closing signal SIGTERM
[2024-02-09 16:38:32,417] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 5 (pid: 246243) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_16:38:31
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 246243)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=1, tp=4, pp=4, mbs=2
LOCAL_IP = 10.64.24.52
DP=1, MP=4, PP=4
[2024-02-09 16:40:45,048] torch.distributed.run: [WARNING] 
[2024-02-09 16:40:45,048] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 16:40:45,048] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 16:40:45,048] torch.distributed.run: [WARNING] *****************************************

SYM206-GPU-A0206-P2-Node52:246593:246766 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:246590:246748 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:246592:246743 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:246596:246744 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:246597:246776 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain

SYM206-GPU-A0206-P2-Node52:246591:246774 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies

SYM206-GPU-A0206-P2-Node52:246595:246775 [5] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:246594:246777 [4] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
[2024-02-09 16:41:00,071] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 246590 closing signal SIGTERM
[2024-02-09 16:41:00,071] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 246591 closing signal SIGTERM
[2024-02-09 16:41:00,071] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 246594 closing signal SIGTERM
[2024-02-09 16:41:00,071] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 246595 closing signal SIGTERM
[2024-02-09 16:41:00,072] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 246596 closing signal SIGTERM
[2024-02-09 16:41:00,512] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 2 (pid: 246592) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_16:41:00
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 246593)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_16:41:00
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 246597)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_16:41:00
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 246592)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
13b, 16k, gbs=512: dp=1, tp=4, pp=4, mbs=1
LOCAL_IP = 10.64.24.52
DP=1, MP=4, PP=4
[2024-02-09 16:43:27,204] torch.distributed.run: [WARNING] 
[2024-02-09 16:43:27,204] torch.distributed.run: [WARNING] *****************************************
[2024-02-09 16:43:27,204] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-09 16:43:27,204] torch.distributed.run: [WARNING] *****************************************

SYM206-GPU-A0206-P2-Node52:246946:247107 [4] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:246944:247127 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:246945:247119 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:246943:247120 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:246949:247115 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:246942:247104 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:246947:247128 [5] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:246948:247129 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.51<56991> failed : Software caused connection abort
[2024-02-09 16:43:47,234] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 246942) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-09_16:43:47
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 246943)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-09_16:43:47
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 246944)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-09_16:43:47
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 246945)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-09_16:43:47
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 246946)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-09_16:43:47
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 246947)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-09_16:43:47
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 246948)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-09_16:43:47
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 246949)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-09_16:43:47
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 246942)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

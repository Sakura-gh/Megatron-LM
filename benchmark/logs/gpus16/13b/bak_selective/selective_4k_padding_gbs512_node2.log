13b, 4k, gbs=512: dp=8, tp=2, pp=1, mbs=1
LOCAL_IP = 10.64.24.52
DP=8, MP=2, PP=1
[2024-02-12 04:48:40,926] torch.distributed.run: [WARNING] 
[2024-02-12 04:48:40,926] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 04:48:40,926] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 04:48:40,926] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 19
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 19
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 19
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 19
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper

  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 19
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 19
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 19
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 19
[2024-02-12 04:48:55,950] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 376305) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_04:48:55
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 376306)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-12_04:48:55
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 376307)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-12_04:48:55
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 376308)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-12_04:48:55
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 376309)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-12_04:48:55
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 376310)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-12_04:48:55
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 376311)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-12_04:48:55
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 376312)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_04:48:55
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 376305)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (478065): No such process
kill: (478071): No such process
kill: (478077): No such process
kill: (478083): No such process
10.64.24.50 kill done.
13b, 4k, gbs=512: dp=4, tp=2, pp=2, mbs=1
LOCAL_IP = 10.64.24.52
DP=4, MP=2, PP=2
[2024-02-12 04:52:17,485] torch.distributed.run: [WARNING] 
[2024-02-12 04:52:17,485] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 04:52:17,485] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 04:52:17,485] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 45
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 45
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 45
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[2024-02-12 04:52:32,504] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 376559 closing signal SIGTERM
[2024-02-12 04:52:32,869] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 376557) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_04:52:32
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 376558)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-12_04:52:32
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 376560)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-12_04:52:32
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 376561)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-12_04:52:32
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 376562)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-12_04:52:32
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 376563)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-12_04:52:32
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 376564)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_04:52:32
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 376557)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (478123): No such process
kill: (478129): No such process
kill: (478135): No such process
kill: (478141): No such process
10.64.24.50 kill done.
13b, 4k, gbs=512: dp=4, tp=4, pp=1, mbs=2
LOCAL_IP = 10.64.24.52
DP=4, MP=4, PP=1
[2024-02-12 04:55:49,831] torch.distributed.run: [WARNING] 
[2024-02-12 04:55:49,831] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 04:55:49,831] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 04:55:49,831] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 45
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 45
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 45
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 45
[2024-02-12 04:56:04,854] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 376812 closing signal SIGTERM
[2024-02-12 04:56:04,855] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 376815 closing signal SIGTERM
[2024-02-12 04:56:05,333] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 376809) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_04:56:04
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 376810)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-12_04:56:04
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 376811)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-12_04:56:04
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 376813)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-12_04:56:04
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 376814)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-12_04:56:04
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 376816)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_04:56:04
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 376809)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (478181): No such process
kill: (478187): No such process
kill: (478193): No such process
kill: (478199): No such process
10.64.24.50 kill done.
13b, 4k, gbs=512: dp=4, tp=4, pp=1, mbs=1
LOCAL_IP = 10.64.24.52
DP=4, MP=4, PP=1
[2024-02-12 04:59:12,298] torch.distributed.run: [WARNING] 
[2024-02-12 04:59:12,298] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 04:59:12,298] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 04:59:12,298] torch.distributed.run: [WARNING] *****************************************
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 45
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 45
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
      File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
_initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper

  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
        func_return = func(*args, **kwargs)func_return = func(*args, **kwargs)

  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 45
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 45
[2024-02-12 04:59:27,322] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 377068 closing signal SIGTERM
[2024-02-12 04:59:27,687] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 377061) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_04:59:27
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 377062)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-12_04:59:27
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 377063)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-12_04:59:27
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 377064)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-12_04:59:27
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 377065)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-12_04:59:27
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 377066)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-12_04:59:27
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 377067)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_04:59:27
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 377061)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (478239): No such process
kill: (478245): No such process
kill: (478251): No such process
kill: (478257): No such process
10.64.24.50 kill done.
13b, 4k, gbs=512: dp=4, tp=1, pp=4, mbs=1
LOCAL_IP = 10.64.24.52
DP=4, MP=1, PP=4
[2024-02-12 05:02:44,668] torch.distributed.run: [WARNING] 
[2024-02-12 05:02:44,668] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 05:02:44,668] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 05:02:44,668] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error

SYM206-GPU-A0206-P2-Node52:377320:377473 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:377319:377472 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>

SYM206-GPU-A0206-P2-Node52:377318:377475 [5] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain

SYM206-GPU-A0206-P2-Node52:377317:377474 [4] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
[2024-02-12 05:02:59,691] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 377315 closing signal SIGTERM
[2024-02-12 05:02:59,692] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 377316 closing signal SIGTERM
[2024-02-12 05:02:59,692] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 377317 closing signal SIGTERM
[2024-02-12 05:02:59,693] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 377318 closing signal SIGTERM
[2024-02-12 05:02:59,693] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 377319 closing signal SIGTERM
[2024-02-12 05:02:59,693] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 377320 closing signal SIGTERM
[2024-02-12 05:03:00,270] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 377313) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_05:02:59
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 377314)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_05:02:59
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 377313)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (478355): No such process
kill: (478361): No such process
kill: (478367): No such process
kill: (478373): No such process
10.64.24.50 kill done.
13b, 4k, gbs=512: dp=2, tp=4, pp=2, mbs=2
LOCAL_IP = 10.64.24.52
DP=2, MP=4, PP=2
[2024-02-12 05:05:27,209] torch.distributed.run: [WARNING] 
[2024-02-12 05:05:27,209] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 05:05:27,209] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 05:05:27,209] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1638548480
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1638548480
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 1638548480
 > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 1638548480
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.361 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.412 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.973 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.227 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (906.80, 973.34)
    train/valid/test-data-iterators-setup ..........: (0.02, 12366.92)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[2024-02-12 06:05:25,014] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-12 06:05:25,016] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 377625 closing signal SIGTERM
[2024-02-12 06:05:25,016] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 377626 closing signal SIGTERM
[2024-02-12 06:05:25,016] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 377627 closing signal SIGTERM
[2024-02-12 06:05:25,016] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 377628 closing signal SIGTERM
[2024-02-12 06:05:25,016] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 377629 closing signal SIGTERM
[2024-02-12 06:05:25,016] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 377630 closing signal SIGTERM
[2024-02-12 06:05:25,016] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 377631 closing signal SIGTERM
[2024-02-12 06:05:25,016] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 377632 closing signal SIGTERM
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (478471): No such process
kill: (478477): No such process
kill: (478483): No such process
kill: (478489): No such process
10.64.24.50 kill done.
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py", line 877, in _invoke_run
    time.sleep(monitor_interval)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 377551 got signal: 15
13b, 4k, gbs=512: dp=2, tp=4, pp=2, mbs=1
LOCAL_IP = 10.64.24.52
DP=2, MP=4, PP=2
[2024-02-12 06:06:01,419] torch.distributed.run: [WARNING] 
[2024-02-12 06:06:01,419] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 06:06:01,419] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 06:06:01,419] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 80, in __init__
    self.initialize_word_embeddings(init_method_normal)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 106, in initialize_word_embeddings
    torch.distributed.all_reduce(self.word_embeddings_weight().data,
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
    work = group.allreduce([tensor], opts)
RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f3b79db0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f3b79d6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f3b3659123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f3b3659150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f3b36591816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f3b3654a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f3b3654a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f3b3654a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f3b3654a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f3b3654a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f3ae98d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f3ae98da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0xe27d37 (0x7f3ae98e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f3ae98e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f3ae98ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4936f0a (0x7f3b36538f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494766f (0x7f3b3654966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4950b97 (0x7f3b36552b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x495f1ee (0x7f3b365611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb8cc15 (0x7f3b3cd22c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f3b3c532407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x564f053d7e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x564f053ce5eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x564f053e67bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x564f053c68a2 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x564f053d870c in /usr/bin/python)
frame #26: PyObject_Call + 0x122 (0x564f053e7192 in /usr/bin/python)
frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x564f053c32c1 in /usr/bin/python)
frame #28: _PyFunction_Vectorcall + 0x7c (0x564f053d870c in /usr/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x564f053c20d1 in /usr/bin/python)
frame #30: <unknown function> + 0x16e4e1 (0x564f053e64e1 in /usr/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x564f053c68a2 in /usr/bin/python)
frame #32: _PyFunction_Vectorcall + 0x7c (0x564f053d870c in /usr/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x16d (0x564f053cd82d in /usr/bin/python)
frame #34: <unknown function> + 0x16a744 (0x564f053e2744 in /usr/bin/python)
frame #35: _PyObject_MakeTpCall + 0x1fc (0x564f053ce58c in /usr/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x564f053c7908 in /usr/bin/python)
frame #37: _PyFunction_Vectorcall + 0x7c (0x564f053d870c in /usr/bin/python)
frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x564f053c20d1 in /usr/bin/python)
frame #39: _PyFunction_Vectorcall + 0x7c (0x564f053d870c in /usr/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x564f053c0e0d in /usr/bin/python)
frame #41: _PyFunction_Vectorcall + 0x7c (0x564f053d870c in /usr/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x564f053c0e0d in /usr/bin/python)
frame #43: _PyFunction_Vectorcall + 0x7c (0x564f053d870c in /usr/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x564f053c20d1 in /usr/bin/python)
frame #45: <unknown function> + 0x239e56 (0x564f054b1e56 in /usr/bin/python)
frame #46: PyEval_EvalCode + 0x86 (0x564f054b1cf6 in /usr/bin/python)
frame #47: <unknown function> + 0x2647d8 (0x564f054dc7d8 in /usr/bin/python)
frame #48: <unknown function> + 0x25e0bb (0x564f054d60bb in /usr/bin/python)
frame #49: <unknown function> + 0x264525 (0x564f054dc525 in /usr/bin/python)
frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x564f054dba08 in /usr/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x564f054db653 in /usr/bin/python)
frame #52: Py_RunMain + 0x2be (0x564f054ce41e in /usr/bin/python)
frame #53: Py_BytesMain + 0x2d (0x564f054a4cad in /usr/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7f3b8507cd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7f3b8507ce40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #56: _start + 0x25 (0x564f054a4ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 80, in __init__
    Traceback (most recent call last):
self.initialize_word_embeddings(init_method_normal)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 106, in initialize_word_embeddings
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    torch.distributed.all_reduce(self.word_embeddings_weight().data,
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
    work = group.allreduce([tensor], opts)
RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fb11cbc8449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7fb11cb831d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7fb0ce79123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7fb0ce79150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7fb0ce791816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fb0ce74a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fb0ce74a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fb0ce74a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fb0ce74a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fb0ce74a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7fb081ad73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7fb081ada908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0xe27d37 (0x7fb081ae6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7fb081ae82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7fb081aea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4936f0a (0x7fb0ce738f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494766f (0x7fb0ce74966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4950b97 (0x7fb0ce752b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x495f1ee (0x7fb0ce7611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb8cc15 (0x7fb0d4f22c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7fb0d4732407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x5628739ade0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x5628739a45eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x5628739bc7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x56287399c8a2 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x5628739ae70c in /usr/bin/python)
frame #26: PyObject_Call + 0x122 (0x5628739bd192 in /usr/bin/python)
frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x5628739992c1 in /usr/bin/python)
frame #28: _PyFunction_Vectorcall + 0x7c (0x5628739ae70c in /usr/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x5628739980d1 in /usr/bin/python)
frame #30: <unknown function> + 0x16e4e1 (0x5628739bc4e1 in /usr/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x56287399c8a2 in /usr/bin/python)
frame #32: _PyFunction_Vectorcall + 0x7c (0x5628739ae70c in /usr/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x16d (0x5628739a382d in /usr/bin/python)
frame #34: <unknown function> + 0x16a744 (0x5628739b8744 in /usr/bin/python)
frame #35: _PyObject_MakeTpCall + 0x1fc (0x5628739a458c in /usr/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x56287399d908 in /usr/bin/python)
frame #37: _PyFunction_Vectorcall + 0x7c (0x5628739ae70c in /usr/bin/python)
frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x5628739980d1 in /usr/bin/python)
frame #39: _PyFunction_Vectorcall + 0x7c (0x5628739ae70c in /usr/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x562873996e0d in /usr/bin/python)
frame #41: _PyFunction_Vectorcall + 0x7c (0x5628739ae70c in /usr/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x562873996e0d in /usr/bin/python)
frame #43: _PyFunction_Vectorcall + 0x7c (0x5628739ae70c in /usr/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x5628739980d1 in /usr/bin/python)
frame #45: <unknown function> + 0x239e56 (0x562873a87e56 in /usr/bin/python)
frame #46: PyEval_EvalCode + 0x86 (0x562873a87cf6 in /usr/bin/python)
frame #47: <unknown function> + 0x2647d8 (0x562873ab27d8 in /usr/bin/python)
frame #48: <unknown function> + 0x25e0bb (0x562873aac0bb in /usr/bin/python)
frame #49: <unknown function> + 0x264525 (0x562873ab2525 in /usr/bin/python)
frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x562873ab1a08 in /usr/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x562873ab1653 in /usr/bin/python)
frame #52: Py_RunMain + 0x2be (0x562873aa441e in /usr/bin/python)
frame #53: Py_BytesMain + 0x2d (0x562873a7acad in /usr/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7fb11d394d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7fb11d394e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #56: _start + 0x25 (0x562873a7aba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 80, in __init__
    self.initialize_word_embeddings(init_method_normal)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 106, in initialize_word_embeddings
    torch.distributed.all_reduce(self.word_embeddings_weight().data,
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
    work = group.allreduce([tensor], opts)
RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f7508fb0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f7508f6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f74c579123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f74c579150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f74c5791816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f74c574a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f74c574a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f74c574a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f74c574a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f74c574a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f7478ad73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f7478ada908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0xe27d37 (0x7f7478ae6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f7478ae82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f7478aea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4936f0a (0x7f74c5738f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494766f (0x7f74c574966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4950b97 (0x7f74c5752b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x495f1ee (0x7f74c57611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb8cc15 (0x7f74cbf22c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f74cb732407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55fcf0023e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55fcf001a5eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55fcf00327bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x55fcf00128a2 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55fcf002470c in /usr/bin/python)
frame #26: PyObject_Call + 0x122 (0x55fcf0033192 in /usr/bin/python)
frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x55fcf000f2c1 in /usr/bin/python)
frame #28: _PyFunction_Vectorcall + 0x7c (0x55fcf002470c in /usr/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x55fcf000e0d1 in /usr/bin/python)
frame #30: <unknown function> + 0x16e4e1 (0x55fcf00324e1 in /usr/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x55fcf00128a2 in /usr/bin/python)
frame #32: _PyFunction_Vectorcall + 0x7c (0x55fcf002470c in /usr/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x16d (0x55fcf001982d in /usr/bin/python)
frame #34: <unknown function> + 0x16a744 (0x55fcf002e744 in /usr/bin/python)
frame #35: _PyObject_MakeTpCall + 0x1fc (0x55fcf001a58c in /usr/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x55fcf0013908 in /usr/bin/python)
frame #37: _PyFunction_Vectorcall + 0x7c (0x55fcf002470c in /usr/bin/python)
frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x55fcf000e0d1 in /usr/bin/python)
frame #39: _PyFunction_Vectorcall + 0x7c (0x55fcf002470c in /usr/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x55fcf000ce0d in /usr/bin/python)
frame #41: _PyFunction_Vectorcall + 0x7c (0x55fcf002470c in /usr/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x55fcf000ce0d in /usr/bin/python)
frame #43: _PyFunction_Vectorcall + 0x7c (0x55fcf002470c in /usr/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x55fcf000e0d1 in /usr/bin/python)
frame #45: <unknown function> + 0x239e56 (0x55fcf00fde56 in /usr/bin/python)
frame #46: PyEval_EvalCode + 0x86 (0x55fcf00fdcf6 in /usr/bin/python)
frame #47: <unknown function> + 0x2647d8 (0x55fcf01287d8 in /usr/bin/python)
frame #48: <unknown function> + 0x25e0bb (0x55fcf01220bb in /usr/bin/python)
frame #49: <unknown function> + 0x264525 (0x55fcf0128525 in /usr/bin/python)
frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x55fcf0127a08 in /usr/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x55fcf0127653 in /usr/bin/python)
frame #52: Py_RunMain + 0x2be (0x55fcf011a41e in /usr/bin/python)
frame #53: Py_BytesMain + 0x2d (0x55fcf00f0cad in /usr/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7f7514250d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7f7514250e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #56: _start + 0x25 (0x55fcf00f0ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 80, in __init__
    self.initialize_word_embeddings(init_method_normal)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 106, in initialize_word_embeddings
    torch.distributed.all_reduce(self.word_embeddings_weight().data,
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
    work = group.allreduce([tensor], opts)
RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7ff1e73b7449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7ff1e73721d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7ff198f9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7ff198f9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7ff198f91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7ff198f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7ff198f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7ff198f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7ff198f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7ff198f4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7ff14c2d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7ff14c2da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0xe27d37 (0x7ff14c2e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7ff14c2e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7ff14c2ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4936f0a (0x7ff198f38f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494766f (0x7ff198f4966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4950b97 (0x7ff198f52b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x495f1ee (0x7ff198f611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb8cc15 (0x7ff19f722c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7ff19ef32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x5607f2f00e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x5607f2ef75eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x5607f2f0f7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x5607f2eef8a2 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x5607f2f0170c in /usr/bin/python)
frame #26: PyObject_Call + 0x122 (0x5607f2f10192 in /usr/bin/python)
frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x5607f2eec2c1 in /usr/bin/python)
frame #28: _PyFunction_Vectorcall + 0x7c (0x5607f2f0170c in /usr/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x5607f2eeb0d1 in /usr/bin/python)
frame #30: <unknown function> + 0x16e4e1 (0x5607f2f0f4e1 in /usr/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x5607f2eef8a2 in /usr/bin/python)
frame #32: _PyFunction_Vectorcall + 0x7c (0x5607f2f0170c in /usr/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x16d (0x5607f2ef682d in /usr/bin/python)
frame #34: <unknown function> + 0x16a744 (0x5607f2f0b744 in /usr/bin/python)
frame #35: _PyObject_MakeTpCall + 0x1fc (0x5607f2ef758c in /usr/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x5607f2ef0908 in /usr/bin/python)
frame #37: _PyFunction_Vectorcall + 0x7c (0x5607f2f0170c in /usr/bin/python)
frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x5607f2eeb0d1 in /usr/bin/python)
frame #39: _PyFunction_Vectorcall + 0x7c (0x5607f2f0170c in /usr/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x5607f2ee9e0d in /usr/bin/python)
frame #41: _PyFunction_Vectorcall + 0x7c (0x5607f2f0170c in /usr/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x5607f2ee9e0d in /usr/bin/python)
frame #43: _PyFunction_Vectorcall + 0x7c (0x5607f2f0170c in /usr/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x5607f2eeb0d1 in /usr/bin/python)
frame #45: <unknown function> + 0x239e56 (0x5607f2fdae56 in /usr/bin/python)
frame #46: PyEval_EvalCode + 0x86 (0x5607f2fdacf6 in /usr/bin/python)
frame #47: <unknown function> + 0x2647d8 (0x5607f30057d8 in /usr/bin/python)
frame #48: <unknown function> + 0x25e0bb (0x5607f2fff0bb in /usr/bin/python)
frame #49: <unknown function> + 0x264525 (0x5607f3005525 in /usr/bin/python)
frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x5607f3004a08 in /usr/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x5607f3004653 in /usr/bin/python)
frame #52: Py_RunMain + 0x2be (0x5607f2ff741e in /usr/bin/python)
frame #53: Py_BytesMain + 0x2d (0x5607f2fcdcad in /usr/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7ff1e7b83d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7ff1e7b83e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #56: _start + 0x25 (0x5607f2fcdba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 80, in __init__
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 80, in __init__
    self.initialize_word_embeddings(init_method_normal)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 106, in initialize_word_embeddings
    self.initialize_word_embeddings(init_method_normal)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 106, in initialize_word_embeddings
    torch.distributed.all_reduce(self.word_embeddings_weight().data,
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
    work = group.allreduce([tensor], opts)
RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f37fddb0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f37fdd6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f37b079123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f37b079150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f37b0791816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f37b074a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f37b074a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f37b074a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f37b074a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f37b074a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f3763ad73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f3763ada908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0xe27d37 (0x7f3763ae6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f3763ae82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f3763aea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4936f0a (0x7f37b0738f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494766f (0x7f37b074966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4950b97 (0x7f37b0752b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x495f1ee (0x7f37b07611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb8cc15 (0x7f37b6f22c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f37b6732407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x56207a1fde0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x56207a1f45eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x56207a20c7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x56207a1ec8a2 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x56207a1fe70c in /usr/bin/python)
frame #26: PyObject_Call + 0x122 (0x56207a20d192 in /usr/bin/python)
frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x56207a1e92c1 in /usr/bin/python)
frame #28: _PyFunction_Vectorcall + 0x7c (0x56207a1fe70c in /usr/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x56207a1e80d1 in /usr/bin/python)
frame #30: <unknown function> + 0x16e4e1 (0x56207a20c4e1 in /usr/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x56207a1ec8a2 in /usr/bin/python)
frame #32: _PyFunction_Vectorcall + 0x7c (0x56207a1fe70c in /usr/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x16d (0x56207a1f382d in /usr/bin/python)
frame #34: <unknown function> + 0x16a744 (0x56207a208744 in /usr/bin/python)
frame #35: _PyObject_MakeTpCall + 0x1fc (0x56207a1f458c in /usr/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x56207a1ed908 in /usr/bin/python)
frame #37: _PyFunction_Vectorcall + 0x7c (0x56207a1fe70c in /usr/bin/python)
frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x56207a1e80d1 in /usr/bin/python)
frame #39: _PyFunction_Vectorcall + 0x7c (0x56207a1fe70c in /usr/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x56207a1e6e0d in /usr/bin/python)
frame #41: _PyFunction_Vectorcall + 0x7c (0x56207a1fe70c in /usr/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x56207a1e6e0d in /usr/bin/python)
frame #43: _PyFunction_Vectorcall + 0x7c (0x56207a1fe70c in /usr/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x56207a1e80d1 in /usr/bin/python)
frame #45: <unknown function> + 0x239e56 (0x56207a2d7e56 in /usr/bin/python)
frame #46: PyEval_EvalCode + 0x86 (0x56207a2d7cf6 in /usr/bin/python)
frame #47: <unknown function> + 0x2647d8 (0x56207a3027d8 in /usr/bin/python)
frame #48: <unknown function> + 0x25e0bb (0x56207a2fc0bb in /usr/bin/python)
frame #49: <unknown function> + 0x264525 (0x56207a302525 in /usr/bin/python)
frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x56207a301a08 in /usr/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x56207a301653 in /usr/bin/python)
frame #52: Py_RunMain + 0x2be (0x56207a2f441e in /usr/bin/python)
frame #53: Py_BytesMain + 0x2d (0x56207a2cacad in /usr/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7f37ff2f6d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7f37ff2f6e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #56: _start + 0x25 (0x56207a2caba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
    torch.distributed.all_reduce(self.word_embeddings_weight().data,
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
    work = group.allreduce([tensor], opts)
RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fca365b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7fca3656b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7fc9f2d9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7fc9f2d9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7fc9f2d91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fc9f2d4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fc9f2d4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fc9f2d4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fc9f2d4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fc9f2d4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7fc9a60d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7fc9a60da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0xe27d37 (0x7fc9a60e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7fc9a60e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7fc9a60ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4936f0a (0x7fc9f2d38f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494766f (0x7fc9f2d4966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4950b97 (0x7fc9f2d52b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x495f1ee (0x7fc9f2d611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb8cc15 (0x7fc9f9522c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7fc9f8d32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55e176132e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55e1761295eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55e1761417bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x55e1761218a2 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55e17613370c in /usr/bin/python)
frame #26: PyObject_Call + 0x122 (0x55e176142192 in /usr/bin/python)
frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x55e17611e2c1 in /usr/bin/python)
frame #28: _PyFunction_Vectorcall + 0x7c (0x55e17613370c in /usr/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x55e17611d0d1 in /usr/bin/python)
frame #30: <unknown function> + 0x16e4e1 (0x55e1761414e1 in /usr/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x55e1761218a2 in /usr/bin/python)
frame #32: _PyFunction_Vectorcall + 0x7c (0x55e17613370c in /usr/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x16d (0x55e17612882d in /usr/bin/python)
frame #34: <unknown function> + 0x16a744 (0x55e17613d744 in /usr/bin/python)
frame #35: _PyObject_MakeTpCall + 0x1fc (0x55e17612958c in /usr/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x55e176122908 in /usr/bin/python)
frame #37: _PyFunction_Vectorcall + 0x7c (0x55e17613370c in /usr/bin/python)
frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x55e17611d0d1 in /usr/bin/python)
frame #39: _PyFunction_Vectorcall + 0x7c (0x55e17613370c in /usr/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x55e17611be0d in /usr/bin/python)
frame #41: _PyFunction_Vectorcall + 0x7c (0x55e17613370c in /usr/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x55e17611be0d in /usr/bin/python)
frame #43: _PyFunction_Vectorcall + 0x7c (0x55e17613370c in /usr/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x55e17611d0d1 in /usr/bin/python)
frame #45: <unknown function> + 0x239e56 (0x55e17620ce56 in /usr/bin/python)
frame #46: PyEval_EvalCode + 0x86 (0x55e17620ccf6 in /usr/bin/python)
frame #47: <unknown function> + 0x2647d8 (0x55e1762377d8 in /usr/bin/python)
frame #48: <unknown function> + 0x25e0bb (0x55e1762310bb in /usr/bin/python)
frame #49: <unknown function> + 0x264525 (0x55e176237525 in /usr/bin/python)
frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x55e176236a08 in /usr/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x55e176236653 in /usr/bin/python)
frame #52: Py_RunMain + 0x2be (0x55e17622941e in /usr/bin/python)
frame #53: Py_BytesMain + 0x2d (0x55e1761ffcad in /usr/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7fca41869d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7fca41869e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #56: _start + 0x25 (0x55e1761ffba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 80, in __init__
    self.initialize_word_embeddings(init_method_normal)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 106, in initialize_word_embeddings
    torch.distributed.all_reduce(self.word_embeddings_weight().data,
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    work = group.allreduce([tensor], opts)
RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f7ff8bb0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f7ff8b6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f7fb539123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f7fb539150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f7fb5391816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f7fb534a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f7fb534a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f7fb534a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f7fb534a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f7fb534a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f7f686d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f7f686da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0xe27d37 (0x7f7f686e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f7f686e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f7f686ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4936f0a (0x7f7fb5338f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494766f (0x7f7fb534966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4950b97 (0x7f7fb5352b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x495f1ee (0x7f7fb53611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb8cc15 (0x7f7fbbb22c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f7fbb332407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x56512a3fce0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x56512a3f35eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x56512a40b7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x56512a3eb8a2 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x56512a3fd70c in /usr/bin/python)
frame #26: PyObject_Call + 0x122 (0x56512a40c192 in /usr/bin/python)
frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x56512a3e82c1 in /usr/bin/python)
frame #28: _PyFunction_Vectorcall + 0x7c (0x56512a3fd70c in /usr/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x56512a3e70d1 in /usr/bin/python)
frame #30: <unknown function> + 0x16e4e1 (0x56512a40b4e1 in /usr/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x56512a3eb8a2 in /usr/bin/python)
frame #32: _PyFunction_Vectorcall + 0x7c (0x56512a3fd70c in /usr/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x16d (0x56512a3f282d in /usr/bin/python)
frame #34: <unknown function> + 0x16a744 (0x56512a407744 in /usr/bin/python)
frame #35: _PyObject_MakeTpCall + 0x1fc (0x56512a3f358c in /usr/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x56512a3ec908 in /usr/bin/python)
frame #37: _PyFunction_Vectorcall + 0x7c (0x56512a3fd70c in /usr/bin/python)
frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x56512a3e70d1 in /usr/bin/python)
frame #39: _PyFunction_Vectorcall + 0x7c (0x56512a3fd70c in /usr/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x56512a3e5e0d in /usr/bin/python)
frame #41: _PyFunction_Vectorcall + 0x7c (0x56512a3fd70c in /usr/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x56512a3e5e0d in /usr/bin/python)
frame #43: _PyFunction_Vectorcall + 0x7c (0x56512a3fd70c in /usr/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x56512a3e70d1 in /usr/bin/python)
frame #45: <unknown function> + 0x239e56 (0x56512a4d6e56 in /usr/bin/python)
frame #46: PyEval_EvalCode + 0x86 (0x56512a4d6cf6 in /usr/bin/python)
frame #47: <unknown function> + 0x2647d8 (0x56512a5017d8 in /usr/bin/python)
frame #48: <unknown function> + 0x25e0bb (0x56512a4fb0bb in /usr/bin/python)
frame #49: <unknown function> + 0x264525 (0x56512a501525 in /usr/bin/python)
frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x56512a500a08 in /usr/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x56512a500653 in /usr/bin/python)
frame #52: Py_RunMain + 0x2be (0x56512a4f341e in /usr/bin/python)
frame #53: Py_BytesMain + 0x2d (0x56512a4c9cad in /usr/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7f8003ea7d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7f8003ea7e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #56: _start + 0x25 (0x56512a4c9ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 80, in __init__
    self.initialize_word_embeddings(init_method_normal)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 106, in initialize_word_embeddings
    torch.distributed.all_reduce(self.word_embeddings_weight().data,
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
    work = group.allreduce([tensor], opts)
RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fa007fb0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7fa007f6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f9fc479123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f9fc479150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f9fc4791816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f9fc474a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f9fc474a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f9fc474a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f9fc474a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f9fc474a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f9f77ad73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f9f77ada908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0xe27d37 (0x7f9f77ae6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f9f77ae82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f9f77aea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4936f0a (0x7f9fc4738f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494766f (0x7f9fc474966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4950b97 (0x7f9fc4752b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x495f1ee (0x7f9fc47611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb8cc15 (0x7f9fcaf22c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f9fca732407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x5610bff88e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x5610bff7f5eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x5610bff977bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x5610bff778a2 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x5610bff8970c in /usr/bin/python)
frame #26: PyObject_Call + 0x122 (0x5610bff98192 in /usr/bin/python)
frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x5610bff742c1 in /usr/bin/python)
frame #28: _PyFunction_Vectorcall + 0x7c (0x5610bff8970c in /usr/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x5610bff730d1 in /usr/bin/python)
frame #30: <unknown function> + 0x16e4e1 (0x5610bff974e1 in /usr/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x5610bff778a2 in /usr/bin/python)
frame #32: _PyFunction_Vectorcall + 0x7c (0x5610bff8970c in /usr/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x16d (0x5610bff7e82d in /usr/bin/python)
frame #34: <unknown function> + 0x16a744 (0x5610bff93744 in /usr/bin/python)
frame #35: _PyObject_MakeTpCall + 0x1fc (0x5610bff7f58c in /usr/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x5610bff78908 in /usr/bin/python)
frame #37: _PyFunction_Vectorcall + 0x7c (0x5610bff8970c in /usr/bin/python)
frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x5610bff730d1 in /usr/bin/python)
frame #39: _PyFunction_Vectorcall + 0x7c (0x5610bff8970c in /usr/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x5610bff71e0d in /usr/bin/python)
frame #41: _PyFunction_Vectorcall + 0x7c (0x5610bff8970c in /usr/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x5610bff71e0d in /usr/bin/python)
frame #43: _PyFunction_Vectorcall + 0x7c (0x5610bff8970c in /usr/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x5610bff730d1 in /usr/bin/python)
frame #45: <unknown function> + 0x239e56 (0x5610c0062e56 in /usr/bin/python)
frame #46: PyEval_EvalCode + 0x86 (0x5610c0062cf6 in /usr/bin/python)
frame #47: <unknown function> + 0x2647d8 (0x5610c008d7d8 in /usr/bin/python)
frame #48: <unknown function> + 0x25e0bb (0x5610c00870bb in /usr/bin/python)
frame #49: <unknown function> + 0x264525 (0x5610c008d525 in /usr/bin/python)
frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x5610c008ca08 in /usr/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x5610c008c653 in /usr/bin/python)
frame #52: Py_RunMain + 0x2be (0x5610c007f41e in /usr/bin/python)
frame #53: Py_BytesMain + 0x2d (0x5610c0055cad in /usr/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7fa013278d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7fa013278e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #56: _start + 0x25 (0x5610c0055ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
[2024-02-12 06:16:27,056] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 378444) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_06:16:27
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 378445)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-12_06:16:27
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 378446)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-12_06:16:27
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 378447)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-12_06:16:27
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 378448)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-12_06:16:27
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 378449)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-12_06:16:27
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 378450)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-02-12_06:16:27
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 378451)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_06:16:27
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 378444)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (478587): No such process
kill: (478593): No such process
kill: (478599): No such process
kill: (478605): No such process
10.64.24.50 kill done.
13b, 4k, gbs=512: dp=2, tp=2, pp=4, mbs=2
LOCAL_IP = 10.64.24.52
DP=2, MP=2, PP=4
[2024-02-12 06:19:54,076] torch.distributed.run: [WARNING] 
[2024-02-12 06:19:54,076] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 06:19:54,076] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 06:19:54,076] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 1573350400
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1573350400
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1702466560
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 1702466560
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...

> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.832 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.848 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.891 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.483 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.175 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.245 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.289 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.257 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (42.85, 624.14)
    train/valid/test-data-iterators-setup ..........: (0.02, 11062.69)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[2024-02-12 07:19:51,924] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-12 07:19:51,925] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 378841 closing signal SIGTERM
[2024-02-12 07:19:51,925] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 378842 closing signal SIGTERM
[2024-02-12 07:19:51,925] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 378843 closing signal SIGTERM
[2024-02-12 07:19:51,925] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 378844 closing signal SIGTERM
[2024-02-12 07:19:51,925] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 378845 closing signal SIGTERM
[2024-02-12 07:19:51,925] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 378846 closing signal SIGTERM
[2024-02-12 07:19:51,925] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 378847 closing signal SIGTERM
[2024-02-12 07:19:51,925] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 378848 closing signal SIGTERM
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (478703): No such process
kill: (478709): No such process
kill: (478715): No such process
kill: (478721): No such process
10.64.24.50 kill done.
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py", line 877, in _invoke_run
    time.sleep(monitor_interval)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 378767 got signal: 15
13b, 4k, gbs=512: dp=2, tp=2, pp=4, mbs=1
LOCAL_IP = 10.64.24.52
DP=2, MP=2, PP=4
[2024-02-12 07:20:28,362] torch.distributed.run: [WARNING] 
[2024-02-12 07:20:28,362] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 07:20:28,362] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 07:20:28,362] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1573350400
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 1573350400
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 1702466560
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1702466560
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.534 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.706 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.725 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.734 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.085 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  4.993 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.025 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.201 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (45.75, 619.41)
    train/valid/test-data-iterators-setup ..........: (0.02, 10400.70)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[2024-02-12 08:20:26,188] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-12 08:20:26,189] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 380034 closing signal SIGTERM
[2024-02-12 08:20:26,190] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 380035 closing signal SIGTERM
[2024-02-12 08:20:26,190] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 380036 closing signal SIGTERM
[2024-02-12 08:20:26,190] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 380037 closing signal SIGTERM
[2024-02-12 08:20:26,190] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 380038 closing signal SIGTERM
[2024-02-12 08:20:26,190] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 380039 closing signal SIGTERM
[2024-02-12 08:20:26,190] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 380040 closing signal SIGTERM
[2024-02-12 08:20:26,190] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 380041 closing signal SIGTERM
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (478819): No such process
kill: (478825): No such process
kill: (478831): No such process
kill: (478837): No such process
10.64.24.50 kill done.
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py", line 877, in _invoke_run
    time.sleep(monitor_interval)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 379960 got signal: 15
13b, 4k, gbs=512: dp=2, tp=8, pp=1, mbs=2
LOCAL_IP = 10.64.24.52
DP=2, MP=8, PP=1
[2024-02-12 08:21:02,607] torch.distributed.run: [WARNING] 
[2024-02-12 08:21:02,607] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 08:21:02,607] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 08:21:02,607] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.283 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.046 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (450.59, 491.90)
    train/valid/test-data-iterators-setup ..........: (0.02, 10650.21)
NCCL version 2.19.3+cuda12.2
[E ProcessGroupNCCL.cpp:467] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=65, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800005 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=65, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800344 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=65, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800486 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=65, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800510 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=65, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800538 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=65, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800572 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=65, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800578 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=65, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800596 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 1] NCCL watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=65, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800486 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 1] NCCL watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=65, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800486 milliseconds before timing out.
[2024-02-12 08:52:09,490] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 381227 closing signal SIGTERM
[2024-02-12 08:52:09,491] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 381228 closing signal SIGTERM
[2024-02-12 08:52:09,492] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 381229 closing signal SIGTERM
[2024-02-12 08:52:09,493] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 381231 closing signal SIGTERM
[2024-02-12 08:52:09,493] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 381232 closing signal SIGTERM
[2024-02-12 08:52:09,494] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 381233 closing signal SIGTERM
[2024-02-12 08:52:09,494] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 381234 closing signal SIGTERM
[2024-02-12 08:52:10,550] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 3 (pid: 381230) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
pretrain_gpt.py FAILED
-------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_08:52:09
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : -6 (pid: 381230)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 381230
=======================================================
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (478935): No such process
kill: (478941): No such process
kill: (478947): No such process
kill: (478953): No such process
10.64.24.50 kill done.
13b, 4k, gbs=512: dp=2, tp=8, pp=1, mbs=1
LOCAL_IP = 10.64.24.52
DP=2, MP=8, PP=1
[2024-02-12 08:54:27,443] torch.distributed.run: [WARNING] 
[2024-02-12 08:54:27,443] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 08:54:27,443] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 08:54:27,443] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.207 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.008 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (440.03, 536.00)
    train/valid/test-data-iterators-setup ..........: (0.02, 11339.72)
NCCL version 2.19.3+cuda12.2
[E ProcessGroupNCCL.cpp:467] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=129, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800189 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=129, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800682 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=129, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800688 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=129, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800708 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=129, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800745 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=129, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800824 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=129, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800880 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=129, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800926 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 1] NCCL watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=129, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800682 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 1] NCCL watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=129, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800682 milliseconds before timing out.
[2024-02-12 09:25:39,309] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 381940 closing signal SIGTERM
[2024-02-12 09:25:39,310] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 381941 closing signal SIGTERM
[2024-02-12 09:25:39,311] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 381942 closing signal SIGTERM
[2024-02-12 09:25:39,312] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 381943 closing signal SIGTERM
[2024-02-12 09:25:39,313] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 381945 closing signal SIGTERM
[2024-02-12 09:25:39,314] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 381946 closing signal SIGTERM
[2024-02-12 09:25:39,315] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 381947 closing signal SIGTERM
[2024-02-12 09:25:40,270] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 4 (pid: 381944) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
pretrain_gpt.py FAILED
-------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_09:25:39
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : -6 (pid: 381944)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 381944
=======================================================
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (479051): No such process
kill: (479057): No such process
kill: (479063): No such process
kill: (479069): No such process
10.64.24.50 kill done.
13b, 4k, gbs=512: dp=2, tp=1, pp=8, mbs=1
LOCAL_IP = 10.64.24.52
DP=2, MP=1, PP=8
[2024-02-12 09:27:57,145] torch.distributed.run: [WARNING] 
[2024-02-12 09:27:57,145] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 09:27:57,145] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 09:27:57,145] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 1573196800
 > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 1573196800
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 1573196800
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 1830763520
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...

> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.673 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.815 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.818 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.823 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.821 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.862 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.878 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.754 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.989 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.054 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.065 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.084 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.228 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.248 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.259 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.692 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (37.04, 472.70)
    train/valid/test-data-iterators-setup ..........: (9976.05, 11793.23)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())

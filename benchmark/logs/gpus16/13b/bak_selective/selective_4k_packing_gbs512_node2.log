13b, 4k, gbs=512: dp=4, tp=2, pp=2, mbs=2
LOCAL_IP = 10.64.24.52
DP=4, MP=2, PP=2
[2024-02-12 02:38:14,816] torch.distributed.run: [WARNING] 
[2024-02-12 02:38:14,816] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 02:38:14,816] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 02:38:14,816] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 3275816960
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 3275816960
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...

 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.171 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.357 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.525 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.555 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.128 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.195 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.067 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.326 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (921.93, 981.69)
    train/valid/test-data-iterators-setup ..........: (0.02, 12966.90)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 10996.3 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.095017E+01 | loss scale: 1.0 | grad norm: 8.128 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 9] (after 10 iterations) memory (MB) | allocated: 28242.5107421875 | max allocated: 45603.71435546875 | reserved: 49544.0 | max reserved: 49544.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 28241.7177734375 | max allocated: 45600.60693359375 | reserved: 49518.0 | max reserved: 49518.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (10343.80, 10432.95)
    forward-compute ................................: (1762.48, 5893.82)
    backward-compute ...............................: (3246.46, 3851.94)
    batch-generator ................................: (249.85, 274.59)
    forward-recv ...................................: (264.10, 281.02)
    forward-send ...................................: (2.53, 7.52)
    backward-recv ..................................: (94.43, 123.57)
    backward-send ..................................: (0.67, 0.94)
    forward-send-backward-recv .....................: (4910.01, 5252.52)
    backward-send-forward-recv .....................: (555.86, 724.34)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (11.19, 11.45)
    grads-reduce-scatter ...........................: (36.20, 447.65)
    params-all-gather ..............................: (19.53, 19.74)
    optimizer-copy-to-main-grad ....................: (0.36, 0.46)
    optimizer-clip-main-grad .......................: (7.33, 7.34)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.32, 9.60)
    optimizer-copy-main-to-model-params ............: (2.67, 2.74)
    optimizer ......................................: (20.89, 20.99)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 9066.8 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.082488E+01 | loss scale: 1.0 | grad norm: 5.741 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8876.73, 8929.77)
    forward-compute ................................: (1685.16, 4780.30)
    backward-compute ...............................: (3004.12, 3436.04)
    batch-generator ................................: (51.07, 63.44)
    forward-recv ...................................: (17.35, 22.04)
    forward-send ...................................: (0.41, 0.50)
    backward-recv ..................................: (54.74, 94.38)
    backward-send ..................................: (0.57, 2.55)
    forward-send-backward-recv .....................: (3850.97, 4120.39)
    backward-send-forward-recv .....................: (681.56, 856.03)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (11.03, 11.69)
    grads-reduce-scatter ...........................: (36.29, 36.74)
    params-all-gather ..............................: (19.51, 19.72)
    optimizer-copy-to-main-grad ....................: (0.34, 0.45)
    optimizer-clip-main-grad .......................: (4.26, 4.34)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.07, 9.33)
    optimizer-copy-main-to-model-params ............: (2.67, 2.74)
    optimizer ......................................: (17.25, 17.32)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 9666.5 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.059042E+01 | loss scale: 1.0 | grad norm: 2.231 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9476.48, 9550.00)
    forward-compute ................................: (1844.74, 4986.14)
    backward-compute ...............................: (3319.85, 3718.67)
    batch-generator ................................: (49.48, 68.00)
    forward-recv ...................................: (20.43, 29.18)
    forward-send ...................................: (0.46, 0.64)
    backward-recv ..................................: (64.93, 80.96)
    backward-send ..................................: (0.48, 18.48)
    forward-send-backward-recv .....................: (4178.03, 4274.54)
    backward-send-forward-recv .....................: (816.06, 851.98)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (11.05, 11.33)
    grads-reduce-scatter ...........................: (36.13, 36.75)
    params-all-gather ..............................: (19.57, 19.75)
    optimizer-copy-to-main-grad ....................: (0.34, 0.48)
    optimizer-clip-main-grad .......................: (4.24, 4.26)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.08, 9.31)
    optimizer-copy-main-to-model-params ............: (2.67, 2.74)
    optimizer ......................................: (17.13, 17.20)
Mon Feb 12 02:45:17 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   36C    P0             451W / 700W |  55802MiB / 81559MiB |     76%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   42C    P0             455W / 700W |  56068MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   42C    P0             328W / 700W |  55906MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0             392W / 700W |  55920MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   36C    P0             234W / 700W |  56966MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   42C    P0             241W / 700W |  56934MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   40C    P0             223W / 700W |  56846MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   35C    P0             226W / 700W |  56696MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 8881.2 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.057100E+01 | loss scale: 1.0 | grad norm: 1.542 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8593.15, 8675.31)
    forward-compute ................................: (1690.93, 4452.43)
    backward-compute ...............................: (3049.00, 3480.35)
    batch-generator ................................: (51.50, 67.94)
    forward-recv ...................................: (20.53, 32.12)
    forward-send ...................................: (0.47, 0.68)
    backward-recv ..................................: (86.97, 98.76)
    backward-send ..................................: (0.46, 0.69)
    forward-send-backward-recv .....................: (3556.04, 3778.23)
    backward-send-forward-recv .....................: (719.85, 759.80)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (10.95, 11.61)
    grads-reduce-scatter ...........................: (36.37, 36.79)
    params-all-gather ..............................: (19.54, 19.76)
    optimizer-copy-to-main-grad ....................: (0.33, 0.44)
    optimizer-clip-main-grad .......................: (3.98, 4.03)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.07, 9.25)
    optimizer-copy-main-to-model-params ............: (2.67, 2.74)
    optimizer ......................................: (16.84, 16.92)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 9799.3 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.047755E+01 | loss scale: 1.0 | grad norm: 2.600 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9599.58, 9680.38)
    forward-compute ................................: (1591.71, 5318.78)
    backward-compute ...............................: (3336.91, 3779.61)
    batch-generator ................................: (52.65, 66.70)
    forward-recv ...................................: (24.15, 28.22)
    forward-send ...................................: (0.54, 0.59)
    backward-recv ..................................: (62.90, 122.15)
    backward-send ..................................: (0.54, 20.40)
    forward-send-backward-recv .....................: (4406.87, 4619.25)
    backward-send-forward-recv .....................: (531.31, 633.70)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (11.05, 11.46)
    grads-reduce-scatter ...........................: (36.23, 36.74)
    params-all-gather ..............................: (19.55, 19.74)
    optimizer-copy-to-main-grad ....................: (0.34, 0.43)
    optimizer-clip-main-grad .......................: (4.24, 4.26)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.07, 9.23)
    optimizer-copy-main-to-model-params ............: (2.67, 2.74)
    optimizer ......................................: (17.04, 17.10)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 9244.4 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049476E+01 | loss scale: 1.0 | grad norm: 1.704 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9046.43, 9110.87)
    forward-compute ................................: (1781.43, 4782.46)
    backward-compute ...............................: (3218.08, 3660.94)
    batch-generator ................................: (51.21, 66.85)
    forward-recv ...................................: (17.97, 25.12)
    forward-send ...................................: (0.41, 0.55)
    backward-recv ..................................: (71.22, 98.28)
    backward-send ..................................: (0.70, 17.61)
    forward-send-backward-recv .....................: (3754.76, 3970.93)
    backward-send-forward-recv .....................: (689.27, 903.72)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (11.01, 11.45)
    grads-reduce-scatter ...........................: (36.26, 36.78)
    params-all-gather ..............................: (19.55, 19.75)
    optimizer-copy-to-main-grad ....................: (0.34, 0.43)
    optimizer-clip-main-grad .......................: (4.24, 4.26)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.07, 9.23)
    optimizer-copy-main-to-model-params ............: (2.67, 2.74)
    optimizer ......................................: (17.02, 17.09)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 9333.3 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047546E+01 | loss scale: 1.0 | grad norm: 1.209 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9134.42, 9200.33)
    forward-compute ................................: (1772.55, 4850.65)
    backward-compute ...............................: (3203.05, 3591.59)
    batch-generator ................................: (51.40, 67.35)
    forward-recv ...................................: (18.59, 28.37)
    forward-send ...................................: (0.43, 0.58)
    backward-recv ..................................: (50.13, 93.36)
    backward-send ..................................: (0.61, 7.86)
    forward-send-backward-recv .....................: (3943.43, 4107.29)
    backward-send-forward-recv .....................: (685.97, 814.74)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (11.03, 11.30)
    grads-reduce-scatter ...........................: (36.34, 36.86)
    params-all-gather ..............................: (19.56, 19.74)
    optimizer-copy-to-main-grad ....................: (0.33, 0.44)
    optimizer-clip-main-grad .......................: (3.99, 4.00)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.06, 9.37)
    optimizer-copy-main-to-model-params ............: (2.67, 2.74)
    optimizer ......................................: (17.09, 17.16)
Mon Feb 12 02:51:31 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   38C    P0             490W / 700W |  55802MiB / 81559MiB |     56%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   44C    P0             416W / 700W |  56070MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   44C    P0             474W / 700W |  55906MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             273W / 700W |  55920MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             253W / 700W |  56966MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             340W / 700W |  56934MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   42C    P0             346W / 700W |  56846MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             434W / 700W |  56696MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 9040.5 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044217E+01 | loss scale: 1.0 | grad norm: 1.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8752.86, 8831.14)
    forward-compute ................................: (1551.80, 4647.12)
    backward-compute ...............................: (3269.59, 3691.70)
    batch-generator ................................: (51.04, 68.26)
    forward-recv ...................................: (18.85, 25.18)
    forward-send ...................................: (0.43, 0.56)
    backward-recv ..................................: (83.03, 99.06)
    backward-send ..................................: (0.59, 6.01)
    forward-send-backward-recv .....................: (3717.53, 3851.99)
    backward-send-forward-recv .....................: (496.71, 579.46)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (10.97, 11.47)
    grads-reduce-scatter ...........................: (36.29, 36.85)
    params-all-gather ..............................: (19.56, 19.73)
    optimizer-copy-to-main-grad ....................: (0.34, 0.43)
    optimizer-clip-main-grad .......................: (4.23, 4.25)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.07, 9.21)
    optimizer-copy-main-to-model-params ............: (2.67, 2.74)
    optimizer ......................................: (16.98, 17.06)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 9418.6 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.043006E+01 | loss scale: 1.0 | grad norm: 1.318 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9194.67, 9276.48)
    forward-compute ................................: (1764.62, 5000.06)
    backward-compute ...............................: (3234.07, 3730.38)
    batch-generator ................................: (51.93, 67.42)
    forward-recv ...................................: (21.16, 32.25)
    forward-send ...................................: (0.48, 0.69)
    backward-recv ..................................: (59.11, 85.93)
    backward-send ..................................: (0.60, 11.37)
    forward-send-backward-recv .....................: (3905.37, 4186.00)
    backward-send-forward-recv .....................: (646.44, 884.09)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (11.04, 11.29)
    grads-reduce-scatter ...........................: (36.26, 36.74)
    params-all-gather ..............................: (19.57, 19.74)
    optimizer-copy-to-main-grad ....................: (0.34, 0.43)
    optimizer-clip-main-grad .......................: (3.74, 3.76)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.07, 9.23)
    optimizer-copy-main-to-model-params ............: (2.67, 2.74)
    optimizer ......................................: (16.51, 16.58)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 9102.8 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041189E+01 | loss scale: 1.0 | grad norm: 0.743 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8882.17, 8961.12)
    forward-compute ................................: (1738.11, 4678.54)
    backward-compute ...............................: (3170.13, 3741.41)
    batch-generator ................................: (51.22, 67.87)
    forward-recv ...................................: (18.16, 26.40)
    forward-send ...................................: (0.42, 0.59)
    backward-recv ..................................: (66.70, 98.23)
    backward-send ..................................: (0.78, 13.50)
    forward-send-backward-recv .....................: (3603.32, 3931.35)
    backward-send-forward-recv .....................: (701.08, 807.32)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (11.04, 11.41)
    grads-reduce-scatter ...........................: (36.29, 36.72)
    params-all-gather ..............................: (19.59, 19.70)
    optimizer-copy-to-main-grad ....................: (0.34, 0.43)
    optimizer-clip-main-grad .......................: (3.49, 3.66)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.07, 9.23)
    optimizer-copy-main-to-model-params ............: (2.67, 2.74)
    optimizer ......................................: (16.42, 16.50)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (476862): No such process
kill: (476893): No such process
kill: (476905): No such process
kill: (476917): No such process
10.64.24.50 kill done.
13b, 4k, gbs=512: dp=4, tp=4, pp=1, mbs=2
LOCAL_IP = 10.64.24.52
DP=4, MP=4, PP=1
[2024-02-12 02:56:53,693] torch.distributed.run: [WARNING] 
[2024-02-12 02:56:53,693] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 02:56:53,693] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 02:56:53,693] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.095 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.230 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.997 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.018 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (471.47, 506.38)
    train/valid/test-data-iterators-setup ..........: (0.02, 10574.75)
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 11218.4 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.094169E+01 | loss scale: 1.0 | grad norm: 7.903 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10969.34, 10990.93)
    forward-compute ................................: (6447.52, 6674.36)
    backward-compute ...............................: (4225.02, 4504.91)
    batch-generator ................................: (452.16, 505.94)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (107.64, 107.74)
    params-all-gather ..............................: (55.96, 56.33)
    optimizer-copy-to-main-grad ....................: (0.76, 1.11)
    optimizer-clip-main-grad .......................: (7.51, 7.53)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.26, 9.44)
    optimizer-copy-main-to-model-params ............: (2.90, 2.98)
    optimizer ......................................: (21.61, 21.70)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 9663.0 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.082710E+01 | loss scale: 1.0 | grad norm: 4.066 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9443.78, 9448.87)
    forward-compute ................................: (5342.64, 5478.44)
    backward-compute ...............................: (3927.10, 4066.16)
    batch-generator ................................: (64.26, 113.41)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (107.30, 107.51)
    params-all-gather ..............................: (55.93, 56.05)
    optimizer-copy-to-main-grad ....................: (0.68, 1.11)
    optimizer-clip-main-grad .......................: (4.49, 4.51)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.99, 9.08)
    optimizer-copy-main-to-model-params ............: (2.89, 2.98)
    optimizer ......................................: (18.16, 18.24)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 10644.0 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.059243E+01 | loss scale: 1.0 | grad norm: 1.720 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10427.88, 10439.31)
    forward-compute ................................: (6045.65, 6130.13)
    backward-compute ...............................: (4251.58, 4350.25)
    batch-generator ................................: (65.02, 115.27)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (107.34, 107.59)
    params-all-gather ..............................: (55.98, 56.22)
    optimizer-copy-to-main-grad ....................: (0.69, 1.11)
    optimizer-clip-main-grad .......................: (4.52, 4.54)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.01, 9.14)
    optimizer-copy-main-to-model-params ............: (2.89, 2.98)
    optimizer ......................................: (18.34, 18.42)
Mon Feb 12 03:04:32 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   35C    P0             208W / 700W |  65976MiB / 81559MiB |     34%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   40C    P0             230W / 700W |  65618MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   41C    P0             221W / 700W |  65574MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   35C    P0             252W / 700W |  65756MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   34C    P0             231W / 700W |  71870MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   40C    P0             241W / 700W |  71582MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   39C    P0             271W / 700W |  71060MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   34C    P0             193W / 700W |  71366MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 10749.3 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.057414E+01 | loss scale: 1.0 | grad norm: 1.674 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10428.18, 10454.45)
    forward-compute ................................: (6263.54, 6485.07)
    backward-compute ...............................: (3899.57, 4142.86)
    batch-generator ................................: (65.15, 114.34)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (107.29, 107.57)
    params-all-gather ..............................: (55.93, 56.05)
    optimizer-copy-to-main-grad ....................: (0.68, 1.14)
    optimizer-clip-main-grad .......................: (4.46, 4.49)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.00, 9.09)
    optimizer-copy-main-to-model-params ............: (2.89, 2.97)
    optimizer ......................................: (18.23, 18.32)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 11163.7 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.048092E+01 | loss scale: 1.0 | grad norm: 4.385 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10940.05, 10955.23)
    forward-compute ................................: (6419.34, 6546.61)
    backward-compute ...............................: (4335.81, 4491.26)
    batch-generator ................................: (67.01, 110.78)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (107.31, 107.49)
    params-all-gather ..............................: (55.95, 56.10)
    optimizer-copy-to-main-grad ....................: (0.65, 1.15)
    optimizer-clip-main-grad .......................: (4.52, 4.54)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.00, 9.10)
    optimizer-copy-main-to-model-params ............: (2.89, 2.97)
    optimizer ......................................: (18.19, 18.29)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 10632.1 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.050226E+01 | loss scale: 1.0 | grad norm: 2.773 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10406.84, 10427.54)
    forward-compute ................................: (6099.07, 6259.46)
    backward-compute ...............................: (4120.59, 4285.14)
    batch-generator ................................: (66.72, 85.23)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (107.28, 107.61)
    params-all-gather ..............................: (55.95, 56.07)
    optimizer-copy-to-main-grad ....................: (0.64, 0.95)
    optimizer-clip-main-grad .......................: (4.52, 4.54)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.01, 9.17)
    optimizer-copy-main-to-model-params ............: (2.89, 2.98)
    optimizer ......................................: (18.36, 18.43)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 10481.4 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047758E+01 | loss scale: 1.0 | grad norm: 1.214 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10255.24, 10268.81)
    forward-compute ................................: (5992.29, 6075.32)
    backward-compute ...............................: (4146.53, 4225.45)
    batch-generator ................................: (65.66, 80.00)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (107.30, 107.63)
    params-all-gather ..............................: (55.97, 56.09)
    optimizer-copy-to-main-grad ....................: (0.67, 0.91)
    optimizer-clip-main-grad .......................: (4.26, 4.28)
    optimizer-count-zeros ..........................: (0.01, 0.02)
    optimizer-inner-step ...........................: (9.01, 9.08)
    optimizer-copy-main-to-model-params ............: (2.89, 2.98)
    optimizer ......................................: (17.86, 17.95)
Mon Feb 12 03:11:53 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   34C    P0             294W / 700W |  65984MiB / 81559MiB |     81%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   40C    P0             328W / 700W |  65624MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   41C    P0             321W / 700W |  65584MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   34C    P0             364W / 700W |  65766MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   34C    P0             392W / 700W |  71870MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   40C    P0             344W / 700W |  71584MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   38C    P0             346W / 700W |  71060MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   34C    P0             371W / 700W |  71366MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 11891.3 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044111E+01 | loss scale: 1.0 | grad norm: 0.897 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (11577.24, 11594.19)
    forward-compute ................................: (7225.10, 7360.19)
    backward-compute ...............................: (4177.61, 4314.20)
    batch-generator ................................: (66.02, 79.84)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (107.32, 107.54)
    params-all-gather ..............................: (55.93, 56.07)
    optimizer-copy-to-main-grad ....................: (0.67, 0.92)
    optimizer-clip-main-grad .......................: (4.04, 4.06)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.99, 9.07)
    optimizer-copy-main-to-model-params ............: (2.89, 2.98)
    optimizer ......................................: (17.56, 17.64)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 10620.6 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.042868E+01 | loss scale: 1.0 | grad norm: 0.803 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10386.33, 10409.10)
    forward-compute ................................: (5984.24, 6240.80)
    backward-compute ...............................: (4117.35, 4364.22)
    batch-generator ................................: (65.76, 80.10)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (107.28, 108.55)
    params-all-gather ..............................: (55.96, 56.09)
    optimizer-copy-to-main-grad ....................: (0.70, 0.89)
    optimizer-clip-main-grad .......................: (4.32, 4.34)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.01, 9.06)
    optimizer-copy-main-to-model-params ............: (2.89, 2.97)
    optimizer ......................................: (17.84, 17.93)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 10532.0 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041510E+01 | loss scale: 1.0 | grad norm: 3.052 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10304.46, 10321.22)
    forward-compute ................................: (5900.79, 6217.01)
    backward-compute ...............................: (4048.28, 4378.99)
    batch-generator ................................: (65.66, 79.52)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (107.30, 107.36)
    params-all-gather ..............................: (55.97, 56.07)
    optimizer-copy-to-main-grad ....................: (0.68, 0.95)
    optimizer-clip-main-grad .......................: (4.54, 4.56)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.02, 9.07)
    optimizer-copy-main-to-model-params ............: (2.89, 2.97)
    optimizer ......................................: (18.13, 18.21)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (476975): No such process
kill: (477009): No such process
kill: (477021): No such process
kill: (477033): No such process
10.64.24.50 kill done.
13b, 4k, gbs=512: dp=4, tp=1, pp=4, mbs=2
LOCAL_IP = 10.64.24.52
DP=4, MP=1, PP=4
[2024-02-12 03:17:42,701] torch.distributed.run: [WARNING] 
[2024-02-12 03:17:42,701] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 03:17:42,701] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 03:17:42,701] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 3146393600
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 3403960320
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304) > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)

 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.916 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.925 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.928 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.956 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.982 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.439 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.909 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  6.207 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.137 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.283 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.315 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.275 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.380 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.430 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.369 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.289 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (45.40, 635.70)
    train/valid/test-data-iterators-setup ..........: (10322.07, 11852.07)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 11037.6 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.095132E+01 | loss scale: 1.0 | grad norm: 8.065 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 27090.8671875 | max allocated: 43428.1337890625 | reserved: 46620.0 | max reserved: 46620.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 29304.5380859375 | max allocated: 45163.19384765625 | reserved: 46088.0 | max reserved: 46088.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (10219.47, 10453.92)
    forward-compute ................................: (1120.16, 5748.13)
    backward-compute ...............................: (2675.56, 3808.59)
    batch-generator ................................: (67.41, 89.77)
    forward-recv ...................................: (87.52, 253.72)
    forward-send ...................................: (3.66, 145.78)
    backward-recv ..................................: (78.18, 402.61)
    backward-send ..................................: (0.63, 43.94)
    forward-send-backward-recv .....................: (4719.81, 5944.82)
    backward-send-forward-recv .....................: (616.73, 1150.74)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 22.32)
    grads-reduce-scatter ...........................: (37.90, 430.14)
    params-all-gather ..............................: (18.43, 20.06)
    optimizer-copy-to-main-grad ....................: (0.20, 0.27)
    optimizer-clip-main-grad .......................: (7.13, 7.35)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.89, 9.70)
    optimizer-copy-main-to-model-params ............: (2.45, 2.70)
    optimizer ......................................: (20.57, 20.82)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 9058.6 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.084526E+01 | loss scale: 1.0 | grad norm: 5.280 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8717.73, 8891.39)
    forward-compute ................................: (967.30, 4663.50)
    backward-compute ...............................: (2477.17, 3430.92)
    batch-generator ................................: (47.99, 53.82)
    forward-recv ...................................: (27.97, 402.12)
    forward-send ...................................: (0.43, 16.80)
    backward-recv ..................................: (40.88, 346.23)
    backward-send ..................................: (0.57, 53.58)
    forward-send-backward-recv .....................: (3913.41, 4667.61)
    backward-send-forward-recv .....................: (482.03, 1122.22)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.28)
    grads-reduce-scatter ...........................: (34.92, 38.22)
    params-all-gather ..............................: (18.42, 20.06)
    optimizer-copy-to-main-grad ....................: (0.18, 0.24)
    optimizer-clip-main-grad .......................: (4.06, 4.28)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.68, 9.44)
    optimizer-copy-main-to-model-params ............: (2.44, 2.69)
    optimizer ......................................: (16.83, 17.07)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 10395.1 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.059603E+01 | loss scale: 1.0 | grad norm: 2.675 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10014.27, 10197.47)
    forward-compute ................................: (1107.00, 5586.90)
    backward-compute ...............................: (2726.81, 3641.89)
    batch-generator ................................: (46.70, 53.95)
    forward-recv ...................................: (20.25, 334.67)
    forward-send ...................................: (0.45, 292.29)
    backward-recv ..................................: (51.55, 374.85)
    backward-send ..................................: (0.51, 50.71)
    forward-send-backward-recv .....................: (4661.82, 5901.58)
    backward-send-forward-recv .....................: (592.21, 1178.35)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.12)
    grads-reduce-scatter ...........................: (35.00, 38.08)
    params-all-gather ..............................: (18.44, 20.07)
    optimizer-copy-to-main-grad ....................: (0.18, 0.27)
    optimizer-clip-main-grad .......................: (4.06, 4.28)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.68, 9.42)
    optimizer-copy-main-to-model-params ............: (2.45, 2.69)
    optimizer ......................................: (16.83, 17.08)
Mon Feb 12 03:25:01 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   33C    P0             332W / 700W |  51562MiB / 81559MiB |     10%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   38C    P0             274W / 700W |  56010MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   38C    P0             153W / 700W |  52672MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   33C    P0             203W / 700W |  51320MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   36C    P0             254W / 700W |  50846MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   42C    P0             238W / 700W |  52238MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   40C    P0             180W / 700W |  52032MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   35C    P0             194W / 700W |  50768MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 9951.2 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.057427E+01 | loss scale: 1.0 | grad norm: 1.904 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9523.40, 9707.64)
    forward-compute ................................: (970.87, 5251.96)
    backward-compute ...............................: (2458.54, 3467.21)
    batch-generator ................................: (48.08, 54.94)
    forward-recv ...................................: (22.49, 71.09)
    forward-send ...................................: (0.48, 25.47)
    backward-recv ..................................: (76.72, 244.56)
    backward-send ..................................: (0.46, 23.99)
    forward-send-backward-recv .....................: (4520.70, 5829.63)
    backward-send-forward-recv .....................: (450.60, 1081.00)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 22.41)
    grads-reduce-scatter ...........................: (34.91, 38.14)
    params-all-gather ..............................: (18.42, 20.07)
    optimizer-copy-to-main-grad ....................: (0.18, 0.27)
    optimizer-clip-main-grad .......................: (4.06, 4.28)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.68, 9.43)
    optimizer-copy-main-to-model-params ............: (2.44, 2.69)
    optimizer ......................................: (16.86, 17.11)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 11541.3 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.047336E+01 | loss scale: 1.0 | grad norm: 2.989 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (11187.44, 11393.67)
    forward-compute ................................: (1131.17, 6853.15)
    backward-compute ...............................: (2777.10, 3723.36)
    batch-generator ................................: (48.82, 53.83)
    forward-recv ...................................: (25.02, 68.32)
    forward-send ...................................: (0.55, 20.99)
    backward-recv ..................................: (67.60, 280.88)
    backward-send ..................................: (0.57, 51.26)
    forward-send-backward-recv .....................: (5794.46, 7048.70)
    backward-send-forward-recv .....................: (609.81, 1154.04)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 22.22)
    grads-reduce-scatter ...........................: (34.93, 38.13)
    params-all-gather ..............................: (18.44, 20.04)
    optimizer-copy-to-main-grad ....................: (0.18, 0.24)
    optimizer-clip-main-grad .......................: (4.07, 4.37)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.69, 9.43)
    optimizer-copy-main-to-model-params ............: (2.45, 2.69)
    optimizer ......................................: (16.93, 17.18)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 9269.3 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.048897E+01 | loss scale: 1.0 | grad norm: 1.944 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8889.76, 9051.61)
    forward-compute ................................: (1054.81, 4755.34)
    backward-compute ...............................: (2641.65, 3637.75)
    batch-generator ................................: (47.65, 52.45)
    forward-recv ...................................: (18.40, 57.82)
    forward-send ...................................: (0.41, 19.13)
    backward-recv ..................................: (66.17, 286.29)
    backward-send ..................................: (0.63, 41.76)
    forward-send-backward-recv .....................: (3872.12, 4976.90)
    backward-send-forward-recv .....................: (547.59, 1028.49)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.30)
    grads-reduce-scatter ...........................: (34.90, 38.21)
    params-all-gather ..............................: (18.40, 20.08)
    optimizer-copy-to-main-grad ....................: (0.18, 0.23)
    optimizer-clip-main-grad .......................: (3.82, 4.02)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.67, 9.43)
    optimizer-copy-main-to-model-params ............: (2.45, 2.69)
    optimizer ......................................: (16.55, 16.80)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 9106.1 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.046844E+01 | loss scale: 1.0 | grad norm: 1.320 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8758.55, 8925.01)
    forward-compute ................................: (1064.83, 4712.59)
    backward-compute ...............................: (2643.06, 3570.26)
    batch-generator ................................: (47.84, 53.02)
    forward-recv ...................................: (21.60, 60.83)
    forward-send ...................................: (0.43, 24.18)
    backward-recv ..................................: (59.28, 305.83)
    backward-send ..................................: (0.67, 26.25)
    forward-send-backward-recv .....................: (3959.80, 4837.27)
    backward-send-forward-recv .....................: (490.99, 863.39)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.12)
    grads-reduce-scatter ...........................: (34.93, 38.27)
    params-all-gather ..............................: (18.43, 20.08)
    optimizer-copy-to-main-grad ....................: (0.18, 0.23)
    optimizer-clip-main-grad .......................: (3.58, 3.76)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.67, 9.42)
    optimizer-copy-main-to-model-params ............: (2.45, 2.69)
    optimizer ......................................: (16.31, 16.60)
Mon Feb 12 03:31:33 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   35C    P0             451W / 700W |  51608MiB / 81559MiB |     27%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   39C    P0             414W / 700W |  56010MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   40C    P0             298W / 700W |  52674MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   35C    P0             329W / 700W |  51640MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             508W / 700W |  50846MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   44C    P0             389W / 700W |  52238MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             361W / 700W |  52032MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             402W / 700W |  50768MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 9289.0 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.043756E+01 | loss scale: 1.0 | grad norm: 1.172 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8846.33, 9053.30)
    forward-compute ................................: (1079.80, 4654.36)
    backward-compute ...............................: (2686.43, 3630.51)
    batch-generator ................................: (46.74, 53.12)
    forward-recv ...................................: (25.70, 55.92)
    forward-send ...................................: (0.40, 17.31)
    backward-recv ..................................: (67.79, 304.69)
    backward-send ..................................: (0.58, 40.68)
    forward-send-backward-recv .....................: (3992.08, 4875.46)
    backward-send-forward-recv .....................: (555.47, 846.99)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 22.34)
    grads-reduce-scatter ...........................: (34.78, 38.28)
    params-all-gather ..............................: (18.41, 20.06)
    optimizer-copy-to-main-grad ....................: (0.18, 0.23)
    optimizer-clip-main-grad .......................: (3.82, 4.02)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.68, 9.43)
    optimizer-copy-main-to-model-params ............: (2.45, 2.69)
    optimizer ......................................: (16.55, 16.79)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 9277.5 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.043193E+01 | loss scale: 1.0 | grad norm: 1.461 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8886.24, 9080.67)
    forward-compute ................................: (1054.17, 4870.71)
    backward-compute ...............................: (2636.58, 3656.72)
    batch-generator ................................: (46.67, 53.80)
    forward-recv ...................................: (24.83, 73.22)
    forward-send ...................................: (0.50, 21.51)
    backward-recv ..................................: (67.10, 304.66)
    backward-send ..................................: (0.62, 24.20)
    forward-send-backward-recv .....................: (3767.83, 4955.56)
    backward-send-forward-recv .....................: (456.34, 1065.88)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 22.25)
    grads-reduce-scatter ...........................: (34.98, 38.25)
    params-all-gather ..............................: (18.43, 21.09)
    optimizer-copy-to-main-grad ....................: (0.18, 0.24)
    optimizer-clip-main-grad .......................: (4.06, 4.28)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.68, 9.42)
    optimizer-copy-main-to-model-params ............: (2.45, 2.69)
    optimizer ......................................: (16.84, 17.08)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 9085.3 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041737E+01 | loss scale: 1.0 | grad norm: 1.605 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8684.57, 8867.68)
    forward-compute ................................: (1027.75, 4663.28)
    backward-compute ...............................: (2583.26, 3668.53)
    batch-generator ................................: (46.47, 53.42)
    forward-recv ...................................: (21.03, 58.50)
    forward-send ...................................: (0.43, 15.62)
    backward-recv ..................................: (83.49, 322.64)
    backward-send ..................................: (0.59, 30.74)
    forward-send-backward-recv .....................: (3715.94, 4777.50)
    backward-send-forward-recv .....................: (490.52, 912.58)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 22.43)
    grads-reduce-scatter ...........................: (35.01, 38.13)
    params-all-gather ..............................: (18.42, 20.07)
    optimizer-copy-to-main-grad ....................: (0.18, 0.24)
    optimizer-clip-main-grad .......................: (4.06, 4.28)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.68, 9.43)
    optimizer-copy-main-to-model-params ............: (2.45, 2.69)
    optimizer ......................................: (16.82, 17.07)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (477103): No such process
kill: (477125): No such process
kill: (477137): No such process
kill: (477149): No such process
kill: (477152): No such process
10.64.24.50 kill done.
13b, 4k, gbs=512: dp=2, tp=4, pp=2, mbs=8
LOCAL_IP = 10.64.24.52
DP=2, MP=4, PP=2
[2024-02-12 03:36:51,605] torch.distributed.run: [WARNING] 
[2024-02-12 03:36:51,605] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 03:36:51,605] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 03:36:51,605] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 1638548480
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1638548480
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 1638548480
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1638548480
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.310 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.336 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.116 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.151 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (1055.86, 1177.29)
    train/valid/test-data-iterators-setup ..........: (0.02, 10875.11)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 8260.7 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.092942E+01 | loss scale: 1.0 | grad norm: 8.031 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 11] (after 10 iterations) memory (MB) | allocated: 18882.9130859375 | max allocated: 45549.03955078125 | reserved: 56138.0 | max reserved: 56138.0
[Rank 10] (after 10 iterations) memory (MB) | allocated: 18882.9130859375 | max allocated: 45549.03955078125 | reserved: 54836.0 | max reserved: 54836.0
[Rank 9] (after 10 iterations) memory (MB) | allocated: 18882.9130859375 | max allocated: 45549.03955078125 | reserved: 56380.0 | max reserved: 56380.0[Rank 8] (after 10 iterations) memory (MB) | allocated: 18882.9130859375 | max allocated: 45549.7451171875 | reserved: 53838.0 | max reserved: 53838.0

(min, max) time across ranks (ms):
    forward-backward ...............................: (7852.49, 7964.85)
    forward-compute ................................: (2028.33, 3804.24)
    backward-compute ...............................: (2588.48, 3265.68)
    batch-generator ................................: (425.01, 442.03)
    forward-recv ...................................: (496.72, 515.31)
    forward-send ...................................: (3.95, 7.57)
    backward-recv ..................................: (131.56, 183.36)
    backward-send ..................................: (1.72, 10.52)
    forward-send-backward-recv .....................: (2791.35, 3115.12)
    backward-send-forward-recv .....................: (473.85, 546.08)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.76, 6.07)
    grads-reduce-scatter ...........................: (15.42, 232.61)
    params-all-gather ..............................: (8.34, 8.89)
    optimizer-copy-to-main-grad ....................: (0.66, 1.04)
    optimizer-clip-main-grad .......................: (8.32, 8.36)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.40, 9.73)
    optimizer-copy-main-to-model-params ............: (2.96, 3.05)
    optimizer ......................................: (22.78, 22.89)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 6016.1 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.085283E+01 | loss scale: 1.0 | grad norm: 5.190 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5854.83, 5939.72)
    forward-compute ................................: (1354.62, 2767.50)
    backward-compute ...............................: (2286.66, 2847.91)
    batch-generator ................................: (28.49, 38.62)
    forward-recv ...................................: (36.23, 58.58)
    forward-send ...................................: (1.04, 1.60)
    backward-recv ..................................: (87.09, 108.32)
    backward-send ..................................: (1.20, 11.08)
    forward-send-backward-recv .....................: (1999.54, 2157.10)
    backward-send-forward-recv .....................: (286.63, 501.14)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.69, 6.21)
    grads-reduce-scatter ...........................: (15.49, 15.94)
    params-all-gather ..............................: (8.41, 8.84)
    optimizer-copy-to-main-grad ....................: (0.66, 0.80)
    optimizer-clip-main-grad .......................: (4.50, 4.53)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.15, 9.33)
    optimizer-copy-main-to-model-params ............: (2.96, 3.04)
    optimizer ......................................: (18.04, 18.12)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 6991.7 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.060152E+01 | loss scale: 1.0 | grad norm: 2.452 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6792.55, 6910.70)
    forward-compute ................................: (1839.57, 2974.70)
    backward-compute ...............................: (2631.15, 3110.58)
    batch-generator ................................: (29.12, 32.75)
    forward-recv ...................................: (48.05, 50.16)
    forward-send ...................................: (1.32, 1.38)
    backward-recv ..................................: (98.01, 132.67)
    backward-send ..................................: (7.75, 17.39)
    forward-send-backward-recv .....................: (2255.74, 2274.78)
    backward-send-forward-recv .....................: (661.83, 784.61)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.70, 5.95)
    grads-reduce-scatter ...........................: (15.29, 15.98)
    params-all-gather ..............................: (8.40, 8.83)
    optimizer-copy-to-main-grad ....................: (0.65, 0.79)
    optimizer-clip-main-grad .......................: (4.49, 4.52)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.16, 9.34)
    optimizer-copy-main-to-model-params ............: (2.96, 3.04)
    optimizer ......................................: (18.04, 18.13)
Mon Feb 12 03:42:06 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   37C    P0             283W / 700W |  65992MiB / 81559MiB |     87%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   43C    P0             330W / 700W |  67354MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   44C    P0             308W / 700W |  67572MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             323W / 700W |  67496MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             469W / 700W |  58544MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             402W / 700W |  58844MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             390W / 700W |  59610MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             486W / 700W |  57796MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 6675.5 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.057884E+01 | loss scale: 1.0 | grad norm: 1.452 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6422.68, 6522.21)
    forward-compute ................................: (1391.51, 3166.38)
    backward-compute ...............................: (2380.00, 2822.53)
    batch-generator ................................: (27.99, 36.36)
    forward-recv ...................................: (47.34, 53.14)
    forward-send ...................................: (1.31, 1.47)
    backward-recv ..................................: (66.77, 76.71)
    backward-send ..................................: (1.38, 10.50)
    forward-send-backward-recv .....................: (2581.46, 2622.85)
    backward-send-forward-recv .....................: (367.25, 393.51)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.70, 6.26)
    grads-reduce-scatter ...........................: (15.34, 15.92)
    params-all-gather ..............................: (8.40, 8.85)
    optimizer-copy-to-main-grad ....................: (0.67, 0.88)
    optimizer-clip-main-grad .......................: (4.50, 4.53)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.16, 9.31)
    optimizer-copy-main-to-model-params ............: (2.96, 3.04)
    optimizer ......................................: (18.13, 18.22)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 7094.4 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.047930E+01 | loss scale: 1.0 | grad norm: 1.764 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6937.68, 7017.00)
    forward-compute ................................: (1875.34, 3007.67)
    backward-compute ...............................: (2708.86, 3219.44)
    batch-generator ................................: (28.11, 36.76)
    forward-recv ...................................: (52.31, 64.17)
    forward-send ...................................: (1.43, 1.75)
    backward-recv ..................................: (69.42, 97.42)
    backward-send ..................................: (5.22, 7.24)
    forward-send-backward-recv .....................: (2172.27, 2321.53)
    backward-send-forward-recv .....................: (678.31, 709.49)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.72, 6.12)
    grads-reduce-scatter ...........................: (15.19, 15.96)
    params-all-gather ..............................: (8.38, 8.88)
    optimizer-copy-to-main-grad ....................: (0.69, 0.83)
    optimizer-clip-main-grad .......................: (4.49, 4.52)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.16, 9.31)
    optimizer-copy-main-to-model-params ............: (2.96, 3.04)
    optimizer ......................................: (18.05, 18.13)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 6383.7 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.051097E+01 | loss scale: 1.0 | grad norm: 5.509 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6188.10, 6302.45)
    forward-compute ................................: (1489.78, 2818.68)
    backward-compute ...............................: (2528.92, 2991.26)
    batch-generator ................................: (27.05, 37.25)
    forward-recv ...................................: (39.84, 59.75)
    forward-send ...................................: (1.12, 1.64)
    backward-recv ..................................: (111.10, 121.77)
    backward-send ..................................: (1.35, 1.68)
    forward-send-backward-recv .....................: (2013.49, 2148.48)
    backward-send-forward-recv .....................: (366.33, 390.16)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.68, 6.04)
    grads-reduce-scatter ...........................: (15.34, 16.00)
    params-all-gather ..............................: (8.30, 8.84)
    optimizer-copy-to-main-grad ....................: (0.65, 0.79)
    optimizer-clip-main-grad .......................: (4.51, 4.54)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.15, 9.36)
    optimizer-copy-main-to-model-params ............: (2.96, 3.05)
    optimizer ......................................: (18.12, 18.21)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 6249.7 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.048534E+01 | loss scale: 1.0 | grad norm: 0.918 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6066.89, 6171.29)
    forward-compute ................................: (1466.57, 2843.17)
    backward-compute ...............................: (2484.24, 3087.26)
    batch-generator ................................: (26.97, 34.06)
    forward-recv ...................................: (41.77, 49.19)
    forward-send ...................................: (1.16, 1.37)
    backward-recv ..................................: (103.97, 118.14)
    backward-send ..................................: (1.39, 2.84)
    forward-send-backward-recv .....................: (1834.41, 2063.50)
    backward-send-forward-recv .....................: (280.04, 436.34)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (5.70, 6.00)
    grads-reduce-scatter ...........................: (15.32, 15.98)
    params-all-gather ..............................: (8.42, 8.85)
    optimizer-copy-to-main-grad ....................: (0.67, 0.77)
    optimizer-clip-main-grad .......................: (4.27, 4.29)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.16, 9.31)
    optimizer-copy-main-to-model-params ............: (2.96, 3.05)
    optimizer ......................................: (17.80, 17.89)
Mon Feb 12 03:46:30 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   39C    P0             528W / 700W |  69952MiB / 81559MiB |     55%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   45C    P0             533W / 700W |  71314MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   46C    P0             461W / 700W |  71532MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0             457W / 700W |  71456MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             449W / 700W |  68604MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   46C    P0             507W / 700W |  68600MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   44C    P0             508W / 700W |  68556MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             446W / 700W |  68056MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 6684.2 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044992E+01 | loss scale: 1.0 | grad norm: 0.769 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6428.33, 6522.17)
    forward-compute ................................: (1802.18, 2677.16)
    backward-compute ...............................: (2618.97, 3071.93)
    batch-generator ................................: (28.05, 32.83)
    forward-recv ...................................: (41.61, 52.77)
    forward-send ...................................: (1.16, 1.46)
    backward-recv ..................................: (74.13, 99.52)
    backward-send ..................................: (1.23, 4.24)
    forward-send-backward-recv .....................: (1921.97, 1963.71)
    backward-send-forward-recv .....................: (648.23, 651.73)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.63, 6.23)
    grads-reduce-scatter ...........................: (15.30, 15.91)
    params-all-gather ..............................: (8.38, 8.85)
    optimizer-copy-to-main-grad ....................: (0.66, 0.76)
    optimizer-clip-main-grad .......................: (4.25, 4.28)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.16, 9.86)
    optimizer-copy-main-to-model-params ............: (2.96, 3.04)
    optimizer ......................................: (18.45, 18.53)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 6882.1 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.043459E+01 | loss scale: 1.0 | grad norm: 1.428 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6703.32, 6804.47)
    forward-compute ................................: (1540.65, 3303.86)
    backward-compute ...............................: (2578.29, 3057.91)
    batch-generator ................................: (28.89, 32.43)
    forward-recv ...................................: (52.73, 53.90)
    forward-send ...................................: (1.42, 1.51)
    backward-recv ..................................: (110.64, 121.86)
    backward-send ..................................: (1.49, 1.64)
    forward-send-backward-recv .....................: (2432.49, 2546.91)
    backward-send-forward-recv .....................: (333.08, 407.18)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.69, 5.96)
    grads-reduce-scatter ...........................: (15.48, 15.97)
    params-all-gather ..............................: (8.29, 8.88)
    optimizer-copy-to-main-grad ....................: (0.64, 0.77)
    optimizer-clip-main-grad .......................: (4.26, 4.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.15, 9.33)
    optimizer-copy-main-to-model-params ............: (2.96, 3.05)
    optimizer ......................................: (18.05, 18.14)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 6214.0 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.042520E+01 | loss scale: 1.0 | grad norm: 2.886 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6019.89, 6135.06)
    forward-compute ................................: (1509.03, 2657.06)
    backward-compute ...............................: (2568.12, 3033.66)
    batch-generator ................................: (28.91, 32.47)
    forward-recv ...................................: (42.73, 44.64)
    forward-send ...................................: (1.20, 1.26)
    backward-recv ..................................: (107.18, 147.77)
    backward-send ..................................: (1.47, 5.35)
    forward-send-backward-recv .....................: (1864.42, 1875.95)
    backward-send-forward-recv .....................: (323.13, 340.33)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (5.69, 6.19)
    grads-reduce-scatter ...........................: (15.18, 15.90)
    params-all-gather ..............................: (8.34, 8.88)
    optimizer-copy-to-main-grad ....................: (0.65, 0.76)
    optimizer-clip-main-grad .......................: (4.25, 4.28)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.15, 9.33)
    optimizer-copy-main-to-model-params ............: (2.96, 3.05)
    optimizer ......................................: (17.78, 17.87)
[2024-02-12 03:53:52,457] torch.distributed.elastic.agent.server.api: [ERROR] Error waiting on exit barrier. Elapsed: 300.1056206226349 seconds
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (477195): No such process
kill: (477201): No such process
kill: (477207): No such process
kill: (477213): No such process
10.64.24.50 kill done.
13b, 4k, gbs=512: dp=2, tp=4, pp=2, mbs=4
LOCAL_IP = 10.64.24.52
DP=2, MP=4, PP=2
[2024-02-12 03:55:59,315] torch.distributed.run: [WARNING] 
[2024-02-12 03:55:59,315] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 03:55:59,315] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 03:55:59,315] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error

SYM206-GPU-A0206-P2-Node52:372275:372417 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<37301> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<37301> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:372279:372421 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<37301> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<37301> failed : Software caused connection abort
[2024-02-12 03:56:14,338] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 372276 closing signal SIGTERM
[2024-02-12 03:56:14,339] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 372278 closing signal SIGTERM
[2024-02-12 03:56:14,339] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 372280 closing signal SIGTERM
[2024-02-12 03:56:14,818] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 372273) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_03:56:14
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 372274)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-12_03:56:14
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 372275)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-12_03:56:14
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 372277)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-12_03:56:14
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 372279)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_03:56:14
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 372273)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (477253): No such process
kill: (477259): No such process
kill: (477265): No such process
kill: (477271): No such process
10.64.24.50 kill done.
13b, 4k, gbs=512: dp=2, tp=4, pp=2, mbs=2
LOCAL_IP = 10.64.24.52
DP=2, MP=4, PP=2
[2024-02-12 03:59:02,888] torch.distributed.run: [WARNING] 
[2024-02-12 03:59:02,888] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 03:59:02,888] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 03:59:02,888] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error

SYM206-GPU-A0206-P2-Node52:372556:372711 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<37301> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:372562:372714 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<37301> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>

SYM206-GPU-A0206-P2-Node52:372560:372712 [5] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<37301> failed : Software caused connection abort
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<37301> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<37301> failed : Software caused connection abort
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<37301> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:372558:372713 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<37301> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<37301> failed : Software caused connection abort
[2024-02-12 03:59:17,911] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 372556 closing signal SIGTERM
[2024-02-12 03:59:17,912] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 372558 closing signal SIGTERM
[2024-02-12 03:59:17,912] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 372559 closing signal SIGTERM
[2024-02-12 03:59:17,912] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 372560 closing signal SIGTERM
[2024-02-12 03:59:17,913] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 372562 closing signal SIGTERM
[2024-02-12 03:59:18,440] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 372555) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_03:59:17
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 372557)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-12_03:59:17
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 372561)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_03:59:17
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 372555)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (477311): No such process
kill: (477317): No such process
kill: (477323): No such process
kill: (477329): No such process
10.64.24.50 kill done.
13b, 4k, gbs=512: dp=2, tp=2, pp=4, mbs=8
LOCAL_IP = 10.64.24.52
DP=2, MP=2, PP=4
[2024-02-12 04:01:48,038] torch.distributed.run: [WARNING] 
[2024-02-12 04:01:48,038] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 04:01:48,038] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 04:01:48,038] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error

SYM206-GPU-A0206-P2-Node52:372870:373025 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<37301> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<37301> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:372873:373024 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<37301> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:372868:373027 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<37301> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain

SYM206-GPU-A0206-P2-Node52:372871:373026 [4] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<37301> failed : Software caused connection abort
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<37301> failed : Software caused connection abort
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<37301> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<37301> failed : Software caused connection abort
[2024-02-12 04:02:03,057] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 372868 closing signal SIGTERM
[2024-02-12 04:02:03,057] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 372869 closing signal SIGTERM
[2024-02-12 04:02:03,058] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 372871 closing signal SIGTERM
[2024-02-12 04:02:03,058] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 372873 closing signal SIGTERM
[2024-02-12 04:02:03,059] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 372874 closing signal SIGTERM
[2024-02-12 04:02:03,600] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 372867) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_04:02:03
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 372870)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-12_04:02:03
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 372872)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_04:02:03
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 372867)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (477427): No such process
kill: (477433): No such process
kill: (477439): No such process
kill: (477445): No such process
10.64.24.50 kill done.
13b, 4k, gbs=512: dp=2, tp=2, pp=4, mbs=4
LOCAL_IP = 10.64.24.52
DP=2, MP=2, PP=4
[2024-02-12 04:04:38,189] torch.distributed.run: [WARNING] 
[2024-02-12 04:04:38,189] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 04:04:38,189] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 04:04:38,189] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1573350400
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 1573350400
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
[E ProcessGroupNCCL.cpp:467] [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600058 milliseconds before timing out.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 80, in __init__
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    self.initialize_word_embeddings(init_method_normal)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 106, in initialize_word_embeddings
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 80, in __init__
    self.initialize_word_embeddings(init_method_normal)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 106, in initialize_word_embeddings
    torch.distributed.all_reduce(self.word_embeddings_weight().data,
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
    torch.distributed.all_reduce(self.word_embeddings_weight().data,
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    work = group.allreduce([tensor], opts)
    return func(*args, **kwargs)
RuntimeError  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f28e4fb0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f28e4f6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f289799123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f289799150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f2897991816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f289794a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f289794a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f289794a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f289794a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f289794a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f284acd73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f284acda908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0xe27d37 (0x7f284ace6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f284ace82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f284acea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4936f0a (0x7f2897938f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494766f (0x7f289794966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4950b97 (0x7f2897952b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x495f1ee (0x7f28979611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb8cc15 (0x7f289e122c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f289d932407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x5584251cce0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x5584251c35eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x5584251db7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x5584251bb8a2 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x5584251cd70c in /usr/bin/python)
frame #26: PyObject_Call + 0x122 (0x5584251dc192 in /usr/bin/python)
frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x5584251b82c1 in /usr/bin/python)
frame #28: _PyFunction_Vectorcall + 0x7c (0x5584251cd70c in /usr/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x5584251b70d1 in /usr/bin/python)
frame #30: <unknown function> + 0x16e4e1 (0x5584251db4e1 in /usr/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x5584251bb8a2 in /usr/bin/python)
frame #32: _PyFunction_Vectorcall + 0x7c (0x5584251cd70c in /usr/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x16d (0x5584251c282d in /usr/bin/python)
frame #34: <unknown function> + 0x16a744 (0x5584251d7744 in /usr/bin/python)
frame #35: _PyObject_MakeTpCall + 0x1fc (0x5584251c358c in /usr/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x5584251bc908 in /usr/bin/python)
frame #37: _PyFunction_Vectorcall + 0x7c (0x5584251cd70c in /usr/bin/python)
frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x5584251b70d1 in /usr/bin/python)
frame #39: _PyFunction_Vectorcall + 0x7c (0x5584251cd70c in /usr/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x5584251b5e0d in /usr/bin/python)
frame #41: _PyFunction_Vectorcall + 0x7c (0x5584251cd70c in /usr/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x5584251b5e0d in /usr/bin/python)
frame #43: _PyFunction_Vectorcall + 0x7c (0x5584251cd70c in /usr/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x5584251b70d1 in /usr/bin/python)
frame #45: <unknown function> + 0x239e56 (0x5584252a6e56 in /usr/bin/python)
frame #46: PyEval_EvalCode + 0x86 (0x5584252a6cf6 in /usr/bin/python)
frame #47: <unknown function> + 0x2647d8 (0x5584252d17d8 in /usr/bin/python)
frame #48: <unknown function> + 0x25e0bb (0x5584252cb0bb in /usr/bin/python)
frame #49: <unknown function> + 0x264525 (0x5584252d1525 in /usr/bin/python)
frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x5584252d0a08 in /usr/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x5584252d0653 in /usr/bin/python)
frame #52: Py_RunMain + 0x2be (0x5584252c341e in /usr/bin/python)
frame #53: Py_BytesMain + 0x2d (0x558425299cad in /usr/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7f28e6534d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7f28e6534e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #56: _start + 0x25 (0x558425299ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
    work = group.allreduce([tensor], opts)
RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f1571fb0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f1571f6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f152499123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f152499150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f1524991816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f152494a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f152494a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f152494a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f152494a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f152494a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f14d7cd73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f14d7cda908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0xe27d37 (0x7f14d7ce6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f14d7ce82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f14d7cea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4936f0a (0x7f1524938f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494766f (0x7f152494966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4950b97 (0x7f1524952b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x495f1ee (0x7f15249611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb8cc15 (0x7f152b122c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f152a932407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55abbc5c0e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55abbc5b75eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55abbc5cf7bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x55abbc5af8a2 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55abbc5c170c in /usr/bin/python)
frame #26: PyObject_Call + 0x122 (0x55abbc5d0192 in /usr/bin/python)
frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x55abbc5ac2c1 in /usr/bin/python)
frame #28: _PyFunction_Vectorcall + 0x7c (0x55abbc5c170c in /usr/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x55abbc5ab0d1 in /usr/bin/python)
frame #30: <unknown function> + 0x16e4e1 (0x55abbc5cf4e1 in /usr/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x55abbc5af8a2 in /usr/bin/python)
frame #32: _PyFunction_Vectorcall + 0x7c (0x55abbc5c170c in /usr/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x16d (0x55abbc5b682d in /usr/bin/python)
frame #34: <unknown function> + 0x16a744 (0x55abbc5cb744 in /usr/bin/python)
frame #35: _PyObject_MakeTpCall + 0x1fc (0x55abbc5b758c in /usr/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x55abbc5b0908 in /usr/bin/python)
frame #37: _PyFunction_Vectorcall + 0x7c (0x55abbc5c170c in /usr/bin/python)
frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x55abbc5ab0d1 in /usr/bin/python)
frame #39: _PyFunction_Vectorcall + 0x7c (0x55abbc5c170c in /usr/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x55abbc5a9e0d in /usr/bin/python)
frame #41: _PyFunction_Vectorcall + 0x7c (0x55abbc5c170c in /usr/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x55abbc5a9e0d in /usr/bin/python)
frame #43: _PyFunction_Vectorcall + 0x7c (0x55abbc5c170c in /usr/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x55abbc5ab0d1 in /usr/bin/python)
frame #45: <unknown function> + 0x239e56 (0x55abbc69ae56 in /usr/bin/python)
frame #46: PyEval_EvalCode + 0x86 (0x55abbc69acf6 in /usr/bin/python)
frame #47: <unknown function> + 0x2647d8 (0x55abbc6c57d8 in /usr/bin/python)
frame #48: <unknown function> + 0x25e0bb (0x55abbc6bf0bb in /usr/bin/python)
frame #49: <unknown function> + 0x264525 (0x55abbc6c5525 in /usr/bin/python)
frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x55abbc6c4a08 in /usr/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x55abbc6c4653 in /usr/bin/python)
frame #52: Py_RunMain + 0x2be (0x55abbc6b741e in /usr/bin/python)
frame #53: Py_BytesMain + 0x2d (0x55abbc68dcad in /usr/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7f15734b9d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7f15734b9e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #56: _start + 0x25 (0x55abbc68dba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 80, in __init__
    self.initialize_word_embeddings(init_method_normal)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 106, in initialize_word_embeddings
    torch.distributed.all_reduce(self.word_embeddings_weight().data,
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
    work = group.allreduce([tensor], opts)
RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f91929b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f919296b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f914539123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f914539150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f9145391816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f914534a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f914534a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f914534a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f914534a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f914534a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f90f86d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f90f86da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0xe27d37 (0x7f90f86e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f90f86e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f90f86ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4936f0a (0x7f9145338f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494766f (0x7f914534966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4950b97 (0x7f9145352b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x495f1ee (0x7f91453611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb8cc15 (0x7f914bb22c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7f914b332407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55dc14beae0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55dc14be15eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55dc14bf97bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x55dc14bd98a2 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55dc14beb70c in /usr/bin/python)
frame #26: PyObject_Call + 0x122 (0x55dc14bfa192 in /usr/bin/python)
frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x55dc14bd62c1 in /usr/bin/python)
frame #28: _PyFunction_Vectorcall + 0x7c (0x55dc14beb70c in /usr/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x55dc14bd50d1 in /usr/bin/python)
frame #30: <unknown function> + 0x16e4e1 (0x55dc14bf94e1 in /usr/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x55dc14bd98a2 in /usr/bin/python)
frame #32: _PyFunction_Vectorcall + 0x7c (0x55dc14beb70c in /usr/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x16d (0x55dc14be082d in /usr/bin/python)
frame #34: <unknown function> + 0x16a744 (0x55dc14bf5744 in /usr/bin/python)
frame #35: _PyObject_MakeTpCall + 0x1fc (0x55dc14be158c in /usr/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x55dc14bda908 in /usr/bin/python)
frame #37: _PyFunction_Vectorcall + 0x7c (0x55dc14beb70c in /usr/bin/python)
frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x55dc14bd50d1 in /usr/bin/python)
frame #39: _PyFunction_Vectorcall + 0x7c (0x55dc14beb70c in /usr/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x55dc14bd3e0d in /usr/bin/python)
frame #41: _PyFunction_Vectorcall + 0x7c (0x55dc14beb70c in /usr/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x55dc14bd3e0d in /usr/bin/python)
frame #43: _PyFunction_Vectorcall + 0x7c (0x55dc14beb70c in /usr/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x55dc14bd50d1 in /usr/bin/python)
frame #45: <unknown function> + 0x239e56 (0x55dc14cc4e56 in /usr/bin/python)
frame #46: PyEval_EvalCode + 0x86 (0x55dc14cc4cf6 in /usr/bin/python)
frame #47: <unknown function> + 0x2647d8 (0x55dc14cef7d8 in /usr/bin/python)
frame #48: <unknown function> + 0x25e0bb (0x55dc14ce90bb in /usr/bin/python)
frame #49: <unknown function> + 0x264525 (0x55dc14cef525 in /usr/bin/python)
frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x55dc14ceea08 in /usr/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x55dc14cee653 in /usr/bin/python)
frame #52: Py_RunMain + 0x2be (0x55dc14ce141e in /usr/bin/python)
frame #53: Py_BytesMain + 0x2d (0x55dc14cb7cad in /usr/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7f9193f3cd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7f9193f3ce40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #56: _start + 0x25 (0x55dc14cb7ba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 113, in pretrain
    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 366, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 246, in get_model
    model = model_provider_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 27, in model_provider
    model = GPTModel(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 80, in __init__
    self.initialize_word_embeddings(init_method_normal)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 106, in initialize_word_embeddings
    torch.distributed.all_reduce(self.word_embeddings_weight().data,
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
    work = group.allreduce([tensor], opts)
RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fbf57bb0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7fbf57b6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7fbf1f19123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7fbf1f19150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7fbf1f191816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fbf1f14a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fbf1f14a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fbf1f14a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fbf1f14a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7fbf1f14a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7fbed24d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7fbed24da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0xe27d37 (0x7fbed24e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7fbed24e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7fbed24ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #15: <unknown function> + 0x4936f0a (0x7fbf1f138f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x494766f (0x7fbf1f14966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x4950b97 (0x7fbf1f152b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0x495f1ee (0x7fbf1f1611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xb8cc15 (0x7fbf25922c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x39c407 (0x7fbf25132407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #21: <unknown function> + 0x15fe0e (0x55eadf8c2e0e in /usr/bin/python)
frame #22: _PyObject_MakeTpCall + 0x25b (0x55eadf8b95eb in /usr/bin/python)
frame #23: <unknown function> + 0x16e7bb (0x55eadf8d17bb in /usr/bin/python)
frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x55eadf8b18a2 in /usr/bin/python)
frame #25: _PyFunction_Vectorcall + 0x7c (0x55eadf8c370c in /usr/bin/python)
frame #26: PyObject_Call + 0x122 (0x55eadf8d2192 in /usr/bin/python)
frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x55eadf8ae2c1 in /usr/bin/python)
frame #28: _PyFunction_Vectorcall + 0x7c (0x55eadf8c370c in /usr/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x55eadf8ad0d1 in /usr/bin/python)
frame #30: <unknown function> + 0x16e4e1 (0x55eadf8d14e1 in /usr/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x55eadf8b18a2 in /usr/bin/python)
frame #32: _PyFunction_Vectorcall + 0x7c (0x55eadf8c370c in /usr/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x16d (0x55eadf8b882d in /usr/bin/python)
frame #34: <unknown function> + 0x16a744 (0x55eadf8cd744 in /usr/bin/python)
frame #35: _PyObject_MakeTpCall + 0x1fc (0x55eadf8b958c in /usr/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x55eadf8b2908 in /usr/bin/python)
frame #37: _PyFunction_Vectorcall + 0x7c (0x55eadf8c370c in /usr/bin/python)
frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x55eadf8ad0d1 in /usr/bin/python)
frame #39: _PyFunction_Vectorcall + 0x7c (0x55eadf8c370c in /usr/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x55eadf8abe0d in /usr/bin/python)
frame #41: _PyFunction_Vectorcall + 0x7c (0x55eadf8c370c in /usr/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x55eadf8abe0d in /usr/bin/python)
frame #43: _PyFunction_Vectorcall + 0x7c (0x55eadf8c370c in /usr/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x55eadf8ad0d1 in /usr/bin/python)
frame #45: <unknown function> + 0x239e56 (0x55eadf99ce56 in /usr/bin/python)
frame #46: PyEval_EvalCode + 0x86 (0x55eadf99ccf6 in /usr/bin/python)
frame #47: <unknown function> + 0x2647d8 (0x55eadf9c77d8 in /usr/bin/python)
frame #48: <unknown function> + 0x25e0bb (0x55eadf9c10bb in /usr/bin/python)
frame #49: <unknown function> + 0x264525 (0x55eadf9c7525 in /usr/bin/python)
frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x55eadf9c6a08 in /usr/bin/python)
frame #51: _PyRun_AnyFileObject + 0x43 (0x55eadf9c6653 in /usr/bin/python)
frame #52: Py_RunMain + 0x2be (0x55eadf9b941e in /usr/bin/python)
frame #53: Py_BytesMain + 0x2d (0x55eadf98fcad in /usr/bin/python)
frame #54: <unknown function> + 0x29d90 (0x7fbf6dbe0d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #55: __libc_start_main + 0x80 (0x7fbf6dbe0e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #56: _start + 0x25 (0x55eadf98fba5 in /usr/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
[E ProcessGroupNCCL.cpp:467] [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600420 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600893 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:467] [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600987 milliseconds before timing out.
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 8] NCCL watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600420 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 8] NCCL watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600420 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 10] NCCL watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600893 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 10] NCCL watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600893 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 11] NCCL watchdog thread terminated with exception: [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600058 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 11] NCCL watchdog thread terminated with exception: [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600058 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:852] [Rank 9] NCCL watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600987 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 9] NCCL watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600987 milliseconds before timing out.
[2024-02-12 04:15:03,838] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 373177 closing signal SIGTERM
[2024-02-12 04:15:03,839] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 373178 closing signal SIGTERM
[2024-02-12 04:15:03,839] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 373179 closing signal SIGTERM
[2024-02-12 04:15:03,839] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 373180 closing signal SIGTERM
[2024-02-12 04:15:33,839] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 373177 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-12 04:15:34,302] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 4 (pid: 373181) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_04:15:03
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 373182)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-12_04:15:03
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 373183)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-12_04:15:03
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 373184)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_04:15:03
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 373181)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (477485): No such process
kill: (477491): No such process
kill: (477497): No such process
kill: (477503): No such process
10.64.24.50 kill done.
13b, 4k, gbs=512: dp=2, tp=2, pp=4, mbs=2
LOCAL_IP = 10.64.24.52
DP=2, MP=2, PP=4
[2024-02-12 04:18:21,231] torch.distributed.run: [WARNING] 
[2024-02-12 04:18:21,231] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 04:18:21,231] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 04:18:21,231] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error

SYM206-GPU-A0206-P2-Node52:373575:373709 [5] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:373577:373708 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
[2024-02-12 04:18:36,254] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 373573 closing signal SIGTERM
[2024-02-12 04:18:36,669] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 373570) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_04:18:36
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 373571)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-12_04:18:36
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 373572)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-12_04:18:36
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 373574)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-12_04:18:36
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 373575)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-12_04:18:36
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 373576)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-12_04:18:36
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 373577)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_04:18:36
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 373570)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (477543): No such process
kill: (477549): No such process
kill: (477555): No such process
kill: (477561): No such process
10.64.24.50 kill done.
13b, 4k, gbs=512: dp=2, tp=8, pp=1, mbs=4
LOCAL_IP = 10.64.24.52
DP=2, MP=8, PP=1
[2024-02-12 04:21:44,323] torch.distributed.run: [WARNING] 
[2024-02-12 04:21:44,323] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 04:21:44,323] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 04:21:44,323] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error

SYM206-GPU-A0206-P2-Node52:373852:373987 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:373854:374014 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:373855:374015 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:373853:374016 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
[2024-02-12 04:21:59,347] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 373856 closing signal SIGTERM
[2024-02-12 04:21:59,712] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 373852) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_04:21:59
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 373853)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-12_04:21:59
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 373854)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-12_04:21:59
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 373855)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-12_04:21:59
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 373857)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-12_04:21:59
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 373858)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-12_04:21:59
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 373859)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_04:21:59
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 373852)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (477601): No such process
kill: (477607): No such process
kill: (477613): No such process
kill: (477619): No such process
10.64.24.50 kill done.
13b, 4k, gbs=512: dp=2, tp=8, pp=1, mbs=2
LOCAL_IP = 10.64.24.52
DP=2, MP=8, PP=1
[2024-02-12 04:25:16,673] torch.distributed.run: [WARNING] 
[2024-02-12 04:25:16,673] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 04:25:16,673] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 04:25:16,673] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error

SYM206-GPU-A0206-P2-Node52:374169:374315 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:374170:374325 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:374171:374336 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:374172:374314 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
        return func(*args, **kwargs)return func(*args, **kwargs)

  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
        work = default_pg.barrier(opts=opts)work = default_pg.barrier(opts=opts)

torch.distributedtorch.distributed..DistBackendErrorDistBackendError: : NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abortNCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort

[2024-02-12 04:25:31,693] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 374173 closing signal SIGTERM
[2024-02-12 04:25:31,693] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 374174 closing signal SIGTERM
[2024-02-12 04:25:32,071] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 374169) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_04:25:31
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 374170)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-12_04:25:31
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 374171)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-12_04:25:31
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 374172)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-12_04:25:31
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 374175)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-12_04:25:31
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 374176)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_04:25:31
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 374169)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (477659): No such process
kill: (477665): No such process
kill: (477671): No such process
kill: (477677): No such process
10.64.24.50 kill done.
13b, 4k, gbs=512: dp=2, tp=1, pp=8, mbs=4
LOCAL_IP = 10.64.24.52
DP=2, MP=1, PP=8
[2024-02-12 04:28:39,012] torch.distributed.run: [WARNING] 
[2024-02-12 04:28:39,012] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 04:28:39,012] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 04:28:39,012] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error

SYM206-GPU-A0206-P2-Node52:374493:374630 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:374492:374631 [6] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
[2024-02-12 04:28:54,036] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 374486 closing signal SIGTERM
[2024-02-12 04:28:54,036] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 374487 closing signal SIGTERM
[2024-02-12 04:28:54,037] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 374490 closing signal SIGTERM
[2024-02-12 04:28:54,478] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 2 (pid: 374488) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_04:28:54
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 374489)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-12_04:28:54
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 374491)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-12_04:28:54
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 374492)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-12_04:28:54
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 374493)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_04:28:54
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 374488)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (477717): No such process
kill: (477723): No such process
kill: (477729): No such process
kill: (477735): No such process
10.64.24.50 kill done.
13b, 4k, gbs=512: dp=2, tp=1, pp=8, mbs=2
LOCAL_IP = 10.64.24.52
DP=2, MP=1, PP=8
[2024-02-12 04:31:51,409] torch.distributed.run: [WARNING] 
[2024-02-12 04:31:51,409] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 04:31:51,409] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 04:31:51,409] torch.distributed.run: [WARNING] *****************************************
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
[E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 76, in initialize_megatron
    finish_mpu_init()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 56, in finish_mpu_init
    _initialize_distributed()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 185, in _initialize_distributed
    mpu.initialize_model_parallel(args.tensor_model_parallel_size,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/parallel_state.py", line 166, in initialize_model_parallel
    group_gloo = torch.distributed.new_group(ranks, backend="gloo")
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
    func_return = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
    return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
    pg, pg_store = _new_process_group_helper(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error

SYM206-GPU-A0206-P2-Node52:374775:374899 [7] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:374771:374925 [3] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:374769:374926 [1] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:374770:374928 [2] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort

SYM206-GPU-A0206-P2-Node52:374768:374934 [0] misc/socket.cc:484 NCCL WARN socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 91, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 82, in initialize_megatron
    _compile_dependencies()
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py", line 133, in _compile_dependencies
    torch.distributed.barrier()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3685, in barrier
    work = default_pg.barrier(opts=opts)
torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1207, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
socketStartConnect: Connect to 10.64.24.49<40755> failed : Software caused connection abort
[2024-02-12 04:32:06,433] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 374772 closing signal SIGTERM
[2024-02-12 04:32:06,748] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 374768) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_04:32:06
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 374769)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-12_04:32:06
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 374770)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-12_04:32:06
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 374771)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-02-12_04:32:06
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 374773)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-02-12_04:32:06
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 374774)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-02-12_04:32:06
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 374775)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_04:32:06
  host      : SYM206-GPU-A0206-P2-Node52
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 374768)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (477775): No such process
kill: (477781): No such process
kill: (477787): No such process
kill: (477793): No such process
10.64.24.50 kill done.

7b, 8k, gbs=512: dp=8, tp=1, pp=2, mbs=2
LOCAL_IP = 10.64.24.51
DP=8, MP=1, PP=2
[2024-02-12 15:20:17,321] torch.distributed.run: [WARNING] 
[2024-02-12 15:20:17,321] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 15:20:17,321] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 15:20:17,321] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 3428130816
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.441 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  5.450 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  5.466 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  5.558 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  5.576 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  6.114 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  7.103 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  7.545 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Cutting or padding data end, time cost:  10.878 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.919 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.920 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.846 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.956 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.959 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.886 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  11.048 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (1022.13, 1120.70)
    train/valid/test-data-iterators-setup ..........: (16652.11, 19096.35)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 9357.6 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.087327E+01 | loss scale: 1.0 | grad norm: 3.359 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 24591.4169921875 | max allocated: 56476.0859375 | reserved: 59894.0 | max reserved: 59894.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (8337.62, 8474.17)
    forward-compute ................................: (765.40, 5799.31)
    backward-compute ...............................: (1570.99, 2563.53)
    batch-generator ................................: (39.88, 96.53)
    forward-recv ...................................: (114.62, 152.51)
    forward-send ...................................: (2.65, 7.14)
    backward-recv ..................................: (191.88, 228.75)
    backward-send ..................................: (0.45, 18.81)
    forward-send-backward-recv .....................: (5457.28, 5842.59)
    backward-send-forward-recv .....................: (158.67, 288.69)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.58, 18.19)
    grads-reduce-scatter ...........................: (42.03, 674.90)
    params-all-gather ..............................: (22.43, 22.65)
    optimizer-copy-to-main-grad ....................: (0.13, 0.28)
    optimizer-clip-main-grad .......................: (5.44, 5.56)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.90, 4.99)
    optimizer-copy-main-to-model-params ............: (1.36, 1.43)
    optimizer ......................................: (13.01, 13.11)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 6526.2 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.085007E+01 | loss scale: 1.0 | grad norm: 11.135 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6249.11, 6355.56)
    forward-compute ................................: (554.26, 4023.88)
    backward-compute ...............................: (1395.66, 2205.33)
    batch-generator ................................: (26.51, 32.13)
    forward-recv ...................................: (13.17, 33.10)
    forward-send ...................................: (0.39, 0.66)
    backward-recv ..................................: (81.23, 175.76)
    backward-send ..................................: (0.45, 51.69)
    forward-send-backward-recv .....................: (3967.80, 4159.24)
    backward-send-forward-recv .....................: (129.71, 215.72)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.45, 17.67)
    grads-reduce-scatter ...........................: (42.06, 42.63)
    params-all-gather ..............................: (22.45, 22.66)
    optimizer-copy-to-main-grad ....................: (0.13, 0.22)
    optimizer-clip-main-grad .......................: (2.30, 2.32)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 5.19)
    optimizer-copy-main-to-model-params ............: (1.35, 1.43)
    optimizer ......................................: (9.53, 9.60)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 9543.6 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.071949E+01 | loss scale: 1.0 | grad norm: 1.055 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9258.54, 9379.89)
    forward-compute ................................: (1151.27, 6379.68)
    backward-compute ...............................: (1593.19, 2327.13)
    batch-generator ................................: (26.16, 31.64)
    forward-recv ...................................: (15.59, 743.22)
    forward-send ...................................: (0.42, 0.73)
    backward-recv ..................................: (150.57, 673.73)
    backward-send ..................................: (0.58, 34.02)
    forward-send-backward-recv .....................: (5686.72, 6305.54)
    backward-send-forward-recv .....................: (127.28, 728.44)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.50, 18.29)
    grads-reduce-scatter ...........................: (41.96, 42.67)
    params-all-gather ..............................: (22.44, 22.67)
    optimizer-copy-to-main-grad ....................: (0.13, 0.22)
    optimizer-clip-main-grad .......................: (2.17, 2.19)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.97)
    optimizer-copy-main-to-model-params ............: (1.35, 1.43)
    optimizer ......................................: (9.19, 9.27)
Mon Feb 12 15:26:22 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   34C    P0             285W / 700W |  63090MiB / 81559MiB |     26%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   41C    P0             183W / 700W |  69552MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   42C    P0             204W / 700W |  65040MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0             292W / 700W |  67782MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   36C    P0             442W / 700W |  68316MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   42C    P0             339W / 700W |  68366MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   39C    P0             234W / 700W |  74486MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   35C    P0             262W / 700W |  71206MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 6900.8 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.069817E+01 | loss scale: 1.0 | grad norm: 0.570 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6564.62, 6661.38)
    forward-compute ................................: (585.80, 4345.52)
    backward-compute ...............................: (1467.92, 2237.43)
    batch-generator ................................: (26.33, 33.74)
    forward-recv ...................................: (16.27, 30.04)
    forward-send ...................................: (0.44, 0.62)
    backward-recv ..................................: (104.22, 147.04)
    backward-send ..................................: (0.40, 19.28)
    forward-send-backward-recv .....................: (4244.29, 4422.74)
    backward-send-forward-recv .....................: (117.63, 249.50)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.49, 17.76)
    grads-reduce-scatter ...........................: (41.97, 42.57)
    params-all-gather ..............................: (22.44, 22.66)
    optimizer-copy-to-main-grad ....................: (0.13, 0.22)
    optimizer-clip-main-grad .......................: (1.10, 1.12)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.85)
    optimizer-copy-main-to-model-params ............: (1.35, 1.43)
    optimizer ......................................: (8.04, 8.12)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 9436.3 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.064097E+01 | loss scale: 1.0 | grad norm: 0.345 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9146.54, 9272.03)
    forward-compute ................................: (666.71, 6729.93)
    backward-compute ...............................: (1616.90, 2446.73)
    batch-generator ................................: (26.36, 34.12)
    forward-recv ...................................: (19.52, 46.17)
    forward-send ...................................: (0.49, 0.85)
    backward-recv ..................................: (129.59, 205.82)
    backward-send ..................................: (0.43, 36.32)
    forward-send-backward-recv .....................: (6461.34, 6807.59)
    backward-send-forward-recv .....................: (124.75, 332.81)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (17.49, 17.72)
    grads-reduce-scatter ...........................: (41.98, 42.74)
    params-all-gather ..............................: (22.45, 22.67)
    optimizer-copy-to-main-grad ....................: (0.13, 0.22)
    optimizer-clip-main-grad .......................: (0.99, 0.99)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 5.00)
    optimizer-copy-main-to-model-params ............: (1.35, 1.42)
    optimizer ......................................: (8.07, 8.14)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 8112.3 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.065188E+01 | loss scale: 1.0 | grad norm: 0.411 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7800.82, 7902.56)
    forward-compute ................................: (1098.28, 5450.88)
    backward-compute ...............................: (1538.45, 2273.41)
    batch-generator ................................: (26.31, 32.69)
    forward-recv ...................................: (13.79, 37.54)
    forward-send ...................................: (0.40, 1.20)
    backward-recv ..................................: (161.83, 618.76)
    backward-send ..................................: (0.51, 19.75)
    forward-send-backward-recv .....................: (4433.28, 5010.47)
    backward-send-forward-recv .....................: (145.51, 628.01)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.44, 17.80)
    grads-reduce-scatter ...........................: (42.02, 42.87)
    params-all-gather ..............................: (22.45, 22.64)
    optimizer-copy-to-main-grad ....................: (0.13, 0.23)
    optimizer-clip-main-grad .......................: (0.98, 0.99)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.85)
    optimizer-copy-main-to-model-params ............: (1.35, 1.42)
    optimizer ......................................: (7.86, 7.94)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 6682.4 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.064216E+01 | loss scale: 1.0 | grad norm: 0.418 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6422.42, 6511.83)
    forward-compute ................................: (608.45, 4165.77)
    backward-compute ...............................: (1494.81, 2290.80)
    batch-generator ................................: (26.00, 35.16)
    forward-recv ...................................: (14.26, 23.82)
    forward-send ...................................: (0.39, 0.53)
    backward-recv ..................................: (104.75, 170.47)
    backward-send ..................................: (0.47, 32.88)
    forward-send-backward-recv .....................: (3970.23, 4200.42)
    backward-send-forward-recv .....................: (93.40, 219.01)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.36, 17.78)
    grads-reduce-scatter ...........................: (41.99, 43.16)
    params-all-gather ..............................: (22.44, 22.67)
    optimizer-copy-to-main-grad ....................: (0.12, 0.23)
    optimizer-clip-main-grad .......................: (1.00, 1.01)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.88)
    optimizer-copy-main-to-model-params ............: (1.35, 1.43)
    optimizer ......................................: (8.01, 8.58)
Mon Feb 12 15:31:33 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   36C    P0             341W / 700W |  63090MiB / 81559MiB |     63%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   42C    P0             513W / 700W |  69552MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   43C    P0             406W / 700W |  72900MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             401W / 700W |  67782MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   36C    P0             412W / 700W |  68316MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   42C    P0             384W / 700W |  68368MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   41C    P0             298W / 700W |  74486MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             429W / 700W |  71206MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 6915.9 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.062330E+01 | loss scale: 1.0 | grad norm: 0.301 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6563.21, 6653.85)
    forward-compute ................................: (614.78, 4315.80)
    backward-compute ...............................: (1515.98, 2336.97)
    batch-generator ................................: (26.41, 33.53)
    forward-recv ...................................: (14.45, 33.19)
    forward-send ...................................: (0.41, 0.68)
    backward-recv ..................................: (115.59, 166.09)
    backward-send ..................................: (0.48, 17.20)
    forward-send-backward-recv .....................: (4062.83, 4305.04)
    backward-send-forward-recv .....................: (101.74, 219.23)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.41, 17.76)
    grads-reduce-scatter ...........................: (41.95, 42.76)
    params-all-gather ..............................: (22.43, 22.70)
    optimizer-copy-to-main-grad ....................: (0.12, 0.23)
    optimizer-clip-main-grad .......................: (1.00, 1.13)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.89)
    optimizer-copy-main-to-model-params ............: (1.35, 1.47)
    optimizer ......................................: (8.07, 8.19)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 6714.9 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.061710E+01 | loss scale: 1.0 | grad norm: 0.362 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6444.55, 6539.53)
    forward-compute ................................: (595.10, 4222.32)
    backward-compute ...............................: (1475.76, 2344.01)
    batch-generator ................................: (26.24, 32.21)
    forward-recv ...................................: (15.29, 34.65)
    forward-send ...................................: (0.43, 0.68)
    backward-recv ..................................: (122.03, 180.05)
    backward-send ..................................: (0.46, 18.39)
    forward-send-backward-recv .....................: (3920.54, 4276.98)
    backward-send-forward-recv .....................: (93.64, 242.19)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.40, 17.60)
    grads-reduce-scatter ...........................: (41.99, 42.58)
    params-all-gather ..............................: (22.43, 22.65)
    optimizer-copy-to-main-grad ....................: (0.13, 0.22)
    optimizer-clip-main-grad .......................: (1.10, 1.10)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 5.29)
    optimizer-copy-main-to-model-params ............: (1.36, 1.49)
    optimizer ......................................: (8.43, 8.56)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 10082.0 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.061789E+01 | loss scale: 1.0 | grad norm: 1.248 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9726.96, 9860.15)
    forward-compute ................................: (1064.32, 7501.46)
    backward-compute ...............................: (1478.52, 2366.38)
    batch-generator ................................: (26.21, 33.01)
    forward-recv ...................................: (14.07, 25.03)
    forward-send ...................................: (0.40, 0.55)
    backward-recv ..................................: (166.78, 215.77)
    backward-send ..................................: (0.50, 18.78)
    forward-send-backward-recv .....................: (6645.45, 7019.67)
    backward-send-forward-recv .....................: (94.19, 731.67)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.47, 17.72)
    grads-reduce-scatter ...........................: (41.97, 42.63)
    params-all-gather ..............................: (22.44, 22.67)
    optimizer-copy-to-main-grad ....................: (0.13, 0.22)
    optimizer-clip-main-grad .......................: (1.57, 2.09)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.85)
    optimizer-copy-main-to-model-params ............: (1.36, 1.47)
    optimizer ......................................: (8.97, 9.08)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (525848): No such process
kill: (525854): No such process
kill: (525860): No such process
kill: (525866): No such process
10.64.24.50 kill done.
7b, 8k, gbs=512: dp=4, tp=2, pp=2, mbs=4
LOCAL_IP = 10.64.24.51
DP=4, MP=2, PP=2
[2024-02-12 15:36:40,048] torch.distributed.run: [WARNING] 
[2024-02-12 15:36:40,048] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 15:36:40,048] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 15:36:40,048] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1714528256
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1714528256
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.449 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  5.515 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  5.668 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  6.090 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Cutting or padding data end, time cost:  10.729 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.821 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.809 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  11.007 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (1022.85, 1087.58)
    train/valid/test-data-iterators-setup ..........: (0.02, 17568.82)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 7497.5 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.089509E+01 | loss scale: 1.0 | grad norm: 3.532 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 9] (after 10 iterations) memory (MB) | allocated: 14792.7294921875 | max allocated: 48772.82666015625 | reserved: 54086.0 | max reserved: 54086.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 14792.7294921875 | max allocated: 48772.82666015625 | reserved: 54086.0 | max reserved: 54086.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (6845.08, 6943.29)
    forward-compute ................................: (1086.76, 4067.99)
    backward-compute ...............................: (1539.66, 2419.28)
    batch-generator ................................: (199.47, 251.11)
    forward-recv ...................................: (295.05, 308.74)
    forward-send ...................................: (3.22, 11.02)
    backward-recv ..................................: (144.25, 161.01)
    backward-send ..................................: (2.24, 9.22)
    forward-send-backward-recv .....................: (3863.71, 4074.09)
    backward-send-forward-recv .....................: (214.24, 275.81)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.95, 9.30)
    grads-reduce-scatter ...........................: (19.20, 432.18)
    params-all-gather ..............................: (10.63, 10.85)
    optimizer-copy-to-main-grad ....................: (0.26, 0.44)
    optimizer-clip-main-grad .......................: (7.23, 7.28)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.92, 5.24)
    optimizer-copy-main-to-model-params ............: (1.47, 1.57)
    optimizer ......................................: (15.18, 15.30)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 5296.5 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.086979E+01 | loss scale: 1.0 | grad norm: 13.830 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5137.01, 5199.31)
    forward-compute ................................: (684.49, 2946.69)
    backward-compute ...............................: (1329.73, 2134.38)
    batch-generator ................................: (26.15, 33.43)
    forward-recv ...................................: (17.21, 37.63)
    forward-send ...................................: (0.53, 1.02)
    backward-recv ..................................: (65.43, 139.33)
    backward-send ..................................: (0.69, 21.48)
    forward-send-backward-recv .....................: (2863.42, 3019.34)
    backward-send-forward-recv .....................: (143.72, 209.68)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.88, 9.85)
    grads-reduce-scatter ...........................: (19.16, 19.72)
    params-all-gather ..............................: (10.60, 10.85)
    optimizer-copy-to-main-grad ....................: (0.24, 0.41)
    optimizer-clip-main-grad .......................: (2.39, 2.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 5.02)
    optimizer-copy-main-to-model-params ............: (1.47, 1.57)
    optimizer ......................................: (9.83, 9.93)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 7121.2 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.073828E+01 | loss scale: 1.0 | grad norm: 1.618 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6948.59, 7036.37)
    forward-compute ................................: (1304.94, 4006.22)
    backward-compute ...............................: (1557.50, 2296.84)
    batch-generator ................................: (26.63, 31.60)
    forward-recv ...................................: (27.75, 716.26)
    forward-send ...................................: (0.71, 0.87)
    backward-recv ..................................: (135.35, 734.97)
    backward-send ..................................: (6.09, 13.35)
    forward-send-backward-recv .....................: (3197.99, 3742.44)
    backward-send-forward-recv .....................: (184.23, 854.49)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.92, 9.21)
    grads-reduce-scatter ...........................: (19.12, 19.82)
    params-all-gather ..............................: (10.64, 10.89)
    optimizer-copy-to-main-grad ....................: (0.25, 0.35)
    optimizer-clip-main-grad .......................: (2.38, 2.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.98)
    optimizer-copy-main-to-model-params ............: (1.47, 1.57)
    optimizer ......................................: (9.64, 9.74)
Mon Feb 12 15:41:33 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   37C    P0             274W / 700W |  57760MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   43C    P0             298W / 700W |  57760MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   45C    P0             296W / 700W |  61118MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             233W / 700W |  61118MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             501W / 700W |  59640MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             490W / 700W |  59640MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   42C    P0             173W / 700W |  66934MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             249W / 700W |  66934MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 5452.7 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.070692E+01 | loss scale: 1.0 | grad norm: 0.781 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5211.22, 5285.76)
    forward-compute ................................: (747.20, 2925.68)
    backward-compute ...............................: (1427.31, 2150.29)
    batch-generator ................................: (25.61, 31.39)
    forward-recv ...................................: (21.50, 33.22)
    forward-send ...................................: (0.62, 0.91)
    backward-recv ..................................: (80.56, 117.04)
    backward-send ..................................: (0.59, 11.17)
    forward-send-backward-recv .....................: (2869.72, 2993.16)
    backward-send-forward-recv .....................: (155.43, 215.35)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.83, 9.17)
    grads-reduce-scatter ...........................: (19.21, 19.71)
    params-all-gather ..............................: (10.62, 10.84)
    optimizer-copy-to-main-grad ....................: (0.24, 0.34)
    optimizer-clip-main-grad .......................: (1.32, 1.33)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.94)
    optimizer-copy-main-to-model-params ............: (1.47, 1.66)
    optimizer ......................................: (8.55, 8.74)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 6757.4 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.064808E+01 | loss scale: 1.0 | grad norm: 0.592 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6582.62, 6671.17)
    forward-compute ................................: (851.43, 4161.05)
    backward-compute ...............................: (1609.99, 2361.75)
    batch-generator ................................: (25.80, 34.57)
    forward-recv ...................................: (27.05, 40.75)
    forward-send ...................................: (0.75, 1.07)
    backward-recv ..................................: (78.63, 128.16)
    backward-send ..................................: (0.66, 11.52)
    forward-send-backward-recv .....................: (3884.41, 4088.14)
    backward-send-forward-recv .....................: (150.99, 349.89)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.88, 9.24)
    grads-reduce-scatter ...........................: (19.13, 19.73)
    params-all-gather ..............................: (10.64, 10.84)
    optimizer-copy-to-main-grad ....................: (0.24, 0.39)
    optimizer-clip-main-grad .......................: (1.08, 1.09)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 5.04)
    optimizer-copy-main-to-model-params ............: (1.47, 1.57)
    optimizer ......................................: (8.50, 8.60)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 6471.8 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.065631E+01 | loss scale: 1.0 | grad norm: 0.381 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6289.59, 6362.28)
    forward-compute ................................: (1264.54, 3493.37)
    backward-compute ...............................: (1498.77, 2205.31)
    batch-generator ................................: (27.31, 31.90)
    forward-recv ...................................: (22.66, 512.81)
    forward-send ...................................: (0.59, 1.11)
    backward-recv ..................................: (106.51, 121.51)
    backward-send ..................................: (0.73, 8.75)
    forward-send-backward-recv .....................: (3291.33, 3468.03)
    backward-send-forward-recv .....................: (126.31, 735.15)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.91, 9.27)
    grads-reduce-scatter ...........................: (19.17, 19.77)
    params-all-gather ..............................: (10.62, 10.86)
    optimizer-copy-to-main-grad ....................: (0.24, 0.36)
    optimizer-clip-main-grad .......................: (1.07, 1.08)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.94)
    optimizer-copy-main-to-model-params ............: (1.47, 1.57)
    optimizer ......................................: (8.31, 8.42)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 5326.9 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.064614E+01 | loss scale: 1.0 | grad norm: 0.420 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5173.08, 5244.80)
    forward-compute ................................: (758.35, 2914.63)
    backward-compute ...............................: (1459.80, 2235.86)
    batch-generator ................................: (26.76, 34.68)
    forward-recv ...................................: (20.00, 27.37)
    forward-send ...................................: (0.60, 0.76)
    backward-recv ..................................: (89.47, 132.13)
    backward-send ..................................: (0.71, 12.62)
    forward-send-backward-recv .....................: (2705.26, 2863.87)
    backward-send-forward-recv .....................: (127.36, 224.63)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.89, 9.17)
    grads-reduce-scatter ...........................: (19.18, 19.87)
    params-all-gather ..............................: (10.63, 10.83)
    optimizer-copy-to-main-grad ....................: (0.24, 0.40)
    optimizer-clip-main-grad .......................: (1.10, 1.10)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.93)
    optimizer-copy-main-to-model-params ............: (1.47, 1.62)
    optimizer ......................................: (8.36, 8.51)
Mon Feb 12 15:45:34 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   39C    P0             544W / 700W |  57760MiB / 81559MiB |     48%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   45C    P0             444W / 700W |  57760MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   46C    P0             524W / 700W |  75842MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0             452W / 700W |  75586MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             488W / 700W |  59640MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             473W / 700W |  59640MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             474W / 700W |  66934MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             462W / 700W |  66934MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 5533.4 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.062683E+01 | loss scale: 1.0 | grad norm: 0.210 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5282.33, 5362.22)
    forward-compute ................................: (800.80, 2934.71)
    backward-compute ...............................: (1519.81, 2277.53)
    batch-generator ................................: (26.79, 33.70)
    forward-recv ...................................: (18.86, 32.27)
    forward-send ...................................: (0.56, 0.91)
    backward-recv ..................................: (85.76, 135.88)
    backward-send ..................................: (0.69, 8.67)
    forward-send-backward-recv .....................: (2786.17, 2869.87)
    backward-send-forward-recv .....................: (130.67, 199.90)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.86, 9.44)
    grads-reduce-scatter ...........................: (19.12, 19.81)
    params-all-gather ..............................: (10.64, 10.84)
    optimizer-copy-to-main-grad ....................: (0.24, 0.37)
    optimizer-clip-main-grad .......................: (1.08, 1.08)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.93)
    optimizer-copy-main-to-model-params ............: (1.47, 1.62)
    optimizer ......................................: (8.29, 8.44)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 5831.2 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.062195E+01 | loss scale: 1.0 | grad norm: 0.499 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5667.40, 5740.90)
    forward-compute ................................: (773.15, 3369.86)
    backward-compute ...............................: (1479.74, 2259.10)
    batch-generator ................................: (26.70, 33.32)
    forward-recv ...................................: (26.34, 31.34)
    forward-send ...................................: (0.73, 0.84)
    backward-recv ..................................: (575.24, 610.80)
    backward-send ..................................: (0.81, 8.11)
    forward-send-backward-recv .....................: (2679.99, 2864.15)
    backward-send-forward-recv .....................: (130.51, 196.10)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.89, 9.15)
    grads-reduce-scatter ...........................: (19.22, 19.71)
    params-all-gather ..............................: (10.60, 10.84)
    optimizer-copy-to-main-grad ....................: (0.24, 0.36)
    optimizer-clip-main-grad .......................: (1.07, 1.08)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.93)
    optimizer-copy-main-to-model-params ............: (1.47, 1.57)
    optimizer ......................................: (8.27, 8.38)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 6408.6 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.062352E+01 | loss scale: 1.0 | grad norm: 0.581 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6210.85, 6293.29)
    forward-compute ................................: (1241.84, 3928.37)
    backward-compute ...............................: (1462.73, 2216.77)
    batch-generator ................................: (26.96, 33.77)
    forward-recv ...................................: (20.64, 25.88)
    forward-send ...................................: (0.62, 0.73)
    backward-recv ..................................: (108.31, 152.26)
    backward-send ..................................: (0.81, 5.54)
    forward-send-backward-recv .....................: (3226.83, 3390.97)
    backward-send-forward-recv .....................: (132.63, 681.09)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.87, 9.26)
    grads-reduce-scatter ...........................: (19.23, 19.76)
    params-all-gather ..............................: (10.61, 10.85)
    optimizer-copy-to-main-grad ....................: (0.24, 0.35)
    optimizer-clip-main-grad .......................: (1.86, 1.89)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.93)
    optimizer-copy-main-to-model-params ............: (1.47, 1.57)
    optimizer ......................................: (9.08, 9.18)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (527354): No such process
kill: (527360): No such process
kill: (527366): No such process
kill: (527372): No such process
10.64.24.50 kill done.
7b, 8k, gbs=512: dp=4, tp=2, pp=2, mbs=2
LOCAL_IP = 10.64.24.51
DP=4, MP=2, PP=2
[2024-02-12 15:49:52,584] torch.distributed.run: [WARNING] 
[2024-02-12 15:49:52,584] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 15:49:52,584] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 15:49:52,584] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1714528256
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1714528256
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.311 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  5.421 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  5.587 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  5.830 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Cutting or padding data end, time cost:  10.671 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.737 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.720 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.807 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (1022.94, 1095.01)
    train/valid/test-data-iterators-setup ..........: (0.02, 17097.94)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 8977.2 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.089509E+01 | loss scale: 1.0 | grad norm: 3.531 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 9] (after 10 iterations) memory (MB) | allocated: 14797.8388671875 | max allocated: 34347.97412109375 | reserved: 38244.0 | max reserved: 38244.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 14797.8388671875 | max allocated: 34347.97412109375 | reserved: 38242.0 | max reserved: 38242.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (8371.09, 8436.28)
    forward-compute ................................: (1371.97, 5115.09)
    backward-compute ...............................: (2063.02, 2874.16)
    batch-generator ................................: (243.63, 273.58)
    forward-recv ...................................: (284.33, 301.26)
    forward-send ...................................: (2.31, 11.57)
    backward-recv ..................................: (72.43, 104.76)
    backward-send ..................................: (0.64, 3.92)
    forward-send-backward-recv .....................: (4592.29, 4858.24)
    backward-send-forward-recv .....................: (299.00, 453.98)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.97, 9.22)
    grads-reduce-scatter ...........................: (19.20, 437.59)
    params-all-gather ..............................: (10.61, 10.85)
    optimizer-copy-to-main-grad ....................: (0.26, 0.40)
    optimizer-clip-main-grad .......................: (5.45, 5.49)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.93, 5.16)
    optimizer-copy-main-to-model-params ............: (1.47, 1.57)
    optimizer ......................................: (13.36, 14.14)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 6859.1 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.086964E+01 | loss scale: 1.0 | grad norm: 13.866 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6733.31, 6780.87)
    forward-compute ................................: (1519.35, 3499.93)
    backward-compute ...............................: (1946.87, 2552.81)
    batch-generator ................................: (49.87, 63.11)
    forward-recv ...................................: (13.32, 15.42)
    forward-send ...................................: (0.37, 0.59)
    backward-recv ..................................: (51.71, 95.54)
    backward-send ..................................: (0.49, 0.65)
    forward-send-backward-recv .....................: (3097.64, 3195.94)
    backward-send-forward-recv .....................: (711.07, 776.53)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.91, 9.06)
    grads-reduce-scatter ...........................: (19.21, 19.69)
    params-all-gather ..............................: (10.62, 10.84)
    optimizer-copy-to-main-grad ....................: (0.25, 0.35)
    optimizer-clip-main-grad .......................: (2.56, 2.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.97)
    optimizer-copy-main-to-model-params ............: (1.47, 1.57)
    optimizer ......................................: (9.85, 9.95)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 8846.1 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.073832E+01 | loss scale: 1.0 | grad norm: 1.645 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8733.88, 8779.75)
    forward-compute ................................: (1614.43, 5672.35)
    backward-compute ...............................: (2146.74, 2715.25)
    batch-generator ................................: (50.89, 62.76)
    forward-recv ...................................: (14.79, 23.50)
    forward-send ...................................: (0.40, 0.65)
    backward-recv ..................................: (48.52, 61.88)
    backward-send ..................................: (0.46, 12.90)
    forward-send-backward-recv .....................: (4881.13, 4940.47)
    backward-send-forward-recv .....................: (327.66, 852.63)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.91, 9.15)
    grads-reduce-scatter ...........................: (19.13, 19.72)
    params-all-gather ..............................: (10.60, 10.84)
    optimizer-copy-to-main-grad ....................: (0.24, 0.34)
    optimizer-clip-main-grad .......................: (2.38, 2.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.94)
    optimizer-copy-main-to-model-params ............: (1.47, 1.57)
    optimizer ......................................: (9.63, 9.73)
Mon Feb 12 15:55:46 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   37C    P0             381W / 700W |  47388MiB / 81559MiB |     18%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   44C    P0             485W / 700W |  47334MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   44C    P0             406W / 700W |  48542MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             385W / 700W |  48604MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   37C    P0             250W / 700W |  52790MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   43C    P0             248W / 700W |  52844MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   41C    P0             248W / 700W |  50796MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             194W / 700W |  50924MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 6766.4 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.070695E+01 | loss scale: 1.0 | grad norm: 0.776 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6555.29, 6603.22)
    forward-compute ................................: (1034.92, 3752.16)
    backward-compute ...............................: (1931.98, 2610.77)
    batch-generator ................................: (50.90, 61.37)
    forward-recv ...................................: (14.40, 21.41)
    forward-send ...................................: (0.42, 0.60)
    backward-recv ..................................: (62.68, 71.78)
    backward-send ..................................: (0.45, 0.63)
    forward-send-backward-recv .....................: (3293.51, 3529.08)
    backward-send-forward-recv .....................: (265.72, 331.54)
    layernorm-grads-all-reduce .....................: (0.02, 0.04)
    embedding-grads-all-reduce .....................: (8.90, 9.35)
    grads-reduce-scatter ...........................: (19.24, 19.67)
    params-all-gather ..............................: (10.62, 11.16)
    optimizer-copy-to-main-grad ....................: (0.24, 0.42)
    optimizer-clip-main-grad .......................: (1.37, 1.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.94)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (8.68, 8.77)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 9365.4 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.064808E+01 | loss scale: 1.0 | grad norm: 0.587 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9242.10, 9294.31)
    forward-compute ................................: (1612.50, 6022.62)
    backward-compute ...............................: (2140.50, 2831.30)
    batch-generator ................................: (50.63, 62.58)
    forward-recv ...................................: (16.64, 21.01)
    forward-send ...................................: (0.48, 0.56)
    backward-recv ..................................: (59.85, 98.93)
    backward-send ..................................: (0.50, 16.12)
    forward-send-backward-recv .....................: (5234.51, 5433.32)
    backward-send-forward-recv .....................: (408.85, 856.26)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.90, 9.23)
    grads-reduce-scatter ...........................: (19.19, 20.03)
    params-all-gather ..............................: (10.64, 10.84)
    optimizer-copy-to-main-grad ....................: (0.25, 0.41)
    optimizer-clip-main-grad .......................: (1.10, 1.11)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.93)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (8.37, 8.47)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 6713.3 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.065631E+01 | loss scale: 1.0 | grad norm: 0.381 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6595.04, 6633.33)
    forward-compute ................................: (1079.32, 3775.14)
    backward-compute ...............................: (2036.54, 2681.63)
    batch-generator ................................: (51.52, 59.93)
    forward-recv ...................................: (13.42, 17.59)
    forward-send ...................................: (0.37, 0.47)
    backward-recv ..................................: (60.11, 87.95)
    backward-send ..................................: (0.58, 10.43)
    forward-send-backward-recv .....................: (3275.51, 3426.32)
    backward-send-forward-recv .....................: (201.97, 334.85)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.84, 9.17)
    grads-reduce-scatter ...........................: (19.22, 19.70)
    params-all-gather ..............................: (10.63, 10.84)
    optimizer-copy-to-main-grad ....................: (0.24, 0.39)
    optimizer-clip-main-grad .......................: (1.07, 1.08)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.93)
    optimizer-copy-main-to-model-params ............: (1.47, 1.57)
    optimizer ......................................: (8.31, 8.41)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 9497.3 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.064614E+01 | loss scale: 1.0 | grad norm: 0.422 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9378.22, 9420.06)
    forward-compute ................................: (1555.38, 6062.71)
    backward-compute ...............................: (2051.03, 2645.06)
    batch-generator ................................: (50.82, 71.11)
    forward-recv ...................................: (13.83, 19.73)
    forward-send ...................................: (0.38, 0.54)
    backward-recv ..................................: (40.15, 70.94)
    backward-send ..................................: (0.52, 1.61)
    forward-send-backward-recv .....................: (5604.45, 5721.34)
    backward-send-forward-recv .....................: (695.86, 785.43)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.84, 9.23)
    grads-reduce-scatter ...........................: (19.18, 19.77)
    params-all-gather ..............................: (10.63, 10.88)
    optimizer-copy-to-main-grad ....................: (0.24, 0.41)
    optimizer-clip-main-grad .......................: (1.08, 1.10)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 5.64)
    optimizer-copy-main-to-model-params ............: (1.47, 1.57)
    optimizer ......................................: (9.08, 9.18)
Mon Feb 12 16:01:31 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   37C    P0             453W / 700W |  47388MiB / 81559MiB |     90%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   43C    P0             467W / 700W |  47334MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   43C    P0             458W / 700W |  48542MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             450W / 700W |  48608MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             448W / 700W |  52790MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   43C    P0             440W / 700W |  52844MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   41C    P0             480W / 700W |  50800MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             444W / 700W |  50924MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 8858.7 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.062683E+01 | loss scale: 1.0 | grad norm: 0.211 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8641.31, 8689.00)
    forward-compute ................................: (1584.61, 5236.74)
    backward-compute ...............................: (2094.84, 2709.05)
    batch-generator ................................: (50.49, 78.91)
    forward-recv ...................................: (14.10, 17.45)
    forward-send ...................................: (0.38, 0.50)
    backward-recv ..................................: (62.47, 80.46)
    backward-send ..................................: (0.49, 1.57)
    forward-send-backward-recv .....................: (4736.87, 4890.57)
    backward-send-forward-recv .....................: (744.60, 810.81)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.88, 9.22)
    grads-reduce-scatter ...........................: (19.17, 19.67)
    params-all-gather ..............................: (10.61, 10.91)
    optimizer-copy-to-main-grad ....................: (0.24, 0.41)
    optimizer-clip-main-grad .......................: (1.08, 1.09)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.95)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (8.37, 8.46)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 6738.2 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.062196E+01 | loss scale: 1.0 | grad norm: 0.662 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6607.05, 6661.71)
    forward-compute ................................: (1068.13, 3802.57)
    backward-compute ...............................: (2034.21, 2727.24)
    batch-generator ................................: (49.88, 77.99)
    forward-recv ...................................: (15.04, 26.96)
    forward-send ...................................: (0.43, 0.70)
    backward-recv ..................................: (50.77, 70.35)
    backward-send ..................................: (0.52, 0.98)
    forward-send-backward-recv .....................: (3153.27, 3474.67)
    backward-send-forward-recv .....................: (204.35, 369.80)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.85, 9.28)
    grads-reduce-scatter ...........................: (19.18, 19.64)
    params-all-gather ..............................: (10.61, 10.82)
    optimizer-copy-to-main-grad ....................: (0.24, 0.40)
    optimizer-clip-main-grad .......................: (1.08, 1.10)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.95)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (8.35, 8.45)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 9591.9 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.061833E+01 | loss scale: 1.0 | grad norm: 0.998 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9440.52, 9497.65)
    forward-compute ................................: (1516.61, 6205.46)
    backward-compute ...............................: (1976.40, 2740.80)
    batch-generator ................................: (50.08, 76.43)
    forward-recv ...................................: (13.75, 19.06)
    forward-send ...................................: (0.39, 0.53)
    backward-recv ..................................: (64.27, 84.22)
    backward-send ..................................: (0.52, 9.48)
    forward-send-backward-recv .....................: (5481.15, 5884.49)
    backward-send-forward-recv .....................: (678.64, 862.87)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.88, 9.19)
    grads-reduce-scatter ...........................: (19.21, 19.71)
    params-all-gather ..............................: (10.62, 10.85)
    optimizer-copy-to-main-grad ....................: (0.24, 0.38)
    optimizer-clip-main-grad .......................: (1.60, 1.62)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.95)
    optimizer-copy-main-to-model-params ............: (1.47, 1.57)
    optimizer ......................................: (8.88, 8.98)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (528860): No such process
kill: (528866): No such process
kill: (528872): No such process
kill: (528878): No such process
10.64.24.50 kill done.
7b, 8k, gbs=512: dp=4, tp=4, pp=1, mbs=4
LOCAL_IP = 10.64.24.51
DP=4, MP=4, PP=1
[2024-02-12 16:06:30,340] torch.distributed.run: [WARNING] 
[2024-02-12 16:06:30,340] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 16:06:30,340] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 16:06:30,340] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.230 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  6.459 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Cutting or padding data end, time cost:  10.509 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.754 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (447.85, 493.87)
    train/valid/test-data-iterators-setup ..........: (0.02, 17748.13)
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 6885.2 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.088693E+01 | loss scale: 1.0 | grad norm: 3.488 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6694.72, 6733.72)
    forward-compute ................................: (4158.42, 4353.95)
    backward-compute ...............................: (2324.42, 2555.48)
    batch-generator ................................: (428.84, 460.40)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (56.77, 58.53)
    params-all-gather ..............................: (29.86, 29.96)
    optimizer-copy-to-main-grad ....................: (0.55, 0.75)
    optimizer-clip-main-grad .......................: (5.35, 5.37)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.97, 5.09)
    optimizer-copy-main-to-model-params ............: (1.66, 1.75)
    optimizer ......................................: (13.49, 13.57)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 6367.7 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.086265E+01 | loss scale: 1.0 | grad norm: 15.063 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6239.32, 6252.38)
    forward-compute ................................: (4016.24, 4147.71)
    backward-compute ...............................: (2074.58, 2212.82)
    batch-generator ................................: (30.19, 36.25)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (56.40, 56.48)
    params-all-gather ..............................: (29.86, 29.98)
    optimizer-copy-to-main-grad ....................: (0.54, 0.72)
    optimizer-clip-main-grad .......................: (2.59, 2.61)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.81, 4.89)
    optimizer-copy-main-to-model-params ............: (1.66, 1.74)
    optimizer ......................................: (10.41, 10.50)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 5784.2 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.072980E+01 | loss scale: 1.0 | grad norm: 0.934 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5637.22, 5665.56)
    forward-compute ................................: (3241.08, 3302.73)
    backward-compute ...............................: (2322.98, 2394.57)
    batch-generator ................................: (29.90, 36.54)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-reduce-scatter ...........................: (56.40, 57.72)
    params-all-gather ..............................: (29.86, 29.96)
    optimizer-copy-to-main-grad ....................: (0.55, 0.68)
    optimizer-clip-main-grad .......................: (2.41, 2.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.82, 4.86)
    optimizer-copy-main-to-model-params ............: (1.66, 1.74)
    optimizer ......................................: (10.15, 10.23)
Mon Feb 12 16:11:22 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   38C    P0             478W / 700W |  75972MiB / 81559MiB |     46%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   44C    P0             510W / 700W |  76402MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   45C    P0             470W / 700W |  75826MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0             454W / 700W |  75844MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   37C    P0             230W / 700W |  74972MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   43C    P0             263W / 700W |  75298MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   41C    P0             199W / 700W |  75244MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             224W / 700W |  74894MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 6270.2 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.070628E+01 | loss scale: 1.0 | grad norm: 0.547 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6045.54, 6069.85)
    forward-compute ................................: (3816.25, 3898.57)
    backward-compute ...............................: (2144.15, 2237.78)
    batch-generator ................................: (29.60, 37.51)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (56.39, 57.63)
    params-all-gather ..............................: (29.86, 30.01)
    optimizer-copy-to-main-grad ....................: (0.52, 0.69)
    optimizer-clip-main-grad .......................: (1.39, 1.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.81, 4.88)
    optimizer-copy-main-to-model-params ............: (1.66, 1.74)
    optimizer ......................................: (9.14, 9.22)
benchmark/test_packing.sh.two.bak: line 139: 460872 Killed                  torchrun $DISTRIBUTED_ARGS pretrain_gpt.py $GPT_ARGS $DATA_ARGS $OUTPUT_ARGS --distributed-backend nccl
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (529915): No such process
kill: (529921): No such process
kill: (529927): No such process
kill: (529933): No such process
10.64.24.50 kill done.
7b, 8k, gbs=512: dp=4, tp=4, pp=1, mbs=2
LOCAL_IP = 10.64.24.51
DP=4, MP=4, PP=1
[2024-02-12 16:14:33,800] torch.distributed.run: [WARNING] 
[2024-02-12 16:14:33,800] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 16:14:33,800] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 16:14:33,800] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.306 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  6.826 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Cutting or padding data end, time cost:  10.505 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.806 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (461.54, 491.70)
    train/valid/test-data-iterators-setup ..........: (0.02, 18127.71)
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 9913.9 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.088687E+01 | loss scale: 1.0 | grad norm: 3.486 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9748.88, 9771.12)
    forward-compute ................................: (6519.06, 6785.13)
    backward-compute ...............................: (2943.52, 3219.36)
    batch-generator ................................: (474.16, 498.97)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (56.86, 56.96)
    params-all-gather ..............................: (29.88, 29.99)
    optimizer-copy-to-main-grad ....................: (0.54, 0.80)
    optimizer-clip-main-grad .......................: (5.36, 5.37)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.97, 5.12)
    optimizer-copy-main-to-model-params ............: (1.66, 1.75)
    optimizer ......................................: (13.48, 13.57)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 8562.8 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.086273E+01 | loss scale: 1.0 | grad norm: 15.069 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8434.26, 8447.98)
    forward-compute ................................: (5559.39, 5600.92)
    backward-compute ...............................: (2770.17, 2842.40)
    batch-generator ................................: (67.20, 113.50)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (56.38, 56.55)
    params-all-gather ..............................: (29.88, 30.01)
    optimizer-copy-to-main-grad ....................: (0.48, 0.88)
    optimizer-clip-main-grad .......................: (2.57, 2.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.83, 4.89)
    optimizer-copy-main-to-model-params ............: (1.66, 1.81)
    optimizer ......................................: (10.74, 10.88)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 9357.9 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.072982E+01 | loss scale: 1.0 | grad norm: 0.934 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9241.95, 9247.50)
    forward-compute ................................: (6142.60, 6226.20)
    backward-compute ...............................: (2986.82, 3070.66)
    batch-generator ................................: (71.39, 108.50)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (56.39, 56.41)
    params-all-gather ..............................: (29.87, 29.96)
    optimizer-copy-to-main-grad ....................: (0.48, 0.81)
    optimizer-clip-main-grad .......................: (2.44, 2.46)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.81, 4.93)
    optimizer-copy-main-to-model-params ............: (1.66, 1.76)
    optimizer ......................................: (10.34, 10.46)
Mon Feb 12 16:21:20 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   36C    P0             287W / 700W |  78308MiB / 81559MiB |     63%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   43C    P0             324W / 700W |  78254MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   44C    P0             249W / 700W |  78360MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             323W / 700W |  78174MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   36C    P0             300W / 700W |  76532MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   42C    P0             266W / 700W |  77308MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   40C    P0             256W / 700W |  77346MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             298W / 700W |  77202MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 8855.5 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.070629E+01 | loss scale: 1.0 | grad norm: 0.548 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8637.80, 8654.29)
    forward-compute ................................: (5732.31, 5859.73)
    backward-compute ...............................: (2742.05, 2881.26)
    batch-generator ................................: (71.70, 111.76)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (56.39, 57.40)
    params-all-gather ..............................: (29.89, 30.05)
    optimizer-copy-to-main-grad ....................: (0.49, 0.81)
    optimizer-clip-main-grad .......................: (1.47, 1.49)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.81, 4.91)
    optimizer-copy-main-to-model-params ............: (1.66, 1.74)
    optimizer ......................................: (9.41, 9.50)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 9658.5 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.064874E+01 | loss scale: 1.0 | grad norm: 0.409 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9537.99, 9547.51)
    forward-compute ................................: (6364.54, 6514.30)
    backward-compute ...............................: (2988.58, 3149.94)
    batch-generator ................................: (70.59, 88.68)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (56.40, 57.06)
    params-all-gather ..............................: (29.88, 30.01)
    optimizer-copy-to-main-grad ....................: (0.48, 0.73)
    optimizer-clip-main-grad .......................: (1.27, 1.28)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.81, 4.89)
    optimizer-copy-main-to-model-params ............: (1.66, 1.83)
    optimizer ......................................: (9.09, 9.28)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 10911.7 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.065951E+01 | loss scale: 1.0 | grad norm: 0.290 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10789.75, 10800.73)
    forward-compute ................................: (7789.69, 7902.93)
    backward-compute ...............................: (2860.88, 2975.69)
    batch-generator ................................: (70.99, 87.74)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (56.39, 57.88)
    params-all-gather ..............................: (29.85, 30.81)
    optimizer-copy-to-main-grad ....................: (0.49, 0.73)
    optimizer-clip-main-grad .......................: (1.28, 1.29)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.81, 4.90)
    optimizer-copy-main-to-model-params ............: (1.66, 1.75)
    optimizer ......................................: (9.14, 9.24)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 8895.5 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.065017E+01 | loss scale: 1.0 | grad norm: 0.581 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8772.70, 8781.25)
    forward-compute ................................: (5808.40, 5875.01)
    backward-compute ...............................: (2868.90, 2931.99)
    batch-generator ................................: (70.52, 89.75)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (56.40, 56.58)
    params-all-gather ..............................: (29.92, 30.04)
    optimizer-copy-to-main-grad ....................: (0.48, 0.68)
    optimizer-clip-main-grad .......................: (1.39, 1.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.82, 4.89)
    optimizer-copy-main-to-model-params ............: (1.66, 1.74)
    optimizer ......................................: (9.27, 9.42)
Mon Feb 12 16:27:45 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   36C    P0             313W / 700W |  79900MiB / 81559MiB |     22%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   43C    P0             385W / 700W |  79838MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   43C    P0             361W / 700W |  79954MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             362W / 700W |  79772MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   37C    P0             362W / 700W |  76532MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   42C    P0             366W / 700W |  77310MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   40C    P0             360W / 700W |  77350MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             364W / 700W |  77204MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 9108.7 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.063189E+01 | loss scale: 1.0 | grad norm: 0.498 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8890.50, 8900.80)
    forward-compute ................................: (5841.10, 5964.98)
    backward-compute ...............................: (2892.30, 3026.46)
    batch-generator ................................: (67.56, 83.63)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (56.40, 56.48)
    params-all-gather ..............................: (29.88, 30.17)
    optimizer-copy-to-main-grad ....................: (0.48, 0.69)
    optimizer-clip-main-grad .......................: (1.27, 1.28)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.81, 4.91)
    optimizer-copy-main-to-model-params ............: (1.66, 1.74)
    optimizer ......................................: (9.23, 9.32)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 8972.5 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.062837E+01 | loss scale: 1.0 | grad norm: 0.705 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8841.39, 8857.19)
    forward-compute ................................: (5760.59, 5950.68)
    backward-compute ...............................: (2865.09, 3048.90)
    batch-generator ................................: (68.62, 83.03)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (56.39, 56.47)
    params-all-gather ..............................: (29.89, 30.04)
    optimizer-copy-to-main-grad ....................: (0.48, 0.70)
    optimizer-clip-main-grad .......................: (1.66, 1.68)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.81, 4.88)
    optimizer-copy-main-to-model-params ............: (1.66, 1.74)
    optimizer ......................................: (9.50, 9.60)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 9351.1 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.062133E+01 | loss scale: 1.0 | grad norm: 0.455 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9218.62, 9235.51)
    forward-compute ................................: (6139.59, 6380.42)
    backward-compute ...............................: (2804.94, 3063.76)
    batch-generator ................................: (68.66, 84.96)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (56.39, 57.06)
    params-all-gather ..............................: (29.86, 30.10)
    optimizer-copy-to-main-grad ....................: (0.47, 0.66)
    optimizer-clip-main-grad .......................: (1.29, 1.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.82, 4.88)
    optimizer-copy-main-to-model-params ............: (1.66, 1.74)
    optimizer ......................................: (9.08, 9.17)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (530981): No such process
kill: (530987): No such process
kill: (530993): No such process
kill: (530999): No such process
10.64.24.50 kill done.
7b, 8k, gbs=512: dp=4, tp=1, pp=4, mbs=4
LOCAL_IP = 10.64.24.51
DP=4, MP=1, PP=4
[2024-02-12 16:33:06,680] torch.distributed.run: [WARNING] 
[2024-02-12 16:33:06,680] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 16:33:06,680] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 16:33:06,680] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1611038720
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1817092096
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...

> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...

 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.798 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.800 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.802 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.812 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.855 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.862 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.884 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.897 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Cutting or padding data end, time cost:  10.716 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.970 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.961 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.981 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.952 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  11.073 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  11.125 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  11.634 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (39.09, 775.78)
    train/valid/test-data-iterators-setup ..........: (15853.86, 17929.72)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 7995.0 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.087887E+01 | loss scale: 1.0 | grad norm: 3.473 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 13904.525390625 | max allocated: 36972.9345703125 | reserved: 41362.0 | max reserved: 41362.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 15673.3857421875 | max allocated: 51657.97119140625 | reserved: 57456.0 | max reserved: 57456.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (7151.66, 7377.48)
    forward-compute ................................: (639.38, 3953.49)
    backward-compute ...............................: (1227.53, 2922.77)
    batch-generator ................................: (47.73, 65.27)
    forward-recv ...................................: (97.98, 259.88)
    forward-send ...................................: (3.26, 150.90)
    backward-recv ..................................: (159.27, 564.34)
    backward-send ..................................: (0.97, 41.54)
    forward-send-backward-recv .....................: (4275.02, 4621.76)
    backward-send-forward-recv .....................: (144.63, 452.66)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.04)
    grads-reduce-scatter ...........................: (20.48, 418.32)
    params-all-gather ..............................: (9.71, 11.12)
    optimizer-copy-to-main-grad ....................: (0.16, 0.25)
    optimizer-clip-main-grad .......................: (5.40, 5.61)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.60, 5.37)
    optimizer-copy-main-to-model-params ............: (1.32, 1.54)
    optimizer ......................................: (13.27, 13.49)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 6366.7 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.085900E+01 | loss scale: 1.0 | grad norm: 12.656 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6050.43, 6224.94)
    forward-compute ................................: (489.31, 3348.27)
    backward-compute ...............................: (1046.46, 2674.35)
    batch-generator ................................: (24.87, 30.59)
    forward-recv ...................................: (17.82, 86.78)
    forward-send ...................................: (0.64, 35.23)
    backward-recv ..................................: (105.72, 499.64)
    backward-send ..................................: (0.74, 59.67)
    forward-send-backward-recv .....................: (3718.87, 4117.59)
    backward-send-forward-recv .....................: (100.70, 419.78)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.36)
    grads-reduce-scatter ...........................: (18.13, 20.92)
    params-all-gather ..............................: (9.73, 11.63)
    optimizer-copy-to-main-grad ....................: (0.13, 0.22)
    optimizer-clip-main-grad .......................: (2.28, 2.54)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.30)
    optimizer-copy-main-to-model-params ............: (1.31, 1.66)
    optimizer ......................................: (9.80, 10.16)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 6840.0 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.073626E+01 | loss scale: 1.0 | grad norm: 2.736 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6487.86, 6692.40)
    forward-compute ................................: (589.73, 3575.03)
    backward-compute ...............................: (1244.39, 2807.90)
    batch-generator ................................: (25.19, 34.85)
    forward-recv ...................................: (24.99, 66.36)
    forward-send ...................................: (0.77, 25.88)
    backward-recv ..................................: (117.71, 535.33)
    backward-send ..................................: (0.89, 34.23)
    forward-send-backward-recv .....................: (3905.67, 4228.44)
    backward-send-forward-recv .....................: (106.20, 461.04)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 17.92)
    grads-reduce-scatter ...........................: (18.13, 20.90)
    params-all-gather ..............................: (9.72, 11.58)
    optimizer-copy-to-main-grad ....................: (0.13, 0.25)
    optimizer-clip-main-grad .......................: (2.35, 2.56)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.24)
    optimizer-copy-main-to-model-params ............: (1.31, 1.53)
    optimizer ......................................: (9.75, 10.33)
Mon Feb 12 16:38:21 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   30C    P0             140W / 700W |  44464MiB / 81559MiB |     26%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   36C    P0             155W / 700W |  50478MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   37C    P0             217W / 700W |  52508MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   32C    P0             129W / 700W |  57296MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   37C    P0             165W / 700W |  60642MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   44C    P0             147W / 700W |  60158MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   42C    P0             280W / 700W |  60268MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             148W / 700W |  66314MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 6393.7 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.070470E+01 | loss scale: 1.0 | grad norm: 0.680 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6012.36, 6170.72)
    forward-compute ................................: (530.53, 3205.33)
    backward-compute ...............................: (1133.30, 2635.46)
    batch-generator ................................: (24.85, 31.80)
    forward-recv ...................................: (26.62, 76.22)
    forward-send ...................................: (0.64, 30.47)
    backward-recv ..................................: (97.08, 449.25)
    backward-send ..................................: (0.68, 40.60)
    forward-send-backward-recv .....................: (3685.27, 4004.60)
    backward-send-forward-recv .....................: (114.39, 377.54)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.10)
    grads-reduce-scatter ...........................: (18.08, 20.96)
    params-all-gather ..............................: (9.73, 11.21)
    optimizer-copy-to-main-grad ....................: (0.13, 0.22)
    optimizer-clip-main-grad .......................: (1.53, 1.61)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.47, 6.03)
    optimizer-copy-main-to-model-params ............: (1.31, 1.53)
    optimizer ......................................: (9.57, 9.79)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 7145.8 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.064603E+01 | loss scale: 1.0 | grad norm: 0.644 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6804.28, 7008.17)
    forward-compute ................................: (608.50, 3890.50)
    backward-compute ...............................: (1279.93, 2877.71)
    batch-generator ................................: (24.50, 28.81)
    forward-recv ...................................: (31.95, 93.79)
    forward-send ...................................: (0.74, 35.63)
    backward-recv ..................................: (102.49, 545.99)
    backward-send ..................................: (0.69, 44.41)
    forward-send-backward-recv .....................: (4112.62, 4482.90)
    backward-send-forward-recv .....................: (116.49, 478.99)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.94)
    grads-reduce-scatter ...........................: (18.12, 20.87)
    params-all-gather ..............................: (9.71, 11.12)
    optimizer-copy-to-main-grad ....................: (0.13, 0.21)
    optimizer-clip-main-grad .......................: (1.03, 1.03)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.19)
    optimizer-copy-main-to-model-params ............: (1.31, 1.53)
    optimizer ......................................: (8.41, 8.64)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 6460.2 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.065400E+01 | loss scale: 1.0 | grad norm: 0.348 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6131.73, 6289.95)
    forward-compute ................................: (560.31, 3329.02)
    backward-compute ...............................: (1185.27, 2716.54)
    batch-generator ................................: (24.66, 29.50)
    forward-recv ...................................: (22.75, 91.79)
    forward-send ...................................: (0.57, 28.49)
    backward-recv ..................................: (140.51, 483.08)
    backward-send ..................................: (0.79, 25.90)
    forward-send-backward-recv .....................: (3788.92, 3975.60)
    backward-send-forward-recv .....................: (69.87, 335.02)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.53)
    grads-reduce-scatter ...........................: (18.04, 20.94)
    params-all-gather ..............................: (9.72, 11.12)
    optimizer-copy-to-main-grad ....................: (0.13, 0.21)
    optimizer-clip-main-grad .......................: (1.02, 1.02)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.47, 5.19)
    optimizer-copy-main-to-model-params ............: (1.31, 1.53)
    optimizer ......................................: (8.13, 8.36)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 7879.5 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.064345E+01 | loss scale: 1.0 | grad norm: 0.389 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7590.86, 7740.95)
    forward-compute ................................: (553.44, 4296.23)
    backward-compute ...............................: (1165.41, 2738.62)
    batch-generator ................................: (25.04, 29.56)
    forward-recv ...................................: (21.23, 1206.53)
    forward-send ...................................: (0.66, 510.73)
    backward-recv ..................................: (120.76, 446.23)
    backward-send ..................................: (0.77, 24.18)
    forward-send-backward-recv .....................: (4135.77, 4977.82)
    backward-send-forward-recv .....................: (199.78, 1240.53)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.92)
    grads-reduce-scatter ...........................: (18.13, 20.92)
    params-all-gather ..............................: (9.72, 11.11)
    optimizer-copy-to-main-grad ....................: (0.13, 0.22)
    optimizer-clip-main-grad .......................: (1.01, 1.02)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.19)
    optimizer-copy-main-to-model-params ............: (1.31, 1.53)
    optimizer ......................................: (8.14, 8.51)
Mon Feb 12 16:43:12 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   32C    P0             256W / 700W |  46260MiB / 81559MiB |     68%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   37C    P0             303W / 700W |  66524MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   37C    P0             270W / 700W |  52508MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   33C    P0             289W / 700W |  57296MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             406W / 700W |  60642MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             464W / 700W |  72734MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   42C    P0             406W / 700W |  60268MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             372W / 700W |  66314MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 7604.0 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.062451E+01 | loss scale: 1.0 | grad norm: 0.280 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7218.85, 7412.59)
    forward-compute ................................: (577.87, 4375.64)
    backward-compute ...............................: (1215.32, 2802.66)
    batch-generator ................................: (24.95, 29.60)
    forward-recv ...................................: (28.51, 73.78)
    forward-send ...................................: (0.61, 23.08)
    backward-recv ..................................: (119.13, 447.71)
    backward-send ..................................: (0.75, 33.04)
    forward-send-backward-recv .....................: (4240.38, 5023.63)
    backward-send-forward-recv .....................: (93.98, 924.78)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.15)
    grads-reduce-scatter ...........................: (18.01, 20.97)
    params-all-gather ..............................: (9.72, 11.11)
    optimizer-copy-to-main-grad ....................: (0.13, 0.21)
    optimizer-clip-main-grad .......................: (1.02, 1.03)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.19)
    optimizer-copy-main-to-model-params ............: (1.31, 1.53)
    optimizer ......................................: (8.16, 8.38)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 6299.6 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.061871E+01 | loss scale: 1.0 | grad norm: 0.667 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6000.39, 6164.31)
    forward-compute ................................: (554.98, 3169.81)
    backward-compute ...............................: (1174.30, 2786.42)
    batch-generator ................................: (25.11, 28.96)
    forward-recv ...................................: (28.51, 70.86)
    forward-send ...................................: (0.80, 25.13)
    backward-recv ..................................: (118.59, 462.20)
    backward-send ..................................: (0.80, 28.92)
    forward-send-backward-recv .....................: (3553.45, 3898.65)
    backward-send-forward-recv .....................: (78.54, 373.06)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.81)
    grads-reduce-scatter ...........................: (18.18, 20.85)
    params-all-gather ..............................: (9.71, 11.10)
    optimizer-copy-to-main-grad ....................: (0.13, 0.21)
    optimizer-clip-main-grad .......................: (1.02, 1.02)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.19)
    optimizer-copy-main-to-model-params ............: (1.31, 1.53)
    optimizer ......................................: (9.03, 9.25)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 6301.1 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.061303E+01 | loss scale: 1.0 | grad norm: 0.466 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5931.09, 6121.00)
    forward-compute ................................: (547.78, 3129.35)
    backward-compute ...............................: (1135.87, 2722.25)
    batch-generator ................................: (24.61, 29.04)
    forward-recv ...................................: (18.42, 58.12)
    forward-send ...................................: (0.66, 21.46)
    backward-recv ..................................: (140.58, 537.53)
    backward-send ..................................: (0.86, 26.79)
    forward-send-backward-recv .....................: (3552.26, 3797.13)
    backward-send-forward-recv .....................: (78.16, 355.91)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.01)
    grads-reduce-scatter ...........................: (18.09, 20.84)
    params-all-gather ..............................: (9.70, 11.12)
    optimizer-copy-to-main-grad ....................: (0.13, 0.22)
    optimizer-clip-main-grad .......................: (1.14, 1.16)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.19)
    optimizer-copy-main-to-model-params ............: (1.31, 1.53)
    optimizer ......................................: (8.29, 8.50)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (533259): No such process
kill: (533265): No such process
kill: (533271): No such process
kill: (533277): No such process
10.64.24.50 kill done.
7b, 8k, gbs=512: dp=4, tp=1, pp=4, mbs=2
LOCAL_IP = 10.64.24.51
DP=4, MP=1, PP=4
[2024-02-12 16:47:39,267] torch.distributed.run: [WARNING] 
[2024-02-12 16:47:39,267] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 16:47:39,267] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 16:47:39,267] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1611038720
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1817092096
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...

> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ... > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)


Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.812 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.823 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.820 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.824 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.825 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.933 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  5.040 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  5.928 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Cutting or padding data end, time cost:  10.796 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.854 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.889 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.893 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.896 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.861 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.984 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.852 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (37.20, 746.74)
    train/valid/test-data-iterators-setup ..........: (15765.74, 17144.67)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 9312.5 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.087883E+01 | loss scale: 1.0 | grad norm: 3.474 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 13908.955078125 | max allocated: 29000.6513671875 | reserved: 32340.0 | max reserved: 32340.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 15678.4951171875 | max allocated: 35651.93896484375 | reserved: 38630.0 | max reserved: 38630.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (8568.64, 8713.65)
    forward-compute ................................: (739.92, 5031.61)
    backward-compute ...............................: (1561.28, 3183.89)
    batch-generator ................................: (62.54, 91.94)
    forward-recv ...................................: (79.04, 224.35)
    forward-send ...................................: (4.98, 127.30)
    backward-recv ..................................: (59.05, 361.71)
    backward-send ..................................: (0.74, 19.91)
    forward-send-backward-recv .....................: (5017.60, 5759.70)
    backward-send-forward-recv .....................: (318.11, 738.92)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.22)
    grads-reduce-scatter ...........................: (20.45, 430.14)
    params-all-gather ..............................: (9.72, 11.10)
    optimizer-copy-to-main-grad ....................: (0.16, 0.23)
    optimizer-clip-main-grad .......................: (5.49, 5.70)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.61, 5.36)
    optimizer-copy-main-to-model-params ............: (1.32, 1.52)
    optimizer ......................................: (14.31, 14.53)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 7212.9 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.085857E+01 | loss scale: 1.0 | grad norm: 12.619 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6969.55, 7091.45)
    forward-compute ................................: (634.78, 3814.12)
    backward-compute ...............................: (1465.82, 2909.49)
    batch-generator ................................: (47.57, 53.70)
    forward-recv ...................................: (13.08, 37.19)
    forward-send ...................................: (0.37, 12.08)
    backward-recv ..................................: (52.14, 324.75)
    backward-send ..................................: (0.52, 62.04)
    forward-send-backward-recv .....................: (4179.44, 4569.91)
    backward-send-forward-recv .....................: (253.28, 489.80)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.00)
    grads-reduce-scatter ...........................: (18.19, 20.86)
    params-all-gather ..............................: (9.72, 11.10)
    optimizer-copy-to-main-grad ....................: (0.13, 0.22)
    optimizer-clip-main-grad .......................: (2.26, 2.46)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.20)
    optimizer-copy-main-to-model-params ............: (1.31, 1.52)
    optimizer ......................................: (9.59, 9.80)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 8103.5 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.073596E+01 | loss scale: 1.0 | grad norm: 2.762 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7884.43, 7998.82)
    forward-compute ................................: (726.88, 4496.55)
    backward-compute ...............................: (1648.91, 3046.67)
    batch-generator ................................: (47.20, 55.41)
    forward-recv ...................................: (12.60, 49.96)
    forward-send ...................................: (0.40, 19.72)
    backward-recv ..................................: (48.05, 328.82)
    backward-send ..................................: (0.50, 34.85)
    forward-send-backward-recv .....................: (4768.40, 5215.53)
    backward-send-forward-recv .....................: (347.11, 627.15)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 17.87)
    grads-reduce-scatter ...........................: (18.13, 20.82)
    params-all-gather ..............................: (9.71, 11.10)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (2.26, 2.46)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.54)
    optimizer-copy-main-to-model-params ............: (1.32, 1.52)
    optimizer ......................................: (10.02, 10.23)
Mon Feb 12 16:53:40 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   31C    P0             393W / 700W |  40176MiB / 81559MiB |     90%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   36C    P0             232W / 700W |  41730MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   36C    P0             214W / 700W |  40564MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   32C    P0             177W / 700W |  46704MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             396W / 700W |  44962MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   44C    P0             360W / 700W |  44476MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   42C    P0             200W / 700W |  46166MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             227W / 700W |  47488MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 7580.2 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.070471E+01 | loss scale: 1.0 | grad norm: 0.682 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7270.43, 7390.20)
    forward-compute ................................: (638.87, 4081.63)
    backward-compute ...............................: (1461.62, 2952.24)
    batch-generator ................................: (47.09, 54.88)
    forward-recv ...................................: (14.30, 44.91)
    forward-send ...................................: (0.37, 13.51)
    backward-recv ..................................: (64.94, 268.17)
    backward-send ..................................: (0.45, 34.16)
    forward-send-backward-recv .....................: (4301.85, 4829.62)
    backward-send-forward-recv .....................: (261.74, 562.57)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.06)
    grads-reduce-scatter ...........................: (18.07, 20.80)
    params-all-gather ..............................: (9.74, 11.11)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (1.52, 1.60)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.31)
    optimizer-copy-main-to-model-params ............: (1.31, 1.90)
    optimizer ......................................: (8.85, 9.44)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 10100.4 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.064604E+01 | loss scale: 1.0 | grad norm: 0.650 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9860.88, 9986.42)
    forward-compute ................................: (722.96, 5979.95)
    backward-compute ...............................: (1639.44, 3130.30)
    batch-generator ................................: (47.20, 53.90)
    forward-recv ...................................: (17.12, 823.48)
    forward-send ...................................: (0.44, 14.81)
    backward-recv ..................................: (76.06, 286.87)
    backward-send ..................................: (0.51, 45.74)
    forward-send-backward-recv .....................: (5714.42, 6625.74)
    backward-send-forward-recv .....................: (295.48, 1302.93)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.99)
    grads-reduce-scatter ...........................: (18.11, 20.90)
    params-all-gather ..............................: (9.72, 11.11)
    optimizer-copy-to-main-grad ....................: (0.14, 0.23)
    optimizer-clip-main-grad .......................: (1.02, 1.03)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.21)
    optimizer-copy-main-to-model-params ............: (1.32, 1.52)
    optimizer ......................................: (8.71, 8.91)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 8393.4 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.065402E+01 | loss scale: 1.0 | grad norm: 0.346 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8154.87, 8250.70)
    forward-compute ................................: (688.14, 4464.16)
    backward-compute ...............................: (1564.81, 3000.68)
    batch-generator ................................: (47.16, 53.96)
    forward-recv ...................................: (11.65, 565.15)
    forward-send ...................................: (0.36, 545.54)
    backward-recv ..................................: (66.29, 265.65)
    backward-send ..................................: (0.60, 34.07)
    forward-send-backward-recv .....................: (4624.81, 5617.85)
    backward-send-forward-recv .....................: (288.93, 1070.86)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.00)
    grads-reduce-scatter ...........................: (18.02, 20.87)
    params-all-gather ..............................: (9.75, 11.08)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (1.02, 1.02)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.19)
    optimizer-copy-main-to-model-params ............: (1.32, 1.52)
    optimizer ......................................: (8.16, 8.37)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 8523.9 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.064345E+01 | loss scale: 1.0 | grad norm: 0.390 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8325.83, 8418.17)
    forward-compute ................................: (679.26, 5148.79)
    backward-compute ...............................: (1556.75, 2984.83)
    batch-generator ................................: (47.47, 52.89)
    forward-recv ...................................: (13.22, 39.90)
    forward-send ...................................: (0.44, 13.83)
    backward-recv ..................................: (58.85, 296.57)
    backward-send ..................................: (0.53, 21.72)
    forward-send-backward-recv .....................: (5395.21, 5833.37)
    backward-send-forward-recv .....................: (213.49, 489.82)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.85)
    grads-reduce-scatter ...........................: (18.14, 20.83)
    params-all-gather ..............................: (9.72, 11.11)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (1.02, 1.02)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.18)
    optimizer-copy-main-to-model-params ............: (1.32, 1.52)
    optimizer ......................................: (8.14, 8.35)
Mon Feb 12 16:59:37 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   32C    P0             291W / 700W |  41168MiB / 81559MiB |     62%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   37C    P0             319W / 700W |  41730MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   37C    P0             277W / 700W |  40566MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   33C    P0             265W / 700W |  46704MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             455W / 700W |  44962MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   44C    P0             442W / 700W |  44476MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             416W / 700W |  46166MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             444W / 700W |  47488MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 8696.7 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.062456E+01 | loss scale: 1.0 | grad norm: 0.323 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8381.74, 8498.48)
    forward-compute ................................: (703.99, 5124.71)
    backward-compute ...............................: (1584.51, 3020.69)
    batch-generator ................................: (46.56, 53.43)
    forward-recv ...................................: (17.12, 37.45)
    forward-send ...................................: (0.38, 11.69)
    backward-recv ..................................: (62.57, 268.83)
    backward-send ..................................: (0.48, 27.77)
    forward-send-backward-recv .....................: (5334.06, 5804.50)
    backward-send-forward-recv .....................: (249.68, 522.45)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.10)
    grads-reduce-scatter ...........................: (18.02, 20.90)
    params-all-gather ..............................: (9.72, 11.10)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (1.03, 1.03)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.18)
    optimizer-copy-main-to-model-params ............: (1.32, 1.52)
    optimizer ......................................: (8.16, 8.37)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 8056.0 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.061891E+01 | loss scale: 1.0 | grad norm: 0.726 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7822.54, 7936.83)
    forward-compute ................................: (686.14, 4689.04)
    backward-compute ...............................: (1559.29, 3057.19)
    batch-generator ................................: (46.99, 54.81)
    forward-recv ...................................: (17.33, 54.11)
    forward-send ...................................: (0.41, 18.67)
    backward-recv ..................................: (66.04, 273.58)
    backward-send ..................................: (0.54, 13.42)
    forward-send-backward-recv .....................: (4034.35, 5296.73)
    backward-send-forward-recv .....................: (199.52, 1045.63)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.02)
    grads-reduce-scatter ...........................: (18.09, 20.80)
    params-all-gather ..............................: (9.71, 11.09)
    optimizer-copy-to-main-grad ....................: (0.13, 0.22)
    optimizer-clip-main-grad .......................: (1.02, 1.03)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.18)
    optimizer-copy-main-to-model-params ............: (1.32, 1.52)
    optimizer ......................................: (8.15, 8.39)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 7861.9 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.061449E+01 | loss scale: 1.0 | grad norm: 0.750 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7580.94, 7706.07)
    forward-compute ................................: (659.69, 4221.28)
    backward-compute ...............................: (1512.10, 3070.42)
    batch-generator ................................: (46.81, 52.75)
    forward-recv ...................................: (13.64, 36.81)
    forward-send ...................................: (0.38, 8.46)
    backward-recv ..................................: (80.32, 328.23)
    backward-send ..................................: (0.58, 21.81)
    forward-send-backward-recv .....................: (4368.26, 5073.90)
    backward-send-forward-recv .....................: (246.42, 747.40)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 17.98)
    grads-reduce-scatter ...........................: (18.10, 20.91)
    params-all-gather ..............................: (9.72, 11.11)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (1.15, 1.17)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.68)
    optimizer-copy-main-to-model-params ............: (1.32, 1.52)
    optimizer ......................................: (9.12, 9.32)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (535537): No such process
kill: (535543): No such process
kill: (535549): No such process
kill: (535555): No such process
10.64.24.50 kill done.
7b, 8k, gbs=512: dp=2, tp=4, pp=2, mbs=8
LOCAL_IP = 10.64.24.51
DP=2, MP=4, PP=2
[2024-02-12 17:04:37,022] torch.distributed.run: [WARNING] 
[2024-02-12 17:04:37,022] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 17:04:37,022] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 17:04:37,022] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 857726976
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 857726976
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.272 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  5.366 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Cutting or padding data end, time cost:  10.587 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.633 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (999.56, 1068.03)
    train/valid/test-data-iterators-setup ..........: (0.02, 16461.87)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 6923.9 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.090099E+01 | loss scale: 1.0 | grad norm: 3.570 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 9919.6787109375 | max allocated: 41387.22705078125 | reserved: 51922.0 | max reserved: 51922.0
[Rank 10] (after 10 iterations) memory (MB) | allocated: 9919.6787109375 | max allocated: 41387.22705078125 | reserved: 52066.0 | max reserved: 52066.0
[Rank 11] (after 10 iterations) memory (MB) | allocated: 9920.0927734375 | max allocated: 41387.64111328125 | reserved: 52020.0 | max reserved: 52020.0
[Rank 9] (after 10 iterations) memory (MB) | allocated: 9920.0927734375 | max allocated: 41387.64111328125 | reserved: 52056.0 | max reserved: 52056.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (6572.64, 6658.16)
    forward-compute ................................: (1547.94, 3332.98)
    backward-compute ...............................: (1702.05, 2592.28)
    batch-generator ................................: (400.09, 444.47)
    forward-recv ...................................: (474.51, 517.03)
    forward-send ...................................: (3.43, 7.02)
    backward-recv ..................................: (154.22, 161.87)
    backward-send ..................................: (1.64, 10.46)
    forward-send-backward-recv .....................: (3027.97, 3205.36)
    backward-send-forward-recv .....................: (271.72, 296.39)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.65, 4.94)
    grads-reduce-scatter ...........................: (8.02, 215.48)
    params-all-gather ..............................: (4.79, 5.04)
    optimizer-copy-to-main-grad ....................: (0.53, 0.71)
    optimizer-clip-main-grad .......................: (5.76, 5.83)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.02, 5.38)
    optimizer-copy-main-to-model-params ............: (1.70, 1.83)
    optimizer ......................................: (14.35, 14.51)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 4797.1 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.086953E+01 | loss scale: 1.0 | grad norm: 13.134 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4689.13, 4751.29)
    forward-compute ................................: (883.13, 2329.42)
    backward-compute ...............................: (1440.61, 2296.55)
    batch-generator ................................: (28.62, 33.50)
    forward-recv ...................................: (33.35, 49.05)
    forward-send ...................................: (0.88, 1.45)
    backward-recv ..................................: (78.16, 109.20)
    backward-send ..................................: (1.03, 9.40)
    forward-send-backward-recv .....................: (2163.59, 2284.28)
    backward-send-forward-recv .....................: (110.48, 258.30)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.59, 4.88)
    grads-reduce-scatter ...........................: (8.15, 8.65)
    params-all-gather ..............................: (4.78, 5.04)
    optimizer-copy-to-main-grad ....................: (0.49, 0.68)
    optimizer-clip-main-grad .......................: (2.57, 2.64)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.86, 5.11)
    optimizer-copy-main-to-model-params ............: (1.70, 1.82)
    optimizer ......................................: (10.57, 10.69)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 6215.6 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.074391E+01 | loss scale: 1.0 | grad norm: 1.245 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6080.00, 6171.21)
    forward-compute ................................: (1517.54, 2924.97)
    backward-compute ...............................: (1665.87, 2472.42)
    batch-generator ................................: (28.40, 35.74)
    forward-recv ...................................: (33.38, 35.51)
    forward-send ...................................: (1.19, 1.26)
    backward-recv ..................................: (76.21, 115.76)
    backward-send ..................................: (4.10, 5.37)
    forward-send-backward-recv .....................: (2797.76, 2846.85)
    backward-send-forward-recv .....................: (658.56, 718.80)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.62, 4.94)
    grads-reduce-scatter ...........................: (8.03, 8.78)
    params-all-gather ..............................: (4.79, 5.03)
    optimizer-copy-to-main-grad ....................: (0.49, 0.66)
    optimizer-clip-main-grad .......................: (2.57, 2.64)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.86, 5.17)
    optimizer-copy-main-to-model-params ............: (1.70, 1.82)
    optimizer ......................................: (10.63, 10.76)
Mon Feb 12 17:09:03 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   38C    P0             333W / 700W |  58764MiB / 81559MiB |     72%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   45C    P0             394W / 700W |  58946MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   46C    P0             379W / 700W |  58956MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0             384W / 700W |  58670MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   40C    P0             440W / 700W |  56736MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   47C    P0             444W / 700W |  56890MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   45C    P0             406W / 700W |  56890MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   40C    P0             393W / 700W |  56650MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 4894.0 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.071058E+01 | loss scale: 1.0 | grad norm: 0.603 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4705.71, 4770.60)
    forward-compute ................................: (941.81, 2227.10)
    backward-compute ...............................: (1540.05, 2276.50)
    batch-generator ................................: (28.75, 34.31)
    forward-recv ...................................: (30.29, 35.55)
    forward-send ...................................: (1.11, 1.30)
    backward-recv ..................................: (69.71, 81.47)
    backward-send ..................................: (1.75, 4.30)
    forward-send-backward-recv .....................: (2170.56, 2175.15)
    backward-send-forward-recv .....................: (163.16, 205.29)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.62, 4.86)
    grads-reduce-scatter ...........................: (8.12, 8.62)
    params-all-gather ..............................: (4.78, 5.07)
    optimizer-copy-to-main-grad ....................: (0.49, 0.69)
    optimizer-clip-main-grad .......................: (1.42, 1.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.14)
    optimizer-copy-main-to-model-params ............: (1.70, 1.82)
    optimizer ......................................: (9.40, 9.52)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 6452.4 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.065219E+01 | loss scale: 1.0 | grad norm: 0.584 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6351.62, 6403.85)
    forward-compute ................................: (1098.00, 3542.14)
    backward-compute ...............................: (1772.64, 2547.89)
    batch-generator ................................: (29.16, 36.01)
    forward-recv ...................................: (35.55, 44.61)
    forward-send ...................................: (1.26, 1.58)
    backward-recv ..................................: (71.56, 94.30)
    backward-send ..................................: (1.20, 3.61)
    forward-send-backward-recv .....................: (3390.24, 3426.80)
    backward-send-forward-recv .....................: (218.38, 268.04)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.61, 4.83)
    grads-reduce-scatter ...........................: (8.07, 8.66)
    params-all-gather ..............................: (4.81, 5.65)
    optimizer-copy-to-main-grad ....................: (0.49, 0.66)
    optimizer-clip-main-grad .......................: (1.29, 1.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.12)
    optimizer-copy-main-to-model-params ............: (1.70, 1.82)
    optimizer ......................................: (9.24, 9.37)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 5845.1 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.066105E+01 | loss scale: 1.0 | grad norm: 0.424 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5722.96, 5801.31)
    forward-compute ................................: (1454.59, 2751.83)
    backward-compute ...............................: (1609.41, 2355.39)
    batch-generator ................................: (27.77, 37.10)
    forward-recv ...................................: (25.61, 39.52)
    forward-send ...................................: (0.93, 1.42)
    backward-recv ..................................: (94.44, 108.21)
    backward-send ..................................: (1.43, 3.27)
    forward-send-backward-recv .....................: (2558.08, 2598.59)
    backward-send-forward-recv .....................: (584.01, 643.21)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.59, 4.98)
    grads-reduce-scatter ...........................: (8.12, 8.72)
    params-all-gather ..............................: (4.79, 5.08)
    optimizer-copy-to-main-grad ....................: (0.49, 0.67)
    optimizer-clip-main-grad .......................: (1.27, 1.28)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.12)
    optimizer-copy-main-to-model-params ............: (1.70, 1.89)
    optimizer ......................................: (9.22, 9.42)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 4911.3 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.065093E+01 | loss scale: 1.0 | grad norm: 0.392 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4802.40, 4865.40)
    forward-compute ................................: (948.11, 2358.53)
    backward-compute ...............................: (1566.65, 2426.53)
    batch-generator ................................: (29.91, 36.51)
    forward-recv ...................................: (26.40, 31.70)
    forward-send ...................................: (0.98, 1.18)
    backward-recv ..................................: (89.46, 102.24)
    backward-send ..................................: (1.15, 3.06)
    forward-send-backward-recv .....................: (2025.80, 2220.51)
    backward-send-forward-recv .....................: (113.10, 189.97)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.62, 4.87)
    grads-reduce-scatter ...........................: (8.11, 8.70)
    params-all-gather ..............................: (4.80, 5.08)
    optimizer-copy-to-main-grad ....................: (0.50, 0.66)
    optimizer-clip-main-grad .......................: (1.29, 1.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.11)
    optimizer-copy-main-to-model-params ............: (1.70, 1.82)
    optimizer ......................................: (9.38, 9.95)
Mon Feb 12 17:12:46 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   39C    P0             528W / 700W |  65100MiB / 81559MiB |     51%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   46C    P0             510W / 700W |  65282MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   47C    P0             460W / 700W |  65292MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   40C    P0             507W / 700W |  65006MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   40C    P0             450W / 700W |  56736MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   46C    P0             421W / 700W |  56890MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   44C    P0             467W / 700W |  56890MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   40C    P0             442W / 700W |  56650MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 5087.8 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.063203E+01 | loss scale: 1.0 | grad norm: 0.270 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4894.67, 4954.52)
    forward-compute ................................: (1037.52, 2263.78)
    backward-compute ...............................: (1683.89, 2414.42)
    batch-generator ................................: (29.36, 36.40)
    forward-recv ...................................: (26.46, 34.20)
    forward-send ...................................: (0.97, 1.25)
    backward-recv ..................................: (79.15, 94.10)
    backward-send ..................................: (1.02, 1.23)
    forward-send-backward-recv .....................: (2104.10, 2136.50)
    backward-send-forward-recv .....................: (179.41, 183.28)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.55, 4.97)
    grads-reduce-scatter ...........................: (8.17, 8.61)
    params-all-gather ..............................: (4.82, 5.05)
    optimizer-copy-to-main-grad ....................: (0.49, 0.65)
    optimizer-clip-main-grad .......................: (1.31, 1.32)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.12)
    optimizer-copy-main-to-model-params ............: (1.69, 1.92)
    optimizer ......................................: (9.27, 9.50)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 5907.8 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.062580E+01 | loss scale: 1.0 | grad norm: 0.578 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5802.14, 5860.22)
    forward-compute ................................: (1030.78, 3235.45)
    backward-compute ...............................: (1645.63, 2419.45)
    batch-generator ................................: (28.98, 37.27)
    forward-recv ...................................: (36.53, 36.95)
    forward-send ...................................: (1.29, 1.33)
    backward-recv ..................................: (94.18, 108.91)
    backward-send ..................................: (1.27, 1.44)
    forward-send-backward-recv .....................: (2602.57, 3025.17)
    backward-send-forward-recv .....................: (147.80, 180.33)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.62, 4.93)
    grads-reduce-scatter ...........................: (8.21, 8.64)
    params-all-gather ..............................: (4.75, 5.05)
    optimizer-copy-to-main-grad ....................: (0.49, 0.67)
    optimizer-clip-main-grad .......................: (1.31, 1.32)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.12)
    optimizer-copy-main-to-model-params ............: (1.70, 1.82)
    optimizer ......................................: (9.29, 9.42)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 5345.2 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.062372E+01 | loss scale: 1.0 | grad norm: 1.220 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5213.97, 5296.68)
    forward-compute ................................: (1031.97, 2638.13)
    backward-compute ...............................: (1609.47, 2392.50)
    batch-generator ................................: (28.66, 36.59)
    forward-recv ...................................: (27.52, 29.54)
    forward-send ...................................: (1.01, 1.15)
    backward-recv ..................................: (98.73, 139.29)
    backward-send ..................................: (1.28, 3.00)
    forward-send-backward-recv .....................: (2042.20, 2480.69)
    backward-send-forward-recv .....................: (157.78, 586.78)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.59, 5.10)
    grads-reduce-scatter ...........................: (8.18, 8.76)
    params-all-gather ..............................: (4.79, 5.71)
    optimizer-copy-to-main-grad ....................: (0.49, 0.69)
    optimizer-clip-main-grad .......................: (1.83, 1.86)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.15)
    optimizer-copy-main-to-model-params ............: (1.70, 1.83)
    optimizer ......................................: (9.91, 10.04)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (536651): No such process
kill: (536657): No such process
kill: (536663): No such process
kill: (536669): No such process
10.64.24.50 kill done.
7b, 8k, gbs=512: dp=2, tp=4, pp=2, mbs=4
LOCAL_IP = 10.64.24.51
DP=2, MP=4, PP=2
[2024-02-12 17:16:54,517] torch.distributed.run: [WARNING] 
[2024-02-12 17:16:54,517] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 17:16:54,517] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 17:16:54,517] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 857726976
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.328 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  5.345 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Cutting or padding data end, time cost:  10.573 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.674 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (1593.25, 1625.54)
    train/valid/test-data-iterators-setup ..........: (0.02, 16479.38)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 7768.5 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.090103E+01 | loss scale: 1.0 | grad norm: 3.570 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 11] (after 10 iterations) memory (MB) | allocated: 9930.2255859375 | max allocated: 32928.5634765625 | reserved: 38100.0 | max reserved: 38100.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 9930.2255859375 | max allocated: 32926.5693359375 | reserved: 37974.0 | max reserved: 37974.0[Rank 10] (after 10 iterations) memory (MB) | allocated: 9930.2255859375 | max allocated: 32928.1650390625 | reserved: 37994.0 | max reserved: 37994.0[Rank 9] (after 10 iterations) memory (MB) | allocated: 9930.2255859375 | max allocated: 32928.1650390625 | reserved: 37994.0 | max reserved: 37994.0


(min, max) time across ranks (ms):
    forward-backward ...............................: (7440.14, 7485.19)
    forward-compute ................................: (1746.31, 3739.74)
    backward-compute ...............................: (2132.12, 2878.98)
    batch-generator ................................: (440.74, 471.21)
    forward-recv ...................................: (447.87, 483.96)
    forward-send ...................................: (2.44, 6.72)
    backward-recv ..................................: (65.34, 88.26)
    backward-send ..................................: (0.97, 1.02)
    forward-send-backward-recv .....................: (3398.71, 3493.44)
    backward-send-forward-recv .....................: (440.38, 442.71)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.71, 4.98)
    grads-reduce-scatter ...........................: (8.16, 225.60)
    params-all-gather ..............................: (4.79, 5.06)
    optimizer-copy-to-main-grad ....................: (0.54, 0.72)
    optimizer-clip-main-grad .......................: (5.54, 5.61)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.00, 5.36)
    optimizer-copy-main-to-model-params ............: (1.69, 1.83)
    optimizer ......................................: (14.11, 14.27)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 6087.3 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.086959E+01 | loss scale: 1.0 | grad norm: 13.168 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6009.56, 6047.78)
    forward-compute ................................: (1667.34, 2654.75)
    backward-compute ...............................: (1905.15, 2566.60)
    batch-generator ................................: (52.89, 63.34)
    forward-recv ...................................: (14.92, 16.56)
    forward-send ...................................: (0.52, 0.59)
    backward-recv ..................................: (47.45, 71.09)
    backward-send ..................................: (0.67, 0.83)
    forward-send-backward-recv .....................: (2354.96, 2379.84)
    backward-send-forward-recv .....................: (781.02, 827.85)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.64, 4.81)
    grads-reduce-scatter ...........................: (8.15, 8.50)
    params-all-gather ..............................: (4.75, 5.02)
    optimizer-copy-to-main-grad ....................: (0.50, 0.66)
    optimizer-clip-main-grad .......................: (2.60, 2.67)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.14)
    optimizer-copy-main-to-model-params ............: (1.69, 1.82)
    optimizer ......................................: (10.62, 10.76)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 7205.3 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.074413E+01 | loss scale: 1.0 | grad norm: 1.307 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7136.74, 7167.58)
    forward-compute ................................: (1778.80, 3502.26)
    backward-compute ...............................: (2136.85, 2761.95)
    batch-generator ................................: (54.81, 63.43)
    forward-recv ...................................: (21.21, 21.80)
    forward-send ...................................: (0.73, 0.84)
    backward-recv ..................................: (31.94, 41.19)
    backward-send ..................................: (3.50, 9.08)
    forward-send-backward-recv .....................: (3154.97, 3187.95)
    backward-send-forward-recv .....................: (830.00, 889.05)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.66, 4.73)
    grads-reduce-scatter ...........................: (8.12, 8.63)
    params-all-gather ..............................: (4.78, 5.02)
    optimizer-copy-to-main-grad ....................: (0.50, 0.65)
    optimizer-clip-main-grad .......................: (2.61, 2.68)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.15)
    optimizer-copy-main-to-model-params ............: (1.69, 1.82)
    optimizer ......................................: (10.60, 10.74)
Mon Feb 12 17:22:02 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   38C    P0             374W / 700W |  44816MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   45C    P0             448W / 700W |  44884MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   46C    P0             449W / 700W |  44884MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0             448W / 700W |  44750MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             261W / 700W |  49730MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   44C    P0             230W / 700W |  49324MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   42C    P0             248W / 700W |  49850MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             216W / 700W |  49438MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 5819.9 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.071070E+01 | loss scale: 1.0 | grad norm: 0.609 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5649.10, 5687.10)
    forward-compute ................................: (1218.18, 2723.44)
    backward-compute ...............................: (1937.27, 2603.69)
    batch-generator ................................: (53.12, 63.81)
    forward-recv ...................................: (17.49, 21.35)
    forward-send ...................................: (0.61, 0.75)
    backward-recv ..................................: (58.93, 62.49)
    backward-send ..................................: (0.70, 0.83)
    forward-send-backward-recv .....................: (2353.25, 2441.13)
    backward-send-forward-recv .....................: (318.87, 335.01)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.59, 4.77)
    grads-reduce-scatter ...........................: (8.10, 8.61)
    params-all-gather ..............................: (4.83, 5.21)
    optimizer-copy-to-main-grad ....................: (0.50, 0.64)
    optimizer-clip-main-grad .......................: (1.44, 1.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.12)
    optimizer-copy-main-to-model-params ............: (1.68, 1.82)
    optimizer ......................................: (9.38, 9.52)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 6874.3 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.065235E+01 | loss scale: 1.0 | grad norm: 0.575 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6794.48, 6829.69)
    forward-compute ................................: (1824.71, 3593.29)
    backward-compute ...............................: (2210.05, 2861.74)
    batch-generator ................................: (53.28, 63.71)
    forward-recv ...................................: (20.78, 21.59)
    forward-send ...................................: (0.73, 0.78)
    backward-recv ..................................: (37.50, 74.22)
    backward-send ..................................: (0.75, 6.64)
    forward-send-backward-recv .....................: (2655.03, 2732.78)
    backward-send-forward-recv .....................: (355.69, 438.84)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.67, 4.77)
    grads-reduce-scatter ...........................: (8.10, 8.66)
    params-all-gather ..............................: (4.79, 5.02)
    optimizer-copy-to-main-grad ....................: (0.50, 0.65)
    optimizer-clip-main-grad .......................: (1.29, 1.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.13)
    optimizer-copy-main-to-model-params ............: (1.69, 1.82)
    optimizer ......................................: (9.19, 9.33)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 5813.8 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.066113E+01 | loss scale: 1.0 | grad norm: 0.443 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5740.71, 5770.76)
    forward-compute ................................: (1239.04, 2734.14)
    backward-compute ...............................: (2043.28, 2666.64)
    batch-generator ................................: (52.47, 62.64)
    forward-recv ...................................: (15.74, 17.76)
    forward-send ...................................: (0.54, 0.63)
    backward-recv ..................................: (46.49, 49.21)
    backward-send ..................................: (3.34, 3.45)
    forward-send-backward-recv .....................: (2395.65, 2419.42)
    backward-send-forward-recv .....................: (299.66, 340.71)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.61, 4.83)
    grads-reduce-scatter ...........................: (8.20, 8.59)
    params-all-gather ..............................: (4.78, 5.04)
    optimizer-copy-to-main-grad ....................: (0.51, 0.66)
    optimizer-clip-main-grad .......................: (1.28, 1.29)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.84, 5.12)
    optimizer-copy-main-to-model-params ............: (1.69, 1.82)
    optimizer ......................................: (9.26, 9.40)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 7162.0 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.065100E+01 | loss scale: 1.0 | grad norm: 0.400 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7087.16, 7122.47)
    forward-compute ................................: (1714.41, 3623.50)
    backward-compute ...............................: (2048.08, 2668.13)
    batch-generator ................................: (51.66, 63.38)
    forward-recv ...................................: (18.22, 21.32)
    forward-send ...................................: (0.64, 0.78)
    backward-recv ..................................: (34.66, 56.94)
    backward-send ..................................: (0.77, 0.89)
    forward-send-backward-recv .....................: (3264.13, 3294.37)
    backward-send-forward-recv .....................: (764.81, 769.95)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.63, 4.86)
    grads-reduce-scatter ...........................: (8.12, 8.58)
    params-all-gather ..............................: (4.76, 5.03)
    optimizer-copy-to-main-grad ....................: (0.50, 0.64)
    optimizer-clip-main-grad .......................: (1.29, 1.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.13)
    optimizer-copy-main-to-model-params ............: (1.68, 1.82)
    optimizer ......................................: (9.24, 9.38)
Mon Feb 12 17:26:24 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   39C    P0             484W / 700W |  44816MiB / 81559MiB |     78%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   45C    P0             449W / 700W |  44884MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   46C    P0             447W / 700W |  44884MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   40C    P0             482W / 700W |  44750MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             467W / 700W |  56576MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   46C    P0             467W / 700W |  56170MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             428W / 700W |  56696MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             447W / 700W |  56342MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 6403.6 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.063207E+01 | loss scale: 1.0 | grad norm: 0.243 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6228.35, 6273.30)
    forward-compute ................................: (1731.27, 3155.92)
    backward-compute ...............................: (2099.08, 2722.56)
    batch-generator ................................: (51.57, 62.38)
    forward-recv ...................................: (15.64, 18.06)
    forward-send ...................................: (0.55, 0.66)
    backward-recv ..................................: (56.16, 60.78)
    backward-send ..................................: (0.69, 0.86)
    forward-send-backward-recv .....................: (2331.63, 2355.00)
    backward-send-forward-recv .....................: (320.25, 746.90)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.61, 4.79)
    grads-reduce-scatter ...........................: (8.11, 8.58)
    params-all-gather ..............................: (4.81, 5.02)
    optimizer-copy-to-main-grad ....................: (0.50, 0.65)
    optimizer-clip-main-grad .......................: (1.30, 1.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.13)
    optimizer-copy-main-to-model-params ............: (1.68, 1.84)
    optimizer ......................................: (9.23, 9.38)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 5852.5 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.062571E+01 | loss scale: 1.0 | grad norm: 0.594 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5766.87, 5817.23)
    forward-compute ................................: (1244.79, 2790.54)
    backward-compute ...............................: (2061.53, 2724.47)
    batch-generator ................................: (54.95, 61.99)
    forward-recv ...................................: (20.59, 23.46)
    forward-send ...................................: (0.72, 0.84)
    backward-recv ..................................: (50.36, 51.34)
    backward-send ..................................: (0.68, 1.18)
    forward-send-backward-recv .....................: (2332.72, 2436.16)
    backward-send-forward-recv .....................: (261.15, 353.25)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.62, 4.73)
    grads-reduce-scatter ...........................: (8.16, 8.58)
    params-all-gather ..............................: (4.79, 5.02)
    optimizer-copy-to-main-grad ....................: (0.51, 0.62)
    optimizer-clip-main-grad .......................: (1.27, 1.28)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.13)
    optimizer-copy-main-to-model-params ............: (1.69, 1.82)
    optimizer ......................................: (9.19, 9.33)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 7189.1 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.062047E+01 | loss scale: 1.0 | grad norm: 0.905 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7105.19, 7140.59)
    forward-compute ................................: (1693.38, 4173.73)
    backward-compute ...............................: (2009.80, 2706.48)
    batch-generator ................................: (52.83, 63.01)
    forward-recv ...................................: (16.71, 17.55)
    forward-send ...................................: (0.60, 0.63)
    backward-recv ..................................: (50.18, 54.85)
    backward-send ..................................: (0.82, 5.04)
    forward-send-backward-recv .....................: (3195.27, 3358.74)
    backward-send-forward-recv .....................: (262.95, 804.08)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.59, 4.90)
    grads-reduce-scatter ...........................: (8.09, 8.66)
    params-all-gather ..............................: (4.80, 5.03)
    optimizer-copy-to-main-grad ....................: (0.50, 0.68)
    optimizer-clip-main-grad .......................: (1.42, 1.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.84, 5.12)
    optimizer-copy-main-to-model-params ............: (1.68, 1.82)
    optimizer ......................................: (9.39, 9.53)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (537765): No such process
kill: (537771): No such process
kill: (537777): No such process
kill: (537783): No such process
10.64.24.50 kill done.
7b, 8k, gbs=512: dp=2, tp=4, pp=2, mbs=2
LOCAL_IP = 10.64.24.51
DP=2, MP=4, PP=2
[2024-02-12 17:30:52,135] torch.distributed.run: [WARNING] 
[2024-02-12 17:30:52,135] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 17:30:52,135] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 17:30:52,135] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 857726976
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.237 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  5.236 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Cutting or padding data end, time cost:  10.536 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.565 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (1014.87, 1119.27)
    train/valid/test-data-iterators-setup ..........: (0.02, 16439.82)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 10601.3 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.090099E+01 | loss scale: 1.0 | grad norm: 3.570 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 11] (after 10 iterations) memory (MB) | allocated: 9897.5224609375 | max allocated: 24935.927734375 | reserved: 30078.0 | max reserved: 30078.0
[Rank 9] (after 10 iterations) memory (MB) | allocated: 9897.5224609375 | max allocated: 24935.927734375 | reserved: 30052.0 | max reserved: 30052.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 9897.5224609375 | max allocated: 24935.927734375 | reserved: 30068.0 | max reserved: 30068.0[Rank 10] (after 10 iterations) memory (MB) | allocated: 9897.3037109375 | max allocated: 24935.708984375 | reserved: 29956.0 | max reserved: 29956.0

(min, max) time across ranks (ms):
    forward-backward ...............................: (10303.29, 10346.82)
    forward-compute ................................: (2980.06, 5151.58)
    backward-compute ...............................: (3061.71, 3502.76)
    batch-generator ................................: (499.98, 536.51)
    forward-recv ...................................: (447.39, 477.64)
    forward-send ...................................: (2.45, 11.58)
    backward-recv ..................................: (42.21, 48.99)
    backward-send ..................................: (5.94, 6.44)
    forward-send-backward-recv .....................: (4144.95, 4180.01)
    backward-send-forward-recv .....................: (1153.78, 1181.59)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.77, 5.06)
    grads-reduce-scatter ...........................: (8.12, 217.14)
    params-all-gather ..............................: (4.80, 5.08)
    optimizer-copy-to-main-grad ....................: (0.54, 0.86)
    optimizer-clip-main-grad .......................: (6.49, 6.57)
    optimizer-count-zeros ..........................: (0.01, 0.02)
    optimizer-inner-step ...........................: (5.01, 5.48)
    optimizer-copy-main-to-model-params ............: (1.69, 1.83)
    optimizer ......................................: (15.38, 15.54)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 9250.0 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.086965E+01 | loss scale: 1.0 | grad norm: 13.137 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9181.31, 9210.26)
    forward-compute ................................: (2425.13, 5012.87)
    backward-compute ...............................: (2877.12, 3254.74)
    batch-generator ................................: (109.73, 157.24)
    forward-recv ...................................: (13.29, 14.83)
    forward-send ...................................: (0.37, 0.42)
    backward-recv ..................................: (26.84, 31.64)
    backward-send ..................................: (1.01, 6.01)
    forward-send-backward-recv .....................: (3754.99, 3813.21)
    backward-send-forward-recv .....................: (907.06, 974.40)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.69, 4.96)
    grads-reduce-scatter ...........................: (8.04, 8.58)
    params-all-gather ..............................: (4.81, 5.03)
    optimizer-copy-to-main-grad ....................: (0.52, 0.80)
    optimizer-clip-main-grad .......................: (2.68, 2.75)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.86, 5.22)
    optimizer-copy-main-to-model-params ............: (1.69, 2.33)
    optimizer ......................................: (10.93, 11.57)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 9882.1 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.074391E+01 | loss scale: 1.0 | grad norm: 1.245 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9822.13, 9847.47)
    forward-compute ................................: (2507.01, 5290.15)
    backward-compute ...............................: (3062.00, 3439.14)
    batch-generator ................................: (107.78, 154.96)
    forward-recv ...................................: (13.55, 18.65)
    forward-send ...................................: (0.40, 0.63)
    backward-recv ..................................: (25.14, 26.32)
    backward-send ..................................: (0.46, 0.55)
    forward-send-backward-recv .....................: (4122.32, 4200.86)
    backward-send-forward-recv .....................: (1058.71, 1086.72)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.66, 4.81)
    grads-reduce-scatter ...........................: (8.18, 8.51)
    params-all-gather ..............................: (4.80, 5.08)
    optimizer-copy-to-main-grad ....................: (0.50, 0.74)
    optimizer-clip-main-grad .......................: (2.62, 2.69)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.86, 5.19)
    optimizer-copy-main-to-model-params ............: (1.69, 1.82)
    optimizer ......................................: (10.99, 11.27)
Mon Feb 12 17:38:11 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   36C    P0             381W / 700W |  38408MiB / 81559MiB |     25%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   42C    P0             395W / 700W |  38784MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   43C    P0             424W / 700W |  38752MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             383W / 700W |  38632MiB / 81559MiB |     97%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   36C    P0             345W / 700W |  39506MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   42C    P0             394W / 700W |  39612MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   40C    P0             366W / 700W |  39552MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             383W / 700W |  39484MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 10422.6 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.071060E+01 | loss scale: 1.0 | grad norm: 0.602 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10263.67, 10300.97)
    forward-compute ................................: (2915.36, 5511.11)
    backward-compute ...............................: (2878.17, 3260.29)
    batch-generator ................................: (110.17, 155.49)
    forward-recv ...................................: (13.74, 17.87)
    forward-send ...................................: (0.42, 0.61)
    backward-recv ..................................: (23.11, 504.30)
    backward-send ..................................: (0.45, 3.47)
    forward-send-backward-recv .....................: (3839.65, 4416.99)
    backward-send-forward-recv .....................: (1446.88, 1467.59)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.68, 4.87)
    grads-reduce-scatter ...........................: (8.11, 8.57)
    params-all-gather ..............................: (4.81, 5.02)
    optimizer-copy-to-main-grad ....................: (0.51, 0.76)
    optimizer-clip-main-grad .......................: (1.47, 1.49)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.21)
    optimizer-copy-main-to-model-params ............: (1.69, 1.82)
    optimizer ......................................: (9.59, 9.73)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 10243.6 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.065224E+01 | loss scale: 1.0 | grad norm: 0.583 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10179.95, 10209.75)
    forward-compute ................................: (2551.91, 5586.32)
    backward-compute ...............................: (3126.07, 3536.18)
    batch-generator ................................: (112.20, 153.51)
    forward-recv ...................................: (15.57, 16.54)
    forward-send ...................................: (0.48, 0.55)
    backward-recv ..................................: (26.00, 28.39)
    backward-send ..................................: (0.50, 0.60)
    forward-send-backward-recv .....................: (4396.59, 4451.16)
    backward-send-forward-recv .....................: (1068.26, 1167.71)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.63, 4.84)
    grads-reduce-scatter ...........................: (8.07, 8.63)
    params-all-gather ..............................: (4.82, 5.03)
    optimizer-copy-to-main-grad ....................: (0.52, 0.75)
    optimizer-clip-main-grad .......................: (1.33, 1.34)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.87, 5.61)
    optimizer-copy-main-to-model-params ............: (1.69, 1.82)
    optimizer ......................................: (10.23, 10.36)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 9598.1 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.066109E+01 | loss scale: 1.0 | grad norm: 0.434 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9534.98, 9561.98)
    forward-compute ................................: (2450.95, 5189.78)
    backward-compute ...............................: (2973.19, 3329.70)
    batch-generator ................................: (108.93, 162.12)
    forward-recv ...................................: (12.93, 16.32)
    forward-send ...................................: (0.37, 0.45)
    backward-recv ..................................: (31.34, 32.60)
    backward-send ..................................: (0.46, 0.82)
    forward-send-backward-recv .....................: (3806.68, 4059.60)
    backward-send-forward-recv .....................: (963.65, 1077.46)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.65, 4.88)
    grads-reduce-scatter ...........................: (8.16, 8.65)
    params-all-gather ..............................: (4.79, 5.03)
    optimizer-copy-to-main-grad ....................: (0.49, 0.78)
    optimizer-clip-main-grad .......................: (1.34, 1.35)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.87, 5.20)
    optimizer-copy-main-to-model-params ............: (1.69, 1.82)
    optimizer ......................................: (9.50, 9.63)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 9681.4 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.065097E+01 | loss scale: 1.0 | grad norm: 0.400 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9619.13, 9647.17)
    forward-compute ................................: (2911.61, 4789.90)
    backward-compute ...............................: (2966.27, 3332.52)
    batch-generator ................................: (110.32, 153.36)
    forward-recv ...................................: (13.78, 16.55)
    forward-send ...................................: (0.38, 0.54)
    backward-recv ..................................: (26.36, 26.86)
    backward-send ..................................: (0.46, 2.44)
    forward-send-backward-recv .....................: (3353.74, 3697.21)
    backward-send-forward-recv .....................: (1440.75, 1615.91)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.64, 4.81)
    grads-reduce-scatter ...........................: (8.21, 8.55)
    params-all-gather ..............................: (4.79, 5.01)
    optimizer-copy-to-main-grad ....................: (0.51, 0.75)
    optimizer-clip-main-grad .......................: (1.37, 1.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.86, 5.17)
    optimizer-copy-main-to-model-params ............: (1.69, 1.82)
    optimizer ......................................: (9.43, 9.56)
Mon Feb 12 17:44:45 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   37C    P0             385W / 700W |  39200MiB / 81559MiB |     64%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   44C    P0             405W / 700W |  39576MiB / 81559MiB |     97%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   45C    P0             411W / 700W |  39544MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0             407W / 700W |  39424MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   37C    P0             393W / 700W |  39512MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   43C    P0             405W / 700W |  39616MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   41C    P0             381W / 700W |  39552MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             392W / 700W |  39488MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 9807.4 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.063203E+01 | loss scale: 1.0 | grad norm: 0.250 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9650.47, 9677.14)
    forward-compute ................................: (2438.69, 5338.82)
    backward-compute ...............................: (2960.06, 3452.11)
    batch-generator ................................: (109.09, 152.28)
    forward-recv ...................................: (13.76, 14.23)
    forward-send ...................................: (0.38, 0.62)
    backward-recv ..................................: (35.00, 37.45)
    backward-send ..................................: (0.61, 0.68)
    forward-send-backward-recv .....................: (3835.97, 4192.61)
    backward-send-forward-recv .....................: (929.97, 1107.49)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.64, 4.92)
    grads-reduce-scatter ...........................: (8.03, 8.59)
    params-all-gather ..............................: (4.72, 5.03)
    optimizer-copy-to-main-grad ....................: (0.51, 0.75)
    optimizer-clip-main-grad .......................: (1.34, 1.35)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.86, 5.21)
    optimizer-copy-main-to-model-params ............: (1.69, 1.82)
    optimizer ......................................: (9.43, 9.56)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 9832.4 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.062574E+01 | loss scale: 1.0 | grad norm: 0.599 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9758.37, 9789.73)
    forward-compute ................................: (2452.04, 5399.29)
    backward-compute ...............................: (2987.43, 3432.49)
    batch-generator ................................: (110.99, 153.66)
    forward-recv ...................................: (14.53, 16.83)
    forward-send ...................................: (0.47, 0.50)
    backward-recv ..................................: (32.75, 43.48)
    backward-send ..................................: (0.56, 0.68)
    forward-send-backward-recv .....................: (4007.90, 4260.19)
    backward-send-forward-recv .....................: (946.94, 1077.27)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.65, 4.87)
    grads-reduce-scatter ...........................: (8.22, 8.51)
    params-all-gather ..............................: (4.79, 5.02)
    optimizer-copy-to-main-grad ....................: (0.50, 0.75)
    optimizer-clip-main-grad .......................: (1.33, 1.34)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.86, 5.20)
    optimizer-copy-main-to-model-params ............: (1.69, 1.82)
    optimizer ......................................: (9.42, 9.55)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 9688.3 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.062323E+01 | loss scale: 1.0 | grad norm: 0.908 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9622.42, 9648.21)
    forward-compute ................................: (2422.95, 5340.23)
    backward-compute ...............................: (2936.72, 3414.28)
    batch-generator ................................: (112.18, 154.89)
    forward-recv ...................................: (13.94, 14.49)
    forward-send ...................................: (0.41, 0.47)
    backward-recv ..................................: (31.04, 46.70)
    backward-send ..................................: (0.60, 7.03)
    forward-send-backward-recv .....................: (3925.54, 4193.59)
    backward-send-forward-recv .....................: (911.30, 1061.43)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.64, 4.84)
    grads-reduce-scatter ...........................: (8.17, 8.61)
    params-all-gather ..............................: (4.81, 5.05)
    optimizer-copy-to-main-grad ....................: (0.51, 0.77)
    optimizer-clip-main-grad .......................: (1.71, 1.74)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.87, 5.21)
    optimizer-copy-main-to-model-params ............: (1.69, 1.82)
    optimizer ......................................: (9.81, 9.94)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (538879): No such process
kill: (538885): No such process
kill: (538891): No such process
kill: (538897): No such process
10.64.24.50 kill done.
7b, 8k, gbs=512: dp=2, tp=2, pp=4, mbs=8
LOCAL_IP = 10.64.24.51
DP=2, MP=2, PP=4
[2024-02-12 17:50:20,056] torch.distributed.run: [WARNING] 
[2024-02-12 17:50:20,056] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 17:50:20,056] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 17:50:20,056] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 805617664
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 805617664
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 908910592
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 908910592
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.819 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.811 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  5.204 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  5.958 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Cutting or padding data end, time cost:  10.812 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.869 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.954 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.832 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (51.87, 770.88)
    train/valid/test-data-iterators-setup ..........: (0.02, 17181.59)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 7268.9 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.089583E+01 | loss scale: 1.0 | grad norm: 3.563 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 9] (after 10 iterations) memory (MB) | allocated: 9322.189453125 | max allocated: 33686.5419921875 | reserved: 49512.0 | max reserved: 49512.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 9322.189453125 | max allocated: 33686.4365234375 | reserved: 48822.0 | max reserved: 48822.0[Rank 13] (after 10 iterations) memory (MB) | allocated: 10506.1669921875 | max allocated: 42694.65673828125 | reserved: 52744.0 | max reserved: 52744.0

[Rank 12] (after 10 iterations) memory (MB) | allocated: 10506.1669921875 | max allocated: 42695.57080078125 | reserved: 52744.0 | max reserved: 52744.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (6771.72, 6966.38)
    forward-compute ................................: (879.25, 3177.94)
    backward-compute ...............................: (1290.87, 3042.84)
    batch-generator ................................: (128.12, 158.16)
    forward-recv ...................................: (194.72, 539.17)
    forward-send ...................................: (4.92, 321.47)
    backward-recv ..................................: (151.20, 473.11)
    backward-send ..................................: (1.44, 22.57)
    forward-send-backward-recv .....................: (3533.61, 3834.87)
    backward-send-forward-recv .....................: (113.30, 320.46)
    layernorm-grads-all-reduce .....................: (0.02, 0.04)
    embedding-grads-all-reduce .....................: (0.02, 9.39)
    grads-reduce-scatter ...........................: (8.74, 217.31)
    params-all-gather ..............................: (4.18, 4.90)
    optimizer-copy-to-main-grad ....................: (0.28, 0.40)
    optimizer-clip-main-grad .......................: (6.81, 7.05)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.65, 5.58)
    optimizer-copy-main-to-model-params ............: (1.42, 1.68)
    optimizer ......................................: (15.21, 15.48)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 5723.7 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.088896E+01 | loss scale: 1.0 | grad norm: 16.395 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5517.76, 5665.51)
    forward-compute ................................: (614.21, 2692.77)
    backward-compute ...............................: (1081.90, 2788.05)
    batch-generator ................................: (27.90, 32.11)
    forward-recv ...................................: (27.90, 90.55)
    forward-send ...................................: (0.72, 31.30)
    backward-recv ..................................: (111.85, 379.54)
    backward-send ..................................: (0.93, 33.20)
    forward-send-backward-recv .....................: (3168.74, 3547.66)
    backward-send-forward-recv .....................: (50.06, 296.44)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.54)
    grads-reduce-scatter ...........................: (7.66, 9.17)
    params-all-gather ..............................: (4.18, 4.91)
    optimizer-copy-to-main-grad ....................: (0.26, 0.36)
    optimizer-clip-main-grad .......................: (2.35, 2.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.53, 5.34)
    optimizer-copy-main-to-model-params ............: (1.42, 1.67)
    optimizer ......................................: (10.09, 10.34)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 6089.7 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.075276E+01 | loss scale: 1.0 | grad norm: 1.181 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5826.50, 6025.30)
    forward-compute ................................: (744.78, 2747.82)
    backward-compute ...............................: (1311.68, 2948.57)
    batch-generator ................................: (28.37, 32.65)
    forward-recv ...................................: (31.31, 77.75)
    forward-send ...................................: (0.98, 25.90)
    backward-recv ..................................: (102.10, 481.81)
    backward-send ..................................: (1.34, 36.80)
    forward-send-backward-recv .....................: (3226.41, 3392.28)
    backward-send-forward-recv .....................: (66.54, 287.58)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.34)
    grads-reduce-scatter ...........................: (7.74, 9.28)
    params-all-gather ..............................: (4.21, 4.95)
    optimizer-copy-to-main-grad ....................: (0.26, 0.37)
    optimizer-clip-main-grad .......................: (2.35, 2.56)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.31)
    optimizer-copy-main-to-model-params ............: (1.42, 1.69)
    optimizer ......................................: (10.15, 10.42)
Mon Feb 12 17:55:07 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   31C    P0             228W / 700W |  57166MiB / 81559MiB |     49%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   36C    P0             165W / 700W |  57856MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   37C    P0             226W / 700W |  59300MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   33C    P0             285W / 700W |  59226MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             261W / 700W |  56172MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             294W / 700W |  56172MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   44C    P0             459W / 700W |  54904MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             305W / 700W |  54904MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 5748.9 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.070690E+01 | loss scale: 1.0 | grad norm: 0.578 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5459.05, 5603.74)
    forward-compute ................................: (662.85, 2560.16)
    backward-compute ...............................: (1172.76, 2775.77)
    batch-generator ................................: (27.90, 34.44)
    forward-recv ...................................: (32.23, 79.13)
    forward-send ...................................: (0.94, 23.91)
    backward-recv ..................................: (105.45, 408.20)
    backward-send ..................................: (0.75, 28.29)
    forward-send-backward-recv .....................: (3138.56, 3287.67)
    backward-send-forward-recv .....................: (51.03, 229.76)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.51)
    grads-reduce-scatter ...........................: (7.76, 9.04)
    params-all-gather ..............................: (4.24, 4.94)
    optimizer-copy-to-main-grad ....................: (0.26, 0.36)
    optimizer-clip-main-grad .......................: (1.36, 1.51)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.53, 5.30)
    optimizer-copy-main-to-model-params ............: (1.42, 1.67)
    optimizer ......................................: (9.07, 9.32)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 6268.2 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.064815E+01 | loss scale: 1.0 | grad norm: 0.460 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6047.89, 6196.37)
    forward-compute ................................: (770.46, 2839.48)
    backward-compute ...............................: (1360.86, 3004.13)
    batch-generator ................................: (28.64, 38.44)
    forward-recv ...................................: (34.97, 99.55)
    forward-send ...................................: (1.06, 31.62)
    backward-recv ..................................: (102.02, 410.76)
    backward-send ..................................: (1.07, 33.33)
    forward-send-backward-recv .....................: (3320.36, 3521.24)
    backward-send-forward-recv .....................: (96.93, 291.14)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.49)
    grads-reduce-scatter ...........................: (7.72, 9.17)
    params-all-gather ..............................: (4.21, 4.97)
    optimizer-copy-to-main-grad ....................: (0.26, 0.58)
    optimizer-clip-main-grad .......................: (1.11, 1.12)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.30)
    optimizer-copy-main-to-model-params ............: (1.42, 1.68)
    optimizer ......................................: (8.91, 9.17)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 5811.6 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.065656E+01 | loss scale: 1.0 | grad norm: 0.424 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5588.71, 5752.62)
    forward-compute ................................: (703.84, 2628.02)
    backward-compute ...............................: (1241.32, 2859.68)
    batch-generator ................................: (28.35, 34.64)
    forward-recv ...................................: (31.11, 88.83)
    forward-send ...................................: (0.79, 24.25)
    backward-recv ..................................: (126.61, 408.31)
    backward-send ..................................: (1.00, 21.31)
    forward-send-backward-recv .....................: (3187.47, 3296.45)
    backward-send-forward-recv .....................: (51.79, 226.73)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.58)
    grads-reduce-scatter ...........................: (7.75, 9.18)
    params-all-gather ..............................: (4.15, 4.94)
    optimizer-copy-to-main-grad ....................: (0.25, 0.37)
    optimizer-clip-main-grad .......................: (1.11, 1.11)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.31)
    optimizer-copy-main-to-model-params ............: (1.42, 1.68)
    optimizer ......................................: (8.60, 8.87)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 6959.3 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.064613E+01 | loss scale: 1.0 | grad norm: 0.383 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6755.54, 6906.15)
    forward-compute ................................: (687.90, 2743.90)
    backward-compute ...............................: (1208.69, 2908.86)
    batch-generator ................................: (27.89, 34.45)
    forward-recv ...................................: (58.16, 1263.81)
    forward-send ...................................: (0.84, 505.37)
    backward-recv ..................................: (125.55, 407.54)
    backward-send ..................................: (1.04, 16.03)
    forward-send-backward-recv .....................: (3229.91, 3374.31)
    backward-send-forward-recv .....................: (159.08, 1257.33)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.35)
    grads-reduce-scatter ...........................: (7.71, 9.13)
    params-all-gather ..............................: (4.22, 5.43)
    optimizer-copy-to-main-grad ....................: (0.25, 0.37)
    optimizer-clip-main-grad .......................: (1.11, 1.12)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.53, 5.31)
    optimizer-copy-main-to-model-params ............: (1.42, 1.69)
    optimizer ......................................: (8.63, 8.90)
Mon Feb 12 17:59:17 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   32C    P0             210W / 700W |  64636MiB / 81559MiB |      7%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   37C    P0             286W / 700W |  65326MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   37C    P0             247W / 700W |  60248MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   33C    P0             301W / 700W |  60174MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   40C    P0             421W / 700W |  62476MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   46C    P0             410W / 700W |  62476MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   44C    P0             434W / 700W |  54904MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             402W / 700W |  54904MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 5914.2 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.062702E+01 | loss scale: 1.0 | grad norm: 0.294 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5632.54, 5767.72)
    forward-compute ................................: (727.63, 2598.01)
    backward-compute ...............................: (1280.72, 2898.70)
    batch-generator ................................: (28.23, 32.62)
    forward-recv ...................................: (35.37, 75.56)
    forward-send ...................................: (0.84, 16.36)
    backward-recv ..................................: (117.32, 387.65)
    backward-send ..................................: (0.95, 10.41)
    forward-send-backward-recv .....................: (3165.86, 3257.04)
    backward-send-forward-recv .....................: (73.93, 234.07)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.65)
    grads-reduce-scatter ...........................: (7.64, 8.94)
    params-all-gather ..............................: (4.16, 4.92)
    optimizer-copy-to-main-grad ....................: (0.24, 0.37)
    optimizer-clip-main-grad .......................: (1.11, 1.12)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.29)
    optimizer-copy-main-to-model-params ............: (1.42, 1.67)
    optimizer ......................................: (8.61, 8.86)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 5808.7 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.062242E+01 | loss scale: 1.0 | grad norm: 0.588 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5610.86, 5746.01)
    forward-compute ................................: (711.18, 2584.41)
    backward-compute ...............................: (1251.75, 2895.08)
    batch-generator ................................: (28.29, 34.49)
    forward-recv ...................................: (36.54, 81.99)
    forward-send ...................................: (1.08, 29.07)
    backward-recv ..................................: (127.35, 373.50)
    backward-send ..................................: (1.11, 15.57)
    forward-send-backward-recv .....................: (3065.27, 3297.82)
    backward-send-forward-recv .....................: (69.27, 260.11)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.32)
    grads-reduce-scatter ...........................: (7.72, 9.15)
    params-all-gather ..............................: (4.17, 4.89)
    optimizer-copy-to-main-grad ....................: (0.25, 0.37)
    optimizer-clip-main-grad .......................: (1.11, 1.74)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.31)
    optimizer-copy-main-to-model-params ............: (1.42, 1.69)
    optimizer ......................................: (9.28, 9.54)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 6465.0 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.062364E+01 | loss scale: 1.0 | grad norm: 0.672 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6215.62, 6406.20)
    forward-compute ................................: (703.72, 3265.18)
    backward-compute ...............................: (1239.27, 2888.48)
    batch-generator ................................: (28.14, 32.09)
    forward-recv ...................................: (22.45, 64.86)
    forward-send ...................................: (0.86, 14.95)
    backward-recv ..................................: (132.73, 469.03)
    backward-send ..................................: (1.14, 21.72)
    forward-send-backward-recv .....................: (3825.06, 3936.46)
    backward-send-forward-recv .....................: (63.19, 184.75)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.53)
    grads-reduce-scatter ...........................: (7.72, 9.17)
    params-all-gather ..............................: (4.22, 4.97)
    optimizer-copy-to-main-grad ....................: (0.26, 0.37)
    optimizer-clip-main-grad .......................: (1.73, 1.84)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.31)
    optimizer-copy-main-to-model-params ............: (1.42, 1.69)
    optimizer ......................................: (9.36, 9.63)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (540371): No such process
kill: (540377): No such process
kill: (540383): No such process
kill: (540389): No such process
10.64.24.50 kill done.
7b, 8k, gbs=512: dp=2, tp=2, pp=4, mbs=4
LOCAL_IP = 10.64.24.51
DP=2, MP=2, PP=4
[2024-02-12 18:03:37,595] torch.distributed.run: [WARNING] 
[2024-02-12 18:03:37,595] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 18:03:37,595] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 18:03:37,595] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 805617664
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 805617664
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 908910592
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 908910592
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.767 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.766 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.808 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.904 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Cutting or padding data end, time cost:  10.711 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.805 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.760 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  11.252 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (36.25, 785.34)
    train/valid/test-data-iterators-setup ..........: (0.02, 16602.84)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 7828.6 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.089586E+01 | loss scale: 1.0 | grad norm: 3.564 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 9] (after 10 iterations) memory (MB) | allocated: 9332.736328125 | max allocated: 23854.8466796875 | reserved: 37214.0 | max reserved: 37214.0[Rank 8] (after 10 iterations) memory (MB) | allocated: 9332.736328125 | max allocated: 23855.0419921875 | reserved: 36660.0 | max reserved: 36660.0

[Rank 13] (after 10 iterations) memory (MB) | allocated: 10517.5341796875 | max allocated: 31602.2392578125 | reserved: 34398.0 | max reserved: 34398.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 10517.0263671875 | max allocated: 31603.8876953125 | reserved: 34398.0 | max reserved: 34398.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (7415.01, 7521.63)
    forward-compute ................................: (1019.17, 3481.23)
    backward-compute ...............................: (1573.03, 3110.09)
    batch-generator ................................: (152.52, 188.25)
    forward-recv ...................................: (200.99, 509.97)
    forward-send ...................................: (6.88, 326.98)
    backward-recv ..................................: (61.67, 276.64)
    backward-send ..................................: (0.83, 21.09)
    forward-send-backward-recv .....................: (3595.10, 4135.55)
    backward-send-forward-recv .....................: (374.04, 680.42)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.42)
    grads-reduce-scatter ...........................: (8.70, 217.20)
    params-all-gather ..............................: (4.21, 4.91)
    optimizer-copy-to-main-grad ....................: (0.28, 0.42)
    optimizer-clip-main-grad .......................: (6.56, 6.79)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.65, 5.60)
    optimizer-copy-main-to-model-params ............: (1.42, 1.68)
    optimizer ......................................: (15.06, 15.33)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 6150.5 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.088895E+01 | loss scale: 1.0 | grad norm: 16.391 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6002.17, 6081.30)
    forward-compute ................................: (784.71, 2889.30)
    backward-compute ...............................: (1445.09, 2849.71)
    batch-generator ................................: (49.14, 56.90)
    forward-recv ...................................: (19.03, 43.15)
    forward-send ...................................: (0.41, 13.70)
    backward-recv ..................................: (51.98, 262.16)
    backward-send ..................................: (0.68, 30.78)
    forward-send-backward-recv .....................: (3204.54, 3460.25)
    backward-send-forward-recv .....................: (240.76, 435.19)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.26)
    grads-reduce-scatter ...........................: (7.71, 9.11)
    params-all-gather ..............................: (4.22, 4.92)
    optimizer-copy-to-main-grad ....................: (0.26, 0.37)
    optimizer-clip-main-grad .......................: (2.35, 2.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.51, 5.32)
    optimizer-copy-main-to-model-params ............: (1.42, 1.67)
    optimizer ......................................: (10.08, 10.33)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 6656.0 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.075293E+01 | loss scale: 1.0 | grad norm: 1.198 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6520.00, 6609.86)
    forward-compute ................................: (884.79, 3148.50)
    backward-compute ...............................: (1624.35, 3010.03)
    batch-generator ................................: (49.51, 56.89)
    forward-recv ...................................: (21.76, 45.97)
    forward-send ...................................: (0.59, 14.73)
    backward-recv ..................................: (43.23, 225.60)
    backward-send ..................................: (2.76, 25.05)
    forward-send-backward-recv .....................: (3343.83, 3733.57)
    backward-send-forward-recv .....................: (299.12, 563.71)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.12)
    grads-reduce-scatter ...........................: (7.73, 8.99)
    params-all-gather ..............................: (4.18, 4.90)
    optimizer-copy-to-main-grad ....................: (0.26, 0.37)
    optimizer-clip-main-grad .......................: (2.35, 2.56)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.53, 5.32)
    optimizer-copy-main-to-model-params ............: (1.42, 1.67)
    optimizer ......................................: (10.09, 10.34)
Mon Feb 12 18:08:49 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   31C    P0             250W / 700W |  47402MiB / 81559MiB |     84%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   36C    P0             291W / 700W |  47956MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   36C    P0             188W / 700W |  50442MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   32C    P0             149W / 700W |  50742MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             407W / 700W |  40976MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   44C    P0             457W / 700W |  40976MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   41C    P0             312W / 700W |  43884MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             288W / 700W |  42308MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 6774.4 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.070695E+01 | loss scale: 1.0 | grad norm: 0.579 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6545.71, 6632.63)
    forward-compute ................................: (795.74, 3377.43)
    backward-compute ...............................: (1479.96, 2876.77)
    batch-generator ................................: (48.98, 60.92)
    forward-recv ...................................: (18.67, 45.28)
    forward-send ...................................: (0.49, 13.90)
    backward-recv ..................................: (58.55, 221.92)
    backward-send ..................................: (0.73, 16.95)
    forward-send-backward-recv .....................: (3252.08, 3916.96)
    backward-send-forward-recv .....................: (234.74, 916.23)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.20)
    grads-reduce-scatter ...........................: (7.69, 9.05)
    params-all-gather ..............................: (4.19, 4.90)
    optimizer-copy-to-main-grad ....................: (0.25, 0.38)
    optimizer-clip-main-grad .......................: (1.36, 1.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.29)
    optimizer-copy-main-to-model-params ............: (1.42, 1.67)
    optimizer ......................................: (8.92, 9.16)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 7937.6 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.064817E+01 | loss scale: 1.0 | grad norm: 0.459 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7787.52, 7889.05)
    forward-compute ................................: (941.26, 3545.95)
    backward-compute ...............................: (1662.64, 3094.20)
    batch-generator ................................: (48.42, 61.27)
    forward-recv ...................................: (25.12, 683.99)
    forward-send ...................................: (0.60, 649.21)
    backward-recv ..................................: (48.12, 210.29)
    backward-send ..................................: (0.69, 38.21)
    forward-send-backward-recv .....................: (3638.14, 4185.13)
    backward-send-forward-recv .....................: (499.06, 1209.24)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.20)
    grads-reduce-scatter ...........................: (7.61, 9.18)
    params-all-gather ..............................: (4.20, 4.93)
    optimizer-copy-to-main-grad ....................: (0.25, 0.38)
    optimizer-clip-main-grad .......................: (1.16, 1.23)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.53, 5.30)
    optimizer-copy-main-to-model-params ............: (1.42, 1.67)
    optimizer ......................................: (8.76, 9.01)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 7498.5 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.065655E+01 | loss scale: 1.0 | grad norm: 0.421 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7347.27, 7435.33)
    forward-compute ................................: (834.44, 4157.31)
    backward-compute ...............................: (1534.34, 2928.13)
    batch-generator ................................: (49.44, 58.52)
    forward-recv ...................................: (18.03, 43.45)
    forward-send ...................................: (0.44, 10.83)
    backward-recv ..................................: (56.70, 223.25)
    backward-send ..................................: (0.73, 18.42)
    forward-send-backward-recv .....................: (4413.41, 4692.35)
    backward-send-forward-recv .....................: (214.19, 415.11)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.38)
    grads-reduce-scatter ...........................: (7.68, 8.99)
    params-all-gather ..............................: (4.17, 5.33)
    optimizer-copy-to-main-grad ....................: (0.26, 0.36)
    optimizer-clip-main-grad .......................: (1.10, 1.11)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.30)
    optimizer-copy-main-to-model-params ............: (1.42, 1.67)
    optimizer ......................................: (8.60, 8.86)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 7276.2 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.064613E+01 | loss scale: 1.0 | grad norm: 0.375 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7147.58, 7225.15)
    forward-compute ................................: (838.95, 3441.31)
    backward-compute ...............................: (1545.06, 2930.38)
    batch-generator ................................: (49.90, 59.32)
    forward-recv ...................................: (16.02, 45.76)
    forward-send ...................................: (0.50, 15.27)
    backward-recv ..................................: (54.09, 191.77)
    backward-send ..................................: (0.60, 10.88)
    forward-send-backward-recv .....................: (3630.46, 4503.81)
    backward-send-forward-recv .....................: (297.48, 818.40)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.25)
    grads-reduce-scatter ...........................: (7.68, 9.14)
    params-all-gather ..............................: (4.20, 4.89)
    optimizer-copy-to-main-grad ....................: (0.26, 0.36)
    optimizer-clip-main-grad .......................: (1.11, 1.11)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.30)
    optimizer-copy-main-to-model-params ............: (1.42, 1.67)
    optimizer ......................................: (8.60, 8.84)
Mon Feb 12 18:13:41 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   32C    P0             254W / 700W |  47402MiB / 81559MiB |     15%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   38C    P0             311W / 700W |  47956MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   38C    P0             235W / 700W |  61182MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   34C    P0             277W / 700W |  61708MiB / 81559MiB |     97%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   40C    P0             484W / 700W |  40976MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   46C    P0             440W / 700W |  40976MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   44C    P0             420W / 700W |  50188MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   40C    P0             452W / 700W |  48612MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 6458.5 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.062702E+01 | loss scale: 1.0 | grad norm: 0.290 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6215.53, 6316.85)
    forward-compute ................................: (855.54, 3005.99)
    backward-compute ...............................: (1579.93, 2976.97)
    batch-generator ................................: (49.39, 59.61)
    forward-recv ...................................: (20.01, 39.64)
    forward-send ...................................: (0.44, 7.09)
    backward-recv ..................................: (64.07, 218.82)
    backward-send ..................................: (0.59, 15.30)
    forward-send-backward-recv .....................: (3195.58, 3503.97)
    backward-send-forward-recv .....................: (201.80, 454.58)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.35)
    grads-reduce-scatter ...........................: (7.67, 9.05)
    params-all-gather ..............................: (4.20, 4.92)
    optimizer-copy-to-main-grad ....................: (0.25, 0.35)
    optimizer-clip-main-grad .......................: (1.11, 1.12)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.29)
    optimizer-copy-main-to-model-params ............: (1.42, 1.66)
    optimizer ......................................: (8.64, 8.89)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 6284.2 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.062237E+01 | loss scale: 1.0 | grad norm: 0.731 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6133.54, 6240.02)
    forward-compute ................................: (849.72, 2968.33)
    backward-compute ...............................: (1565.47, 2972.18)
    batch-generator ................................: (49.35, 59.85)
    forward-recv ...................................: (21.83, 52.39)
    forward-send ...................................: (0.58, 12.26)
    backward-recv ..................................: (59.41, 227.19)
    backward-send ..................................: (0.62, 11.95)
    forward-send-backward-recv .....................: (3027.72, 3422.60)
    backward-send-forward-recv .....................: (167.00, 479.12)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.34)
    grads-reduce-scatter ...........................: (7.67, 9.05)
    params-all-gather ..............................: (4.17, 4.92)
    optimizer-copy-to-main-grad ....................: (0.25, 0.36)
    optimizer-clip-main-grad .......................: (1.11, 1.11)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.30)
    optimizer-copy-main-to-model-params ............: (1.42, 1.67)
    optimizer ......................................: (8.60, 8.85)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 6209.7 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.061928E+01 | loss scale: 1.0 | grad norm: 0.848 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6043.63, 6141.12)
    forward-compute ................................: (835.41, 2923.14)
    backward-compute ...............................: (1531.92, 2958.00)
    batch-generator ................................: (47.83, 59.84)
    forward-recv ...................................: (15.70, 37.48)
    forward-send ...................................: (0.48, 11.08)
    backward-recv ..................................: (61.66, 232.12)
    backward-send ..................................: (0.71, 17.15)
    forward-send-backward-recv .....................: (3047.89, 3376.09)
    backward-send-forward-recv .....................: (170.79, 407.35)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.41)
    grads-reduce-scatter ...........................: (7.68, 9.06)
    params-all-gather ..............................: (4.23, 4.91)
    optimizer-copy-to-main-grad ....................: (0.25, 0.36)
    optimizer-clip-main-grad .......................: (1.60, 1.69)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.30)
    optimizer-copy-main-to-model-params ............: (1.42, 1.67)
    optimizer ......................................: (9.20, 9.44)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (541863): No such process
kill: (541869): No such process
kill: (541875): No such process
kill: (541881): No such process
10.64.24.50 kill done.
7b, 8k, gbs=512: dp=2, tp=2, pp=4, mbs=2
LOCAL_IP = 10.64.24.51
DP=2, MP=2, PP=4
[2024-02-12 18:18:05,227] torch.distributed.run: [WARNING] 
[2024-02-12 18:18:05,227] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 18:18:05,227] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 18:18:05,227] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 805617664
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 805617664
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 908910592
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 908910592
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.685 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.725 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.750 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.968 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Cutting or padding data end, time cost:  10.759 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.842 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.868 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.825 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (37.50, 760.44)
    train/valid/test-data-iterators-setup ..........: (0.02, 16638.20)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 9478.2 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.089586E+01 | loss scale: 1.0 | grad norm: 3.565 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 9299.814453125 | max allocated: 22778.59619140625 | reserved: 28654.0 | max reserved: 28654.0
[Rank 13] (after 10 iterations) memory (MB) | allocated: 10483.2294921875 | max allocated: 23470.927734375 | reserved: 27336.0 | max reserved: 27336.0
[Rank 9] (after 10 iterations) memory (MB) | allocated: 9299.814453125 | max allocated: 22778.59619140625 | reserved: 28770.0 | max reserved: 28770.0[Rank 12] (after 10 iterations) memory (MB) | allocated: 10483.7919921875 | max allocated: 23470.927734375 | reserved: 27152.0 | max reserved: 27152.0

(min, max) time across ranks (ms):
    forward-backward ...............................: (9100.42, 9199.78)
    forward-compute ................................: (1422.67, 4287.44)
    backward-compute ...............................: (2226.69, 3546.43)
    batch-generator ................................: (216.48, 238.20)
    forward-recv ...................................: (179.92, 482.93)
    forward-send ...................................: (3.79, 298.88)
    backward-recv ..................................: (41.33, 174.41)
    backward-send ..................................: (4.01, 14.66)
    forward-send-backward-recv .....................: (4009.78, 4965.56)
    backward-send-forward-recv .....................: (628.54, 991.51)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.25)
    grads-reduce-scatter ...........................: (8.70, 224.71)
    params-all-gather ..............................: (4.21, 4.92)
    optimizer-copy-to-main-grad ....................: (0.28, 0.47)
    optimizer-clip-main-grad .......................: (5.51, 5.73)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.66, 5.53)
    optimizer-copy-main-to-model-params ............: (1.43, 1.68)
    optimizer ......................................: (14.03, 14.29)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 7377.3 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.088876E+01 | loss scale: 1.0 | grad norm: 16.329 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7262.91, 7329.52)
    forward-compute ................................: (1197.28, 3517.14)
    backward-compute ...............................: (2073.18, 3329.58)
    batch-generator ................................: (92.93, 116.84)
    forward-recv ...................................: (13.07, 27.24)
    forward-send ...................................: (0.30, 2.78)
    backward-recv ..................................: (31.53, 138.00)
    backward-send ..................................: (0.42, 7.13)
    forward-send-backward-recv .....................: (3093.75, 3740.51)
    backward-send-forward-recv .....................: (397.05, 618.80)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.20)
    grads-reduce-scatter ...........................: (7.60, 8.96)
    params-all-gather ..............................: (4.19, 4.95)
    optimizer-copy-to-main-grad ....................: (0.27, 0.38)
    optimizer-clip-main-grad .......................: (2.35, 2.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.36)
    optimizer-copy-main-to-model-params ............: (1.42, 1.67)
    optimizer ......................................: (10.14, 10.39)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 9596.9 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.075277E+01 | loss scale: 1.0 | grad norm: 1.231 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9495.43, 9556.58)
    forward-compute ................................: (1285.08, 4883.12)
    backward-compute ...............................: (2246.58, 3489.20)
    batch-generator ................................: (92.37, 118.10)
    forward-recv ...................................: (27.56, 1002.84)
    forward-send ...................................: (0.32, 499.41)
    backward-recv ..................................: (29.48, 97.40)
    backward-send ..................................: (0.37, 11.76)
    forward-send-backward-recv .....................: (3912.78, 5230.83)
    backward-send-forward-recv .....................: (533.29, 1692.29)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.18)
    grads-reduce-scatter ...........................: (7.62, 8.98)
    params-all-gather ..............................: (4.16, 4.91)
    optimizer-copy-to-main-grad ....................: (0.26, 0.38)
    optimizer-clip-main-grad .......................: (2.35, 2.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.53, 5.34)
    optimizer-copy-main-to-model-params ............: (1.42, 1.67)
    optimizer ......................................: (10.47, 10.72)
Mon Feb 12 18:24:49 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   33C    P0             274W / 700W |  39302MiB / 81559MiB |     69%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   38C    P0             233W / 700W |  39432MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   37C    P0             265W / 700W |  39684MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   33C    P0             263W / 700W |  39534MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             443W / 700W |  33732MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             418W / 700W |  33914MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             380W / 700W |  33792MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             379W / 700W |  32878MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 9947.6 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.070701E+01 | loss scale: 1.0 | grad norm: 0.586 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9745.86, 9820.16)
    forward-compute ................................: (1234.50, 5788.45)
    backward-compute ...............................: (2105.32, 3338.70)
    batch-generator ................................: (93.68, 116.78)
    forward-recv ...................................: (13.29, 37.85)
    forward-send ...................................: (0.34, 8.67)
    backward-recv ..................................: (33.72, 117.58)
    backward-send ..................................: (0.37, 5.34)
    forward-send-backward-recv .....................: (4379.78, 6159.23)
    backward-send-forward-recv .....................: (457.93, 1790.86)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.17)
    grads-reduce-scatter ...........................: (7.75, 9.05)
    params-all-gather ..............................: (4.21, 5.33)
    optimizer-copy-to-main-grad ....................: (0.26, 0.39)
    optimizer-clip-main-grad .......................: (1.37, 1.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.53, 5.33)
    optimizer-copy-main-to-model-params ............: (1.42, 1.67)
    optimizer ......................................: (9.01, 9.26)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 8823.1 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.064820E+01 | loss scale: 1.0 | grad norm: 0.462 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8721.87, 8786.68)
    forward-compute ................................: (1291.36, 4496.74)
    backward-compute ...............................: (2264.78, 3576.90)
    batch-generator ................................: (91.61, 115.84)
    forward-recv ...................................: (14.40, 33.87)
    forward-send ...................................: (0.40, 7.52)
    backward-recv ..................................: (29.75, 113.08)
    backward-send ..................................: (0.40, 10.99)
    forward-send-backward-recv .....................: (3424.01, 4846.82)
    backward-send-forward-recv .....................: (579.98, 1279.18)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.17)
    grads-reduce-scatter ...........................: (7.67, 9.05)
    params-all-gather ..............................: (4.21, 4.95)
    optimizer-copy-to-main-grad ....................: (0.27, 0.38)
    optimizer-clip-main-grad .......................: (1.12, 1.13)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.33)
    optimizer-copy-main-to-model-params ............: (1.42, 1.67)
    optimizer ......................................: (8.71, 8.95)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 7660.7 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.065656E+01 | loss scale: 1.0 | grad norm: 0.425 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7557.27, 7614.47)
    forward-compute ................................: (1238.20, 3678.42)
    backward-compute ...............................: (2145.62, 3403.03)
    batch-generator ................................: (94.46, 115.34)
    forward-recv ...................................: (10.35, 28.97)
    forward-send ...................................: (0.30, 4.77)
    backward-recv ..................................: (31.34, 125.05)
    backward-send ..................................: (0.48, 15.92)
    forward-send-backward-recv .....................: (3200.43, 3893.65)
    backward-send-forward-recv .....................: (429.48, 731.26)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.18)
    grads-reduce-scatter ...........................: (7.72, 9.77)
    params-all-gather ..............................: (4.23, 4.94)
    optimizer-copy-to-main-grad ....................: (0.26, 0.37)
    optimizer-clip-main-grad .......................: (1.11, 1.12)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.51, 5.34)
    optimizer-copy-main-to-model-params ............: (1.42, 1.67)
    optimizer ......................................: (8.68, 8.93)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 7604.1 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.064613E+01 | loss scale: 1.0 | grad norm: 0.382 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7502.01, 7565.21)
    forward-compute ................................: (1233.38, 3651.98)
    backward-compute ...............................: (2158.51, 3403.83)
    batch-generator ................................: (94.28, 117.47)
    forward-recv ...................................: (11.96, 35.93)
    forward-send ...................................: (0.30, 7.15)
    backward-recv ..................................: (30.12, 105.05)
    backward-send ..................................: (0.40, 8.36)
    forward-send-backward-recv .....................: (3195.46, 3823.86)
    backward-send-forward-recv .....................: (409.18, 677.17)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.14)
    grads-reduce-scatter ...........................: (7.65, 9.11)
    params-all-gather ..............................: (4.20, 4.90)
    optimizer-copy-to-main-grad ....................: (0.26, 0.37)
    optimizer-clip-main-grad .......................: (1.11, 1.12)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.33)
    optimizer-copy-main-to-model-params ............: (1.42, 1.67)
    optimizer ......................................: (8.67, 8.91)
Mon Feb 12 18:30:08 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   33C    P0             248W / 700W |  39304MiB / 81559MiB |     79%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   38C    P0             302W / 700W |  39434MiB / 81559MiB |     96%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   38C    P0             364W / 700W |  39684MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   34C    P0             247W / 700W |  39534MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             441W / 700W |  33732MiB / 81559MiB |     96%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   46C    P0             435W / 700W |  33914MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             449W / 700W |  33792MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             424W / 700W |  32878MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 7838.9 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.062706E+01 | loss scale: 1.0 | grad norm: 0.306 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7632.27, 7694.96)
    forward-compute ................................: (1265.00, 3743.69)
    backward-compute ...............................: (2187.04, 3475.46)
    batch-generator ................................: (93.82, 115.88)
    forward-recv ...................................: (12.05, 27.18)
    forward-send ...................................: (0.30, 3.88)
    backward-recv ..................................: (38.32, 115.61)
    backward-send ..................................: (0.48, 2.99)
    forward-send-backward-recv .....................: (3302.55, 3936.59)
    backward-send-forward-recv .....................: (437.81, 707.14)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.30)
    grads-reduce-scatter ...........................: (7.63, 8.99)
    params-all-gather ..............................: (4.19, 4.89)
    optimizer-copy-to-main-grad ....................: (0.27, 0.37)
    optimizer-clip-main-grad .......................: (1.11, 1.11)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.34)
    optimizer-copy-main-to-model-params ............: (1.42, 1.67)
    optimizer ......................................: (8.67, 8.92)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 7682.0 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.062234E+01 | loss scale: 1.0 | grad norm: 0.770 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7559.46, 7627.38)
    forward-compute ................................: (1245.98, 3588.42)
    backward-compute ...............................: (2200.59, 3428.13)
    batch-generator ................................: (93.37, 115.19)
    forward-recv ...................................: (13.32, 32.10)
    forward-send ...................................: (0.38, 7.25)
    backward-recv ..................................: (33.97, 119.64)
    backward-send ..................................: (0.46, 9.90)
    forward-send-backward-recv .....................: (3262.11, 3827.13)
    backward-send-forward-recv .....................: (463.90, 655.35)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.15)
    grads-reduce-scatter ...........................: (7.74, 9.04)
    params-all-gather ..............................: (4.21, 4.94)
    optimizer-copy-to-main-grad ....................: (0.26, 0.37)
    optimizer-clip-main-grad .......................: (1.11, 1.12)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.32)
    optimizer-copy-main-to-model-params ............: (1.42, 1.67)
    optimizer ......................................: (8.65, 8.90)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 7605.4 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.061756E+01 | loss scale: 1.0 | grad norm: 0.687 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7476.50, 7545.50)
    forward-compute ................................: (1237.16, 3562.62)
    backward-compute ...............................: (2168.67, 3414.52)
    batch-generator ................................: (91.80, 118.47)
    forward-recv ...................................: (11.28, 29.68)
    forward-send ...................................: (0.33, 5.05)
    backward-recv ..................................: (31.21, 160.78)
    backward-send ..................................: (2.54, 21.81)
    forward-send-backward-recv .....................: (3252.29, 3772.19)
    backward-send-forward-recv .....................: (441.07, 622.48)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.26)
    grads-reduce-scatter ...........................: (7.60, 9.10)
    params-all-gather ..............................: (4.21, 4.90)
    optimizer-copy-to-main-grad ....................: (0.26, 0.38)
    optimizer-clip-main-grad .......................: (1.36, 1.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.53, 5.33)
    optimizer-copy-main-to-model-params ............: (1.42, 1.67)
    optimizer ......................................: (8.98, 9.23)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (543355): No such process
kill: (543361): No such process
kill: (543367): No such process
kill: (543373): No such process
10.64.24.50 kill done.
7b, 8k, gbs=512: dp=2, tp=8, pp=1, mbs=8
LOCAL_IP = 10.64.24.51
DP=2, MP=8, PP=1
[2024-02-12 18:34:59,007] torch.distributed.run: [WARNING] 
[2024-02-12 18:34:59,007] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 18:34:59,007] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 18:34:59,007] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.142 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Cutting or padding data end, time cost:  10.655 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (540.35, 588.80)
    train/valid/test-data-iterators-setup ..........: (0.02, 16246.02)
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 6820.4 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.090243E+01 | loss scale: 1.0 | grad norm: 3.581 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6699.92, 6729.38)
    forward-compute ................................: (3697.96, 3863.87)
    backward-compute ...............................: (2819.24, 3015.48)
    batch-generator ................................: (667.35, 690.94)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (39.30, 39.81)
    params-all-gather ..............................: (21.16, 21.40)
    optimizer-copy-to-main-grad ....................: (1.03, 1.32)
    optimizer-clip-main-grad .......................: (6.14, 6.20)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.25, 5.42)
    optimizer-copy-main-to-model-params ............: (2.09, 2.29)
    optimizer ......................................: (17.03, 17.21)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 6351.7 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.086250E+01 | loss scale: 1.0 | grad norm: 11.710 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6255.18, 6264.47)
    forward-compute ................................: (3630.28, 3755.01)
    backward-compute ...............................: (2483.46, 2618.70)
    batch-generator ................................: (33.07, 39.22)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.98, 39.53)
    params-all-gather ..............................: (21.11, 21.38)
    optimizer-copy-to-main-grad ....................: (1.02, 1.30)
    optimizer-clip-main-grad .......................: (3.19, 3.32)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.01, 5.12)
    optimizer-copy-main-to-model-params ............: (2.09, 2.25)
    optimizer ......................................: (13.32, 13.64)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 5822.5 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.074137E+01 | loss scale: 1.0 | grad norm: 1.185 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5703.13, 5736.50)
    forward-compute ................................: (2882.40, 2883.94)
    backward-compute ...............................: (2801.89, 2838.27)
    batch-generator ................................: (33.47, 42.25)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (39.04, 39.56)
    params-all-gather ..............................: (21.13, 21.46)
    optimizer-copy-to-main-grad ....................: (1.03, 1.28)
    optimizer-clip-main-grad .......................: (3.18, 3.26)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.00, 5.09)
    optimizer-copy-main-to-model-params ............: (2.09, 2.25)
    optimizer ......................................: (13.74, 14.11)
Mon Feb 12 18:39:50 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   39C    P0             447W / 700W |  76776MiB / 81559MiB |     86%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   45C    P0             447W / 700W |  77308MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   46C    P0             450W / 700W |  77350MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   40C    P0             430W / 700W |  75558MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             472W / 700W |  77344MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             439W / 700W |  77262MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             421W / 700W |  77322MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             405W / 700W |  77110MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 6245.7 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.071772E+01 | loss scale: 1.0 | grad norm: 0.681 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6055.83, 6080.54)
    forward-compute ................................: (3467.89, 3474.10)
    backward-compute ...............................: (2564.77, 2596.95)
    batch-generator ................................: (32.49, 44.81)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (39.01, 39.45)
    params-all-gather ..............................: (21.13, 21.35)
    optimizer-copy-to-main-grad ....................: (1.02, 1.27)
    optimizer-clip-main-grad .......................: (2.11, 2.26)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.00, 5.08)
    optimizer-copy-main-to-model-params ............: (2.09, 2.30)
    optimizer ......................................: (12.23, 12.43)
benchmark/test_packing.sh.two.bak: line 139: 474521 Killed                  torchrun $DISTRIBUTED_ARGS pretrain_gpt.py $GPT_ARGS $DATA_ARGS $OUTPUT_ARGS --distributed-backend nccl
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (544218): No such process
kill: (544224): No such process
kill: (544230): No such process
kill: (544236): No such process
10.64.24.50 kill done.
7b, 8k, gbs=512: dp=2, tp=8, pp=1, mbs=4
LOCAL_IP = 10.64.24.51
DP=2, MP=8, PP=1
[2024-02-12 18:42:37,398] torch.distributed.run: [WARNING] 
[2024-02-12 18:42:37,398] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 18:42:37,398] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 18:42:37,398] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  6.633 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Cutting or padding data end, time cost:  10.836 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (557.42, 629.03)
    train/valid/test-data-iterators-setup ..........: (0.02, 17910.21)
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 9249.2 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.090240E+01 | loss scale: 1.0 | grad norm: 3.581 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9142.89, 9145.76)
    forward-compute ................................: (5585.06, 5688.30)
    backward-compute ...............................: (3405.84, 3526.82)
    batch-generator ................................: (705.99, 720.82)
    layernorm-grads-all-reduce .....................: (0.02, 0.04)
    embedding-grads-all-reduce .....................: (0.03, 0.04)
    grads-reduce-scatter ...........................: (39.35, 39.80)
    params-all-gather ..............................: (21.17, 21.66)
    optimizer-copy-to-main-grad ....................: (1.08, 1.45)
    optimizer-clip-main-grad .......................: (6.04, 6.09)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.27, 5.46)
    optimizer-copy-main-to-model-params ............: (2.10, 2.25)
    optimizer ......................................: (17.21, 17.41)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 7393.7 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.086266E+01 | loss scale: 1.0 | grad norm: 11.624 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7303.19, 7310.00)
    forward-compute ................................: (4152.21, 4211.17)
    backward-compute ...............................: (3065.18, 3119.29)
    batch-generator ................................: (69.38, 81.65)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (39.05, 39.43)
    params-all-gather ..............................: (21.08, 21.34)
    optimizer-copy-to-main-grad ....................: (1.08, 1.31)
    optimizer-clip-main-grad .......................: (3.16, 3.26)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.02, 5.11)
    optimizer-copy-main-to-model-params ............: (2.10, 2.30)
    optimizer ......................................: (12.95, 13.21)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 8044.4 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.074115E+01 | loss scale: 1.0 | grad norm: 1.187 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7959.37, 7960.50)
    forward-compute ................................: (4543.84, 4562.47)
    backward-compute ...............................: (3364.13, 3384.49)
    batch-generator ................................: (69.13, 81.71)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.98, 39.31)
    params-all-gather ..............................: (21.16, 21.40)
    optimizer-copy-to-main-grad ....................: (1.08, 1.30)
    optimizer-clip-main-grad .......................: (3.20, 3.23)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.02, 5.12)
    optimizer-copy-main-to-model-params ............: (2.10, 2.40)
    optimizer ......................................: (13.14, 13.45)
Mon Feb 12 18:48:40 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   36C    P0             306W / 700W |  76420MiB / 81559MiB |     90%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   43C    P0             276W / 700W |  76876MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   44C    P0             320W / 700W |  77070MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             274W / 700W |  76960MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   37C    P0             281W / 700W |  76964MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   43C    P0             289W / 700W |  77044MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   41C    P0             214W / 700W |  76484MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             280W / 700W |  76462MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 7599.9 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.071769E+01 | loss scale: 1.0 | grad norm: 0.679 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7421.19, 7427.75)
    forward-compute ................................: (4237.80, 4285.72)
    backward-compute ...............................: (3101.90, 3158.69)
    batch-generator ................................: (69.74, 85.04)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.90, 39.37)
    params-all-gather ..............................: (21.14, 22.28)
    optimizer-copy-to-main-grad ....................: (1.07, 1.27)
    optimizer-clip-main-grad .......................: (2.10, 2.15)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.00, 5.09)
    optimizer-copy-main-to-model-params ............: (2.10, 2.25)
    optimizer ......................................: (11.94, 12.04)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 9211.8 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.065994E+01 | loss scale: 1.0 | grad norm: 0.364 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9122.59, 9124.31)
    forward-compute ................................: (5572.26, 5664.14)
    backward-compute ...............................: (3424.30, 3520.56)
    batch-generator ................................: (70.11, 80.95)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (39.00, 39.33)
    params-all-gather ..............................: (21.23, 21.43)
    optimizer-copy-to-main-grad ....................: (1.06, 1.27)
    optimizer-clip-main-grad .......................: (1.97, 2.07)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.01, 5.20)
    optimizer-copy-main-to-model-params ............: (2.10, 2.29)
    optimizer ......................................: (12.51, 12.77)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 7680.1 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.067004E+01 | loss scale: 1.0 | grad norm: 0.353 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7597.77, 7598.26)
    forward-compute ................................: (4305.59, 4363.86)
    backward-compute ...............................: (3200.07, 3261.43)
    batch-generator ................................: (70.03, 82.64)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.79, 39.34)
    params-all-gather ..............................: (21.14, 21.38)
    optimizer-copy-to-main-grad ....................: (1.08, 1.26)
    optimizer-clip-main-grad .......................: (1.81, 1.88)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.00, 5.09)
    optimizer-copy-main-to-model-params ............: (2.10, 2.25)
    optimizer ......................................: (11.54, 11.72)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 7603.4 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.066021E+01 | loss scale: 1.0 | grad norm: 0.346 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7516.57, 7520.17)
    forward-compute ................................: (4223.25, 4259.17)
    backward-compute ...............................: (3227.29, 3262.32)
    batch-generator ................................: (67.35, 77.95)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.94, 39.24)
    params-all-gather ..............................: (21.23, 21.42)
    optimizer-copy-to-main-grad ....................: (1.06, 1.26)
    optimizer-clip-main-grad .......................: (1.76, 1.84)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.99, 5.07)
    optimizer-copy-main-to-model-params ............: (2.10, 2.25)
    optimizer ......................................: (11.90, 12.16)
Mon Feb 12 18:54:03 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   37C    P0             409W / 700W |  69434MiB / 81559MiB |     91%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   44C    P0             417W / 700W |  69588MiB / 81559MiB |     96%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   45C    P0             404W / 700W |  71290MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0             377W / 700W |  71240MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             384W / 700W |  71244MiB / 81559MiB |     96%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   44C    P0             341W / 700W |  71256MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   42C    P0             394W / 700W |  69936MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             350W / 700W |  69498MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 7850.8 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.064119E+01 | loss scale: 1.0 | grad norm: 0.348 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7664.60, 7676.99)
    forward-compute ................................: (4299.27, 4334.01)
    backward-compute ...............................: (3296.87, 3346.62)
    batch-generator ................................: (67.28, 77.59)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.88, 39.51)
    params-all-gather ..............................: (21.17, 21.39)
    optimizer-copy-to-main-grad ....................: (1.07, 1.30)
    optimizer-clip-main-grad .......................: (1.84, 1.94)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.01, 5.07)
    optimizer-copy-main-to-model-params ............: (2.10, 2.25)
    optimizer ......................................: (11.72, 11.89)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 7747.4 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.063668E+01 | loss scale: 1.0 | grad norm: 0.947 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7651.31, 7666.62)
    forward-compute ................................: (4300.55, 4353.62)
    backward-compute ...............................: (3279.45, 3319.74)
    batch-generator ................................: (68.97, 77.19)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (39.04, 39.42)
    params-all-gather ..............................: (21.19, 21.38)
    optimizer-copy-to-main-grad ....................: (1.07, 1.27)
    optimizer-clip-main-grad .......................: (2.57, 2.66)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.01, 5.08)
    optimizer-copy-main-to-model-params ............: (2.10, 2.27)
    optimizer ......................................: (12.73, 12.90)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 8099.4 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.063340E+01 | loss scale: 1.0 | grad norm: 1.296 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8010.02, 8012.89)
    forward-compute ................................: (4682.20, 4759.05)
    backward-compute ...............................: (3217.46, 3299.53)
    batch-generator ................................: (67.93, 78.85)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.97, 39.38)
    params-all-gather ..............................: (21.17, 21.55)
    optimizer-copy-to-main-grad ....................: (1.07, 1.24)
    optimizer-clip-main-grad .......................: (2.23, 2.36)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.01, 5.08)
    optimizer-copy-main-to-model-params ............: (2.10, 2.28)
    optimizer ......................................: (12.07, 12.21)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (545091): No such process
kill: (545097): No such process
kill: (545103): No such process
kill: (545109): No such process
10.64.24.50 kill done.
7b, 8k, gbs=512: dp=2, tp=8, pp=1, mbs=2
LOCAL_IP = 10.64.24.51
DP=2, MP=8, PP=1
[2024-02-12 18:59:00,124] torch.distributed.run: [WARNING] 
[2024-02-12 18:59:00,124] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 18:59:00,124] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 18:59:00,124] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.138 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Cutting or padding data end, time cost:  10.612 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (547.18, 599.87)
    train/valid/test-data-iterators-setup ..........: (0.02, 16170.53)
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 12526.0 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.090238E+01 | loss scale: 1.0 | grad norm: 3.579 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (12424.75, 12436.22)
    forward-compute ................................: (7397.55, 7496.85)
    backward-compute ...............................: (4868.97, 4960.89)
    batch-generator ................................: (816.04, 905.93)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.04)
    grads-reduce-scatter ...........................: (39.26, 39.90)
    params-all-gather ..............................: (21.13, 21.45)
    optimizer-copy-to-main-grad ....................: (1.11, 1.44)
    optimizer-clip-main-grad .......................: (6.11, 7.20)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.27, 6.12)
    optimizer-copy-main-to-model-params ............: (2.10, 2.26)
    optimizer ......................................: (18.97, 19.13)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 11508.8 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.086227E+01 | loss scale: 1.0 | grad norm: 11.699 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (11424.44, 11426.45)
    forward-compute ................................: (6724.68, 6808.25)
    backward-compute ...............................: (4548.75, 4633.65)
    batch-generator ................................: (199.30, 236.27)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.04)
    grads-reduce-scatter ...........................: (38.86, 39.40)
    params-all-gather ..............................: (21.20, 21.46)
    optimizer-copy-to-main-grad ....................: (1.04, 1.33)
    optimizer-clip-main-grad .......................: (3.06, 3.14)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.02, 5.12)
    optimizer-copy-main-to-model-params ............: (2.10, 2.25)
    optimizer ......................................: (12.73, 12.90)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 12646.7 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.074124E+01 | loss scale: 1.0 | grad norm: 1.175 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (12564.70, 12565.28)
    forward-compute ................................: (7632.50, 7683.73)
    backward-compute ...............................: (4812.40, 4867.13)
    batch-generator ................................: (194.51, 232.33)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.04)
    grads-reduce-scatter ...........................: (39.00, 39.57)
    params-all-gather ..............................: (21.13, 21.42)
    optimizer-copy-to-main-grad ....................: (1.06, 1.28)
    optimizer-clip-main-grad .......................: (3.11, 3.16)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.02, 5.11)
    optimizer-copy-main-to-model-params ............: (2.10, 2.26)
    optimizer ......................................: (12.64, 12.84)
Mon Feb 12 19:07:47 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   34C    P0             327W / 700W |  67306MiB / 81559MiB |     82%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   41C    P0             334W / 700W |  68346MiB / 81559MiB |     96%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   42C    P0             307W / 700W |  69402MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0             307W / 700W |  68408MiB / 81559MiB |     97%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   36C    P0             297W / 700W |  68340MiB / 81559MiB |     97%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   41C    P0             304W / 700W |  69806MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   39C    P0             323W / 700W |  69726MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   35C    P0             262W / 700W |  69400MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 12211.6 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.071766E+01 | loss scale: 1.0 | grad norm: 0.675 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (12037.61, 12045.91)
    forward-compute ................................: (7324.67, 7347.24)
    backward-compute ...............................: (4629.84, 4647.40)
    batch-generator ................................: (194.71, 234.58)
    layernorm-grads-all-reduce .....................: (0.02, 0.04)
    embedding-grads-all-reduce .....................: (0.03, 0.04)
    grads-reduce-scatter ...........................: (38.91, 39.50)
    params-all-gather ..............................: (21.07, 21.39)
    optimizer-copy-to-main-grad ....................: (1.08, 1.29)
    optimizer-clip-main-grad .......................: (2.01, 2.07)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.01, 5.09)
    optimizer-copy-main-to-model-params ............: (2.10, 2.25)
    optimizer ......................................: (11.48, 11.64)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 12348.7 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.065989E+01 | loss scale: 1.0 | grad norm: 0.372 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (12264.83, 12270.99)
    forward-compute ................................: (7172.39, 7323.43)
    backward-compute ...............................: (4872.76, 5033.53)
    batch-generator ................................: (184.89, 227.54)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.87, 39.56)
    params-all-gather ..............................: (21.07, 21.39)
    optimizer-copy-to-main-grad ....................: (1.06, 1.34)
    optimizer-clip-main-grad .......................: (1.80, 1.83)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.02, 5.09)
    optimizer-copy-main-to-model-params ............: (2.10, 2.25)
    optimizer ......................................: (11.22, 11.35)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 11765.7 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.067001E+01 | loss scale: 1.0 | grad norm: 0.355 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (11682.00, 11687.00)
    forward-compute ................................: (6846.48, 6945.76)
    backward-compute ...............................: (4666.82, 4775.31)
    batch-generator ................................: (186.05, 225.77)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.04)
    grads-reduce-scatter ...........................: (39.02, 39.39)
    params-all-gather ..............................: (21.12, 21.40)
    optimizer-copy-to-main-grad ....................: (1.05, 1.27)
    optimizer-clip-main-grad .......................: (1.77, 1.82)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.01, 5.10)
    optimizer-copy-main-to-model-params ............: (2.10, 2.37)
    optimizer ......................................: (11.40, 11.59)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 12854.6 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.066021E+01 | loss scale: 1.0 | grad norm: 0.348 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (12771.06, 12776.17)
    forward-compute ................................: (7954.81, 8005.48)
    backward-compute ...............................: (4696.41, 4756.03)
    batch-generator ................................: (190.62, 224.18)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.04)
    grads-reduce-scatter ...........................: (39.00, 39.54)
    params-all-gather ..............................: (21.21, 21.39)
    optimizer-copy-to-main-grad ....................: (1.08, 1.29)
    optimizer-clip-main-grad .......................: (1.87, 1.92)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.02, 5.11)
    optimizer-copy-main-to-model-params ............: (2.10, 2.28)
    optimizer ......................................: (11.75, 11.89)
Mon Feb 12 19:15:57 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   35C    P0             339W / 700W |  67318MiB / 81559MiB |     70%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   41C    P0             326W / 700W |  68356MiB / 81559MiB |     97%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   43C    P0             327W / 700W |  69410MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             358W / 700W |  68418MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   36C    P0             332W / 700W |  68352MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   42C    P0             317W / 700W |  69816MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   39C    P0             312W / 700W |  69738MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   36C    P0             317W / 700W |  69412MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 11989.9 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.064118E+01 | loss scale: 1.0 | grad norm: 0.329 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (11816.33, 11816.85)
    forward-compute ................................: (6910.65, 7025.14)
    backward-compute ...............................: (4720.18, 4838.77)
    batch-generator ................................: (190.84, 217.67)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (38.93, 39.34)
    params-all-gather ..............................: (21.16, 21.43)
    optimizer-copy-to-main-grad ....................: (1.05, 1.30)
    optimizer-clip-main-grad .......................: (1.83, 1.93)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.01, 5.09)
    optimizer-copy-main-to-model-params ............: (2.10, 2.25)
    optimizer ......................................: (11.71, 11.89)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 12395.1 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.063646E+01 | loss scale: 1.0 | grad norm: 0.932 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (12306.03, 12311.24)
    forward-compute ................................: (7453.76, 7478.37)
    backward-compute ...............................: (4760.59, 4785.27)
    batch-generator ................................: (187.50, 225.36)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.04)
    grads-reduce-scatter ...........................: (38.91, 39.36)
    params-all-gather ..............................: (21.24, 21.42)
    optimizer-copy-to-main-grad ....................: (1.07, 1.30)
    optimizer-clip-main-grad .......................: (1.91, 1.99)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.01, 5.09)
    optimizer-copy-main-to-model-params ............: (2.10, 2.25)
    optimizer ......................................: (11.86, 12.00)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 12306.0 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.063297E+01 | loss scale: 1.0 | grad norm: 1.524 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (12221.98, 12228.02)
    forward-compute ................................: (7418.71, 7461.73)
    backward-compute ...............................: (4693.76, 4738.11)
    batch-generator ................................: (186.84, 221.60)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.04)
    grads-reduce-scatter ...........................: (39.09, 39.77)
    params-all-gather ..............................: (21.20, 21.43)
    optimizer-copy-to-main-grad ....................: (1.07, 1.31)
    optimizer-clip-main-grad .......................: (2.01, 2.05)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.01, 5.10)
    optimizer-copy-main-to-model-params ............: (2.10, 2.27)
    optimizer ......................................: (11.45, 11.62)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (545964): No such process
kill: (545970): No such process
kill: (545976): No such process
kill: (545982): No such process
10.64.24.50 kill done.
7b, 8k, gbs=512: dp=2, tp=1, pp=8, mbs=8
LOCAL_IP = 10.64.24.51
DP=2, MP=1, PP=8
[2024-02-12 19:22:23,287] torch.distributed.run: [WARNING] 
[2024-02-12 19:22:23,287] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 19:22:23,287] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 19:22:23,287] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 805519360
 > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 805519360
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 805519360
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 1011572736
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...


> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.627 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.708 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.768 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.787 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.796 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.809 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.845 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.871 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Cutting or padding data end, time cost:  10.516 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.762 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.859 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.911 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.893 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.989 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.976 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.932 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (37.61, 614.85)
    train/valid/test-data-iterators-setup ..........: (15471.84, 16293.37)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 9563.9 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.089231E+01 | loss scale: 1.0 | grad norm: 3.495 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 10] (after 10 iterations) memory (MB) | allocated: 9320.251953125 | max allocated: 36432.91748046875 | reserved: 45878.0 | max reserved: 45878.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 9320.533203125 | max allocated: 39646.0712890625 | reserved: 50534.0 | max reserved: 50534.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 9320.251953125 | max allocated: 34816.03173828125 | reserved: 36362.0 | max reserved: 36362.0
[Rank 14] (after 10 iterations) memory (MB) | allocated: 11680.2373046875 | max allocated: 55988.91259765625 | reserved: 68830.0 | max reserved: 68830.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (8811.09, 9189.31)
    forward-compute ................................: (609.18, 4062.88)
    backward-compute ...............................: (1107.94, 4421.82)
    batch-generator ................................: (40.05, 52.01)
    forward-recv ...................................: (102.79, 403.55)
    forward-send ...................................: (4.37, 308.55)
    backward-recv ..................................: (240.40, 1587.37)
    backward-send ..................................: (1.21, 100.46)
    forward-send-backward-recv .....................: (5241.82, 6392.79)
    backward-send-forward-recv .....................: (58.35, 362.64)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.22)
    grads-reduce-scatter ...........................: (9.61, 229.13)
    params-all-gather ..............................: (4.08, 5.23)
    optimizer-copy-to-main-grad ....................: (0.16, 0.24)
    optimizer-clip-main-grad .......................: (5.53, 5.89)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.61, 5.98)
    optimizer-copy-main-to-model-params ............: (1.32, 1.69)
    optimizer ......................................: (14.17, 15.09)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 8374.6 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.085783E+01 | loss scale: 1.0 | grad norm: 13.770 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7994.57, 8291.39)
    forward-compute ................................: (475.31, 3751.66)
    backward-compute ...............................: (906.38, 4207.14)
    batch-generator ................................: (25.83, 32.70)
    forward-recv ...................................: (31.70, 164.39)
    forward-send ...................................: (0.91, 94.08)
    backward-recv ..................................: (194.67, 1476.66)
    backward-send ..................................: (1.06, 63.93)
    forward-send-backward-recv .....................: (5126.54, 6245.61)
    backward-send-forward-recv .....................: (46.64, 285.05)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.20)
    grads-reduce-scatter ...........................: (7.63, 9.98)
    params-all-gather ..............................: (4.07, 5.21)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (2.34, 2.71)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.83)
    optimizer-copy-main-to-model-params ............: (1.31, 1.68)
    optimizer ......................................: (10.46, 10.83)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 8797.9 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.073082E+01 | loss scale: 1.0 | grad norm: 1.090 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8316.70, 8695.52)
    forward-compute ................................: (572.76, 3827.72)
    backward-compute ...............................: (1091.17, 4295.44)
    batch-generator ................................: (25.65, 32.78)
    forward-recv ...................................: (41.13, 163.33)
    forward-send ...................................: (1.07, 91.02)
    backward-recv ..................................: (192.91, 1584.42)
    backward-send ..................................: (1.06, 65.21)
    forward-send-backward-recv .....................: (5163.44, 6235.83)
    backward-send-forward-recv .....................: (56.20, 340.53)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.96)
    grads-reduce-scatter ...........................: (7.67, 10.06)
    params-all-gather ..............................: (4.04, 5.27)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (2.34, 2.71)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.84)
    optimizer-copy-main-to-model-params ............: (1.32, 1.86)
    optimizer ......................................: (10.62, 11.67)
Mon Feb 12 19:28:53 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   29C    P0             150W / 700W |  58202MiB / 81559MiB |     78%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   34C    P0             215W / 700W |  60974MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   33C    P0             177W / 700W |  53532MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   31C    P0             218W / 700W |  56270MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   30C    P0             162W / 700W |  44016MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   33C    P0             149W / 700W |  47628MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             179W / 700W |  71776MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             212W / 700W |  71290MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 8428.8 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.070214E+01 | loss scale: 1.0 | grad norm: 0.606 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7981.04, 8256.82)
    forward-compute ................................: (510.53, 3648.05)
    backward-compute ...............................: (972.09, 4182.06)
    batch-generator ................................: (25.39, 34.46)
    forward-recv ...................................: (34.25, 147.13)
    forward-send ...................................: (1.04, 79.40)
    backward-recv ..................................: (194.86, 1517.51)
    backward-send ..................................: (0.84, 52.02)
    forward-send-backward-recv .....................: (5050.34, 6104.75)
    backward-send-forward-recv .....................: (47.54, 241.91)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.23)
    grads-reduce-scatter ...........................: (7.67, 10.21)
    params-all-gather ..............................: (4.05, 5.11)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (1.24, 1.28)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.85)
    optimizer-copy-main-to-model-params ............: (1.31, 1.69)
    optimizer ......................................: (9.14, 9.51)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 8852.7 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.064424E+01 | loss scale: 1.0 | grad norm: 0.593 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8426.77, 8738.95)
    forward-compute ................................: (596.50, 3886.31)
    backward-compute ...............................: (1140.55, 4389.12)
    batch-generator ................................: (25.73, 33.90)
    forward-recv ...................................: (42.78, 181.87)
    forward-send ...................................: (1.27, 96.00)
    backward-recv ..................................: (203.49, 1509.13)
    backward-send ..................................: (1.09, 79.41)
    forward-send-backward-recv .....................: (5067.34, 6187.68)
    backward-send-forward-recv .....................: (58.57, 319.62)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.06)
    grads-reduce-scatter ...........................: (7.72, 10.03)
    params-all-gather ..............................: (4.09, 5.22)
    optimizer-copy-to-main-grad ....................: (0.15, 0.24)
    optimizer-clip-main-grad .......................: (1.13, 1.14)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.87)
    optimizer-copy-main-to-model-params ............: (1.31, 1.70)
    optimizer ......................................: (9.14, 9.53)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 8512.9 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.065355E+01 | loss scale: 1.0 | grad norm: 0.351 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8114.84, 8419.68)
    forward-compute ................................: (539.55, 3750.58)
    backward-compute ...............................: (1007.89, 4230.29)
    batch-generator ................................: (26.44, 34.96)
    forward-recv ...................................: (31.68, 177.45)
    forward-send ...................................: (0.97, 91.13)
    backward-recv ..................................: (212.50, 1500.08)
    backward-send ..................................: (1.10, 43.23)
    forward-send-backward-recv .....................: (5071.29, 6162.68)
    backward-send-forward-recv .....................: (51.62, 237.99)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.07)
    grads-reduce-scatter ...........................: (7.70, 10.06)
    params-all-gather ..............................: (4.10, 5.22)
    optimizer-copy-to-main-grad ....................: (0.15, 0.34)
    optimizer-clip-main-grad .......................: (1.10, 1.12)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.85)
    optimizer-copy-main-to-model-params ............: (1.31, 1.92)
    optimizer ......................................: (9.09, 9.70)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 8933.5 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.064338E+01 | loss scale: 1.0 | grad norm: 0.500 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8564.74, 8860.02)
    forward-compute ................................: (523.93, 3783.67)
    backward-compute ...............................: (979.86, 4252.39)
    batch-generator ................................: (26.82, 33.11)
    forward-recv ...................................: (33.99, 662.70)
    forward-send ...................................: (1.03, 620.48)
    backward-recv ..................................: (210.48, 1439.27)
    backward-send ..................................: (1.12, 41.52)
    forward-send-backward-recv .....................: (4961.44, 6236.07)
    backward-send-forward-recv .....................: (112.74, 658.63)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.05)
    grads-reduce-scatter ...........................: (7.68, 10.02)
    params-all-gather ..............................: (4.08, 5.18)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (1.10, 1.10)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.84)
    optimizer-copy-main-to-model-params ............: (1.31, 1.82)
    optimizer ......................................: (8.89, 9.39)
Mon Feb 12 19:34:47 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   29C    P0             201W / 700W |  64262MiB / 81559MiB |     76%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   34C    P0             160W / 700W |  60974MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   34C    P0             217W / 700W |  59592MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   30C    P0             173W / 700W |  56270MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   30C    P0             216W / 700W |  50076MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   33C    P0             154W / 700W |  47628MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   44C    P0             345W / 700W |  71776MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             399W / 700W |  71290MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 9102.7 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.062437E+01 | loss scale: 1.0 | grad norm: 0.275 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8671.17, 8934.24)
    forward-compute ................................: (562.88, 4186.40)
    backward-compute ...............................: (1061.85, 4285.86)
    batch-generator ................................: (26.49, 32.87)
    forward-recv ...................................: (42.53, 746.55)
    forward-send ...................................: (1.05, 78.06)
    backward-recv ..................................: (204.09, 1503.42)
    backward-send ..................................: (1.01, 39.99)
    forward-send-backward-recv .....................: (5006.94, 6637.54)
    backward-send-forward-recv .....................: (57.58, 724.85)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.31)
    grads-reduce-scatter ...........................: (7.71, 10.07)
    params-all-gather ..............................: (4.05, 5.22)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (1.10, 1.10)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.82)
    optimizer-copy-main-to-model-params ............: (1.31, 1.68)
    optimizer ......................................: (8.87, 9.24)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 9414.2 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.061956E+01 | loss scale: 1.0 | grad norm: 0.412 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9059.83, 9329.87)
    forward-compute ................................: (550.58, 4194.56)
    backward-compute ...............................: (1041.92, 4290.42)
    batch-generator ................................: (26.76, 32.90)
    forward-recv ...................................: (35.85, 665.83)
    forward-send ...................................: (1.24, 78.10)
    backward-recv ..................................: (212.05, 1466.87)
    backward-send ..................................: (1.21, 54.64)
    forward-send-backward-recv .....................: (4954.86, 6590.48)
    backward-send-forward-recv .....................: (149.72, 1312.35)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.26)
    grads-reduce-scatter ...........................: (7.72, 10.04)
    params-all-gather ..............................: (4.07, 5.15)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (1.10, 1.10)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.83)
    optimizer-copy-main-to-model-params ............: (1.31, 1.70)
    optimizer ......................................: (8.88, 9.26)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 8493.6 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.061482E+01 | loss scale: 1.0 | grad norm: 1.262 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8038.90, 8406.41)
    forward-compute ................................: (538.75, 3641.09)
    backward-compute ...............................: (1006.07, 4231.22)
    batch-generator ................................: (26.73, 32.05)
    forward-recv ...................................: (35.24, 123.91)
    forward-send ...................................: (1.06, 62.86)
    backward-recv ..................................: (218.52, 1574.61)
    backward-send ..................................: (1.22, 61.51)
    forward-send-backward-recv .....................: (5061.96, 6078.74)
    backward-send-forward-recv .....................: (51.07, 168.32)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.28)
    grads-reduce-scatter ...........................: (7.63, 10.02)
    params-all-gather ..............................: (4.08, 5.24)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (1.47, 1.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.84)
    optimizer-copy-main-to-model-params ............: (1.31, 1.70)
    optimizer ......................................: (9.37, 9.76)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (548236): No such process
kill: (548242): No such process
kill: (548248): No such process
kill: (548254): No such process
10.64.24.50 kill done.
7b, 8k, gbs=512: dp=2, tp=1, pp=8, mbs=4
LOCAL_IP = 10.64.24.51
DP=2, MP=1, PP=8
[2024-02-12 19:40:06,151] torch.distributed.run: [WARNING] 
[2024-02-12 19:40:06,151] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 19:40:06,151] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 19:40:06,151] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 805519360
 > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 805519360
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 805519360
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 1011572736
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...

> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.802 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.802 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.827 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.866 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.877 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.931 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.967 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  5.516 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Cutting or padding data end, time cost:  10.715 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.666 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.781 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.880 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.914 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.813 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  11.034 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.826 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (34.77, 603.13)
    train/valid/test-data-iterators-setup ..........: (15856.75, 16705.78)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 9725.9 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.089233E+01 | loss scale: 1.0 | grad norm: 3.495 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 12] (after 10 iterations) memory (MB) | allocated: 9330.798828125 | max allocated: 21053.1826171875 | reserved: 25642.0 | max reserved: 25642.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 9330.869140625 | max allocated: 28348.013671875 | reserved: 32032.0 | max reserved: 32032.0
[Rank 10] (after 10 iterations) memory (MB) | allocated: 9330.798828125 | max allocated: 22167.95458984375 | reserved: 27862.0 | max reserved: 27862.0[Rank 14] (after 10 iterations) memory (MB) | allocated: 11690.9169921875 | max allocated: 37702.8232421875 | reserved: 40534.0 | max reserved: 40534.0

(min, max) time across ranks (ms):
    forward-backward ...............................: (9131.30, 9377.61)
    forward-compute ................................: (692.45, 4362.93)
    backward-compute ...............................: (1287.29, 4398.26)
    batch-generator ................................: (62.96, 73.74)
    forward-recv ...................................: (85.00, 360.31)
    forward-send ...................................: (3.60, 275.68)
    backward-recv ..................................: (110.91, 817.73)
    backward-send ..................................: (1.00, 61.98)
    forward-send-backward-recv .....................: (5633.77, 6318.86)
    backward-send-forward-recv .....................: (105.79, 744.28)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.10)
    grads-reduce-scatter ...........................: (9.71, 221.68)
    params-all-gather ..............................: (4.10, 5.23)
    optimizer-copy-to-main-grad ....................: (0.16, 0.24)
    optimizer-clip-main-grad .......................: (5.68, 6.04)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.62, 5.99)
    optimizer-copy-main-to-model-params ............: (1.32, 1.69)
    optimizer ......................................: (14.90, 15.31)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 8559.7 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.085791E+01 | loss scale: 1.0 | grad norm: 13.784 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8269.26, 8455.09)
    forward-compute ................................: (576.16, 4013.70)
    backward-compute ...............................: (1141.15, 4171.97)
    batch-generator ................................: (47.85, 55.03)
    forward-recv ...................................: (23.70, 103.82)
    forward-send ...................................: (0.55, 60.71)
    backward-recv ..................................: (94.46, 804.22)
    backward-send ..................................: (0.80, 63.07)
    forward-send-backward-recv .....................: (5522.95, 6115.16)
    backward-send-forward-recv .....................: (110.70, 565.54)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.17)
    grads-reduce-scatter ...........................: (7.68, 10.01)
    params-all-gather ..............................: (4.08, 5.20)
    optimizer-copy-to-main-grad ....................: (0.15, 0.24)
    optimizer-clip-main-grad .......................: (2.35, 2.71)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.83)
    optimizer-copy-main-to-model-params ............: (1.31, 1.69)
    optimizer ......................................: (11.08, 11.45)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 8913.1 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.073091E+01 | loss scale: 1.0 | grad norm: 1.096 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8646.07, 8833.46)
    forward-compute ................................: (656.72, 4203.83)
    backward-compute ...............................: (1258.50, 4310.40)
    batch-generator ................................: (46.38, 55.04)
    forward-recv ...................................: (21.10, 96.84)
    forward-send ...................................: (0.76, 55.15)
    backward-recv ..................................: (89.20, 812.77)
    backward-send ..................................: (0.70, 54.89)
    forward-send-backward-recv .....................: (5505.41, 6172.91)
    backward-send-forward-recv .....................: (105.50, 623.64)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.92)
    grads-reduce-scatter ...........................: (7.75, 9.98)
    params-all-gather ..............................: (4.07, 5.27)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (2.43, 2.80)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.83)
    optimizer-copy-main-to-model-params ............: (1.31, 1.69)
    optimizer ......................................: (10.57, 10.95)
Mon Feb 12 19:46:44 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   30C    P0             240W / 700W |  39308MiB / 81559MiB |     13%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   34C    P0             146W / 700W |  40384MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   34C    P0             197W / 700W |  33180MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   31C    P0             148W / 700W |  38416MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   31C    P0             253W / 700W |  31576MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   34C    P0             126W / 700W |  34186MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   44C    P0             281W / 700W |  43480MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             163W / 700W |  49284MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 8684.1 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.070222E+01 | loss scale: 1.0 | grad norm: 0.603 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8336.24, 8500.66)
    forward-compute ................................: (586.15, 4006.00)
    backward-compute ...............................: (1151.89, 4156.57)
    batch-generator ................................: (46.26, 54.99)
    forward-recv ...................................: (23.20, 104.69)
    forward-send ...................................: (0.76, 62.20)
    backward-recv ..................................: (105.28, 764.53)
    backward-send ..................................: (0.77, 44.29)
    forward-send-backward-recv .....................: (5503.86, 6121.89)
    backward-send-forward-recv .....................: (111.30, 540.48)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.24)
    grads-reduce-scatter ...........................: (7.73, 9.99)
    params-all-gather ..............................: (4.08, 5.23)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (1.23, 1.27)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.83)
    optimizer-copy-main-to-model-params ............: (1.31, 1.69)
    optimizer ......................................: (9.07, 9.44)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 9596.0 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.064428E+01 | loss scale: 1.0 | grad norm: 0.590 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9300.07, 9521.15)
    forward-compute ................................: (681.73, 4569.07)
    backward-compute ...............................: (1341.61, 4398.78)
    batch-generator ................................: (47.04, 54.62)
    forward-recv ...................................: (44.26, 570.64)
    forward-send ...................................: (0.82, 60.77)
    backward-recv ..................................: (91.45, 823.78)
    backward-send ..................................: (0.75, 70.76)
    forward-send-backward-recv .....................: (5595.74, 6514.58)
    backward-send-forward-recv .....................: (228.20, 1197.83)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.07)
    grads-reduce-scatter ...........................: (7.71, 10.03)
    params-all-gather ..............................: (4.08, 5.30)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (1.11, 1.25)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.84)
    optimizer-copy-main-to-model-params ............: (1.31, 1.70)
    optimizer ......................................: (9.04, 9.66)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 9670.0 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.065359E+01 | loss scale: 1.0 | grad norm: 0.360 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9395.16, 9569.45)
    forward-compute ................................: (622.78, 4142.28)
    backward-compute ...............................: (1235.21, 4242.53)
    batch-generator ................................: (46.99, 55.30)
    forward-recv ...................................: (22.52, 615.27)
    forward-send ...................................: (0.58, 587.34)
    backward-recv ..................................: (102.97, 749.55)
    backward-send ..................................: (0.73, 39.43)
    forward-send-backward-recv .....................: (5467.87, 6294.91)
    backward-send-forward-recv .....................: (288.89, 1374.69)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.07)
    grads-reduce-scatter ...........................: (7.71, 9.95)
    params-all-gather ..............................: (4.08, 5.24)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (1.11, 1.11)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.83)
    optimizer-copy-main-to-model-params ............: (1.31, 1.69)
    optimizer ......................................: (8.89, 9.28)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 8583.0 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.064339E+01 | loss scale: 1.0 | grad norm: 0.484 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8361.85, 8513.18)
    forward-compute ................................: (628.49, 4031.24)
    backward-compute ...............................: (1245.41, 4249.07)
    batch-generator ................................: (47.75, 54.11)
    forward-recv ...................................: (20.20, 89.49)
    forward-send ...................................: (0.71, 58.00)
    backward-recv ..................................: (97.00, 758.05)
    backward-send ..................................: (0.71, 34.54)
    forward-send-backward-recv .....................: (5499.84, 6110.79)
    backward-send-forward-recv .....................: (81.87, 471.18)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.88)
    grads-reduce-scatter ...........................: (7.67, 9.93)
    params-all-gather ..............................: (4.07, 5.19)
    optimizer-copy-to-main-grad ....................: (0.15, 0.24)
    optimizer-clip-main-grad .......................: (1.11, 1.11)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.82)
    optimizer-copy-main-to-model-params ............: (1.31, 1.69)
    optimizer ......................................: (8.88, 9.26)
Mon Feb 12 19:53:11 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   29C    P0             208W / 700W |  39312MiB / 81559MiB |     22%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   34C    P0             184W / 700W |  46714MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   34C    P0             178W / 700W |  33182MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   31C    P0             201W / 700W |  43840MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   30C    P0             165W / 700W |  31576MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   34C    P0             194W / 700W |  40514MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   41C    P0             359W / 700W |  43480MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             366W / 700W |  49284MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 10946.1 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.062438E+01 | loss scale: 1.0 | grad norm: 0.264 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10577.83, 10776.18)
    forward-compute ................................: (639.79, 5127.83)
    backward-compute ...............................: (1263.78, 4293.68)
    batch-generator ................................: (46.66, 53.71)
    forward-recv ...................................: (24.53, 567.64)
    forward-send ...................................: (0.63, 538.48)
    backward-recv ..................................: (106.64, 814.73)
    backward-send ..................................: (0.71, 32.91)
    forward-send-backward-recv .....................: (6460.92, 7346.58)
    backward-send-forward-recv .....................: (279.90, 1745.33)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.33)
    grads-reduce-scatter ...........................: (7.75, 10.09)
    params-all-gather ..............................: (4.06, 5.21)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (1.11, 1.11)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.83)
    optimizer-copy-main-to-model-params ............: (1.31, 1.69)
    optimizer ......................................: (8.93, 9.30)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 10255.4 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.061930E+01 | loss scale: 1.0 | grad norm: 0.346 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9992.79, 10190.81)
    forward-compute ................................: (632.26, 5043.72)
    backward-compute ...............................: (1229.78, 4252.70)
    batch-generator ................................: (46.00, 53.29)
    forward-recv ...................................: (23.23, 96.24)
    forward-send ...................................: (0.75, 56.10)
    backward-recv ..................................: (108.07, 788.91)
    backward-send ..................................: (0.71, 46.22)
    forward-send-backward-recv .....................: (6577.66, 7558.85)
    backward-send-forward-recv .....................: (118.02, 1157.44)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.04)
    grads-reduce-scatter ...........................: (7.66, 9.95)
    params-all-gather ..............................: (4.07, 5.74)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (1.11, 1.11)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.83)
    optimizer-copy-main-to-model-params ............: (1.31, 1.69)
    optimizer ......................................: (8.87, 9.25)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 9220.7 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.061238E+01 | loss scale: 1.0 | grad norm: 0.885 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8915.81, 9105.91)
    forward-compute ................................: (614.34, 4550.09)
    backward-compute ...............................: (1196.16, 4218.24)
    batch-generator ................................: (45.88, 52.70)
    forward-recv ...................................: (18.50, 80.05)
    forward-send ...................................: (0.70, 43.00)
    backward-recv ..................................: (107.81, 777.32)
    backward-send ..................................: (0.86, 43.25)
    forward-send-backward-recv .....................: (6100.80, 6763.02)
    backward-send-forward-recv .....................: (89.57, 465.49)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.14)
    grads-reduce-scatter ...........................: (7.65, 10.07)
    params-all-gather ..............................: (4.02, 5.23)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (1.11, 1.20)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.83)
    optimizer-copy-main-to-model-params ............: (1.31, 1.92)
    optimizer ......................................: (8.97, 9.58)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (550508): No such process
kill: (550514): No such process
kill: (550520): No such process
kill: (550526): No such process
10.64.24.50 kill done.
7b, 8k, gbs=512: dp=2, tp=1, pp=8, mbs=2
LOCAL_IP = 10.64.24.51
DP=2, MP=1, PP=8
[2024-02-12 19:58:44,032] torch.distributed.run: [WARNING] 
[2024-02-12 19:58:44,032] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 19:58:44,032] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 19:58:44,032] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 805519360
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 805519360
 > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 805519360
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 1011572736
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...

 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.837 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.857 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.881 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.895 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  4.898 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  5.077 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  5.226 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Loading exists cache end, time cost:  5.279 s
Cutting or padding data to max_seq_len + 1 = 8193 begin ...
Cutting or padding data end, time cost:  10.947 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.903 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.920 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.905 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  10.974 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  11.026 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  11.000 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  11.095 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (30.61, 597.27)
    train/valid/test-data-iterators-setup ..........: (15890.35, 17912.69)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 10775.9 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.089233E+01 | loss scale: 1.0 | grad norm: 3.494 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 12] (after 10 iterations) memory (MB) | allocated: 9297.416015625 | max allocated: 20039.3193359375 | reserved: 24002.0 | max reserved: 24002.0
[Rank 10] (after 10 iterations) memory (MB) | allocated: 9297.947265625 | max allocated: 22130.30224609375 | reserved: 25328.0 | max reserved: 25328.0[Rank 8] (after 10 iterations) memory (MB) | allocated: 9297.666015625 | max allocated: 23892.734375 | reserved: 27290.0 | max reserved: 27290.0

[Rank 14] (after 10 iterations) memory (MB) | allocated: 11657.2998046875 | max allocated: 26560.25 | reserved: 27978.0 | max reserved: 27978.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (10266.08, 10461.12)
    forward-compute ................................: (888.89, 4811.97)
    backward-compute ...............................: (1721.58, 4563.54)
    batch-generator ................................: (109.19, 126.55)
    forward-recv ...................................: (66.73, 317.27)
    forward-send ...................................: (3.22, 257.15)
    backward-recv ..................................: (48.58, 503.15)
    backward-send ..................................: (1.32, 42.98)
    forward-send-backward-recv .....................: (5853.56, 6801.37)
    backward-send-forward-recv .....................: (543.95, 1300.62)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.30)
    grads-reduce-scatter ...........................: (9.80, 218.22)
    params-all-gather ..............................: (4.08, 5.23)
    optimizer-copy-to-main-grad ....................: (0.16, 0.25)
    optimizer-clip-main-grad .......................: (5.81, 6.18)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.62, 6.10)
    optimizer-copy-main-to-model-params ............: (1.32, 1.68)
    optimizer ......................................: (14.63, 15.03)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 9124.6 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.085792E+01 | loss scale: 1.0 | grad norm: 13.779 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8897.10, 9071.11)
    forward-compute ................................: (777.55, 4273.24)
    backward-compute ...............................: (1583.30, 4399.14)
    batch-generator ................................: (89.43, 107.98)
    forward-recv ...................................: (16.81, 62.56)
    forward-send ...................................: (0.39, 28.56)
    backward-recv ..................................: (39.80, 465.48)
    backward-send ..................................: (0.43, 68.67)
    forward-send-backward-recv .....................: (5342.60, 5910.86)
    backward-send-forward-recv .....................: (262.47, 888.97)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.13)
    grads-reduce-scatter ...........................: (7.64, 9.92)
    params-all-gather ..............................: (4.08, 5.30)
    optimizer-copy-to-main-grad ....................: (0.15, 0.25)
    optimizer-clip-main-grad .......................: (2.37, 2.75)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.87)
    optimizer-copy-main-to-model-params ............: (1.31, 1.68)
    optimizer ......................................: (11.06, 11.43)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 10915.0 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.073095E+01 | loss scale: 1.0 | grad norm: 1.099 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10725.48, 10859.74)
    forward-compute ................................: (850.63, 5304.27)
    backward-compute ...............................: (1749.37, 4547.93)
    batch-generator ................................: (88.41, 102.40)
    forward-recv ...................................: (15.22, 702.99)
    forward-send ...................................: (0.45, 54.33)
    backward-recv ..................................: (46.02, 418.25)
    backward-send ..................................: (0.37, 57.02)
    forward-send-backward-recv .....................: (6100.88, 7005.31)
    backward-send-forward-recv .....................: (535.40, 1799.38)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.97)
    grads-reduce-scatter ...........................: (7.65, 10.14)
    params-all-gather ..............................: (4.06, 5.19)
    optimizer-copy-to-main-grad ....................: (0.15, 0.24)
    optimizer-clip-main-grad .......................: (2.35, 2.78)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.85)
    optimizer-copy-main-to-model-params ............: (1.31, 1.69)
    optimizer ......................................: (10.60, 10.97)
Mon Feb 12 20:06:06 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   31C    P0             206W / 700W |  34520MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   35C    P0             242W / 700W |  31136MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   35C    P0             205W / 700W |  32038MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   32C    P0             232W / 700W |  28840MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   32C    P0             174W / 700W |  30712MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   35C    P0             204W / 700W |  28572MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   45C    P0             360W / 700W |  34070MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   40C    P0             320W / 700W |  33586MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 9462.7 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.070218E+01 | loss scale: 1.0 | grad norm: 0.605 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9172.72, 9324.78)
    forward-compute ................................: (778.53, 4424.81)
    backward-compute ...............................: (1623.08, 4393.92)
    batch-generator ................................: (86.44, 98.65)
    forward-recv ...................................: (16.44, 65.04)
    forward-send ...................................: (0.50, 37.24)
    backward-recv ..................................: (45.56, 426.36)
    backward-send ..................................: (0.39, 39.56)
    forward-send-backward-recv .....................: (5613.40, 6182.11)
    backward-send-forward-recv .....................: (345.00, 941.77)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.17)
    grads-reduce-scatter ...........................: (7.70, 10.05)
    params-all-gather ..............................: (4.04, 5.31)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (1.22, 1.27)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.81)
    optimizer-copy-main-to-model-params ............: (1.31, 1.68)
    optimizer ......................................: (9.01, 9.38)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 12186.0 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.064428E+01 | loss scale: 1.0 | grad norm: 0.591 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (11983.28, 12119.27)
    forward-compute ................................: (859.27, 5779.98)
    backward-compute ...............................: (1776.00, 4620.44)
    batch-generator ................................: (87.57, 98.78)
    forward-recv ...................................: (16.76, 1821.69)
    forward-send ...................................: (0.55, 1805.54)
    backward-recv ..................................: (50.37, 388.17)
    backward-send ..................................: (0.49, 35.81)
    forward-send-backward-recv .....................: (6303.78, 8273.43)
    backward-send-forward-recv .....................: (519.08, 2841.86)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.95)
    grads-reduce-scatter ...........................: (7.65, 9.85)
    params-all-gather ..............................: (4.05, 5.27)
    optimizer-copy-to-main-grad ....................: (0.14, 0.23)
    optimizer-clip-main-grad .......................: (1.10, 1.10)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.81)
    optimizer-copy-main-to-model-params ............: (1.31, 1.68)
    optimizer ......................................: (8.84, 9.21)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 11353.7 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.065359E+01 | loss scale: 1.0 | grad norm: 0.360 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (11138.67, 11279.40)
    forward-compute ................................: (809.68, 5649.18)
    backward-compute ...............................: (1671.05, 4466.92)
    batch-generator ................................: (86.64, 98.20)
    forward-recv ...................................: (16.86, 53.66)
    forward-send ...................................: (0.41, 28.23)
    backward-recv ..................................: (43.36, 413.93)
    backward-send ..................................: (0.48, 45.84)
    forward-send-backward-recv .....................: (6276.47, 8114.90)
    backward-send-forward-recv .....................: (400.93, 2124.58)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.98)
    grads-reduce-scatter ...........................: (7.64, 10.08)
    params-all-gather ..............................: (4.10, 5.20)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (1.10, 1.10)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.81)
    optimizer-copy-main-to-model-params ............: (1.31, 1.68)
    optimizer ......................................: (8.84, 9.20)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 11644.9 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.064336E+01 | loss scale: 1.0 | grad norm: 0.489 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (11475.05, 11591.70)
    forward-compute ................................: (812.07, 6793.97)
    backward-compute ...............................: (1678.71, 4469.41)
    batch-generator ................................: (87.82, 98.29)
    forward-recv ...................................: (12.22, 55.50)
    forward-send ...................................: (0.45, 22.91)
    backward-recv ..................................: (50.30, 418.28)
    backward-send ..................................: (0.41, 20.84)
    forward-send-backward-recv .....................: (6897.44, 8490.04)
    backward-send-forward-recv .....................: (236.64, 1498.10)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.94)
    grads-reduce-scatter ...........................: (7.72, 9.98)
    params-all-gather ..............................: (4.08, 5.22)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (1.10, 1.10)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.80)
    optimizer-copy-main-to-model-params ............: (1.31, 1.68)
    optimizer ......................................: (8.84, 9.21)
Mon Feb 12 20:13:39 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   31C    P0             240W / 700W |  34520MiB / 81559MiB |     57%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   36C    P0             229W / 700W |  31136MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   35C    P0             221W / 700W |  32038MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   32C    P0             190W / 700W |  28840MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   32C    P0             222W / 700W |  30714MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   35C    P0             207W / 700W |  28572MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   45C    P0             461W / 700W |  34070MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   40C    P0             472W / 700W |  33586MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 10109.4 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.062443E+01 | loss scale: 1.0 | grad norm: 0.289 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9813.31, 9945.21)
    forward-compute ................................: (816.46, 5040.96)
    backward-compute ...............................: (1684.95, 4460.99)
    batch-generator ................................: (87.24, 97.79)
    forward-recv ...................................: (15.12, 60.88)
    forward-send ...................................: (0.41, 31.55)
    backward-recv ..................................: (51.72, 366.09)
    backward-send ..................................: (0.49, 30.95)
    forward-send-backward-recv .....................: (5532.02, 6806.07)
    backward-send-forward-recv .....................: (241.48, 1422.94)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.16)
    grads-reduce-scatter ...........................: (7.72, 9.96)
    params-all-gather ..............................: (4.09, 5.18)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (1.10, 1.10)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.81)
    optimizer-copy-main-to-model-params ............: (1.31, 1.68)
    optimizer ......................................: (8.85, 9.22)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 9401.5 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.062185E+01 | loss scale: 1.0 | grad norm: 1.059 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9191.63, 9325.24)
    forward-compute ................................: (819.92, 4396.75)
    backward-compute ...............................: (1701.60, 4495.63)
    batch-generator ................................: (87.16, 98.00)
    forward-recv ...................................: (15.03, 71.06)
    forward-send ...................................: (0.51, 34.92)
    backward-recv ..................................: (54.13, 417.04)
    backward-send ..................................: (0.52, 19.27)
    forward-send-backward-recv .....................: (5591.25, 6110.54)
    backward-send-forward-recv .....................: (298.52, 832.24)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.89)
    grads-reduce-scatter ...........................: (7.72, 10.00)
    params-all-gather ..............................: (4.08, 5.15)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (1.47, 1.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.81)
    optimizer-copy-main-to-model-params ............: (1.31, 1.68)
    optimizer ......................................: (9.32, 9.68)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 9317.8 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.062105E+01 | loss scale: 1.0 | grad norm: 0.926 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9065.93, 9215.31)
    forward-compute ................................: (805.90, 4310.98)
    backward-compute ...............................: (1671.47, 4454.18)
    batch-generator ................................: (86.97, 122.69)
    forward-recv ...................................: (12.21, 49.37)
    forward-send ...................................: (0.46, 20.89)
    backward-recv ..................................: (42.04, 423.72)
    backward-send ..................................: (0.52, 44.11)
    forward-send-backward-recv .....................: (5513.71, 6023.72)
    backward-send-forward-recv .....................: (281.14, 758.31)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.16)
    grads-reduce-scatter ...........................: (7.62, 10.03)
    params-all-gather ..............................: (4.07, 5.24)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (1.72, 1.90)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.81)
    optimizer-copy-main-to-model-params ............: (1.31, 1.68)
    optimizer ......................................: (9.63, 10.00)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (552780): No such process
kill: (552786): No such process
kill: (552792): No such process
kill: (552798): No such process
10.64.24.50 kill done.

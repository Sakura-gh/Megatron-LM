7b, 4k, gbs=512: dp=16, tp=1, pp=1, mbs=1
LOCAL_IP = 10.64.24.51
DP=16, MP=1, PP=1
[2024-02-11 14:44:49,649] torch.distributed.run: [WARNING] 
[2024-02-11 14:44:49,649] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 14:44:49,649] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-11 14:44:49,649] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.558 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.597 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.596 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.600 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.604 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.668 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.708 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  6.935 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.088 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.111 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.124 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.152 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.204 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.210 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.098 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.206 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (280.20, 329.86)
    train/valid/test-data-iterators-setup ..........: (11080.77, 12557.18)
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 13809.6 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 7.569448E+00 | loss scale: 1.0 | grad norm: 635.437 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13637.66, 13650.01)
    forward-compute ................................: (5063.65, 5431.33)
    backward-compute ...............................: (8188.64, 8568.07)
    batch-generator ................................: (93.82, 101.75)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (86.66, 86.86)
    params-all-gather ..............................: (48.04, 48.15)
    optimizer-copy-to-main-grad ....................: (0.13, 0.22)
    optimizer-clip-main-grad .......................: (5.34, 5.49)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.81)
    optimizer-copy-main-to-model-params ............: (1.31, 1.38)
    optimizer ......................................: (12.32, 12.38)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 13676.5 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.536720E+00 | loss scale: 1.0 | grad norm: 12.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13507.47, 13519.53)
    forward-compute ................................: (4956.58, 5300.30)
    backward-compute ...............................: (8179.49, 8544.78)
    batch-generator ................................: (30.72, 38.75)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (87.64, 87.89)
    params-all-gather ..............................: (48.04, 48.12)
    optimizer-copy-to-main-grad ....................: (0.12, 0.23)
    optimizer-clip-main-grad .......................: (2.27, 2.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.63, 4.70)
    optimizer-copy-main-to-model-params ............: (1.31, 1.38)
    optimizer ......................................: (9.14, 9.21)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 13065.8 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.407792E+00 | loss scale: 1.0 | grad norm: 2.905 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (12896.43, 12909.06)
    forward-compute ................................: (4348.22, 4707.82)
    backward-compute ...............................: (8171.67, 8542.87)
    batch-generator ................................: (30.75, 38.38)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (87.02, 87.39)
    params-all-gather ..............................: (48.04, 48.15)
    optimizer-copy-to-main-grad ....................: (0.12, 0.25)
    optimizer-clip-main-grad .......................: (2.26, 2.29)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.63, 4.69)
    optimizer-copy-main-to-model-params ............: (1.31, 1.39)
    optimizer ......................................: (9.02, 9.09)
Sun Feb 11 14:54:26 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             573W / 700W |  68140MiB / 81559MiB |     82%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   49C    P0             562W / 700W |  68188MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   50C    P0             540W / 700W |  68252MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   43C    P0             501W / 700W |  68306MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   43C    P0             518W / 700W |  68308MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   50C    P0             507W / 700W |  68248MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   48C    P0             502W / 700W |  68284MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   43C    P0             534W / 700W |  68316MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 13719.3 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.145983E+00 | loss scale: 1.0 | grad norm: 1.535 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13461.67, 13474.05)
    forward-compute ................................: (4923.09, 5276.78)
    backward-compute ...............................: (8168.12, 8533.07)
    batch-generator ................................: (30.50, 42.14)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (86.66, 86.91)
    params-all-gather ..............................: (48.17, 48.29)
    optimizer-copy-to-main-grad ....................: (0.12, 0.26)
    optimizer-clip-main-grad .......................: (2.25, 2.27)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.63, 4.68)
    optimizer-copy-main-to-model-params ............: (1.31, 1.38)
    optimizer ......................................: (8.99, 9.07)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 13622.2 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.282605E+00 | loss scale: 1.0 | grad norm: 2.366 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13454.39, 13465.85)
    forward-compute ................................: (4921.98, 5280.58)
    backward-compute ...............................: (8157.45, 8525.71)
    batch-generator ................................: (30.56, 41.14)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (86.65, 87.02)
    params-all-gather ..............................: (48.02, 48.14)
    optimizer-copy-to-main-grad ....................: (0.12, 0.23)
    optimizer-clip-main-grad .......................: (2.24, 2.27)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.63, 5.02)
    optimizer-copy-main-to-model-params ............: (1.31, 1.38)
    optimizer ......................................: (9.28, 9.36)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 13023.4 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.155667E+00 | loss scale: 1.0 | grad norm: 2.788 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (12855.78, 12866.89)
    forward-compute ................................: (4335.00, 4676.43)
    backward-compute ...............................: (8163.12, 8513.95)
    batch-generator ................................: (30.69, 40.75)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (86.78, 87.17)
    params-all-gather ..............................: (48.05, 48.39)
    optimizer-copy-to-main-grad ....................: (0.13, 0.23)
    optimizer-clip-main-grad .......................: (2.24, 2.26)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.63, 4.68)
    optimizer-copy-main-to-model-params ............: (1.31, 1.38)
    optimizer ......................................: (8.96, 9.03)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 13604.7 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.145815E+00 | loss scale: 1.0 | grad norm: 1.817 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13437.46, 13448.13)
    forward-compute ................................: (4912.22, 5266.38)
    backward-compute ...............................: (8154.75, 8517.60)
    batch-generator ................................: (30.17, 41.90)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (86.78, 87.42)
    params-all-gather ..............................: (48.31, 48.42)
    optimizer-copy-to-main-grad ....................: (0.12, 0.23)
    optimizer-clip-main-grad .......................: (2.26, 2.28)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.63, 4.70)
    optimizer-copy-main-to-model-params ............: (1.31, 1.38)
    optimizer ......................................: (8.98, 9.05)
Sun Feb 11 15:03:20 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             519W / 700W |  68140MiB / 81559MiB |     65%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   49C    P0             573W / 700W |  68188MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   50C    P0             493W / 700W |  68252MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   43C    P0             553W / 700W |  68306MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   43C    P0             526W / 700W |  68308MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             471W / 700W |  68248MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             424W / 700W |  68284MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             531W / 700W |  68316MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 13127.2 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.165180E+00 | loss scale: 1.0 | grad norm: 1.384 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (12870.69, 12882.51)
    forward-compute ................................: (4339.89, 4690.98)
    backward-compute ...............................: (8163.38, 8524.47)
    batch-generator ................................: (30.23, 41.59)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (86.73, 87.13)
    params-all-gather ..............................: (48.04, 48.12)
    optimizer-copy-to-main-grad ....................: (0.12, 0.24)
    optimizer-clip-main-grad .......................: (2.01, 2.03)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.63, 4.72)
    optimizer-copy-main-to-model-params ............: (1.31, 1.38)
    optimizer ......................................: (8.74, 8.81)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 13615.6 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.127137E+00 | loss scale: 1.0 | grad norm: 0.692 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13447.57, 13459.26)
    forward-compute ................................: (4922.49, 5265.65)
    backward-compute ...............................: (8165.51, 8518.49)
    batch-generator ................................: (30.32, 40.99)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (87.24, 87.67)
    params-all-gather ..............................: (48.14, 48.27)
    optimizer-copy-to-main-grad ....................: (0.12, 0.23)
    optimizer-clip-main-grad .......................: (1.25, 1.26)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.63, 5.00)
    optimizer-copy-main-to-model-params ............: (1.31, 1.38)
    optimizer ......................................: (8.40, 8.52)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 13038.0 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.089767E+00 | loss scale: 1.0 | grad norm: 0.672 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (12870.53, 12881.44)
    forward-compute ................................: (4338.81, 4687.44)
    backward-compute ...............................: (8166.35, 8524.36)
    batch-generator ................................: (30.20, 41.45)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (86.59, 86.84)
    params-all-gather ..............................: (48.79, 48.94)
    optimizer-copy-to-main-grad ....................: (0.12, 0.23)
    optimizer-clip-main-grad .......................: (1.38, 1.54)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.63, 4.71)
    optimizer-copy-main-to-model-params ............: (1.31, 1.44)
    optimizer ......................................: (8.33, 8.45)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (440789): No such process
kill: (440795): No such process
kill: (440801): No such process
kill: (440807): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=8, tp=2, pp=1, mbs=2
LOCAL_IP = 10.64.24.51
DP=8, MP=2, PP=1
[2024-02-11 15:10:02,896] torch.distributed.run: [WARNING] 
[2024-02-11 15:10:02,896] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 15:10:02,896] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-11 15:10:02,896] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.401 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.446 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.493 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  6.933 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.078 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.039 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.077 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.188 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (303.04, 320.40)
    train/valid/test-data-iterators-setup ..........: (0.02, 12618.87)
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 15691.9 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 9.549174E+00 | loss scale: 1.0 | grad norm: 700.999 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15557.62, 15568.31)
    forward-compute ................................: (6177.76, 6528.65)
    backward-compute ...............................: (9011.21, 9369.23)
    batch-generator ................................: (229.90, 252.28)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (68.76, 68.79)
    params-all-gather ..............................: (36.24, 36.35)
    optimizer-copy-to-main-grad ....................: (0.29, 0.44)
    optimizer-clip-main-grad .......................: (5.21, 5.24)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.81, 5.01)
    optimizer-copy-main-to-model-params ............: (1.42, 1.51)
    optimizer ......................................: (12.71, 12.80)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 15332.8 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.521546E+00 | loss scale: 1.0 | grad norm: 13.499 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15202.48, 15212.65)
    forward-compute ................................: (5890.91, 6175.57)
    backward-compute ...............................: (9009.09, 9304.37)
    batch-generator ................................: (29.37, 39.25)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.04)
    grads-reduce-scatter ...........................: (68.43, 68.47)
    params-all-gather ..............................: (36.24, 36.47)
    optimizer-copy-to-main-grad ....................: (0.28, 0.44)
    optimizer-clip-main-grad .......................: (2.36, 2.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.69, 4.83)
    optimizer-copy-main-to-model-params ............: (1.42, 1.51)
    optimizer ......................................: (9.57, 9.65)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 14719.8 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.415806E+00 | loss scale: 1.0 | grad norm: 3.020 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14591.05, 14600.00)
    forward-compute ................................: (5283.73, 5568.63)
    backward-compute ...............................: (9005.06, 9298.97)
    batch-generator ................................: (29.94, 37.97)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.42, 68.47)
    params-all-gather ..............................: (36.24, 36.36)
    optimizer-copy-to-main-grad ....................: (0.28, 0.42)
    optimizer-clip-main-grad .......................: (2.34, 2.36)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.68, 4.78)
    optimizer-copy-main-to-model-params ............: (1.42, 1.51)
    optimizer ......................................: (9.45, 9.53)
Sun Feb 11 15:20:48 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   42C    P0             571W / 700W |  57182MiB / 81559MiB |     32%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   50C    P0             511W / 700W |  57308MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   51C    P0             547W / 700W |  57174MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   44C    P0             494W / 700W |  57300MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   43C    P0             520W / 700W |  57198MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   50C    P0             510W / 700W |  57388MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   48C    P0             505W / 700W |  57150MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   43C    P0             464W / 700W |  57276MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 15320.6 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.150113E+00 | loss scale: 1.0 | grad norm: 1.461 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15099.61, 15110.07)
    forward-compute ................................: (5811.84, 6083.08)
    backward-compute ...............................: (8999.90, 9281.06)
    batch-generator ................................: (30.14, 36.90)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.42, 68.80)
    params-all-gather ..............................: (36.26, 36.34)
    optimizer-copy-to-main-grad ....................: (0.28, 0.40)
    optimizer-clip-main-grad .......................: (2.32, 2.35)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.68, 4.73)
    optimizer-copy-main-to-model-params ............: (1.42, 1.51)
    optimizer ......................................: (9.38, 9.46)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 14700.5 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.286627E+00 | loss scale: 1.0 | grad norm: 1.291 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14570.82, 14580.50)
    forward-compute ................................: (5279.23, 5551.04)
    backward-compute ...............................: (9002.03, 9284.44)
    batch-generator ................................: (29.51, 36.98)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.42, 68.67)
    params-all-gather ..............................: (36.24, 36.34)
    optimizer-copy-to-main-grad ....................: (0.29, 0.45)
    optimizer-clip-main-grad .......................: (2.20, 2.22)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.68, 4.73)
    optimizer-copy-main-to-model-params ............: (1.42, 1.51)
    optimizer ......................................: (9.28, 9.36)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 15213.6 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.154851E+00 | loss scale: 1.0 | grad norm: 1.800 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15083.16, 15093.99)
    forward-compute ................................: (5794.98, 6081.67)
    backward-compute ...............................: (8983.98, 9282.04)
    batch-generator ................................: (30.17, 39.40)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.43, 68.47)
    params-all-gather ..............................: (36.25, 36.35)
    optimizer-copy-to-main-grad ....................: (0.29, 0.44)
    optimizer-clip-main-grad .......................: (2.07, 2.09)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.67, 4.73)
    optimizer-copy-main-to-model-params ............: (1.42, 1.51)
    optimizer ......................................: (9.23, 9.31)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 15226.9 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.139944E+00 | loss scale: 1.0 | grad norm: 0.875 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15096.48, 15106.77)
    forward-compute ................................: (5806.43, 6083.72)
    backward-compute ...............................: (8995.99, 9283.54)
    batch-generator ................................: (29.18, 37.81)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.42, 69.25)
    params-all-gather ..............................: (36.24, 36.53)
    optimizer-copy-to-main-grad ....................: (0.30, 0.44)
    optimizer-clip-main-grad .......................: (1.68, 1.70)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.68, 4.74)
    optimizer-copy-main-to-model-params ............: (1.42, 1.50)
    optimizer ......................................: (8.81, 8.89)
Sun Feb 11 15:30:48 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   42C    P0             545W / 700W |  57182MiB / 81559MiB |     22%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   49C    P0             581W / 700W |  57308MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   50C    P0             555W / 700W |  57174MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   44C    P0             556W / 700W |  57300MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   43C    P0             493W / 700W |  57198MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   50C    P0             487W / 700W |  57388MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   48C    P0             512W / 700W |  57150MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   43C    P0             499W / 700W |  57276MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 14797.3 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.159304E+00 | loss scale: 1.0 | grad norm: 1.489 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14578.82, 14588.13)
    forward-compute ................................: (5285.96, 5569.91)
    backward-compute ...............................: (8991.44, 9285.50)
    batch-generator ................................: (29.57, 36.93)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.41, 68.46)
    params-all-gather ..............................: (36.26, 36.42)
    optimizer-copy-to-main-grad ....................: (0.27, 0.40)
    optimizer-clip-main-grad .......................: (1.56, 1.72)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.68, 5.16)
    optimizer-copy-main-to-model-params ............: (1.42, 1.60)
    optimizer ......................................: (9.23, 9.42)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 15207.0 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.137161E+00 | loss scale: 1.0 | grad norm: 2.300 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15077.11, 15087.07)
    forward-compute ................................: (5793.64, 6067.55)
    backward-compute ...............................: (8992.12, 9276.63)
    batch-generator ................................: (29.30, 36.88)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.43, 68.60)
    params-all-gather ..............................: (36.24, 36.36)
    optimizer-copy-to-main-grad ....................: (0.28, 0.41)
    optimizer-clip-main-grad .......................: (2.31, 2.34)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.68, 4.73)
    optimizer-copy-main-to-model-params ............: (1.42, 1.51)
    optimizer ......................................: (9.38, 9.46)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 14709.6 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.094564E+00 | loss scale: 1.0 | grad norm: 1.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14578.19, 14588.53)
    forward-compute ................................: (5280.07, 5567.66)
    backward-compute ...............................: (8993.05, 9291.59)
    batch-generator ................................: (29.38, 39.37)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.41, 69.97)
    params-all-gather ..............................: (36.23, 36.36)
    optimizer-copy-to-main-grad ....................: (0.30, 0.43)
    optimizer-clip-main-grad .......................: (2.08, 2.10)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.68, 4.73)
    optimizer-copy-main-to-model-params ............: (1.42, 1.51)
    optimizer ......................................: (9.14, 9.23)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (442247): No such process
kill: (442253): No such process
kill: (442259): No such process
kill: (442265): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=8, tp=2, pp=1, mbs=1
LOCAL_IP = 10.64.24.51
DP=8, MP=2, PP=1
[2024-02-11 15:38:02,386] torch.distributed.run: [WARNING] 
[2024-02-11 15:38:02,386] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 15:38:02,386] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-11 15:38:02,386] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.355 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.403 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  6.254 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  7.590 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.974 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  4.981 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.387 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.325 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (296.93, 317.93)
    train/valid/test-data-iterators-setup ..........: (0.02, 13375.94)
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 16882.3 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 9.547960E+00 | loss scale: 1.0 | grad norm: 700.913 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16753.88, 16758.67)
    forward-compute ................................: (6742.81, 7017.10)
    backward-compute ...............................: (9700.80, 9981.90)
    batch-generator ................................: (248.01, 284.67)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.75, 68.79)
    params-all-gather ..............................: (36.25, 36.35)
    optimizer-copy-to-main-grad ....................: (0.26, 0.46)
    optimizer-clip-main-grad .......................: (5.22, 5.24)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.81, 4.95)
    optimizer-copy-main-to-model-params ............: (1.43, 1.51)
    optimizer ......................................: (12.58, 12.66)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 15835.9 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.521668E+00 | loss scale: 1.0 | grad norm: 13.561 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15710.98, 15714.56)
    forward-compute ................................: (5741.57, 5984.75)
    backward-compute ...............................: (9690.19, 9939.66)
    batch-generator ................................: (54.81, 71.00)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.42, 68.46)
    params-all-gather ..............................: (36.25, 37.66)
    optimizer-copy-to-main-grad ....................: (0.25, 0.41)
    optimizer-clip-main-grad .......................: (2.32, 2.34)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.67, 4.74)
    optimizer-copy-main-to-model-params ............: (1.42, 1.51)
    optimizer ......................................: (9.37, 9.45)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 15822.7 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.416010E+00 | loss scale: 1.0 | grad norm: 3.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15699.21, 15702.43)
    forward-compute ................................: (5747.54, 5984.09)
    backward-compute ...............................: (9679.44, 9921.70)
    batch-generator ................................: (54.20, 76.42)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.42, 68.46)
    params-all-gather ..............................: (36.22, 36.34)
    optimizer-copy-to-main-grad ....................: (0.26, 0.43)
    optimizer-clip-main-grad .......................: (2.35, 2.37)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.68, 4.79)
    optimizer-copy-main-to-model-params ............: (1.43, 1.50)
    optimizer ......................................: (9.48, 9.56)
Sun Feb 11 15:49:22 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             513W / 700W |  41264MiB / 81559MiB |     42%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   49C    P0             544W / 700W |  41328MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   50C    P0             488W / 700W |  41272MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   43C    P0             523W / 700W |  41304MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   42C    P0             505W / 700W |  41296MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   50C    P0             521W / 700W |  41344MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             483W / 700W |  41312MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             508W / 700W |  41280MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 15910.3 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.149936E+00 | loss scale: 1.0 | grad norm: 1.416 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15694.25, 15698.69)
    forward-compute ................................: (5744.46, 5991.75)
    backward-compute ...............................: (9666.52, 9921.12)
    batch-generator ................................: (53.61, 77.23)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.42, 68.68)
    params-all-gather ..............................: (36.24, 36.35)
    optimizer-copy-to-main-grad ....................: (0.25, 0.46)
    optimizer-clip-main-grad .......................: (2.34, 2.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.67, 4.78)
    optimizer-copy-main-to-model-params ............: (1.43, 1.50)
    optimizer ......................................: (9.52, 9.60)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 15804.9 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.286555E+00 | loss scale: 1.0 | grad norm: 1.356 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15680.82, 15684.66)
    forward-compute ................................: (5737.09, 5969.25)
    backward-compute ...............................: (9676.20, 9914.11)
    batch-generator ................................: (53.78, 77.00)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.42, 68.62)
    params-all-gather ..............................: (36.25, 36.35)
    optimizer-copy-to-main-grad ....................: (0.25, 0.44)
    optimizer-clip-main-grad .......................: (2.20, 2.22)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.68, 4.74)
    optimizer-copy-main-to-model-params ............: (1.43, 1.50)
    optimizer ......................................: (9.33, 9.41)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 15810.5 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.154938E+00 | loss scale: 1.0 | grad norm: 0.972 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15686.04, 15690.14)
    forward-compute ................................: (5737.16, 5979.81)
    backward-compute ...............................: (9671.15, 9920.20)
    batch-generator ................................: (55.03, 76.93)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.43, 68.99)
    params-all-gather ..............................: (36.25, 36.34)
    optimizer-copy-to-main-grad ....................: (0.25, 0.44)
    optimizer-clip-main-grad .......................: (2.08, 2.09)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.67, 4.75)
    optimizer-copy-main-to-model-params ............: (1.43, 1.51)
    optimizer ......................................: (9.19, 9.28)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 16077.9 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.141452E+00 | loss scale: 1.0 | grad norm: 1.026 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15953.91, 15958.08)
    forward-compute ................................: (6011.77, 6256.83)
    backward-compute ...............................: (9660.89, 9913.48)
    batch-generator ................................: (54.49, 77.28)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.43, 68.47)
    params-all-gather ..............................: (36.28, 36.36)
    optimizer-copy-to-main-grad ....................: (0.26, 0.43)
    optimizer-clip-main-grad .......................: (1.95, 1.97)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.67, 4.75)
    optimizer-copy-main-to-model-params ............: (1.42, 1.51)
    optimizer ......................................: (9.06, 9.18)
Sun Feb 11 15:59:58 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             601W / 700W |  41264MiB / 81559MiB |     51%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   48C    P0             527W / 700W |  41328MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   49C    P0             553W / 700W |  41272MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   43C    P0             561W / 700W |  41304MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   42C    P0             519W / 700W |  41296MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             542W / 700W |  41344MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             506W / 700W |  41312MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             479W / 700W |  41280MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 15886.5 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.160921E+00 | loss scale: 1.0 | grad norm: 1.212 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15675.40, 15679.20)
    forward-compute ................................: (5740.94, 5976.52)
    backward-compute ...............................: (9662.88, 9905.69)
    batch-generator ................................: (53.73, 76.83)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.42, 68.58)
    params-all-gather ..............................: (36.26, 36.37)
    optimizer-copy-to-main-grad ....................: (0.26, 0.41)
    optimizer-clip-main-grad .......................: (2.07, 2.09)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.67, 4.75)
    optimizer-copy-main-to-model-params ............: (1.42, 1.50)
    optimizer ......................................: (9.15, 9.23)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 15787.8 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.124672E+00 | loss scale: 1.0 | grad norm: 0.999 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15663.95, 15667.55)
    forward-compute ................................: (5735.53, 5967.41)
    backward-compute ...............................: (9660.53, 9899.39)
    batch-generator ................................: (52.39, 76.81)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.42, 68.47)
    params-all-gather ..............................: (36.26, 36.56)
    optimizer-copy-to-main-grad ....................: (0.25, 0.41)
    optimizer-clip-main-grad .......................: (1.68, 1.70)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.67, 5.10)
    optimizer-copy-main-to-model-params ............: (1.42, 1.50)
    optimizer ......................................: (9.19, 9.27)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 15785.6 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.088632E+00 | loss scale: 1.0 | grad norm: 1.658 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15662.39, 15666.04)
    forward-compute ................................: (5732.02, 5960.40)
    backward-compute ...............................: (9667.14, 9901.72)
    batch-generator ................................: (52.67, 77.82)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.42, 68.66)
    params-all-gather ..............................: (36.23, 36.35)
    optimizer-copy-to-main-grad ....................: (0.25, 0.40)
    optimizer-clip-main-grad .......................: (1.68, 1.69)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.67, 4.73)
    optimizer-copy-main-to-model-params ............: (1.42, 1.51)
    optimizer ......................................: (8.78, 8.86)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (443705): No such process
kill: (443711): No such process
kill: (443717): No such process
kill: (443723): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=8, tp=1, pp=2, mbs=2
LOCAL_IP = 10.64.24.51
DP=8, MP=1, PP=2
[2024-02-11 16:07:26,873] torch.distributed.run: [WARNING] 
[2024-02-11 16:07:26,873] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 16:07:26,873] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-11 16:07:26,873] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 3428130816
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...

> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.298 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.391 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.428 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.443 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.452 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.481 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.568 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.817 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.005 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.081 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.055 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.198 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.149 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.215 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.233 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.106 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (858.54, 929.43)
    train/valid/test-data-iterators-setup ..........: (10684.55, 13224.36)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 14663.1 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 8.534090E+00 | loss scale: 1.0 | grad norm: 706.950 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 24713.6240234375 | max allocated: 47222.2958984375 | reserved: 48746.0 | max reserved: 48746.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (13678.90, 13920.79)
    forward-compute ................................: (3996.20, 5364.65)
    backward-compute ...............................: (7474.28, 8189.34)
    batch-generator ................................: (70.34, 98.47)
    forward-recv ...................................: (220.65, 235.63)
    forward-send ...................................: (4.50, 7.23)
    backward-recv ..................................: (153.79, 162.96)
    backward-send ..................................: (1.70, 1.75)
    forward-send-backward-recv .....................: (1870.16, 2262.43)
    backward-send-forward-recv .....................: (77.89, 80.53)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (17.68, 18.71)
    grads-reduce-scatter ...........................: (41.95, 678.75)
    params-all-gather ..............................: (22.41, 22.55)
    optimizer-copy-to-main-grad ....................: (0.16, 0.24)
    optimizer-clip-main-grad .......................: (5.37, 5.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.90, 5.44)
    optimizer-copy-main-to-model-params ............: (1.36, 1.43)
    optimizer ......................................: (13.29, 13.36)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 13159.9 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.520242E+00 | loss scale: 1.0 | grad norm: 7.811 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (12816.64, 13059.80)
    forward-compute ................................: (3900.22, 4619.15)
    backward-compute ...............................: (7455.44, 8185.87)
    batch-generator ................................: (27.40, 39.14)
    forward-recv ...................................: (121.38, 124.66)
    forward-send ...................................: (1.66, 1.69)
    backward-recv ..................................: (154.75, 163.58)
    backward-send ..................................: (1.68, 1.73)
    forward-send-backward-recv .....................: (1139.42, 1508.07)
    backward-send-forward-recv .....................: (76.07, 78.17)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (17.61, 17.83)
    grads-reduce-scatter ...........................: (41.96, 42.51)
    params-all-gather ..............................: (22.40, 22.55)
    optimizer-copy-to-main-grad ....................: (0.17, 0.24)
    optimizer-clip-main-grad .......................: (2.31, 2.33)
    optimizer-count-zeros ..........................: (0.01, 0.04)
    optimizer-inner-step ...........................: (4.77, 4.85)
    optimizer-copy-main-to-model-params ............: (1.36, 1.43)
    optimizer ......................................: (9.25, 9.31)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 14688.6 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.397977E+00 | loss scale: 1.0 | grad norm: 3.851 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14345.61, 14587.76)
    forward-compute ................................: (4161.67, 6113.76)
    backward-compute ...............................: (7444.62, 8170.67)
    batch-generator ................................: (26.98, 35.47)
    forward-recv ...................................: (121.23, 469.90)
    forward-send ...................................: (1.66, 1.70)
    backward-recv ..................................: (158.09, 429.43)
    backward-send ..................................: (1.68, 1.76)
    forward-send-backward-recv .....................: (1967.67, 2783.36)
    backward-send-forward-recv .....................: (76.65, 748.28)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (17.53, 17.81)
    grads-reduce-scatter ...........................: (41.92, 42.54)
    params-all-gather ..............................: (22.40, 22.55)
    optimizer-copy-to-main-grad ....................: (0.17, 0.22)
    optimizer-clip-main-grad .......................: (2.30, 2.33)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 5.30)
    optimizer-copy-main-to-model-params ............: (1.37, 1.43)
    optimizer ......................................: (9.68, 9.75)
Sun Feb 11 16:17:20 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             436W / 700W |  51940MiB / 81559MiB |     96%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   48C    P0             463W / 700W |  51996MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   49C    P0             398W / 700W |  52000MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0             424W / 700W |  51998MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   41C    P0             445W / 700W |  51990MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             416W / 700W |  52024MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             407W / 700W |  51980MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   41C    P0             387W / 700W |  51758MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 13231.6 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.141526E+00 | loss scale: 1.0 | grad norm: 2.090 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (12800.42, 13041.67)
    forward-compute ................................: (3899.89, 4614.64)
    backward-compute ...............................: (7443.81, 8172.07)
    batch-generator ................................: (26.76, 35.46)
    forward-recv ...................................: (121.36, 125.54)
    forward-send ...................................: (1.66, 1.68)
    backward-recv ..................................: (154.43, 162.11)
    backward-send ..................................: (1.68, 1.72)
    forward-send-backward-recv .....................: (1140.22, 1512.91)
    backward-send-forward-recv .....................: (76.16, 80.77)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.53, 17.75)
    grads-reduce-scatter ...........................: (41.97, 42.42)
    params-all-gather ..............................: (22.41, 22.58)
    optimizer-copy-to-main-grad ....................: (0.16, 0.22)
    optimizer-clip-main-grad .......................: (2.30, 2.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.84)
    optimizer-copy-main-to-model-params ............: (1.36, 1.43)
    optimizer ......................................: (9.28, 9.34)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 14576.6 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.281627E+00 | loss scale: 1.0 | grad norm: 1.474 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14233.37, 14475.85)
    forward-compute ................................: (3897.93, 6062.17)
    backward-compute ...............................: (7441.09, 8161.15)
    batch-generator ................................: (26.72, 35.12)
    forward-recv ...................................: (121.45, 124.11)
    forward-send ...................................: (1.65, 1.68)
    backward-recv ..................................: (153.53, 162.57)
    backward-send ..................................: (1.67, 1.72)
    forward-send-backward-recv .....................: (2576.08, 2950.26)
    backward-send-forward-recv .....................: (76.31, 80.40)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.54, 17.83)
    grads-reduce-scatter ...........................: (41.93, 42.63)
    params-all-gather ..............................: (22.42, 22.56)
    optimizer-copy-to-main-grad ....................: (0.16, 0.22)
    optimizer-clip-main-grad .......................: (2.30, 2.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.92)
    optimizer-copy-main-to-model-params ............: (1.37, 1.43)
    optimizer ......................................: (9.57, 9.63)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 14473.3 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.151767E+00 | loss scale: 1.0 | grad norm: 1.256 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14129.44, 14371.86)
    forward-compute ................................: (4152.19, 5906.73)
    backward-compute ...............................: (7438.71, 8169.01)
    batch-generator ................................: (27.06, 34.77)
    forward-recv ...................................: (121.37, 428.45)
    forward-send ...................................: (1.65, 1.68)
    backward-recv ..................................: (154.72, 163.31)
    backward-send ..................................: (1.68, 1.72)
    forward-send-backward-recv .....................: (2213.06, 2596.50)
    backward-send-forward-recv .....................: (76.92, 352.24)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.49, 17.96)
    grads-reduce-scatter ...........................: (41.94, 42.58)
    params-all-gather ..............................: (22.43, 22.56)
    optimizer-copy-to-main-grad ....................: (0.16, 0.23)
    optimizer-clip-main-grad .......................: (2.18, 2.20)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.84)
    optimizer-copy-main-to-model-params ............: (1.37, 1.47)
    optimizer ......................................: (9.07, 9.17)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 13133.3 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.138495E+00 | loss scale: 1.0 | grad norm: 1.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (12791.21, 13032.88)
    forward-compute ................................: (3898.75, 4614.66)
    backward-compute ...............................: (7442.02, 8172.01)
    batch-generator ................................: (27.04, 34.99)
    forward-recv ...................................: (121.41, 124.23)
    forward-send ...................................: (1.65, 1.68)
    backward-recv ..................................: (153.87, 162.07)
    backward-send ..................................: (1.68, 1.72)
    forward-send-backward-recv .....................: (1130.52, 1502.81)
    backward-send-forward-recv .....................: (76.63, 77.87)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.57, 17.83)
    grads-reduce-scatter ...........................: (41.93, 42.49)
    params-all-gather ..............................: (22.41, 22.57)
    optimizer-copy-to-main-grad ....................: (0.16, 0.23)
    optimizer-clip-main-grad .......................: (1.91, 1.93)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.84)
    optimizer-copy-main-to-model-params ............: (1.37, 1.43)
    optimizer ......................................: (8.79, 8.85)
Sun Feb 11 16:26:34 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   40C    P0             432W / 700W |  51940MiB / 81559MiB |     49%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   48C    P0             438W / 700W |  51996MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   49C    P0             400W / 700W |  52000MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0             417W / 700W |  51998MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   41C    P0             385W / 700W |  51990MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             413W / 700W |  52024MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             361W / 700W |  51980MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   41C    P0             411W / 700W |  51758MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 13228.8 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.161336E+00 | loss scale: 1.0 | grad norm: 1.594 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (12798.33, 13038.38)
    forward-compute ................................: (3901.49, 4621.14)
    backward-compute ...............................: (7444.08, 8159.23)
    batch-generator ................................: (27.11, 37.92)
    forward-recv ...................................: (121.47, 124.39)
    forward-send ...................................: (1.64, 1.69)
    backward-recv ..................................: (152.42, 161.91)
    backward-send ..................................: (1.68, 1.72)
    forward-send-backward-recv .....................: (1138.77, 1505.21)
    backward-send-forward-recv .....................: (76.52, 78.76)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.44, 18.37)
    grads-reduce-scatter ...........................: (41.89, 42.70)
    params-all-gather ..............................: (22.42, 22.58)
    optimizer-copy-to-main-grad ....................: (0.16, 0.26)
    optimizer-clip-main-grad .......................: (2.06, 2.08)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.84)
    optimizer-copy-main-to-model-params ............: (1.36, 1.43)
    optimizer ......................................: (8.98, 9.05)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 13380.6 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.124885E+00 | loss scale: 1.0 | grad norm: 0.734 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13038.91, 13280.86)
    forward-compute ................................: (3901.89, 4883.28)
    backward-compute ...............................: (7441.32, 8163.66)
    batch-generator ................................: (27.25, 36.96)
    forward-recv ...................................: (121.57, 124.59)
    forward-send ...................................: (1.66, 1.68)
    backward-recv ..................................: (152.14, 160.19)
    backward-send ..................................: (1.68, 1.71)
    forward-send-backward-recv .....................: (1355.98, 1743.90)
    backward-send-forward-recv .....................: (76.37, 326.36)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (17.53, 17.83)
    grads-reduce-scatter ...........................: (41.99, 42.36)
    params-all-gather ..............................: (22.43, 22.57)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (1.91, 1.93)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.85)
    optimizer-copy-main-to-model-params ............: (1.36, 1.43)
    optimizer ......................................: (8.81, 8.88)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 15567.0 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.089103E+00 | loss scale: 1.0 | grad norm: 0.712 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15225.45, 15466.58)
    forward-compute ................................: (3960.33, 7000.12)
    backward-compute ...............................: (7436.02, 8143.08)
    batch-generator ................................: (27.07, 36.89)
    forward-recv ...................................: (121.41, 123.99)
    forward-send ...................................: (1.66, 1.68)
    backward-recv ..................................: (153.68, 161.86)
    backward-send ..................................: (1.68, 1.72)
    forward-send-backward-recv .....................: (3328.44, 3788.06)
    backward-send-forward-recv .....................: (76.58, 361.53)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (17.53, 17.88)
    grads-reduce-scatter ...........................: (41.99, 42.44)
    params-all-gather ..............................: (22.43, 22.80)
    optimizer-copy-to-main-grad ....................: (0.16, 0.22)
    optimizer-clip-main-grad .......................: (1.65, 1.66)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.84)
    optimizer-copy-main-to-model-params ............: (1.36, 1.43)
    optimizer ......................................: (8.54, 8.61)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (446300): No such process
kill: (446306): No such process
kill: (446312): No such process
kill: (446318): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=8, tp=1, pp=2, mbs=1
LOCAL_IP = 10.64.24.51
DP=8, MP=1, PP=2
[2024-02-11 16:33:41,243] torch.distributed.run: [WARNING] 
[2024-02-11 16:33:41,243] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 16:33:41,243] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-11 16:33:41,243] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 3428130816
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...

> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.440 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.447 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.433 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.523 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.521 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.472 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.555 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.766 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.119 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.188 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.311 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.328 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.206 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.284 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.325 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.242 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (867.11, 919.53)
    train/valid/test-data-iterators-setup ..........: (10938.63, 12766.09)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 15056.7 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 8.531790E+00 | loss scale: 1.0 | grad norm: 706.879 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 24649.6240234375 | max allocated: 35391.92919921875 | reserved: 35726.0 | max reserved: 35726.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (14206.52, 14333.63)
    forward-compute ................................: (4098.97, 5497.23)
    backward-compute ...............................: (7874.53, 8601.05)
    batch-generator ................................: (63.81, 124.58)
    forward-recv ...................................: (159.61, 177.45)
    forward-send ...................................: (3.05, 4.79)
    backward-recv ..................................: (80.15, 84.28)
    backward-send ..................................: (0.90, 0.93)
    forward-send-backward-recv .....................: (1849.68, 2244.20)
    backward-send-forward-recv .....................: (84.30, 88.70)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (17.56, 17.85)
    grads-reduce-scatter ...........................: (42.08, 660.09)
    params-all-gather ..............................: (22.45, 22.55)
    optimizer-copy-to-main-grad ....................: (0.16, 0.23)
    optimizer-clip-main-grad .......................: (5.51, 5.53)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.90, 4.98)
    optimizer-copy-main-to-model-params ............: (1.37, 1.43)
    optimizer ......................................: (12.89, 12.99)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 14184.7 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.519829E+00 | loss scale: 1.0 | grad norm: 7.689 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13956.52, 14083.00)
    forward-compute ................................: (4255.20, 5345.13)
    backward-compute ...............................: (7854.18, 8571.69)
    batch-generator ................................: (49.99, 69.02)
    forward-recv ...................................: (61.33, 62.98)
    forward-send ...................................: (0.88, 0.92)
    backward-recv ..................................: (79.49, 83.73)
    backward-send ..................................: (0.89, 0.91)
    forward-send-backward-recv .....................: (1454.07, 1862.25)
    backward-send-forward-recv .....................: (83.70, 387.29)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.55, 17.76)
    grads-reduce-scatter ...........................: (41.98, 43.10)
    params-all-gather ..............................: (22.46, 22.58)
    optimizer-copy-to-main-grad ....................: (0.14, 0.24)
    optimizer-clip-main-grad .......................: (2.30, 2.34)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.85)
    optimizer-copy-main-to-model-params ............: (1.36, 1.42)
    optimizer ......................................: (9.25, 9.31)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 16618.5 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.397533E+00 | loss scale: 1.0 | grad norm: 3.956 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16391.31, 16517.78)
    forward-compute ................................: (4243.83, 7810.21)
    backward-compute ...............................: (7848.92, 8570.93)
    batch-generator ................................: (49.51, 67.05)
    forward-recv ...................................: (61.48, 322.14)
    forward-send ...................................: (0.88, 0.90)
    backward-recv ..................................: (78.11, 343.90)
    backward-send ..................................: (0.90, 0.94)
    forward-send-backward-recv .....................: (3765.10, 4053.84)
    backward-send-forward-recv .....................: (85.30, 394.04)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (17.62, 17.90)
    grads-reduce-scatter ...........................: (41.97, 42.57)
    params-all-gather ..............................: (22.46, 22.55)
    optimizer-copy-to-main-grad ....................: (0.14, 0.23)
    optimizer-clip-main-grad .......................: (2.30, 2.33)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.86)
    optimizer-copy-main-to-model-params ............: (1.36, 1.42)
    optimizer ......................................: (9.27, 9.32)
Sun Feb 11 16:44:10 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             492W / 700W |  38920MiB / 81559MiB |     35%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   49C    P0             468W / 700W |  39344MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   50C    P0             483W / 700W |  39348MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   43C    P0             483W / 700W |  39346MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   42C    P0             509W / 700W |  39338MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             469W / 700W |  39276MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             455W / 700W |  39392MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             459W / 700W |  39042MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 13632.7 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.141388E+00 | loss scale: 1.0 | grad norm: 2.094 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13316.04, 13441.65)
    forward-compute ................................: (3982.61, 4746.70)
    backward-compute ...............................: (7846.01, 8595.10)
    batch-generator ................................: (49.82, 65.98)
    forward-recv ...................................: (61.34, 63.17)
    forward-send ...................................: (0.88, 0.90)
    backward-recv ..................................: (80.35, 84.49)
    backward-send ..................................: (0.89, 0.94)
    forward-send-backward-recv .....................: (1092.77, 1502.17)
    backward-send-forward-recv .....................: (83.77, 88.11)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.52, 17.99)
    grads-reduce-scatter ...........................: (42.00, 42.36)
    params-all-gather ..............................: (22.47, 22.57)
    optimizer-copy-to-main-grad ....................: (0.14, 0.24)
    optimizer-clip-main-grad .......................: (2.32, 2.35)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.86)
    optimizer-copy-main-to-model-params ............: (1.36, 1.43)
    optimizer ......................................: (9.28, 9.34)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 16573.7 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.281626E+00 | loss scale: 1.0 | grad norm: 1.644 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16345.66, 16471.59)
    forward-compute ................................: (4240.07, 7771.09)
    backward-compute ...............................: (7838.34, 8554.82)
    batch-generator ................................: (50.23, 65.22)
    forward-recv ...................................: (61.24, 62.66)
    forward-send ...................................: (0.87, 0.91)
    backward-recv ..................................: (79.14, 83.53)
    backward-send ..................................: (0.90, 0.94)
    forward-send-backward-recv .....................: (3893.70, 4281.97)
    backward-send-forward-recv .....................: (85.10, 393.97)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.60, 17.91)
    grads-reduce-scatter ...........................: (41.99, 42.64)
    params-all-gather ..............................: (22.45, 22.62)
    optimizer-copy-to-main-grad ....................: (0.14, 0.24)
    optimizer-clip-main-grad .......................: (2.33, 2.36)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.95)
    optimizer-copy-main-to-model-params ............: (1.36, 1.52)
    optimizer ......................................: (9.49, 9.65)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 15390.1 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.151914E+00 | loss scale: 1.0 | grad norm: 1.155 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15162.17, 15288.20)
    forward-compute ................................: (4244.50, 6519.17)
    backward-compute ...............................: (7840.50, 8555.55)
    batch-generator ................................: (49.96, 65.90)
    forward-recv ...................................: (61.35, 62.87)
    forward-send ...................................: (0.88, 0.92)
    backward-recv ..................................: (79.81, 82.82)
    backward-send ..................................: (0.89, 0.93)
    forward-send-backward-recv .....................: (2726.32, 3093.72)
    backward-send-forward-recv .....................: (84.72, 394.27)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.62, 18.03)
    grads-reduce-scatter ...........................: (42.02, 42.58)
    params-all-gather ..............................: (22.47, 22.86)
    optimizer-copy-to-main-grad ....................: (0.14, 0.23)
    optimizer-clip-main-grad .......................: (2.17, 2.20)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.84)
    optimizer-copy-main-to-model-params ............: (1.36, 1.42)
    optimizer ......................................: (9.08, 9.13)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 13527.9 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.138442E+00 | loss scale: 1.0 | grad norm: 0.782 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13300.70, 13427.34)
    forward-compute ................................: (3979.36, 4749.20)
    backward-compute ...............................: (7844.00, 8563.17)
    batch-generator ................................: (51.98, 64.73)
    forward-recv ...................................: (61.44, 62.79)
    forward-send ...................................: (0.87, 0.90)
    backward-recv ..................................: (79.00, 83.45)
    backward-send ..................................: (0.89, 0.91)
    forward-send-backward-recv .....................: (1118.83, 1493.01)
    backward-send-forward-recv .....................: (82.80, 86.03)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.61, 17.88)
    grads-reduce-scatter ...........................: (42.07, 42.48)
    params-all-gather ..............................: (22.45, 22.56)
    optimizer-copy-to-main-grad ....................: (0.14, 0.23)
    optimizer-clip-main-grad .......................: (1.64, 1.66)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 5.17)
    optimizer-copy-main-to-model-params ............: (1.36, 1.42)
    optimizer ......................................: (8.88, 8.93)
Sun Feb 11 16:54:37 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             477W / 700W |  38920MiB / 81559MiB |     12%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   49C    P0             505W / 700W |  39344MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   50C    P0             502W / 700W |  39348MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   43C    P0             433W / 700W |  39346MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   42C    P0             441W / 700W |  39338MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             436W / 700W |  39276MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             424W / 700W |  39392MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             412W / 700W |  39042MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 17147.6 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.158736E+00 | loss scale: 1.0 | grad norm: 1.041 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16830.10, 16955.55)
    forward-compute ................................: (4240.94, 8290.73)
    backward-compute ...............................: (7836.21, 8555.04)
    batch-generator ................................: (51.11, 66.02)
    forward-recv ...................................: (61.31, 62.97)
    forward-send ...................................: (0.87, 0.90)
    backward-recv ..................................: (79.31, 83.57)
    backward-send ..................................: (0.89, 0.94)
    forward-send-backward-recv .....................: (4388.94, 4767.55)
    backward-send-forward-recv .....................: (83.07, 394.79)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.61, 17.87)
    grads-reduce-scatter ...........................: (42.02, 42.58)
    params-all-gather ..............................: (22.47, 22.56)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (2.17, 2.20)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.86)
    optimizer-copy-main-to-model-params ............: (1.36, 1.55)
    optimizer ......................................: (9.10, 9.28)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 14046.3 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.121839E+00 | loss scale: 1.0 | grad norm: 1.115 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13819.32, 13945.41)
    forward-compute ................................: (3983.36, 5268.05)
    backward-compute ...............................: (7842.42, 8579.54)
    batch-generator ................................: (50.62, 66.34)
    forward-recv ...................................: (61.40, 62.81)
    forward-send ...................................: (0.88, 0.89)
    backward-recv ..................................: (79.66, 83.45)
    backward-send ..................................: (0.89, 0.92)
    forward-send-backward-recv .....................: (1592.26, 1979.53)
    backward-send-forward-recv .....................: (83.02, 343.27)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (17.55, 18.11)
    grads-reduce-scatter ...........................: (42.01, 42.41)
    params-all-gather ..............................: (22.46, 22.54)
    optimizer-copy-to-main-grad ....................: (0.14, 0.23)
    optimizer-clip-main-grad .......................: (1.77, 1.86)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.85)
    optimizer-copy-main-to-model-params ............: (1.36, 1.42)
    optimizer ......................................: (8.75, 8.80)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 16860.0 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.086209E+00 | loss scale: 1.0 | grad norm: 0.899 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16632.27, 16758.60)
    forward-compute ................................: (3977.39, 8093.27)
    backward-compute ...............................: (7836.94, 8544.89)
    batch-generator ................................: (50.08, 66.18)
    forward-recv ...................................: (61.24, 62.97)
    forward-send ...................................: (0.88, 0.90)
    backward-recv ..................................: (77.84, 83.69)
    backward-send ..................................: (0.89, 0.92)
    forward-send-backward-recv .....................: (4076.48, 4833.35)
    backward-send-forward-recv .....................: (82.93, 413.93)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (17.54, 18.26)
    grads-reduce-scatter ...........................: (42.01, 42.46)
    params-all-gather ..............................: (22.45, 22.55)
    optimizer-copy-to-main-grad ....................: (0.14, 0.23)
    optimizer-clip-main-grad .......................: (1.65, 1.77)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.86)
    optimizer-copy-main-to-model-params ............: (1.36, 1.48)
    optimizer ......................................: (8.69, 8.81)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (448593): No such process
kill: (448599): No such process
kill: (448605): No such process
kill: (448611): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=4, tp=2, pp=2, mbs=4
LOCAL_IP = 10.64.24.51
DP=4, MP=2, PP=2
[2024-02-11 17:01:59,703] torch.distributed.run: [WARNING] 
[2024-02-11 17:01:59,703] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 17:01:59,703] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-11 17:01:59,703] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1714528256
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1714528256
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.309 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  6.015 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  7.042 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  9.770 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.072 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.389 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.403 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.455 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (865.26, 921.57)
    train/valid/test-data-iterators-setup ..........: (0.02, 15684.98)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 15955.6 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 9.803184E+00 | loss scale: 1.0 | grad norm: 722.086 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 9] (after 10 iterations) memory (MB) | allocated: 15036.9169921875 | max allocated: 42806.2294921875 | reserved: 47272.0 | max reserved: 47272.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 15036.9169921875 | max allocated: 42806.2294921875 | reserved: 47548.0 | max reserved: 47548.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (15225.79, 15485.22)
    forward-compute ................................: (5030.03, 6007.39)
    backward-compute ...............................: (8107.99, 8757.55)
    batch-generator ................................: (214.39, 253.73)
    forward-recv ...................................: (397.19, 411.89)
    forward-send ...................................: (5.11, 10.82)
    backward-recv ..................................: (180.50, 187.12)
    backward-send ..................................: (3.21, 3.29)
    forward-send-backward-recv .....................: (1870.58, 2137.50)
    backward-send-forward-recv .....................: (146.16, 152.53)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (9.08, 9.57)
    grads-reduce-scatter ...........................: (19.16, 431.36)
    params-all-gather ..............................: (10.62, 10.76)
    optimizer-copy-to-main-grad ....................: (0.27, 0.45)
    optimizer-clip-main-grad .......................: (5.38, 5.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.93, 5.14)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (13.26, 13.35)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 14591.4 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.499576E+00 | loss scale: 1.0 | grad norm: 11.842 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14275.51, 14534.90)
    forward-compute ................................: (4788.32, 5324.89)
    backward-compute ...............................: (8088.59, 8740.11)
    batch-generator ................................: (26.02, 47.54)
    forward-recv ...................................: (150.67, 152.75)
    forward-send ...................................: (3.10, 3.24)
    backward-recv ..................................: (181.15, 187.06)
    backward-send ..................................: (3.22, 3.27)
    forward-send-backward-recv .....................: (1185.29, 1449.17)
    backward-send-forward-recv .....................: (145.97, 149.05)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.99, 9.66)
    grads-reduce-scatter ...........................: (19.20, 19.58)
    params-all-gather ..............................: (10.63, 10.79)
    optimizer-copy-to-main-grad ....................: (0.27, 0.43)
    optimizer-clip-main-grad .......................: (2.56, 2.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 5.03)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (9.96, 10.05)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 15424.9 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.421718E+00 | loss scale: 1.0 | grad norm: 2.882 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15111.50, 15369.42)
    forward-compute ................................: (5049.15, 5908.87)
    backward-compute ...............................: (8079.07, 8723.35)
    batch-generator ................................: (26.90, 44.04)
    forward-recv ...................................: (150.65, 152.71)
    forward-send ...................................: (3.10, 3.18)
    backward-recv ..................................: (180.06, 523.60)
    backward-send ..................................: (3.19, 3.28)
    forward-send-backward-recv .....................: (1549.62, 1923.05)
    backward-send-forward-recv .....................: (405.07, 488.54)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.99, 9.52)
    grads-reduce-scatter ...........................: (19.11, 19.49)
    params-all-gather ..............................: (10.63, 10.77)
    optimizer-copy-to-main-grad ....................: (0.27, 0.47)
    optimizer-clip-main-grad .......................: (2.43, 2.46)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.96)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (9.83, 9.92)
Sun Feb 11 17:12:44 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             445W / 700W |  51222MiB / 81559MiB |     26%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   49C    P0             485W / 700W |  50946MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   50C    P0             447W / 700W |  51044MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   43C    P0             384W / 700W |  51556MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   42C    P0             431W / 700W |  51006MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             393W / 700W |  51518MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             455W / 700W |  50730MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             383W / 700W |  50690MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 14655.8 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.147997E+00 | loss scale: 1.0 | grad norm: 1.369 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14251.54, 14509.64)
    forward-compute ................................: (4787.78, 5319.48)
    backward-compute ...............................: (8079.01, 8720.86)
    batch-generator ................................: (25.97, 39.94)
    forward-recv ...................................: (150.93, 152.93)
    forward-send ...................................: (3.09, 3.15)
    backward-recv ..................................: (180.69, 187.56)
    backward-send ..................................: (3.22, 3.28)
    forward-send-backward-recv .....................: (1172.76, 1435.67)
    backward-send-forward-recv .....................: (145.75, 149.51)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.96, 9.59)
    grads-reduce-scatter ...........................: (19.19, 19.59)
    params-all-gather ..............................: (10.64, 10.79)
    optimizer-copy-to-main-grad ....................: (0.26, 0.44)
    optimizer-clip-main-grad .......................: (2.41, 2.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.92)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (9.70, 9.79)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 15267.0 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.281608E+00 | loss scale: 1.0 | grad norm: 0.974 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14955.08, 15212.44)
    forward-compute ................................: (4786.03, 6021.80)
    backward-compute ...............................: (8073.68, 8701.10)
    batch-generator ................................: (25.70, 37.83)
    forward-recv ...................................: (150.52, 152.56)
    forward-send ...................................: (3.07, 3.15)
    backward-recv ..................................: (179.74, 186.82)
    backward-send ..................................: (3.22, 3.27)
    forward-send-backward-recv .....................: (1885.12, 2145.25)
    backward-send-forward-recv .....................: (145.67, 149.97)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (9.00, 9.39)
    grads-reduce-scatter ...........................: (19.13, 19.56)
    params-all-gather ..............................: (10.61, 10.77)
    optimizer-copy-to-main-grad ....................: (0.27, 0.43)
    optimizer-clip-main-grad .......................: (2.01, 2.11)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.91)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (9.37, 9.47)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 15609.5 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.154219E+00 | loss scale: 1.0 | grad norm: 1.599 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15294.36, 15554.23)
    forward-compute ................................: (5040.25, 6115.67)
    backward-compute ...............................: (8075.16, 8714.20)
    batch-generator ................................: (26.13, 38.17)
    forward-recv ...................................: (150.41, 152.59)
    forward-send ...................................: (3.10, 3.13)
    backward-recv ..................................: (182.82, 187.23)
    backward-send ..................................: (3.23, 3.28)
    forward-send-backward-recv .....................: (1919.21, 2229.14)
    backward-send-forward-recv .....................: (395.95, 453.70)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.94, 9.55)
    grads-reduce-scatter ...........................: (19.18, 19.54)
    params-all-gather ..............................: (10.61, 10.76)
    optimizer-copy-to-main-grad ....................: (0.26, 0.40)
    optimizer-clip-main-grad .......................: (2.27, 2.29)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.91)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (9.52, 9.73)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 14557.7 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.142092E+00 | loss scale: 1.0 | grad norm: 0.987 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14245.47, 14503.56)
    forward-compute ................................: (4786.73, 5297.09)
    backward-compute ...............................: (8077.54, 8697.98)
    batch-generator ................................: (25.67, 37.63)
    forward-recv ...................................: (150.78, 152.71)
    forward-send ...................................: (3.07, 3.14)
    backward-recv ..................................: (180.07, 186.69)
    backward-send ..................................: (3.23, 3.31)
    forward-send-backward-recv .....................: (1172.09, 1430.40)
    backward-send-forward-recv .....................: (145.84, 148.85)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.95, 9.40)
    grads-reduce-scatter ...........................: (19.21, 19.54)
    params-all-gather ..............................: (10.60, 10.77)
    optimizer-copy-to-main-grad ....................: (0.27, 0.39)
    optimizer-clip-main-grad .......................: (2.13, 2.15)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.90)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (9.34, 9.44)
Sun Feb 11 17:22:45 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             504W / 700W |  51222MiB / 81559MiB |     51%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   49C    P0             423W / 700W |  50946MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   50C    P0             416W / 700W |  51044MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   43C    P0             440W / 700W |  51556MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   42C    P0             392W / 700W |  51006MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             420W / 700W |  51518MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             379W / 700W |  50730MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             384W / 700W |  50690MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 14648.7 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.160226E+00 | loss scale: 1.0 | grad norm: 0.571 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14248.39, 14506.86)
    forward-compute ................................: (4784.83, 5312.81)
    backward-compute ...............................: (8081.31, 8717.80)
    batch-generator ................................: (25.49, 37.33)
    forward-recv ...................................: (150.57, 152.59)
    forward-send ...................................: (3.10, 3.16)
    backward-recv ..................................: (179.90, 184.11)
    backward-send ..................................: (3.21, 3.30)
    forward-send-backward-recv .....................: (1166.37, 1435.79)
    backward-send-forward-recv .....................: (146.56, 149.34)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.95, 9.62)
    grads-reduce-scatter ...........................: (19.14, 19.57)
    params-all-gather ..............................: (10.62, 10.76)
    optimizer-copy-to-main-grad ....................: (0.27, 0.41)
    optimizer-clip-main-grad .......................: (1.87, 1.88)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.89)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (9.10, 9.19)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 14553.2 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.122468E+00 | loss scale: 1.0 | grad norm: 0.819 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14240.26, 14499.13)
    forward-compute ................................: (4786.57, 5307.57)
    backward-compute ...............................: (8078.18, 8695.13)
    batch-generator ................................: (25.88, 37.38)
    forward-recv ...................................: (150.58, 152.71)
    forward-send ...................................: (3.10, 3.24)
    backward-recv ..................................: (181.17, 187.12)
    backward-send ..................................: (3.22, 3.28)
    forward-send-backward-recv .....................: (1159.73, 1425.67)
    backward-send-forward-recv .....................: (145.94, 149.11)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.98, 9.40)
    grads-reduce-scatter ...........................: (19.19, 19.52)
    params-all-gather ..............................: (10.63, 10.77)
    optimizer-copy-to-main-grad ....................: (0.27, 0.40)
    optimizer-clip-main-grad .......................: (1.48, 1.49)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.90)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (8.67, 8.77)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 15912.3 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.087776E+00 | loss scale: 1.0 | grad norm: 1.556 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15599.89, 15857.74)
    forward-compute ................................: (5035.90, 6425.04)
    backward-compute ...............................: (8072.16, 8699.07)
    batch-generator ................................: (25.96, 37.03)
    forward-recv ...................................: (150.39, 152.72)
    forward-send ...................................: (3.08, 3.14)
    backward-recv ..................................: (435.24, 442.48)
    backward-send ..................................: (3.20, 3.27)
    forward-send-backward-recv .....................: (1968.73, 2285.93)
    backward-send-forward-recv .....................: (394.98, 453.82)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.99, 9.60)
    grads-reduce-scatter ...........................: (19.21, 19.52)
    params-all-gather ..............................: (10.64, 10.77)
    optimizer-copy-to-main-grad ....................: (0.28, 0.40)
    optimizer-clip-main-grad .......................: (1.88, 1.98)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.90)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (9.21, 9.30)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (450099): No such process
kill: (450105): No such process
kill: (450111): No such process
kill: (450117): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=4, tp=2, pp=2, mbs=2
LOCAL_IP = 10.64.24.51
DP=4, MP=2, PP=2
[2024-02-11 17:30:03,166] torch.distributed.run: [WARNING] 
[2024-02-11 17:30:03,166] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 17:30:03,166] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-11 17:30:03,166] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1714528256
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1714528256
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.178 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.235 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.352 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.500 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.135 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.087 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.143 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.133 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (844.98, 899.61)
    train/valid/test-data-iterators-setup ..........: (0.02, 13914.21)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 16588.7 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 9.802755E+00 | loss scale: 1.0 | grad norm: 722.220 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 14908.9169921875 | max allocated: 28793.6044921875 | reserved: 31234.0 | max reserved: 31234.0
[Rank 9] (after 10 iterations) memory (MB) | allocated: 14908.9169921875 | max allocated: 28793.6044921875 | reserved: 31204.0 | max reserved: 31204.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (15991.82, 16130.27)
    forward-compute ................................: (5262.14, 6220.19)
    backward-compute ...............................: (8699.42, 9376.10)
    batch-generator ................................: (250.29, 277.03)
    forward-recv ...................................: (335.38, 344.79)
    forward-send ...................................: (3.69, 7.77)
    backward-recv ..................................: (93.85, 97.94)
    backward-send ..................................: (1.65, 1.73)
    forward-send-backward-recv .....................: (1752.43, 2035.15)
    backward-send-forward-recv .....................: (161.50, 171.28)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (9.03, 9.45)
    grads-reduce-scatter ...........................: (19.17, 419.19)
    params-all-gather ..............................: (10.60, 10.78)
    optimizer-copy-to-main-grad ....................: (0.28, 0.44)
    optimizer-clip-main-grad .......................: (5.43, 5.47)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.91, 5.15)
    optimizer-copy-main-to-model-params ............: (1.47, 1.57)
    optimizer ......................................: (13.26, 13.37)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 15597.8 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.499568E+00 | loss scale: 1.0 | grad norm: 12.339 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15404.02, 15541.64)
    forward-compute ................................: (5285.59, 5905.31)
    backward-compute ...............................: (8679.11, 9339.39)
    batch-generator ................................: (50.06, 61.37)
    forward-recv ...................................: (78.00, 79.19)
    forward-send ...................................: (1.58, 1.62)
    backward-recv ..................................: (94.06, 97.47)
    backward-send ..................................: (1.64, 1.70)
    forward-send-backward-recv .....................: (1199.12, 1432.96)
    backward-send-forward-recv .....................: (158.44, 523.55)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.97, 9.66)
    grads-reduce-scatter ...........................: (19.18, 19.50)
    params-all-gather ..............................: (10.62, 10.85)
    optimizer-copy-to-main-grad ....................: (0.26, 0.38)
    optimizer-clip-main-grad .......................: (2.39, 2.50)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.95)
    optimizer-copy-main-to-model-params ............: (1.48, 2.08)
    optimizer ......................................: (9.77, 10.37)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 16808.6 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.423692E+00 | loss scale: 1.0 | grad norm: 2.827 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16614.81, 16753.32)
    forward-compute ................................: (5280.77, 7066.98)
    backward-compute ...............................: (8685.58, 9330.63)
    batch-generator ................................: (49.29, 61.48)
    forward-recv ...................................: (78.05, 79.12)
    forward-send ...................................: (1.59, 1.62)
    backward-recv ..................................: (94.24, 97.05)
    backward-send ..................................: (1.64, 1.70)
    forward-send-backward-recv .....................: (2430.13, 2663.21)
    backward-send-forward-recv .....................: (159.41, 429.61)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.99, 9.39)
    grads-reduce-scatter ...........................: (19.09, 19.56)
    params-all-gather ..............................: (10.62, 10.77)
    optimizer-copy-to-main-grad ....................: (0.26, 0.38)
    optimizer-clip-main-grad .......................: (2.39, 2.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.91)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (9.60, 9.70)
Sun Feb 11 17:41:22 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   42C    P0             546W / 700W |  34908MiB / 81559MiB |     93%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   49C    P0             487W / 700W |  34878MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   51C    P0             515W / 700W |  34996MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   43C    P0             490W / 700W |  35156MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   43C    P0             422W / 700W |  34958MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   50C    P0             509W / 700W |  34938MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   48C    P0             407W / 700W |  34652MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   43C    P0             440W / 700W |  34664MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 15311.4 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.148676E+00 | loss scale: 1.0 | grad norm: 1.342 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15025.70, 15164.00)
    forward-compute ................................: (5004.87, 5546.20)
    backward-compute ...............................: (8668.79, 9333.91)
    batch-generator ................................: (49.25, 66.06)
    forward-recv ...................................: (78.02, 79.05)
    forward-send ...................................: (1.58, 1.61)
    backward-recv ..................................: (94.70, 97.37)
    backward-send ..................................: (1.65, 1.71)
    forward-send-backward-recv .....................: (1108.68, 1366.07)
    backward-send-forward-recv .....................: (158.12, 163.08)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.97, 9.64)
    grads-reduce-scatter ...........................: (19.21, 19.60)
    params-all-gather ..............................: (10.63, 10.78)
    optimizer-copy-to-main-grad ....................: (0.26, 0.38)
    optimizer-clip-main-grad .......................: (2.41, 2.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 5.00)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (9.75, 9.85)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 16836.1 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.282828E+00 | loss scale: 1.0 | grad norm: 2.361 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16642.10, 16781.16)
    forward-compute ................................: (5267.77, 6905.43)
    backward-compute ...............................: (8676.36, 9317.42)
    batch-generator ................................: (49.36, 70.01)
    forward-recv ...................................: (78.04, 79.04)
    forward-send ...................................: (1.59, 1.62)
    backward-recv ..................................: (100.33, 102.66)
    backward-send ..................................: (1.64, 1.72)
    forward-send-backward-recv .....................: (2464.04, 2705.82)
    backward-send-forward-recv .....................: (414.60, 491.53)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (9.05, 9.41)
    grads-reduce-scatter ...........................: (19.13, 19.52)
    params-all-gather ..............................: (10.62, 10.77)
    optimizer-copy-to-main-grad ....................: (0.27, 0.41)
    optimizer-clip-main-grad .......................: (2.01, 2.03)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.93)
    optimizer-copy-main-to-model-params ............: (1.47, 1.57)
    optimizer ......................................: (9.30, 9.41)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 15210.2 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.155002E+00 | loss scale: 1.0 | grad norm: 2.761 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15016.79, 15154.60)
    forward-compute ................................: (5008.38, 5523.42)
    backward-compute ...............................: (8680.47, 9306.68)
    batch-generator ................................: (48.93, 74.68)
    forward-recv ...................................: (77.98, 79.79)
    forward-send ...................................: (1.58, 1.61)
    backward-recv ..................................: (93.64, 97.25)
    backward-send ..................................: (1.65, 1.72)
    forward-send-backward-recv .....................: (1089.85, 1341.15)
    backward-send-forward-recv .....................: (157.26, 162.49)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.92, 9.57)
    grads-reduce-scatter ...........................: (19.15, 19.52)
    params-all-gather ..............................: (10.62, 10.77)
    optimizer-copy-to-main-grad ....................: (0.26, 0.42)
    optimizer-clip-main-grad .......................: (2.13, 2.15)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.91)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (9.36, 9.45)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 16302.5 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.144415E+00 | loss scale: 1.0 | grad norm: 1.224 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16108.69, 16247.17)
    forward-compute ................................: (5268.33, 6367.32)
    backward-compute ...............................: (8684.39, 9297.26)
    batch-generator ................................: (47.90, 72.22)
    forward-recv ...................................: (78.07, 79.09)
    forward-send ...................................: (1.57, 1.62)
    backward-recv ..................................: (93.75, 96.79)
    backward-send ..................................: (1.65, 1.70)
    forward-send-backward-recv .....................: (1922.48, 2170.48)
    backward-send-forward-recv .....................: (415.44, 473.57)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.98, 9.44)
    grads-reduce-scatter ...........................: (19.17, 19.56)
    params-all-gather ..............................: (10.61, 10.78)
    optimizer-copy-to-main-grad ....................: (0.26, 0.40)
    optimizer-clip-main-grad .......................: (2.41, 2.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.80, 4.94)
    optimizer-copy-main-to-model-params ............: (1.47, 1.57)
    optimizer ......................................: (9.73, 9.82)
Sun Feb 11 17:52:19 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             554W / 700W |  34908MiB / 81559MiB |     42%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   49C    P0             479W / 700W |  34878MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   50C    P0             525W / 700W |  34996MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   43C    P0             485W / 700W |  35156MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   42C    P0             456W / 700W |  34958MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   50C    P0             498W / 700W |  34938MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   48C    P0             417W / 700W |  34652MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             431W / 700W |  34664MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 17419.4 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.166119E+00 | loss scale: 1.0 | grad norm: 1.415 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (17136.31, 17275.78)
    forward-compute ................................: (5266.88, 7407.75)
    backward-compute ...............................: (8682.62, 9308.74)
    batch-generator ................................: (48.15, 72.18)
    forward-recv ...................................: (77.95, 79.07)
    forward-send ...................................: (1.58, 1.60)
    backward-recv ..................................: (94.33, 97.11)
    backward-send ..................................: (1.64, 1.72)
    forward-send-backward-recv .....................: (2948.54, 3200.45)
    backward-send-forward-recv .....................: (416.85, 472.72)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.96, 9.67)
    grads-reduce-scatter ...........................: (19.08, 19.55)
    params-all-gather ..............................: (10.62, 10.77)
    optimizer-copy-to-main-grad ....................: (0.26, 0.39)
    optimizer-clip-main-grad .......................: (2.28, 2.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.80, 4.92)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (9.52, 9.62)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 15217.9 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.129469E+00 | loss scale: 1.0 | grad norm: 0.731 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15024.37, 15163.32)
    forward-compute ................................: (5002.77, 5541.53)
    backward-compute ...............................: (8680.70, 9335.46)
    batch-generator ................................: (48.58, 73.32)
    forward-recv ...................................: (81.84, 82.78)
    forward-send ...................................: (1.58, 1.62)
    backward-recv ..................................: (94.67, 96.89)
    backward-send ..................................: (1.65, 1.70)
    forward-send-backward-recv .....................: (1096.41, 1355.99)
    backward-send-forward-recv .....................: (156.04, 160.64)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.99, 9.51)
    grads-reduce-scatter ...........................: (19.21, 19.52)
    params-all-gather ..............................: (10.61, 10.77)
    optimizer-copy-to-main-grad ....................: (0.26, 0.41)
    optimizer-clip-main-grad .......................: (1.87, 1.89)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.91)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (9.11, 9.21)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 17076.8 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.091554E+00 | loss scale: 1.0 | grad norm: 0.606 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16882.38, 17021.99)
    forward-compute ................................: (5266.47, 7150.93)
    backward-compute ...............................: (8683.63, 9318.26)
    batch-generator ................................: (48.93, 73.39)
    forward-recv ...................................: (78.01, 78.99)
    forward-send ...................................: (1.58, 1.61)
    backward-recv ..................................: (94.38, 96.66)
    backward-send ..................................: (1.64, 1.77)
    forward-send-backward-recv .....................: (2704.06, 2947.04)
    backward-send-forward-recv .....................: (416.25, 476.42)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.96, 9.64)
    grads-reduce-scatter ...........................: (19.22, 19.56)
    params-all-gather ..............................: (10.60, 10.86)
    optimizer-copy-to-main-grad ....................: (0.26, 0.39)
    optimizer-clip-main-grad .......................: (1.60, 1.62)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.80, 4.92)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (8.84, 8.93)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (451605): No such process
kill: (451611): No such process
kill: (451617): No such process
kill: (451623): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=4, tp=2, pp=2, mbs=1
LOCAL_IP = 10.64.24.51
DP=4, MP=2, PP=2
[2024-02-11 17:59:56,758] torch.distributed.run: [WARNING] 
[2024-02-11 17:59:56,758] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 17:59:56,758] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-11 17:59:56,758] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1714528256
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1714528256
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.311 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.346 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.380 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  7.378 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.027 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.073 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.173 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.377 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (866.15, 921.27)
    train/valid/test-data-iterators-setup ..........: (0.02, 13632.65)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 17780.7 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 9.802897E+00 | loss scale: 1.0 | grad norm: 722.143 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 9] (after 10 iterations) memory (MB) | allocated: 14844.9169921875 | max allocated: 21531.25341796875 | reserved: 22864.0 | max reserved: 22864.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 14844.9169921875 | max allocated: 21531.25341796875 | reserved: 22512.0 | max reserved: 22512.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (17233.38, 17308.87)
    forward-compute ................................: (5804.54, 6507.07)
    backward-compute ...............................: (9461.57, 9997.70)
    batch-generator ................................: (283.24, 330.12)
    forward-recv ...................................: (286.54, 311.39)
    forward-send ...................................: (2.96, 9.05)
    backward-recv ..................................: (47.90, 49.38)
    backward-send ..................................: (0.86, 0.89)
    forward-send-backward-recv .....................: (1683.55, 1938.64)
    backward-send-forward-recv .....................: (464.32, 519.51)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (9.11, 9.59)
    grads-reduce-scatter ...........................: (19.20, 431.34)
    params-all-gather ..............................: (10.65, 10.76)
    optimizer-copy-to-main-grad ....................: (0.28, 0.42)
    optimizer-clip-main-grad .......................: (5.65, 5.79)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.93, 5.26)
    optimizer-copy-main-to-model-params ............: (1.48, 1.65)
    optimizer ......................................: (13.92, 14.06)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 16709.3 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.499468E+00 | loss scale: 1.0 | grad norm: 12.096 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16577.70, 16653.00)
    forward-compute ................................: (5517.78, 6158.13)
    backward-compute ...............................: (9443.32, 10005.09)
    batch-generator ................................: (99.26, 120.74)
    forward-recv ...................................: (40.83, 41.35)
    forward-send ...................................: (0.83, 0.85)
    backward-recv ..................................: (48.18, 49.39)
    backward-send ..................................: (0.86, 1.48)
    forward-send-backward-recv .....................: (1336.31, 1595.28)
    backward-send-forward-recv .....................: (447.80, 457.69)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.99, 9.62)
    grads-reduce-scatter ...........................: (19.22, 19.47)
    params-all-gather ..............................: (10.64, 11.29)
    optimizer-copy-to-main-grad ....................: (0.26, 0.39)
    optimizer-clip-main-grad .......................: (2.40, 2.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.80, 4.91)
    optimizer-copy-main-to-model-params ............: (1.47, 1.55)
    optimizer ......................................: (9.65, 9.73)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 17382.0 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.423058E+00 | loss scale: 1.0 | grad norm: 2.841 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (17251.01, 17326.00)
    forward-compute ................................: (5784.45, 6571.84)
    backward-compute ...............................: (9445.40, 9951.24)
    batch-generator ................................: (96.64, 122.87)
    forward-recv ...................................: (40.75, 41.33)
    forward-send ...................................: (0.83, 0.87)
    backward-recv ..................................: (48.07, 49.79)
    backward-send ..................................: (0.86, 0.89)
    forward-send-backward-recv .....................: (1762.38, 2000.49)
    backward-send-forward-recv .....................: (709.21, 724.50)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (9.01, 9.41)
    grads-reduce-scatter ...........................: (19.07, 19.82)
    params-all-gather ..............................: (10.62, 10.75)
    optimizer-copy-to-main-grad ....................: (0.26, 0.38)
    optimizer-clip-main-grad .......................: (2.40, 2.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.81, 4.90)
    optimizer-copy-main-to-model-params ............: (1.48, 1.66)
    optimizer ......................................: (9.62, 9.83)
Sun Feb 11 18:12:06 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             510W / 700W |  26186MiB / 81559MiB |     67%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   49C    P0             547W / 700W |  26538MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   50C    P0             539W / 700W |  26566MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   43C    P0             473W / 700W |  26290MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   42C    P0             517W / 700W |  26198MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   50C    P0             494W / 700W |  26662MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   48C    P0             461W / 700W |  25970MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             470W / 700W |  26344MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 17472.5 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.148441E+00 | loss scale: 1.0 | grad norm: 1.374 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (17250.34, 17325.63)
    forward-compute ................................: (5507.49, 6852.16)
    backward-compute ...............................: (9430.20, 9957.33)
    batch-generator ................................: (96.56, 120.10)
    forward-recv ...................................: (40.76, 41.18)
    forward-send ...................................: (0.83, 0.87)
    backward-recv ..................................: (48.51, 49.93)
    backward-send ..................................: (0.86, 0.88)
    forward-send-backward-recv .....................: (2045.92, 2291.70)
    backward-send-forward-recv .....................: (446.21, 455.01)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.90, 9.64)
    grads-reduce-scatter ...........................: (19.21, 19.54)
    params-all-gather ..............................: (10.63, 10.76)
    optimizer-copy-to-main-grad ....................: (0.26, 0.39)
    optimizer-clip-main-grad .......................: (2.75, 2.78)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.89)
    optimizer-copy-main-to-model-params ............: (1.47, 1.55)
    optimizer ......................................: (9.98, 10.06)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 17117.5 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.282984E+00 | loss scale: 1.0 | grad norm: 1.900 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16986.43, 17061.56)
    forward-compute ................................: (5510.89, 6843.57)
    backward-compute ...............................: (9435.72, 9967.78)
    batch-generator ................................: (95.97, 117.01)
    forward-recv ...................................: (40.73, 41.17)
    forward-send ...................................: (0.83, 0.85)
    backward-recv ..................................: (48.13, 49.56)
    backward-send ..................................: (0.86, 0.89)
    forward-send-backward-recv .....................: (1770.86, 2019.26)
    backward-send-forward-recv .....................: (186.67, 451.91)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (9.01, 9.67)
    grads-reduce-scatter ...........................: (19.10, 19.54)
    params-all-gather ..............................: (10.62, 10.79)
    optimizer-copy-to-main-grad ....................: (0.26, 0.38)
    optimizer-clip-main-grad .......................: (2.28, 2.31)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.80, 4.90)
    optimizer-copy-main-to-model-params ............: (1.47, 1.55)
    optimizer ......................................: (9.48, 9.56)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 17368.2 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.154924E+00 | loss scale: 1.0 | grad norm: 2.767 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (17237.31, 17312.71)
    forward-compute ................................: (5770.79, 7093.00)
    backward-compute ...............................: (9431.40, 9952.84)
    batch-generator ................................: (93.63, 117.15)
    forward-recv ...................................: (40.65, 41.28)
    forward-send ...................................: (0.83, 0.85)
    backward-recv ..................................: (48.31, 49.92)
    backward-send ..................................: (0.86, 0.89)
    forward-send-backward-recv .....................: (1733.87, 2015.85)
    backward-send-forward-recv .....................: (186.88, 714.95)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.94, 9.54)
    grads-reduce-scatter ...........................: (19.15, 19.52)
    params-all-gather ..............................: (10.62, 10.76)
    optimizer-copy-to-main-grad ....................: (0.26, 0.39)
    optimizer-clip-main-grad .......................: (2.26, 2.28)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.80, 4.90)
    optimizer-copy-main-to-model-params ............: (1.47, 1.55)
    optimizer ......................................: (9.49, 9.56)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 16298.9 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.144557E+00 | loss scale: 1.0 | grad norm: 1.278 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16168.45, 16243.76)
    forward-compute ................................: (5510.98, 6014.09)
    backward-compute ...............................: (9432.19, 9950.30)
    batch-generator ................................: (93.84, 113.91)
    forward-recv ...................................: (40.72, 41.32)
    forward-send ...................................: (0.83, 0.89)
    backward-recv ..................................: (47.99, 49.72)
    backward-send ..................................: (0.86, 0.94)
    forward-send-backward-recv .....................: (920.35, 1206.10)
    backward-send-forward-recv .....................: (186.96, 459.69)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (9.00, 9.38)
    grads-reduce-scatter ...........................: (19.20, 19.51)
    params-all-gather ..............................: (10.63, 10.76)
    optimizer-copy-to-main-grad ....................: (0.26, 0.38)
    optimizer-clip-main-grad .......................: (2.42, 2.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.80, 4.89)
    optimizer-copy-main-to-model-params ............: (1.47, 1.55)
    optimizer ......................................: (9.65, 9.73)
Sun Feb 11 18:23:28 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             526W / 700W |  26186MiB / 81559MiB |     12%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   49C    P0             543W / 700W |  26538MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   50C    P0             551W / 700W |  26566MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   43C    P0             468W / 700W |  26290MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   42C    P0             506W / 700W |  26198MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   50C    P0             478W / 700W |  26662MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   48C    P0             471W / 700W |  25970MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             461W / 700W |  26344MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 17465.6 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.168018E+00 | loss scale: 1.0 | grad norm: 1.861 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (17243.75, 17319.44)
    forward-compute ................................: (5508.61, 6833.66)
    backward-compute ...............................: (9432.85, 9970.35)
    batch-generator ................................: (90.77, 113.20)
    forward-recv ...................................: (40.72, 41.27)
    forward-send ...................................: (0.83, 0.86)
    backward-recv ..................................: (48.03, 49.42)
    backward-send ..................................: (0.86, 0.89)
    forward-send-backward-recv .....................: (2007.12, 2282.87)
    backward-send-forward-recv .....................: (446.32, 458.93)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.87, 9.63)
    grads-reduce-scatter ...........................: (19.16, 19.59)
    params-all-gather ..............................: (10.62, 10.75)
    optimizer-copy-to-main-grad ....................: (0.26, 0.40)
    optimizer-clip-main-grad .......................: (2.27, 2.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.92)
    optimizer-copy-main-to-model-params ............: (1.47, 1.55)
    optimizer ......................................: (9.50, 9.58)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 17897.5 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.131698E+00 | loss scale: 1.0 | grad norm: 1.064 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (17766.99, 17842.28)
    forward-compute ................................: (5774.74, 7361.69)
    backward-compute ...............................: (9438.85, 9960.98)
    batch-generator ................................: (91.60, 116.03)
    forward-recv ...................................: (40.76, 41.73)
    forward-send ...................................: (0.83, 0.85)
    backward-recv ..................................: (47.90, 49.46)
    backward-send ..................................: (0.86, 0.89)
    forward-send-backward-recv .....................: (2252.92, 2533.90)
    backward-send-forward-recv .....................: (447.39, 724.50)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.97, 9.43)
    grads-reduce-scatter ...........................: (19.20, 19.50)
    params-all-gather ..............................: (10.62, 10.76)
    optimizer-copy-to-main-grad ....................: (0.26, 0.38)
    optimizer-clip-main-grad .......................: (2.29, 2.31)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.80, 4.91)
    optimizer-copy-main-to-model-params ............: (1.47, 1.55)
    optimizer ......................................: (9.53, 9.60)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 17625.8 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.092606E+00 | loss scale: 1.0 | grad norm: 0.806 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (17495.48, 17570.76)
    forward-compute ................................: (5519.12, 7087.42)
    backward-compute ...............................: (9450.22, 9954.62)
    batch-generator ................................: (92.88, 117.75)
    forward-recv ...................................: (40.76, 41.38)
    forward-send ...................................: (0.83, 0.85)
    backward-recv ..................................: (48.03, 49.30)
    backward-send ..................................: (0.86, 0.91)
    forward-send-backward-recv .....................: (2255.46, 2506.06)
    backward-send-forward-recv .....................: (445.05, 459.38)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.86, 9.66)
    grads-reduce-scatter ...........................: (19.26, 19.50)
    params-all-gather ..............................: (10.65, 10.76)
    optimizer-copy-to-main-grad ....................: (0.26, 0.39)
    optimizer-clip-main-grad .......................: (1.37, 1.37)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.90)
    optimizer-copy-main-to-model-params ............: (1.47, 1.55)
    optimizer ......................................: (8.60, 8.68)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (453111): No such process
kill: (453117): No such process
kill: (453123): No such process
kill: (453129): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=4, tp=4, pp=1, mbs=4
LOCAL_IP = 10.64.24.51
DP=4, MP=4, PP=1
[2024-02-11 18:31:40,443] torch.distributed.run: [WARNING] 
[2024-02-11 18:31:40,443] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 18:31:40,443] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-11 18:31:40,443] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.265 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.308 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.994 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.043 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (281.54, 302.21)
    train/valid/test-data-iterators-setup ..........: (0.02, 10717.73)
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 17518.9 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.051785E+01 | loss scale: 1.0 | grad norm: 698.889 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (17406.34, 17415.16)
    forward-compute ................................: (7289.79, 7513.97)
    backward-compute ...............................: (9873.69, 10106.47)
    batch-generator ................................: (514.71, 589.22)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (56.28, 56.38)
    params-all-gather ..............................: (29.60, 29.74)
    optimizer-copy-to-main-grad ....................: (0.58, 0.73)
    optimizer-clip-main-grad .......................: (5.40, 5.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.94, 5.11)
    optimizer-copy-main-to-model-params ............: (1.65, 1.76)
    optimizer ......................................: (13.64, 13.76)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 16966.1 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.450797E+00 | loss scale: 1.0 | grad norm: 17.302 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16859.57, 16866.08)
    forward-compute ................................: (6761.01, 6957.43)
    backward-compute ...............................: (9884.46, 10088.07)
    batch-generator ................................: (29.12, 37.62)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.85, 55.97)
    params-all-gather ..............................: (29.61, 29.70)
    optimizer-copy-to-main-grad ....................: (0.55, 0.76)
    optimizer-clip-main-grad .......................: (2.58, 2.60)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.94)
    optimizer-copy-main-to-model-params ............: (1.64, 1.76)
    optimizer ......................................: (10.44, 10.57)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 16698.1 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.438582E+00 | loss scale: 1.0 | grad norm: 3.260 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16591.06, 16598.27)
    forward-compute ................................: (6495.47, 6714.55)
    backward-compute ...............................: (9857.87, 10085.02)
    batch-generator ................................: (29.59, 37.37)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.84, 55.89)
    params-all-gather ..............................: (29.60, 29.93)
    optimizer-copy-to-main-grad ....................: (0.54, 0.76)
    optimizer-clip-main-grad .......................: (2.55, 2.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.96)
    optimizer-copy-main-to-model-params ............: (1.64, 1.76)
    optimizer ......................................: (10.35, 10.46)
Sun Feb 11 18:43:35 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   42C    P0             550W / 700W |  62022MiB / 81559MiB |     85%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   49C    P0             560W / 700W |  61898MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   51C    P0             550W / 700W |  62218MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   44C    P0             534W / 700W |  61938MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   43C    P0             487W / 700W |  62058MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   50C    P0             522W / 700W |  62134MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   48C    P0             485W / 700W |  62678MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   43C    P0             476W / 700W |  61902MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 17021.3 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.154957E+00 | loss scale: 1.0 | grad norm: 1.246 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16825.44, 16833.22)
    forward-compute ................................: (6745.63, 6951.19)
    backward-compute ...............................: (9856.07, 10069.91)
    batch-generator ................................: (30.13, 36.68)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.84, 55.91)
    params-all-gather ..............................: (29.60, 29.69)
    optimizer-copy-to-main-grad ....................: (0.55, 0.71)
    optimizer-clip-main-grad .......................: (2.55, 2.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.91)
    optimizer-copy-main-to-model-params ............: (1.65, 1.76)
    optimizer ......................................: (10.28, 10.41)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 16948.1 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.286676E+00 | loss scale: 1.0 | grad norm: 1.321 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16840.02, 16847.59)
    forward-compute ................................: (6754.99, 6970.76)
    backward-compute ...............................: (9850.94, 10074.98)
    batch-generator ................................: (29.17, 37.09)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.86, 56.48)
    params-all-gather ..............................: (29.59, 29.72)
    optimizer-copy-to-main-grad ....................: (0.54, 0.74)
    optimizer-clip-main-grad .......................: (2.58, 2.60)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.92)
    optimizer-copy-main-to-model-params ............: (1.64, 1.76)
    optimizer ......................................: (10.42, 10.53)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 16697.4 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.157049E+00 | loss scale: 1.0 | grad norm: 2.883 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16589.77, 16596.88)
    forward-compute ................................: (6502.85, 6700.82)
    backward-compute ...............................: (9871.14, 10076.35)
    batch-generator ................................: (28.55, 39.82)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.83, 55.88)
    params-all-gather ..............................: (29.62, 30.73)
    optimizer-copy-to-main-grad ....................: (0.54, 0.76)
    optimizer-clip-main-grad .......................: (2.44, 2.46)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.92)
    optimizer-copy-main-to-model-params ............: (1.64, 1.76)
    optimizer ......................................: (10.22, 10.34)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 16948.1 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.144443E+00 | loss scale: 1.0 | grad norm: 1.013 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16840.59, 16848.64)
    forward-compute ................................: (6755.78, 6972.77)
    backward-compute ...............................: (9849.10, 10074.94)
    batch-generator ................................: (29.39, 41.78)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (55.85, 56.08)
    params-all-gather ..............................: (29.59, 29.72)
    optimizer-copy-to-main-grad ....................: (0.54, 0.78)
    optimizer-clip-main-grad .......................: (2.18, 2.20)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.92)
    optimizer-copy-main-to-model-params ............: (1.65, 1.76)
    optimizer ......................................: (10.00, 10.11)
Sun Feb 11 18:54:48 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   42C    P0             531W / 700W |  62022MiB / 81559MiB |     52%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   49C    P0             540W / 700W |  61898MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   50C    P0             541W / 700W |  62218MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   43C    P0             544W / 700W |  61938MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   43C    P0             511W / 700W |  62058MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   50C    P0             479W / 700W |  62134MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   48C    P0             489W / 700W |  62678MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   43C    P0             464W / 700W |  61902MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 16783.4 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.163098E+00 | loss scale: 1.0 | grad norm: 1.115 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16584.48, 16593.02)
    forward-compute ................................: (6497.61, 6727.25)
    backward-compute ...............................: (9838.49, 10077.68)
    batch-generator ................................: (30.14, 41.31)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.84, 55.87)
    params-all-gather ..............................: (29.60, 30.29)
    optimizer-copy-to-main-grad ....................: (0.54, 0.81)
    optimizer-clip-main-grad .......................: (2.19, 2.21)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.91)
    optimizer-copy-main-to-model-params ............: (1.64, 1.76)
    optimizer ......................................: (10.13, 10.25)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 17208.5 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.126190E+00 | loss scale: 1.0 | grad norm: 1.651 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (17101.30, 17108.80)
    forward-compute ................................: (7018.12, 7229.66)
    backward-compute ...............................: (9852.84, 10072.75)
    batch-generator ................................: (30.03, 42.42)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.83, 55.89)
    params-all-gather ..............................: (29.60, 29.71)
    optimizer-copy-to-main-grad ....................: (0.54, 0.77)
    optimizer-clip-main-grad .......................: (2.20, 2.35)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.92)
    optimizer-copy-main-to-model-params ............: (1.64, 1.76)
    optimizer ......................................: (10.21, 10.33)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 16687.9 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.089822E+00 | loss scale: 1.0 | grad norm: 0.662 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16579.66, 16588.62)
    forward-compute ................................: (6494.07, 6723.45)
    backward-compute ...............................: (9837.62, 10076.38)
    batch-generator ................................: (29.70, 40.06)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.84, 55.88)
    params-all-gather ..............................: (29.62, 29.74)
    optimizer-copy-to-main-grad ....................: (0.55, 0.78)
    optimizer-clip-main-grad .......................: (2.06, 2.08)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.90)
    optimizer-copy-main-to-model-params ............: (1.64, 1.76)
    optimizer ......................................: (9.92, 10.04)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (454177): No such process
kill: (454183): No such process
kill: (454189): No such process
kill: (454195): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=4, tp=4, pp=1, mbs=2
LOCAL_IP = 10.64.24.51
DP=4, MP=4, PP=1
[2024-02-11 19:02:44,135] torch.distributed.run: [WARNING] 
[2024-02-11 19:02:44,135] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 19:02:44,135] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-11 19:02:44,135] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.121 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.490 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.124 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.087 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (283.12, 331.52)
    train/valid/test-data-iterators-setup ..........: (0.02, 11016.11)
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 19049.1 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.051972E+01 | loss scale: 1.0 | grad norm: 699.055 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (18941.80, 18945.43)
    forward-compute ................................: (7927.28, 8110.97)
    backward-compute ...............................: (10793.88, 10981.08)
    batch-generator ................................: (454.88, 480.62)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (56.22, 56.32)
    params-all-gather ..............................: (29.63, 29.97)
    optimizer-copy-to-main-grad ....................: (0.57, 0.81)
    optimizer-clip-main-grad .......................: (5.44, 5.47)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.94, 5.09)
    optimizer-copy-main-to-model-params ............: (1.65, 1.76)
    optimizer ......................................: (13.49, 13.61)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 18345.9 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.450945E+00 | loss scale: 1.0 | grad norm: 17.188 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (18241.71, 18245.95)
    forward-compute ................................: (7242.61, 7429.42)
    backward-compute ...............................: (10775.78, 10967.16)
    batch-generator ................................: (58.35, 93.76)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.84, 55.88)
    params-all-gather ..............................: (29.61, 29.72)
    optimizer-copy-to-main-grad ....................: (0.56, 0.86)
    optimizer-clip-main-grad .......................: (2.53, 2.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.89)
    optimizer-copy-main-to-model-params ............: (1.65, 1.75)
    optimizer ......................................: (10.38, 10.48)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 18336.9 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.437721E+00 | loss scale: 1.0 | grad norm: 3.260 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (18233.46, 18236.83)
    forward-compute ................................: (7248.22, 7426.69)
    backward-compute ...............................: (10770.46, 10952.74)
    batch-generator ................................: (56.18, 92.64)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.82, 55.88)
    params-all-gather ..............................: (29.62, 29.71)
    optimizer-copy-to-main-grad ....................: (0.55, 0.82)
    optimizer-clip-main-grad .......................: (2.71, 2.73)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.87)
    optimizer-copy-main-to-model-params ............: (1.65, 1.75)
    optimizer ......................................: (10.51, 10.62)
Sun Feb 11 19:15:37 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             500W / 700W |  39358MiB / 81559MiB |     30%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   49C    P0             567W / 700W |  39636MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   50C    P0             527W / 700W |  39732MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   43C    P0             535W / 700W |  39492MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   42C    P0             446W / 700W |  39214MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             520W / 700W |  39326MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             492W / 700W |  39262MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             473W / 700W |  39096MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 18415.6 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.154757E+00 | loss scale: 1.0 | grad norm: 1.255 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (18221.97, 18226.54)
    forward-compute ................................: (7241.13, 7419.01)
    backward-compute ...............................: (10766.68, 10949.64)
    batch-generator ................................: (58.26, 92.89)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.84, 55.90)
    params-all-gather ..............................: (29.61, 29.70)
    optimizer-copy-to-main-grad ....................: (0.55, 0.79)
    optimizer-clip-main-grad .......................: (2.53, 2.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.86)
    optimizer-copy-main-to-model-params ............: (1.65, 1.75)
    optimizer ......................................: (10.29, 10.40)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 18330.3 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.287709E+00 | loss scale: 1.0 | grad norm: 3.130 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (18225.33, 18229.61)
    forward-compute ................................: (7242.40, 7425.32)
    backward-compute ...............................: (10763.63, 10951.67)
    batch-generator ................................: (58.75, 93.54)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.84, 55.90)
    params-all-gather ..............................: (29.61, 30.52)
    optimizer-copy-to-main-grad ....................: (0.55, 0.82)
    optimizer-clip-main-grad .......................: (2.53, 2.56)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.87)
    optimizer-copy-main-to-model-params ............: (1.65, 1.75)
    optimizer ......................................: (10.31, 10.41)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 18583.3 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.154222E+00 | loss scale: 1.0 | grad norm: 2.200 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (18479.47, 18483.09)
    forward-compute ................................: (7499.92, 7671.03)
    backward-compute ...............................: (10772.19, 10948.27)
    batch-generator ................................: (56.81, 93.39)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.84, 56.60)
    params-all-gather ..............................: (29.62, 29.73)
    optimizer-copy-to-main-grad ....................: (0.57, 0.79)
    optimizer-clip-main-grad .......................: (2.14, 2.16)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.85)
    optimizer-copy-main-to-model-params ............: (1.65, 1.75)
    optimizer ......................................: (9.95, 10.05)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 18337.3 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.143139E+00 | loss scale: 1.0 | grad norm: 1.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (18233.48, 18237.32)
    forward-compute ................................: (7243.54, 7430.55)
    backward-compute ...............................: (10766.65, 10958.87)
    batch-generator ................................: (59.31, 94.81)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.85, 56.04)
    params-all-gather ..............................: (29.61, 29.72)
    optimizer-copy-to-main-grad ....................: (0.56, 0.79)
    optimizer-clip-main-grad .......................: (2.28, 2.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.86)
    optimizer-copy-main-to-model-params ............: (1.65, 1.75)
    optimizer ......................................: (10.16, 10.26)
Sun Feb 11 19:27:54 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   42C    P0             586W / 700W |  39358MiB / 81559MiB |     45%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   49C    P0             539W / 700W |  39636MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   51C    P0             511W / 700W |  39732MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   43C    P0             530W / 700W |  39492MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   43C    P0             522W / 700W |  39214MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   50C    P0             462W / 700W |  39326MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   48C    P0             465W / 700W |  39262MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   43C    P0             496W / 700W |  39096MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 18434.3 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.162460E+00 | loss scale: 1.0 | grad norm: 1.090 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (18240.91, 18245.25)
    forward-compute ................................: (7251.19, 7434.60)
    backward-compute ...............................: (10770.14, 10959.04)
    batch-generator ................................: (58.68, 92.87)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.84, 55.89)
    params-all-gather ..............................: (29.60, 29.69)
    optimizer-copy-to-main-grad ....................: (0.55, 0.79)
    optimizer-clip-main-grad .......................: (2.40, 2.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.87)
    optimizer-copy-main-to-model-params ............: (1.65, 1.75)
    optimizer ......................................: (10.17, 10.28)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 18335.2 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.125625E+00 | loss scale: 1.0 | grad norm: 1.085 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (18231.53, 18235.57)
    forward-compute ................................: (7242.79, 7433.08)
    backward-compute ...............................: (10762.24, 10957.83)
    batch-generator ................................: (59.00, 93.40)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.84, 55.86)
    params-all-gather ..............................: (29.61, 29.70)
    optimizer-copy-to-main-grad ....................: (0.54, 0.80)
    optimizer-clip-main-grad .......................: (2.14, 2.16)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.86)
    optimizer-copy-main-to-model-params ............: (1.65, 1.75)
    optimizer ......................................: (9.89, 10.00)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 18318.2 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.089044E+00 | loss scale: 1.0 | grad norm: 1.803 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (18215.29, 18219.18)
    forward-compute ................................: (7232.61, 7410.19)
    backward-compute ...............................: (10768.87, 10952.06)
    batch-generator ................................: (58.74, 92.68)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.84, 55.86)
    params-all-gather ..............................: (29.59, 29.70)
    optimizer-copy-to-main-grad ....................: (0.55, 0.78)
    optimizer-clip-main-grad .......................: (1.76, 1.81)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.86)
    optimizer-copy-main-to-model-params ............: (1.65, 1.75)
    optimizer ......................................: (9.54, 9.64)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (455243): No such process
kill: (455249): No such process
kill: (455255): No such process
kill: (455261): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=4, tp=4, pp=1, mbs=1
LOCAL_IP = 10.64.24.51
DP=4, MP=4, PP=1
[2024-02-11 19:36:17,938] torch.distributed.run: [WARNING] 
[2024-02-11 19:36:17,938] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 19:36:17,938] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-11 19:36:17,938] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.293 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.540 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.085 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  4.980 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (282.63, 336.75)
    train/valid/test-data-iterators-setup ..........: (0.02, 10936.44)
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 21107.0 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.051896E+01 | loss scale: 1.0 | grad norm: 698.929 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (21001.79, 21003.19)
    forward-compute ................................: (8767.40, 8889.57)
    backward-compute ...............................: (12040.06, 12167.84)
    batch-generator ................................: (510.51, 550.09)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (56.26, 56.29)
    params-all-gather ..............................: (29.61, 29.73)
    optimizer-copy-to-main-grad ....................: (0.59, 0.74)
    optimizer-clip-main-grad .......................: (5.44, 5.46)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.95, 5.08)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (13.45, 13.56)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 20410.8 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.450871E+00 | loss scale: 1.0 | grad norm: 17.118 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (20309.55, 20310.55)
    forward-compute ................................: (8089.05, 8220.50)
    backward-compute ...............................: (12019.29, 12155.59)
    batch-generator ................................: (106.07, 143.27)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.84, 55.87)
    params-all-gather ..............................: (29.62, 29.70)
    optimizer-copy-to-main-grad ....................: (0.57, 0.72)
    optimizer-clip-main-grad .......................: (2.53, 2.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.80, 4.87)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (10.31, 10.43)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 20647.5 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.437582E+00 | loss scale: 1.0 | grad norm: 3.261 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (20546.22, 20547.26)
    forward-compute ................................: (8344.80, 8461.64)
    backward-compute ...............................: (12014.02, 12136.71)
    batch-generator ................................: (108.78, 139.92)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.84, 55.88)
    params-all-gather ..............................: (29.59, 29.71)
    optimizer-copy-to-main-grad ....................: (0.56, 0.72)
    optimizer-clip-main-grad .......................: (2.52, 2.54)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.85)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (10.24, 10.35)
Sun Feb 11 19:50:36 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   40C    P0             502W / 700W |  28036MiB / 81559MiB |     72%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   48C    P0             513W / 700W |  28108MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   49C    P0             509W / 700W |  28140MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0             497W / 700W |  27888MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   41C    P0             483W / 700W |  27780MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   48C    P0             479W / 700W |  27968MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   46C    P0             487W / 700W |  28086MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   41C    P0             457W / 700W |  27838MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 20476.5 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.154914E+00 | loss scale: 1.0 | grad norm: 1.284 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (20283.33, 20284.42)
    forward-compute ................................: (8088.07, 8207.05)
    backward-compute ...............................: (12006.71, 12130.86)
    batch-generator ................................: (111.53, 141.90)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.83, 56.89)
    params-all-gather ..............................: (29.61, 29.69)
    optimizer-copy-to-main-grad ....................: (0.57, 0.71)
    optimizer-clip-main-grad .......................: (2.53, 2.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.80, 4.86)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (10.26, 10.37)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 20910.9 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.287692E+00 | loss scale: 1.0 | grad norm: 3.168 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (20809.42, 20810.39)
    forward-compute ................................: (8607.77, 8736.78)
    backward-compute ...............................: (12004.46, 12137.12)
    batch-generator ................................: (109.11, 148.29)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.84, 55.87)
    params-all-gather ..............................: (29.58, 29.70)
    optimizer-copy-to-main-grad ....................: (0.56, 0.86)
    optimizer-clip-main-grad .......................: (2.53, 2.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.80, 4.84)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (10.45, 10.55)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 20916.2 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.155953E+00 | loss scale: 1.0 | grad norm: 2.474 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (20815.14, 20816.21)
    forward-compute ................................: (8610.47, 8742.08)
    backward-compute ...............................: (12005.07, 12140.89)
    batch-generator ................................: (109.92, 147.11)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.83, 55.86)
    params-all-gather ..............................: (29.61, 29.70)
    optimizer-copy-to-main-grad ....................: (0.56, 0.72)
    optimizer-clip-main-grad .......................: (2.26, 2.28)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.85)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (10.02, 10.13)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 21459.9 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.143082E+00 | loss scale: 1.0 | grad norm: 1.051 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (21358.24, 21359.55)
    forward-compute ................................: (9152.49, 9272.64)
    backward-compute ...............................: (12017.07, 12141.49)
    batch-generator ................................: (112.59, 147.78)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.83, 56.06)
    params-all-gather ..............................: (29.60, 29.85)
    optimizer-copy-to-main-grad ....................: (0.57, 0.73)
    optimizer-clip-main-grad .......................: (2.39, 2.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.85)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (10.11, 10.23)
Sun Feb 11 20:04:39 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   40C    P0             548W / 700W |  28036MiB / 81559MiB |     51%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   48C    P0             515W / 700W |  28108MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   49C    P0             510W / 700W |  28140MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0             526W / 700W |  27888MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   41C    P0             514W / 700W |  27780MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   48C    P0             454W / 700W |  28166MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   46C    P0             446W / 700W |  28086MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   41C    P0             483W / 700W |  27838MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 21011.1 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.163433E+00 | loss scale: 1.0 | grad norm: 1.508 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (20818.14, 20819.10)
    forward-compute ................................: (8612.10, 8728.00)
    backward-compute ...............................: (12020.56, 12141.29)
    batch-generator ................................: (109.77, 147.05)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.83, 56.58)
    params-all-gather ..............................: (29.61, 29.75)
    optimizer-copy-to-main-grad ....................: (0.57, 0.71)
    optimizer-clip-main-grad .......................: (2.39, 2.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.85)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (10.09, 10.21)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 20924.7 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.125762E+00 | loss scale: 1.0 | grad norm: 0.966 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (20822.93, 20824.13)
    forward-compute ................................: (8615.61, 8733.80)
    backward-compute ...............................: (12019.78, 12143.50)
    batch-generator ................................: (110.89, 145.15)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.84, 55.88)
    params-all-gather ..............................: (29.61, 29.73)
    optimizer-copy-to-main-grad ....................: (0.58, 0.71)
    optimizer-clip-main-grad .......................: (2.01, 2.02)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.80, 4.85)
    optimizer-copy-main-to-model-params ............: (1.64, 1.76)
    optimizer ......................................: (9.74, 9.88)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 21452.1 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.089245E+00 | loss scale: 1.0 | grad norm: 0.759 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (21351.39, 21352.43)
    forward-compute ................................: (9143.56, 9263.06)
    backward-compute ...............................: (12018.89, 12144.01)
    batch-generator ................................: (109.27, 146.69)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.85, 55.97)
    params-all-gather ..............................: (29.61, 29.70)
    optimizer-copy-to-main-grad ....................: (0.57, 0.69)
    optimizer-clip-main-grad .......................: (2.01, 2.02)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.80, 4.84)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (9.70, 9.81)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (456309): No such process
kill: (456315): No such process
kill: (456321): No such process
kill: (456327): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=4, tp=1, pp=4, mbs=2
LOCAL_IP = 10.64.24.51
DP=4, MP=1, PP=4
[2024-02-11 20:13:56,995] torch.distributed.run: [WARNING] 
[2024-02-11 20:13:56,995] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 20:13:56,995] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-11 20:13:56,995] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1611038720
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1817092096
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...

> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...


 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.670 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.723 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.723 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.754 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.755 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.755 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.759 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.832 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.110 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.165 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.080 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.125 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.235 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.253 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.267 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.301 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (46.11, 581.91)
    train/valid/test-data-iterators-setup ..........: (10136.06, 11588.78)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 15183.6 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 9.096269E+00 | loss scale: 1.0 | grad norm: 689.840 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 14020.033203125 | max allocated: 33014.048828125 | reserved: 34028.0 | max reserved: 34028.0[Rank 12] (after 10 iterations) memory (MB) | allocated: 15789.5732421875 | max allocated: 29073.2529296875 | reserved: 30414.0 | max reserved: 30414.0

(min, max) time across ranks (ms):
    forward-backward ...............................: (14347.05, 14702.05)
    forward-compute ................................: (3998.96, 5342.31)
    backward-compute ...............................: (7361.56, 8639.86)
    batch-generator ................................: (63.35, 89.85)
    forward-recv ...................................: (128.69, 365.70)
    forward-send ...................................: (3.68, 95.13)
    backward-recv ..................................: (94.46, 282.05)
    backward-send ..................................: (1.48, 4.43)
    forward-send-backward-recv .....................: (2356.88, 2656.39)
    backward-send-forward-recv .....................: (142.71, 179.75)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.28)
    grads-reduce-scatter ...........................: (20.46, 429.69)
    params-all-gather ..............................: (9.73, 11.04)
    optimizer-copy-to-main-grad ....................: (0.16, 0.24)
    optimizer-clip-main-grad .......................: (5.60, 5.77)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.61, 5.29)
    optimizer-copy-main-to-model-params ............: (1.32, 1.53)
    optimizer ......................................: (13.41, 13.62)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 14284.5 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.494057E+00 | loss scale: 1.0 | grad norm: 11.047 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13862.65, 14216.80)
    forward-compute ................................: (3948.24, 5039.74)
    backward-compute ...............................: (7354.66, 8643.53)
    batch-generator ................................: (48.18, 61.43)
    forward-recv ...................................: (65.30, 184.68)
    forward-send ...................................: (1.42, 4.83)
    backward-recv ..................................: (93.68, 282.51)
    backward-send ..................................: (1.50, 4.41)
    forward-send-backward-recv .....................: (2089.58, 2376.26)
    backward-send-forward-recv .....................: (128.60, 161.61)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.25)
    grads-reduce-scatter ...........................: (18.13, 20.94)
    params-all-gather ..............................: (9.72, 11.48)
    optimizer-copy-to-main-grad ....................: (0.13, 0.24)
    optimizer-clip-main-grad .......................: (2.28, 2.46)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.17)
    optimizer-copy-main-to-model-params ............: (1.32, 1.53)
    optimizer ......................................: (9.64, 9.85)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 14266.7 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.421039E+00 | loss scale: 1.0 | grad norm: 2.929 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13846.08, 14199.16)
    forward-compute ................................: (3943.69, 5017.37)
    backward-compute ...............................: (7351.20, 8631.85)
    batch-generator ................................: (48.26, 55.73)
    forward-recv ...................................: (65.67, 184.62)
    forward-send ...................................: (1.42, 5.20)
    backward-recv ..................................: (93.93, 281.08)
    backward-send ..................................: (1.50, 4.43)
    forward-send-backward-recv .....................: (2082.84, 2369.97)
    backward-send-forward-recv .....................: (128.30, 162.90)
    layernorm-grads-all-reduce .....................: (0.02, 0.06)
    embedding-grads-all-reduce .....................: (0.02, 17.99)
    grads-reduce-scatter ...........................: (18.16, 20.85)
    params-all-gather ..............................: (9.72, 11.05)
    optimizer-copy-to-main-grad ....................: (0.14, 0.23)
    optimizer-clip-main-grad .......................: (2.27, 2.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.15)
    optimizer-copy-main-to-model-params ............: (1.32, 1.53)
    optimizer ......................................: (9.57, 10.05)
Sun Feb 11 20:24:11 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   40C    P0             338W / 700W |  37128MiB / 81559MiB |     57%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   47C    P0             425W / 700W |  37176MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   48C    P0             393W / 700W |  37176MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0             381W / 700W |  36936MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   41C    P0             377W / 700W |  33600MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             342W / 700W |  33660MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             301W / 700W |  33650MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   41C    P0             300W / 700W |  33400MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 14348.6 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.151264E+00 | loss scale: 1.0 | grad norm: 1.508 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13839.53, 14191.97)
    forward-compute ................................: (3942.72, 4987.82)
    backward-compute ...............................: (7341.14, 8621.32)
    batch-generator ................................: (48.62, 55.37)
    forward-recv ...................................: (65.67, 184.68)
    forward-send ...................................: (1.40, 4.64)
    backward-recv ..................................: (94.74, 280.28)
    backward-send ..................................: (1.51, 4.40)
    forward-send-backward-recv .....................: (2087.02, 2372.37)
    backward-send-forward-recv .....................: (128.27, 162.11)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.15)
    grads-reduce-scatter ...........................: (18.16, 20.83)
    params-all-gather ..............................: (9.73, 11.04)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (2.26, 2.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.14)
    optimizer-copy-main-to-model-params ............: (1.32, 1.53)
    optimizer ......................................: (9.52, 9.74)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 14254.9 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.285612E+00 | loss scale: 1.0 | grad norm: 1.071 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13835.18, 14188.69)
    forward-compute ................................: (3942.73, 5038.03)
    backward-compute ...............................: (7341.82, 8623.69)
    batch-generator ................................: (48.75, 55.97)
    forward-recv ...................................: (65.55, 184.43)
    forward-send ...................................: (1.41, 4.50)
    backward-recv ..................................: (93.98, 283.02)
    backward-send ..................................: (1.49, 4.39)
    forward-send-backward-recv .....................: (2078.96, 2370.00)
    backward-send-forward-recv .....................: (127.96, 162.63)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.05)
    grads-reduce-scatter ...........................: (18.11, 20.77)
    params-all-gather ..............................: (9.73, 11.03)
    optimizer-copy-to-main-grad ....................: (0.13, 0.22)
    optimizer-clip-main-grad .......................: (2.01, 2.15)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.14)
    optimizer-copy-main-to-model-params ............: (1.32, 1.53)
    optimizer ......................................: (9.22, 9.44)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 14253.5 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.155784E+00 | loss scale: 1.0 | grad norm: 1.958 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13833.83, 14187.00)
    forward-compute ................................: (3941.10, 5009.55)
    backward-compute ...............................: (7341.76, 8617.47)
    batch-generator ................................: (48.25, 56.55)
    forward-recv ...................................: (66.01, 184.51)
    forward-send ...................................: (1.42, 4.95)
    backward-recv ..................................: (93.05, 281.05)
    backward-send ..................................: (1.50, 4.41)
    forward-send-backward-recv .....................: (2073.96, 2370.52)
    backward-send-forward-recv .....................: (128.14, 161.96)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.16)
    grads-reduce-scatter ...........................: (18.10, 20.75)
    params-all-gather ..............................: (9.73, 11.06)
    optimizer-copy-to-main-grad ....................: (0.13, 0.22)
    optimizer-clip-main-grad .......................: (2.27, 2.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.13)
    optimizer-copy-main-to-model-params ............: (1.32, 1.53)
    optimizer ......................................: (9.51, 9.73)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 14248.0 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.142288E+00 | loss scale: 1.0 | grad norm: 1.082 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13828.27, 14181.67)
    forward-compute ................................: (3941.93, 5013.60)
    backward-compute ...............................: (7346.49, 8617.01)
    batch-generator ................................: (48.22, 55.35)
    forward-recv ...................................: (66.13, 184.63)
    forward-send ...................................: (1.41, 4.80)
    backward-recv ..................................: (93.95, 281.35)
    backward-send ..................................: (1.50, 4.38)
    forward-send-backward-recv .....................: (2061.95, 2358.75)
    backward-send-forward-recv .....................: (128.17, 161.09)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.00)
    grads-reduce-scatter ...........................: (18.17, 21.51)
    params-all-gather ..............................: (9.73, 11.04)
    optimizer-copy-to-main-grad ....................: (0.13, 0.22)
    optimizer-clip-main-grad .......................: (1.88, 2.16)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.22)
    optimizer-copy-main-to-model-params ............: (1.32, 1.54)
    optimizer ......................................: (9.35, 9.58)
Sun Feb 11 20:33:41 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   39C    P0             428W / 700W |  37128MiB / 81559MiB |     86%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   47C    P0             338W / 700W |  37176MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   48C    P0             419W / 700W |  37176MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0             332W / 700W |  36936MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   41C    P0             386W / 700W |  33600MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             412W / 700W |  33660MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             346W / 700W |  33650MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   41C    P0             297W / 700W |  33400MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 14332.8 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.160803E+00 | loss scale: 1.0 | grad norm: 0.543 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13822.39, 14175.45)
    forward-compute ................................: (3940.96, 5004.19)
    backward-compute ...............................: (7343.42, 8609.10)
    batch-generator ................................: (48.52, 54.51)
    forward-recv ...................................: (66.09, 184.44)
    forward-send ...................................: (1.41, 4.77)
    backward-recv ..................................: (93.46, 280.83)
    backward-send ..................................: (1.49, 4.40)
    forward-send-backward-recv .....................: (2058.81, 2353.68)
    backward-send-forward-recv .....................: (128.09, 160.88)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.12)
    grads-reduce-scatter ...........................: (18.01, 20.80)
    params-all-gather ..............................: (9.72, 11.03)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (1.26, 1.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.14)
    optimizer-copy-main-to-model-params ............: (1.32, 1.53)
    optimizer ......................................: (8.38, 8.59)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 14577.6 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.123370E+00 | loss scale: 1.0 | grad norm: 1.728 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14158.80, 14511.76)
    forward-compute ................................: (3937.98, 5106.84)
    backward-compute ...............................: (7338.92, 8605.69)
    batch-generator ................................: (48.85, 54.91)
    forward-recv ...................................: (183.77, 487.35)
    forward-send ...................................: (1.39, 4.64)
    backward-recv ..................................: (93.02, 282.84)
    backward-send ..................................: (1.48, 4.39)
    forward-send-backward-recv .....................: (2079.30, 2405.66)
    backward-send-forward-recv .....................: (128.02, 483.43)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.04)
    grads-reduce-scatter ...........................: (18.17, 20.84)
    params-all-gather ..............................: (9.72, 11.02)
    optimizer-copy-to-main-grad ....................: (0.13, 0.23)
    optimizer-clip-main-grad .......................: (1.64, 1.72)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.13)
    optimizer-copy-main-to-model-params ............: (1.32, 1.53)
    optimizer ......................................: (8.80, 9.01)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 14235.0 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.092736E+00 | loss scale: 1.0 | grad norm: 1.138 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13815.13, 14168.14)
    forward-compute ................................: (3938.46, 5017.14)
    backward-compute ...............................: (7327.63, 8605.57)
    batch-generator ................................: (48.77, 54.82)
    forward-recv ...................................: (65.94, 184.48)
    forward-send ...................................: (1.41, 4.83)
    backward-recv ..................................: (93.48, 280.23)
    backward-send ..................................: (1.50, 4.49)
    forward-send-backward-recv .....................: (2056.45, 2369.12)
    backward-send-forward-recv .....................: (128.23, 162.36)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.17)
    grads-reduce-scatter ...........................: (18.15, 20.80)
    params-all-gather ..............................: (9.73, 11.03)
    optimizer-copy-to-main-grad ....................: (0.13, 0.22)
    optimizer-clip-main-grad .......................: (2.27, 2.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.14)
    optimizer-copy-main-to-model-params ............: (1.32, 1.53)
    optimizer ......................................: (9.52, 9.73)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (458587): No such process
kill: (458593): No such process
kill: (458599): No such process
kill: (458605): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=4, tp=1, pp=4, mbs=1
LOCAL_IP = 10.64.24.51
DP=4, MP=1, PP=4
[2024-02-11 20:40:45,386] torch.distributed.run: [WARNING] 
[2024-02-11 20:40:45,386] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 20:40:45,386] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-11 20:40:45,386] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1611038720
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1817092096
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...


> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.837 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.836 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.858 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.862 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.862 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.913 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.893 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  6.135 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.274 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.274 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.214 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.277 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.287 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.341 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.367 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.277 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (35.36, 572.66)
    train/valid/test-data-iterators-setup ..........: (10175.08, 12869.21)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 15642.1 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 9.096127E+00 | loss scale: 1.0 | grad norm: 689.659 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 13956.033203125 | max allocated: 23005.048828125 | reserved: 23586.0 | max reserved: 23586.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 15725.5732421875 | max allocated: 22111.40185546875 | reserved: 22980.0 | max reserved: 22980.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (14982.84, 15169.16)
    forward-compute ................................: (4130.37, 5569.38)
    backward-compute ...............................: (7740.14, 9079.78)
    batch-generator ................................: (111.33, 136.54)
    forward-recv ...................................: (97.59, 274.31)
    forward-send ...................................: (3.90, 107.83)
    backward-recv ..................................: (48.25, 144.89)
    backward-send ..................................: (0.74, 2.49)
    forward-send-backward-recv .....................: (2297.89, 2762.49)
    backward-send-forward-recv .....................: (152.41, 214.46)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.14)
    grads-reduce-scatter ...........................: (20.39, 420.09)
    params-all-gather ..............................: (9.73, 11.08)
    optimizer-copy-to-main-grad ....................: (0.16, 0.27)
    optimizer-clip-main-grad .......................: (6.22, 6.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.63, 5.30)
    optimizer-copy-main-to-model-params ............: (1.32, 1.54)
    optimizer ......................................: (14.12, 14.34)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 14715.6 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.494105E+00 | loss scale: 1.0 | grad norm: 11.139 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14462.25, 14649.06)
    forward-compute ................................: (4075.90, 5279.13)
    backward-compute ...............................: (7731.11, 9085.17)
    batch-generator ................................: (90.07, 104.66)
    forward-recv ...................................: (33.40, 94.97)
    forward-send ...................................: (0.82, 2.56)
    backward-recv ..................................: (49.08, 145.09)
    backward-send ..................................: (0.73, 2.51)
    forward-send-backward-recv .....................: (1989.93, 2462.48)
    backward-send-forward-recv .....................: (137.22, 181.40)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.28)
    grads-reduce-scatter ...........................: (18.12, 20.84)
    params-all-gather ..............................: (9.71, 11.04)
    optimizer-copy-to-main-grad ....................: (0.14, 0.23)
    optimizer-clip-main-grad .......................: (2.26, 2.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.14)
    optimizer-copy-main-to-model-params ............: (1.31, 1.53)
    optimizer ......................................: (9.53, 9.74)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 14710.5 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.421919E+00 | loss scale: 1.0 | grad norm: 2.951 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14456.83, 14643.72)
    forward-compute ................................: (4065.08, 5264.43)
    backward-compute ...............................: (7729.72, 9080.48)
    batch-generator ................................: (89.80, 109.79)
    forward-recv ...................................: (33.14, 94.96)
    forward-send ...................................: (0.82, 2.31)
    backward-recv ..................................: (48.64, 144.56)
    backward-send ..................................: (0.72, 2.49)
    forward-send-backward-recv .....................: (1967.50, 2473.19)
    backward-send-forward-recv .....................: (137.38, 180.14)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.02)
    grads-reduce-scatter ...........................: (18.09, 20.75)
    params-all-gather ..............................: (9.71, 11.14)
    optimizer-copy-to-main-grad ....................: (0.14, 0.23)
    optimizer-clip-main-grad .......................: (2.28, 2.46)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.17)
    optimizer-copy-main-to-model-params ............: (1.31, 1.53)
    optimizer ......................................: (9.66, 9.87)
Sun Feb 11 20:51:18 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   40C    P0             482W / 700W |  26686MiB / 81559MiB |     57%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   48C    P0             467W / 700W |  26734MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   49C    P0             513W / 700W |  26734MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0             344W / 700W |  26494MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   42C    P0             421W / 700W |  26166MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   50C    P0             397W / 700W |  26192MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   48C    P0             435W / 700W |  26198MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             434W / 700W |  25948MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 14787.5 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.151304E+00 | loss scale: 1.0 | grad norm: 1.488 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14442.62, 14629.06)
    forward-compute ................................: (4060.54, 5241.70)
    backward-compute ...............................: (7744.50, 9075.58)
    batch-generator ................................: (88.26, 119.74)
    forward-recv ...................................: (33.06, 94.92)
    forward-send ...................................: (0.82, 2.56)
    backward-recv ..................................: (48.15, 145.48)
    backward-send ..................................: (0.73, 2.50)
    forward-send-backward-recv .....................: (1961.08, 2452.06)
    backward-send-forward-recv .....................: (137.00, 179.89)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.07)
    grads-reduce-scatter ...........................: (18.07, 20.79)
    params-all-gather ..............................: (9.72, 11.47)
    optimizer-copy-to-main-grad ....................: (0.14, 0.25)
    optimizer-clip-main-grad .......................: (2.27, 2.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.17)
    optimizer-copy-main-to-model-params ............: (1.31, 1.53)
    optimizer ......................................: (9.57, 9.78)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 14691.0 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.285540E+00 | loss scale: 1.0 | grad norm: 1.588 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14436.69, 14623.80)
    forward-compute ................................: (4059.00, 5244.52)
    backward-compute ...............................: (7737.42, 9070.71)
    batch-generator ................................: (88.98, 108.04)
    forward-recv ...................................: (33.16, 94.70)
    forward-send ...................................: (0.82, 2.51)
    backward-recv ..................................: (48.85, 144.98)
    backward-send ..................................: (0.73, 2.48)
    forward-send-backward-recv .....................: (1964.52, 2453.74)
    backward-send-forward-recv .....................: (137.18, 180.06)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.06)
    grads-reduce-scatter ...........................: (18.14, 20.75)
    params-all-gather ..............................: (9.71, 11.03)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (2.02, 2.16)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.17)
    optimizer-copy-main-to-model-params ............: (1.32, 1.53)
    optimizer ......................................: (9.77, 9.98)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 15299.4 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.154767E+00 | loss scale: 1.0 | grad norm: 0.908 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15045.25, 15231.62)
    forward-compute ................................: (4060.02, 5839.21)
    backward-compute ...............................: (7736.67, 9079.71)
    batch-generator ................................: (88.06, 100.48)
    forward-recv ...................................: (33.20, 94.92)
    forward-send ...................................: (0.82, 2.42)
    backward-recv ..................................: (49.19, 144.93)
    backward-send ..................................: (0.73, 2.48)
    forward-send-backward-recv .....................: (2221.63, 2956.25)
    backward-send-forward-recv .....................: (137.45, 532.72)
    layernorm-grads-all-reduce .....................: (0.01, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.15)
    grads-reduce-scatter ...........................: (18.04, 20.91)
    params-all-gather ..............................: (9.72, 11.04)
    optimizer-copy-to-main-grad ....................: (0.14, 0.23)
    optimizer-clip-main-grad .......................: (2.01, 2.16)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.71)
    optimizer-copy-main-to-model-params ............: (1.32, 1.53)
    optimizer ......................................: (10.00, 10.21)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 15018.6 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.140610E+00 | loss scale: 1.0 | grad norm: 0.776 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14766.24, 14952.66)
    forward-compute ................................: (4058.37, 5565.46)
    backward-compute ...............................: (7739.08, 9073.58)
    batch-generator ................................: (87.99, 101.14)
    forward-recv ...................................: (33.21, 94.79)
    forward-send ...................................: (0.82, 2.38)
    backward-recv ..................................: (48.74, 144.21)
    backward-send ..................................: (0.73, 2.48)
    forward-send-backward-recv .....................: (2304.28, 2784.09)
    backward-send-forward-recv .....................: (136.57, 182.59)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.08)
    grads-reduce-scatter ...........................: (18.15, 20.85)
    params-all-gather ..............................: (9.70, 11.02)
    optimizer-copy-to-main-grad ....................: (0.14, 0.23)
    optimizer-clip-main-grad .......................: (1.89, 2.01)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.14)
    optimizer-copy-main-to-model-params ............: (1.32, 1.53)
    optimizer ......................................: (9.10, 9.31)
Sun Feb 11 21:01:15 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   40C    P0             526W / 700W |  26686MiB / 81559MiB |     51%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   48C    P0             416W / 700W |  26734MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   49C    P0             517W / 700W |  26734MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0             393W / 700W |  26494MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   42C    P0             438W / 700W |  26166MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   50C    P0             417W / 700W |  26192MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             430W / 700W |  26198MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             403W / 700W |  25948MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 14767.8 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.159718E+00 | loss scale: 1.0 | grad norm: 0.800 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14427.57, 14614.10)
    forward-compute ................................: (4060.20, 5213.47)
    backward-compute ...............................: (7710.32, 9075.21)
    batch-generator ................................: (89.98, 100.55)
    forward-recv ...................................: (33.27, 94.84)
    forward-send ...................................: (0.82, 2.30)
    backward-recv ..................................: (48.38, 144.64)
    backward-send ..................................: (0.72, 2.55)
    forward-send-backward-recv .....................: (1963.85, 2467.95)
    backward-send-forward-recv .....................: (136.53, 180.80)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.15)
    grads-reduce-scatter ...........................: (18.04, 20.84)
    params-all-gather ..............................: (9.70, 11.04)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (1.64, 1.73)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.14)
    optimizer-copy-main-to-model-params ............: (1.31, 1.53)
    optimizer ......................................: (8.80, 9.02)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 14671.3 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.123525E+00 | loss scale: 1.0 | grad norm: 1.389 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14418.29, 14604.46)
    forward-compute ................................: (4060.05, 5241.65)
    backward-compute ...............................: (7722.32, 9074.32)
    batch-generator ................................: (87.81, 103.42)
    forward-recv ...................................: (33.05, 94.70)
    forward-send ...................................: (0.81, 2.39)
    backward-recv ..................................: (47.40, 145.10)
    backward-send ..................................: (0.72, 2.50)
    forward-send-backward-recv .....................: (1953.05, 2449.85)
    backward-send-forward-recv .....................: (136.55, 180.48)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.13)
    grads-reduce-scatter ...........................: (18.13, 20.76)
    params-all-gather ..............................: (9.70, 11.02)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (1.88, 2.29)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.14)
    optimizer-copy-main-to-model-params ............: (1.32, 1.53)
    optimizer ......................................: (9.38, 9.59)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 15416.5 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.088638E+00 | loss scale: 1.0 | grad norm: 0.759 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15164.02, 15350.35)
    forward-compute ................................: (4068.27, 5845.70)
    backward-compute ...............................: (7726.81, 9058.17)
    batch-generator ................................: (87.82, 102.81)
    forward-recv ...................................: (33.15, 461.09)
    forward-send ...................................: (0.81, 397.84)
    backward-recv ..................................: (48.43, 144.70)
    backward-send ..................................: (0.72, 2.48)
    forward-send-backward-recv .....................: (2311.93, 3170.63)
    backward-send-forward-recv .....................: (136.59, 857.30)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.28)
    grads-reduce-scatter ...........................: (18.16, 20.85)
    params-all-gather ..............................: (9.73, 11.02)
    optimizer-copy-to-main-grad ....................: (0.14, 0.23)
    optimizer-clip-main-grad .......................: (1.64, 1.73)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.14)
    optimizer-copy-main-to-model-params ............: (1.31, 1.53)
    optimizer ......................................: (8.87, 9.07)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (460865): No such process
kill: (460871): No such process
kill: (460877): No such process
kill: (460883): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=4, pp=2, mbs=4
LOCAL_IP = 10.64.24.51
DP=2, MP=4, PP=2
[2024-02-11 21:08:33,805] torch.distributed.run: [WARNING] 
[2024-02-11 21:08:33,805] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 21:08:33,805] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-11 21:08:33,805] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 857726976
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 857726976
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 857726976
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.216 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.830 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.158 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.165 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (864.90, 929.84)
    train/valid/test-data-iterators-setup ..........: (0.02, 11431.41)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 18730.7 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 9.218100E+00 | loss scale: 1.0 | grad norm: 673.464 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 11] (after 10 iterations) memory (MB) | allocated: 10137.8662109375 | max allocated: 29283.1943359375 | reserved: 33530.0 | max reserved: 33530.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 10137.8662109375 | max allocated: 29283.1943359375 | reserved: 33554.0 | max reserved: 33554.0
[Rank 10] (after 10 iterations) memory (MB) | allocated: 10137.8662109375 | max allocated: 29283.1943359375 | reserved: 33778.0 | max reserved: 33778.0
[Rank 9] (after 10 iterations) memory (MB) | allocated: 10137.8662109375 | max allocated: 29283.1943359375 | reserved: 33810.0 | max reserved: 33810.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (18311.49, 18464.71)
    forward-compute ................................: (6812.55, 7353.89)
    backward-compute ...............................: (9631.22, 10143.42)
    batch-generator ................................: (447.54, 478.20)
    forward-recv ...................................: (561.86, 576.09)
    forward-send ...................................: (4.86, 13.27)
    backward-recv ..................................: (112.16, 114.50)
    backward-send ..................................: (3.05, 3.15)
    forward-send-backward-recv .....................: (1699.12, 1865.55)
    backward-send-forward-recv .....................: (300.01, 304.78)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.74, 5.17)
    grads-reduce-scatter ...........................: (8.14, 236.72)
    params-all-gather ..............................: (4.80, 4.98)
    optimizer-copy-to-main-grad ....................: (0.58, 1.13)
    optimizer-clip-main-grad .......................: (5.48, 5.52)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.04, 5.28)
    optimizer-copy-main-to-model-params ............: (1.70, 1.82)
    optimizer ......................................: (14.43, 14.58)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 17608.1 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.513026E+00 | loss scale: 1.0 | grad norm: 12.742 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (17421.22, 17574.32)
    forward-compute ................................: (6598.80, 6703.90)
    backward-compute ...............................: (9608.08, 10136.65)
    batch-generator ................................: (50.82, 61.23)
    forward-recv ...................................: (101.18, 102.14)
    forward-send ...................................: (3.00, 3.06)
    backward-recv ..................................: (112.59, 114.43)
    backward-send ..................................: (3.06, 3.14)
    forward-send-backward-recv .....................: (1048.56, 1223.29)
    backward-send-forward-recv .....................: (550.59, 554.44)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.69, 5.36)
    grads-reduce-scatter ...........................: (8.15, 8.48)
    params-all-gather ..............................: (4.79, 4.94)
    optimizer-copy-to-main-grad ....................: (0.54, 0.66)
    optimizer-clip-main-grad .......................: (2.56, 2.60)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.87, 5.04)
    optimizer-copy-main-to-model-params ............: (1.69, 1.82)
    optimizer ......................................: (10.43, 10.55)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 18265.9 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.424098E+00 | loss scale: 1.0 | grad norm: 2.920 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (18079.64, 18232.48)
    forward-compute ................................: (6593.72, 7628.77)
    backward-compute ...............................: (9606.81, 10121.48)
    batch-generator ................................: (49.45, 61.72)
    forward-recv ...................................: (101.26, 102.11)
    forward-send ...................................: (3.00, 3.06)
    backward-recv ..................................: (112.26, 114.54)
    backward-send ..................................: (3.06, 3.13)
    forward-send-backward-recv .....................: (1714.55, 1887.01)
    backward-send-forward-recv .....................: (290.97, 548.70)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.77, 5.14)
    grads-reduce-scatter ...........................: (8.19, 8.49)
    params-all-gather ..............................: (4.81, 4.98)
    optimizer-copy-to-main-grad ....................: (0.55, 0.66)
    optimizer-clip-main-grad .......................: (2.56, 2.60)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.87, 5.04)
    optimizer-copy-main-to-model-params ............: (1.69, 1.81)
    optimizer ......................................: (10.42, 10.55)
Sun Feb 11 21:21:07 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             479W / 700W |  37228MiB / 81559MiB |     11%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   48C    P0             499W / 700W |  37532MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   50C    P0             494W / 700W |  38292MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   43C    P0             468W / 700W |  37804MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   42C    P0             426W / 700W |  37304MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             459W / 700W |  37480MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             456W / 700W |  37352MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             431W / 700W |  37284MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 17413.2 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.149386E+00 | loss scale: 1.0 | grad norm: 1.285 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (17136.50, 17289.25)
    forward-compute ................................: (6335.15, 6686.04)
    backward-compute ...............................: (9599.61, 10115.21)
    batch-generator ................................: (48.99, 62.14)
    forward-recv ...................................: (101.20, 102.12)
    forward-send ...................................: (3.02, 3.05)
    backward-recv ..................................: (112.35, 114.65)
    backward-send ..................................: (3.07, 3.17)
    forward-send-backward-recv .....................: (1032.80, 1209.64)
    backward-send-forward-recv .....................: (290.13, 292.57)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.69, 5.38)
    grads-reduce-scatter ...........................: (8.18, 8.46)
    params-all-gather ..............................: (4.84, 4.99)
    optimizer-copy-to-main-grad ....................: (0.54, 0.66)
    optimizer-clip-main-grad .......................: (2.56, 2.60)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.87, 5.04)
    optimizer-copy-main-to-model-params ............: (1.69, 1.82)
    optimizer ......................................: (10.43, 10.56)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 18085.6 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.284454E+00 | loss scale: 1.0 | grad norm: 1.970 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (17899.18, 18052.20)
    forward-compute ................................: (6592.76, 7192.42)
    backward-compute ...............................: (9604.75, 10104.29)
    batch-generator ................................: (48.35, 62.02)
    forward-recv ...................................: (101.21, 102.11)
    forward-send ...................................: (3.00, 3.05)
    backward-recv ..................................: (112.22, 113.87)
    backward-send ..................................: (3.06, 3.18)
    forward-send-backward-recv .....................: (1537.63, 1709.27)
    backward-send-forward-recv .....................: (545.30, 550.23)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.66, 5.09)
    grads-reduce-scatter ...........................: (8.14, 8.49)
    params-all-gather ..............................: (4.82, 4.96)
    optimizer-copy-to-main-grad ....................: (0.53, 0.65)
    optimizer-clip-main-grad .......................: (2.31, 2.34)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.88, 5.04)
    optimizer-copy-main-to-model-params ............: (1.69, 1.82)
    optimizer ......................................: (10.15, 10.28)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 17320.1 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.153484E+00 | loss scale: 1.0 | grad norm: 2.681 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (17132.53, 17285.92)
    forward-compute ................................: (6334.88, 6682.75)
    backward-compute ...............................: (9603.35, 10116.11)
    batch-generator ................................: (48.43, 61.31)
    forward-recv ...................................: (101.15, 102.12)
    forward-send ...................................: (3.01, 3.03)
    backward-recv ..................................: (112.58, 114.23)
    backward-send ..................................: (3.05, 3.16)
    forward-send-backward-recv .....................: (1025.91, 1202.41)
    backward-send-forward-recv .....................: (289.77, 292.42)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.63, 5.20)
    grads-reduce-scatter ...........................: (8.12, 8.51)
    params-all-gather ..............................: (4.78, 4.94)
    optimizer-copy-to-main-grad ....................: (0.54, 0.67)
    optimizer-clip-main-grad .......................: (2.30, 2.89)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.87, 5.03)
    optimizer-copy-main-to-model-params ............: (1.69, 1.82)
    optimizer ......................................: (10.80, 10.93)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 17583.4 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.143118E+00 | loss scale: 1.0 | grad norm: 1.056 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (17397.07, 17550.10)
    forward-compute ................................: (6592.55, 6698.08)
    backward-compute ...............................: (9608.52, 10120.30)
    batch-generator ................................: (48.63, 61.48)
    forward-recv ...................................: (101.18, 102.10)
    forward-send ...................................: (3.00, 3.05)
    backward-recv ..................................: (112.11, 114.61)
    backward-send ..................................: (3.06, 3.13)
    forward-send-backward-recv .....................: (1028.42, 1204.48)
    backward-send-forward-recv .....................: (541.28, 548.95)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.71, 5.08)
    grads-reduce-scatter ...........................: (8.19, 8.45)
    params-all-gather ..............................: (4.78, 4.96)
    optimizer-copy-to-main-grad ....................: (0.53, 0.66)
    optimizer-clip-main-grad .......................: (2.43, 2.47)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.87, 5.03)
    optimizer-copy-main-to-model-params ............: (1.69, 1.82)
    optimizer ......................................: (10.30, 10.42)
Sun Feb 11 21:32:59 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             491W / 700W |  37228MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   49C    P0             484W / 700W |  37532MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   50C    P0             485W / 700W |  38292MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   43C    P0             422W / 700W |  37804MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   42C    P0             407W / 700W |  37304MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             409W / 700W |  37480MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             422W / 700W |  37352MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             402W / 700W |  37284MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 18191.8 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.162782E+00 | loss scale: 1.0 | grad norm: 0.993 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (17912.49, 18065.88)
    forward-compute ................................: (6593.58, 7380.75)
    backward-compute ...............................: (9605.99, 10118.12)
    batch-generator ................................: (49.07, 62.64)
    forward-recv ...................................: (101.20, 102.12)
    forward-send ...................................: (3.00, 3.05)
    backward-recv ..................................: (112.47, 114.06)
    backward-send ..................................: (3.05, 3.15)
    forward-send-backward-recv .....................: (1547.35, 1721.66)
    backward-send-forward-recv .....................: (290.61, 549.70)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.65, 5.29)
    grads-reduce-scatter ...........................: (8.16, 8.51)
    params-all-gather ..............................: (4.83, 4.97)
    optimizer-copy-to-main-grad ....................: (0.53, 0.65)
    optimizer-clip-main-grad .......................: (2.30, 2.33)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.87, 5.04)
    optimizer-copy-main-to-model-params ............: (1.69, 1.82)
    optimizer ......................................: (10.18, 10.30)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 17304.6 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.123943E+00 | loss scale: 1.0 | grad norm: 0.650 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (17118.64, 17271.67)
    forward-compute ................................: (6334.35, 6667.51)
    backward-compute ...............................: (9610.22, 10105.32)
    batch-generator ................................: (49.64, 61.75)
    forward-recv ...................................: (101.19, 102.06)
    forward-send ...................................: (3.00, 3.04)
    backward-recv ..................................: (112.14, 113.86)
    backward-send ..................................: (3.06, 3.13)
    forward-send-backward-recv .....................: (1007.84, 1182.97)
    backward-send-forward-recv .....................: (289.55, 292.80)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.71, 5.58)
    grads-reduce-scatter ...........................: (8.23, 8.46)
    params-all-gather ..............................: (4.74, 4.95)
    optimizer-copy-to-main-grad ....................: (0.53, 0.66)
    optimizer-clip-main-grad .......................: (1.38, 1.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.87, 5.03)
    optimizer-copy-main-to-model-params ............: (1.69, 1.82)
    optimizer ......................................: (9.21, 9.33)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 18092.7 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.091129E+00 | loss scale: 1.0 | grad norm: 1.173 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (17906.33, 18059.37)
    forward-compute ................................: (6592.99, 7385.59)
    backward-compute ...............................: (9595.11, 10105.74)
    batch-generator ................................: (50.12, 62.76)
    forward-recv ...................................: (101.23, 102.14)
    forward-send ...................................: (3.00, 3.04)
    backward-recv ..................................: (112.14, 114.97)
    backward-send ..................................: (3.06, 3.14)
    forward-send-backward-recv .....................: (1549.07, 1726.53)
    backward-send-forward-recv .....................: (290.35, 548.54)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.72, 5.34)
    grads-reduce-scatter ...........................: (8.05, 8.47)
    params-all-gather ..............................: (4.79, 4.97)
    optimizer-copy-to-main-grad ....................: (0.53, 0.65)
    optimizer-clip-main-grad .......................: (2.05, 2.07)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.87, 5.04)
    optimizer-copy-main-to-model-params ............: (1.69, 1.82)
    optimizer ......................................: (9.92, 10.05)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (461979): No such process
kill: (461985): No such process
kill: (461991): No such process
kill: (461997): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=4, pp=2, mbs=2
LOCAL_IP = 10.64.24.51
DP=2, MP=4, PP=2
[2024-02-11 21:41:07,519] torch.distributed.run: [WARNING] 
[2024-02-11 21:41:07,519] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 21:41:07,519] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-11 21:41:07,519] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 857726976
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.259 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.309 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.057 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.220 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (844.35, 888.10)
    train/valid/test-data-iterators-setup ..........: (0.02, 11647.42)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 20521.5 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 9.218110E+00 | loss scale: 1.0 | grad norm: 673.456 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 10] (after 10 iterations) memory (MB) | allocated: 10009.8662109375 | max allocated: 19582.5849609375 | reserved: 21858.0 | max reserved: 21858.0
[Rank 9] (after 10 iterations) memory (MB) | allocated: 10009.8662109375 | max allocated: 19582.5849609375 | reserved: 21890.0 | max reserved: 21890.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 10009.8662109375 | max allocated: 19582.5849609375 | reserved: 21874.0 | max reserved: 21874.0
[Rank 11] (after 10 iterations) memory (MB) | allocated: 10009.8662109375 | max allocated: 19582.5849609375 | reserved: 21858.0 | max reserved: 21858.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (20022.72, 20107.46)
    forward-compute ................................: (7538.37, 7887.06)
    backward-compute ...............................: (10596.70, 11060.25)
    batch-generator ................................: (488.21, 521.03)
    forward-recv ...................................: (496.25, 511.75)
    forward-send ...................................: (3.27, 7.89)
    backward-recv ..................................: (59.60, 60.62)
    backward-send ..................................: (1.58, 1.62)
    forward-send-backward-recv .....................: (1612.67, 1856.05)
    backward-send-forward-recv .....................: (606.91, 653.27)
    layernorm-grads-all-reduce .....................: (0.02, 0.09)
    embedding-grads-all-reduce .....................: (4.78, 5.14)
    grads-reduce-scatter ...........................: (8.05, 384.44)
    params-all-gather ..............................: (4.78, 4.99)
    optimizer-copy-to-main-grad ....................: (0.54, 0.74)
    optimizer-clip-main-grad .......................: (5.51, 5.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.03, 5.36)
    optimizer-copy-main-to-model-params ............: (1.70, 1.82)
    optimizer ......................................: (14.09, 14.27)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 19494.0 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.513203E+00 | loss scale: 1.0 | grad norm: 13.295 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (19375.14, 19459.71)
    forward-compute ................................: (7062.59, 7734.58)
    backward-compute ...............................: (10575.26, 11054.48)
    batch-generator ................................: (96.99, 121.75)
    forward-recv ...................................: (54.10, 54.61)
    forward-send ...................................: (1.56, 1.58)
    backward-recv ..................................: (60.04, 60.71)
    backward-send ..................................: (1.57, 1.63)
    forward-send-backward-recv .....................: (1516.51, 1712.91)
    backward-send-forward-recv .....................: (580.64, 591.32)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.68, 5.36)
    grads-reduce-scatter ...........................: (8.17, 8.44)
    params-all-gather ..............................: (4.83, 4.94)
    optimizer-copy-to-main-grad ....................: (0.54, 0.68)
    optimizer-clip-main-grad .......................: (2.56, 2.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.87, 5.04)
    optimizer-copy-main-to-model-params ............: (1.69, 1.81)
    optimizer ......................................: (10.43, 10.55)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 20227.2 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.425563E+00 | loss scale: 1.0 | grad norm: 2.916 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (20108.22, 20193.03)
    forward-compute ................................: (7328.55, 8213.94)
    backward-compute ...............................: (10585.70, 11019.94)
    batch-generator ................................: (104.74, 123.76)
    forward-recv ...................................: (54.15, 54.49)
    forward-send ...................................: (1.57, 1.59)
    backward-recv ..................................: (325.26, 325.48)
    backward-send ..................................: (1.58, 1.62)
    forward-send-backward-recv .....................: (1727.79, 1905.03)
    backward-send-forward-recv .....................: (843.80, 856.98)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.73, 5.09)
    grads-reduce-scatter ...........................: (8.05, 8.49)
    params-all-gather ..............................: (4.78, 4.94)
    optimizer-copy-to-main-grad ....................: (0.54, 0.67)
    optimizer-clip-main-grad .......................: (2.56, 2.68)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.89, 5.03)
    optimizer-copy-main-to-model-params ............: (1.69, 1.81)
    optimizer ......................................: (10.68, 10.80)
Sun Feb 11 21:55:03 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             525W / 700W |  25548MiB / 81559MiB |     87%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   49C    P0             518W / 700W |  25612MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   50C    P0             535W / 700W |  25580MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   43C    P0             505W / 700W |  25340MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   42C    P0             518W / 700W |  25456MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             507W / 700W |  25552MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             519W / 700W |  25616MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             443W / 700W |  25392MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 19829.2 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.149529E+00 | loss scale: 1.0 | grad norm: 1.283 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (19622.69, 19706.95)
    forward-compute ................................: (7075.41, 7992.00)
    backward-compute ...............................: (10567.38, 11027.64)
    batch-generator ................................: (105.45, 125.44)
    forward-recv ...................................: (54.15, 54.71)
    forward-send ...................................: (1.56, 1.59)
    backward-recv ..................................: (59.82, 60.53)
    backward-send ..................................: (1.57, 1.63)
    forward-send-backward-recv .....................: (1778.60, 1956.24)
    backward-send-forward-recv .....................: (582.02, 597.66)
    layernorm-grads-all-reduce .....................: (0.02, 0.04)
    embedding-grads-all-reduce .....................: (4.65, 5.36)
    grads-reduce-scatter ...........................: (8.03, 8.37)
    params-all-gather ..............................: (4.82, 4.95)
    optimizer-copy-to-main-grad ....................: (0.53, 0.67)
    optimizer-clip-main-grad .......................: (2.56, 2.60)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.89, 5.03)
    optimizer-copy-main-to-model-params ............: (1.69, 1.81)
    optimizer ......................................: (10.45, 10.57)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 19824.1 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.284124E+00 | loss scale: 1.0 | grad norm: 1.636 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (19705.93, 19790.16)
    forward-compute ................................: (7063.65, 8073.68)
    backward-compute ...............................: (10571.49, 11045.44)
    batch-generator ................................: (104.09, 132.15)
    forward-recv ...................................: (54.08, 54.58)
    forward-send ...................................: (1.56, 1.59)
    backward-recv ..................................: (59.69, 60.97)
    backward-send ..................................: (1.57, 1.61)
    forward-send-backward-recv .....................: (1849.89, 2047.01)
    backward-send-forward-recv .....................: (582.17, 596.40)
    layernorm-grads-all-reduce .....................: (0.02, 0.04)
    embedding-grads-all-reduce .....................: (4.66, 5.11)
    grads-reduce-scatter ...........................: (8.16, 8.50)
    params-all-gather ..............................: (4.82, 4.93)
    optimizer-copy-to-main-grad ....................: (0.54, 0.67)
    optimizer-clip-main-grad .......................: (2.43, 2.46)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.89, 5.03)
    optimizer-copy-main-to-model-params ............: (1.69, 1.81)
    optimizer ......................................: (10.33, 10.44)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 19813.8 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.153095E+00 | loss scale: 1.0 | grad norm: 1.084 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (19695.41, 19779.69)
    forward-compute ................................: (7066.50, 8063.82)
    backward-compute ...............................: (10562.71, 11059.73)
    batch-generator ................................: (98.27, 139.12)
    forward-recv ...................................: (54.11, 54.49)
    forward-send ...................................: (1.57, 1.60)
    backward-recv ..................................: (59.60, 60.87)
    backward-send ..................................: (1.57, 1.62)
    forward-send-backward-recv .....................: (1851.06, 2043.39)
    backward-send-forward-recv .....................: (581.71, 593.82)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.63, 5.23)
    grads-reduce-scatter ...........................: (8.11, 8.47)
    params-all-gather ..............................: (4.80, 4.95)
    optimizer-copy-to-main-grad ....................: (0.54, 0.68)
    optimizer-clip-main-grad .......................: (2.30, 2.33)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.89, 5.03)
    optimizer-copy-main-to-model-params ............: (1.69, 1.81)
    optimizer ......................................: (10.18, 10.30)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 19762.3 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.139713E+00 | loss scale: 1.0 | grad norm: 0.600 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (19644.35, 19729.07)
    forward-compute ................................: (7329.21, 7748.44)
    backward-compute ...............................: (10577.15, 11068.86)
    batch-generator ................................: (104.40, 137.70)
    forward-recv ...................................: (54.13, 54.67)
    forward-send ...................................: (1.56, 1.60)
    backward-recv ..................................: (60.41, 61.18)
    backward-send ..................................: (1.57, 1.63)
    forward-send-backward-recv .....................: (1537.19, 1714.38)
    backward-send-forward-recv .....................: (844.76, 855.51)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.67, 5.11)
    grads-reduce-scatter ...........................: (8.09, 8.48)
    params-all-gather ..............................: (4.82, 4.92)
    optimizer-copy-to-main-grad ....................: (0.54, 0.67)
    optimizer-clip-main-grad .......................: (1.91, 1.93)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.88, 5.03)
    optimizer-copy-main-to-model-params ............: (1.69, 1.81)
    optimizer ......................................: (9.77, 9.89)
Sun Feb 11 22:08:16 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   41C    P0             544W / 700W |  25548MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   49C    P0             526W / 700W |  25612MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   50C    P0             527W / 700W |  25580MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   43C    P0             471W / 700W |  25340MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   42C    P0             478W / 700W |  25456MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   50C    P0             489W / 700W |  25552MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             454W / 700W |  25616MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             487W / 700W |  25392MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 19905.1 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.159090E+00 | loss scale: 1.0 | grad norm: 1.142 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (19697.55, 19781.88)
    forward-compute ................................: (7067.72, 8067.18)
    backward-compute ...............................: (10562.98, 11060.36)
    batch-generator ................................: (100.54, 138.23)
    forward-recv ...................................: (54.12, 54.41)
    forward-send ...................................: (1.56, 1.59)
    backward-recv ..................................: (59.65, 60.79)
    backward-send ..................................: (1.56, 1.64)
    forward-send-backward-recv .....................: (1861.34, 2043.88)
    backward-send-forward-recv .....................: (581.74, 593.93)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.67, 6.27)
    grads-reduce-scatter ...........................: (8.19, 8.50)
    params-all-gather ..............................: (4.83, 4.98)
    optimizer-copy-to-main-grad ....................: (0.52, 0.68)
    optimizer-clip-main-grad .......................: (2.03, 2.05)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.88, 5.02)
    optimizer-copy-main-to-model-params ............: (1.69, 1.81)
    optimizer ......................................: (9.88, 10.00)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 19794.0 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.124166E+00 | loss scale: 1.0 | grad norm: 1.071 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (19676.21, 19760.34)
    forward-compute ................................: (7067.71, 8048.82)
    backward-compute ...............................: (10575.41, 11048.01)
    batch-generator ................................: (100.74, 137.48)
    forward-recv ...................................: (54.14, 54.46)
    forward-send ...................................: (1.57, 1.59)
    backward-recv ..................................: (59.58, 60.34)
    backward-send ..................................: (1.58, 1.63)
    forward-send-backward-recv .....................: (1827.51, 2008.43)
    backward-send-forward-recv .....................: (582.12, 594.01)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.73, 5.14)
    grads-reduce-scatter ...........................: (8.12, 8.47)
    params-all-gather ..............................: (4.83, 4.99)
    optimizer-copy-to-main-grad ....................: (0.54, 0.68)
    optimizer-clip-main-grad .......................: (2.19, 2.22)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.89, 5.03)
    optimizer-copy-main-to-model-params ............: (1.69, 1.81)
    optimizer ......................................: (10.07, 10.18)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 20331.4 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.094818E+00 | loss scale: 1.0 | grad norm: 0.621 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (20213.21, 20297.51)
    forward-compute ................................: (7331.25, 8319.63)
    backward-compute ...............................: (10573.86, 11061.16)
    batch-generator ................................: (102.49, 138.03)
    forward-recv ...................................: (54.06, 54.49)
    forward-send ...................................: (1.57, 1.59)
    backward-recv ..................................: (59.70, 61.07)
    backward-send ..................................: (1.59, 1.64)
    forward-send-backward-recv .....................: (2101.84, 2282.10)
    backward-send-forward-recv .....................: (843.51, 855.19)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.69, 5.35)
    grads-reduce-scatter ...........................: (8.20, 8.49)
    params-all-gather ..............................: (4.80, 4.93)
    optimizer-copy-to-main-grad ....................: (0.54, 0.68)
    optimizer-clip-main-grad .......................: (2.17, 2.19)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.89, 5.06)
    optimizer-copy-main-to-model-params ............: (1.69, 1.81)
    optimizer ......................................: (10.05, 10.16)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (463093): No such process
kill: (463099): No such process
kill: (463105): No such process
kill: (463111): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=4, pp=2, mbs=1
LOCAL_IP = 10.64.24.51
DP=2, MP=4, PP=2
[2024-02-11 22:17:12,456] torch.distributed.run: [WARNING] 
[2024-02-11 22:17:12,456] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 22:17:12,456] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-11 22:17:12,456] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 857726976
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.305 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.315 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.124 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.177 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (855.51, 899.85)
    train/valid/test-data-iterators-setup ..........: (0.02, 11026.54)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 22887.7 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 9.217811E+00 | loss scale: 1.0 | grad norm: 673.433 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 11] (after 10 iterations) memory (MB) | allocated: 9945.8662109375 | max allocated: 14605.14111328125 | reserved: 15484.0 | max reserved: 15484.0[Rank 9] (after 10 iterations) memory (MB) | allocated: 9945.8662109375 | max allocated: 14605.14111328125 | reserved: 15670.0 | max reserved: 15670.0

[Rank 10] (after 10 iterations) memory (MB) | allocated: 9945.8662109375 | max allocated: 14605.14111328125 | reserved: 15662.0 | max reserved: 15662.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 9945.8662109375 | max allocated: 14605.14111328125 | reserved: 15654.0 | max reserved: 15654.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (22586.04, 22633.43)
    forward-compute ................................: (8460.26, 8775.10)
    backward-compute ...............................: (11949.57, 12286.00)
    batch-generator ................................: (590.72, 637.62)
    forward-recv ...................................: (481.51, 493.83)
    forward-send ...................................: (2.70, 8.76)
    backward-recv ..................................: (31.83, 32.37)
    backward-send ..................................: (0.85, 0.88)
    forward-send-backward-recv .....................: (1925.72, 2098.03)
    backward-send-forward-recv .....................: (954.35, 992.88)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.82, 4.99)
    grads-reduce-scatter ...........................: (8.23, 225.56)
    params-all-gather ..............................: (4.77, 4.99)
    optimizer-copy-to-main-grad ....................: (0.57, 0.71)
    optimizer-clip-main-grad .......................: (5.55, 5.67)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.01, 5.26)
    optimizer-copy-main-to-model-params ............: (1.69, 1.81)
    optimizer ......................................: (14.09, 14.23)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 22485.1 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.513304E+00 | loss scale: 1.0 | grad norm: 13.274 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (22404.55, 22451.73)
    forward-compute ................................: (8212.92, 8850.27)
    backward-compute ...............................: (11928.83, 12267.68)
    batch-generator ................................: (185.51, 245.99)
    forward-recv ...................................: (29.38, 29.94)
    forward-send ...................................: (0.83, 0.86)
    backward-recv ..................................: (31.93, 32.35)
    backward-send ..................................: (0.85, 0.87)
    forward-send-backward-recv .....................: (1964.70, 2189.73)
    backward-send-forward-recv .....................: (1173.91, 1196.80)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.77, 4.97)
    grads-reduce-scatter ...........................: (8.20, 8.38)
    params-all-gather ..............................: (4.81, 4.95)
    optimizer-copy-to-main-grad ....................: (0.54, 0.67)
    optimizer-clip-main-grad .......................: (2.57, 2.61)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.88, 5.02)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (10.46, 10.57)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 22493.3 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.425542E+00 | loss scale: 1.0 | grad norm: 2.938 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (22413.00, 22460.15)
    forward-compute ................................: (8214.45, 9380.04)
    backward-compute ...............................: (11923.55, 12264.05)
    batch-generator ................................: (185.10, 231.31)
    forward-recv ...................................: (29.66, 29.91)
    forward-send ...................................: (0.83, 0.87)
    backward-recv ..................................: (31.45, 32.41)
    backward-send ..................................: (0.84, 0.88)
    forward-send-backward-recv .....................: (1996.91, 2202.87)
    backward-send-forward-recv .....................: (651.28, 1177.57)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.70, 4.88)
    grads-reduce-scatter ...........................: (8.20, 8.48)
    params-all-gather ..............................: (4.77, 4.96)
    optimizer-copy-to-main-grad ....................: (0.54, 0.68)
    optimizer-clip-main-grad .......................: (2.56, 2.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.86, 5.04)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (10.45, 10.57)
Sun Feb 11 22:32:47 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   40C    P0             505W / 700W |  19328MiB / 81559MiB |     76%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   48C    P0             488W / 700W |  19392MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   49C    P0             513W / 700W |  19384MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0             490W / 700W |  19164MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   41C    P0             506W / 700W |  19324MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   48C    P0             501W / 700W |  19396MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   46C    P0             489W / 700W |  19400MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   41C    P0             435W / 700W |  19314MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 22329.2 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.149500E+00 | loss scale: 1.0 | grad norm: 1.278 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (22159.75, 22207.22)
    forward-compute ................................: (7946.05, 8876.76)
    backward-compute ...............................: (11919.54, 12261.04)
    batch-generator ................................: (188.72, 226.64)
    forward-recv ...................................: (29.37, 29.59)
    forward-send ...................................: (0.83, 0.87)
    backward-recv ..................................: (32.44, 32.54)
    backward-send ..................................: (0.84, 0.89)
    forward-send-backward-recv .....................: (2037.78, 2217.63)
    backward-send-forward-recv .....................: (908.55, 945.30)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.75, 4.94)
    grads-reduce-scatter ...........................: (8.12, 8.39)
    params-all-gather ..............................: (4.82, 4.97)
    optimizer-copy-to-main-grad ....................: (0.53, 0.69)
    optimizer-clip-main-grad .......................: (2.56, 2.96)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.88, 5.08)
    optimizer-copy-main-to-model-params ............: (1.69, 1.81)
    optimizer ......................................: (10.96, 11.08)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 22475.5 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.284129E+00 | loss scale: 1.0 | grad norm: 1.702 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (22395.03, 22442.19)
    forward-compute ................................: (8217.14, 9371.91)
    backward-compute ...............................: (11925.93, 12261.24)
    batch-generator ................................: (185.41, 226.12)
    forward-recv ...................................: (29.37, 29.55)
    forward-send ...................................: (0.83, 0.86)
    backward-recv ..................................: (31.83, 32.95)
    backward-send ..................................: (0.85, 0.88)
    forward-send-backward-recv .....................: (2032.57, 2179.27)
    backward-send-forward-recv .....................: (655.82, 1183.78)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.76, 4.91)
    grads-reduce-scatter ...........................: (8.15, 8.50)
    params-all-gather ..............................: (4.82, 4.96)
    optimizer-copy-to-main-grad ....................: (0.54, 0.68)
    optimizer-clip-main-grad .......................: (2.43, 2.46)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.88, 5.05)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (10.33, 10.45)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 21664.3 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.153023E+00 | loss scale: 1.0 | grad norm: 1.508 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (21583.67, 21630.94)
    forward-compute ................................: (7946.51, 8309.10)
    backward-compute ...............................: (11925.26, 12253.37)
    batch-generator ................................: (183.68, 225.68)
    forward-recv ...................................: (29.42, 29.61)
    forward-send ...................................: (0.83, 0.87)
    backward-recv ..................................: (31.85, 32.22)
    backward-send ..................................: (0.83, 1.09)
    forward-send-backward-recv .....................: (1499.80, 1640.41)
    backward-send-forward-recv .....................: (907.38, 921.45)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.70, 4.91)
    grads-reduce-scatter ...........................: (8.16, 8.46)
    params-all-gather ..............................: (4.85, 4.98)
    optimizer-copy-to-main-grad ....................: (0.53, 0.71)
    optimizer-clip-main-grad .......................: (2.17, 2.20)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.86, 5.03)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (10.05, 10.17)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 22482.0 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.144239E+00 | loss scale: 1.0 | grad norm: 1.115 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (22401.05, 22448.51)
    forward-compute ................................: (8221.90, 8849.14)
    backward-compute ...............................: (11921.55, 12260.33)
    batch-generator ................................: (184.88, 226.61)
    forward-recv ...................................: (29.53, 29.63)
    forward-send ...................................: (0.83, 1.00)
    backward-recv ..................................: (32.44, 33.29)
    backward-send ..................................: (0.84, 0.87)
    forward-send-backward-recv .....................: (2037.95, 2186.81)
    backward-send-forward-recv .....................: (1178.18, 1192.86)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.77, 4.90)
    grads-reduce-scatter ...........................: (8.23, 8.48)
    params-all-gather ..............................: (4.82, 4.92)
    optimizer-copy-to-main-grad ....................: (0.53, 0.69)
    optimizer-clip-main-grad .......................: (2.55, 2.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.86, 5.03)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (10.43, 10.54)
Sun Feb 11 22:47:39 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   40C    P0             467W / 700W |  19328MiB / 81559MiB |     56%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   48C    P0             560W / 700W |  19392MiB / 81559MiB |     97%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   49C    P0             489W / 700W |  19582MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0             493W / 700W |  19164MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   41C    P0             504W / 700W |  19324MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   48C    P0             510W / 700W |  19396MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   46C    P0             440W / 700W |  19400MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   41C    P0             508W / 700W |  19314MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 22556.5 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.162855E+00 | loss scale: 1.0 | grad norm: 0.671 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (22388.40, 22435.70)
    forward-compute ................................: (8216.16, 8846.14)
    backward-compute ...............................: (11924.07, 12257.99)
    batch-generator ................................: (184.74, 223.99)
    forward-recv ...................................: (29.38, 29.75)
    forward-send ...................................: (0.82, 0.87)
    backward-recv ..................................: (31.97, 32.41)
    backward-send ..................................: (0.84, 0.89)
    forward-send-backward-recv .....................: (2032.85, 2178.25)
    backward-send-forward-recv .....................: (1169.42, 1185.64)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.74, 4.89)
    grads-reduce-scatter ...........................: (8.14, 8.50)
    params-all-gather ..............................: (4.76, 4.95)
    optimizer-copy-to-main-grad ....................: (0.53, 0.67)
    optimizer-clip-main-grad .......................: (1.91, 1.93)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.86, 5.04)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (9.78, 9.90)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 21671.5 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.124005E+00 | loss scale: 1.0 | grad norm: 0.709 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (21592.08, 21639.31)
    forward-compute ................................: (7946.73, 8835.88)
    backward-compute ...............................: (11928.17, 12259.86)
    batch-generator ................................: (182.24, 223.24)
    forward-recv ...................................: (29.49, 30.01)
    forward-send ...................................: (0.83, 0.87)
    backward-recv ..................................: (32.31, 32.53)
    backward-send ..................................: (0.84, 0.87)
    forward-send-backward-recv .....................: (1500.12, 1646.83)
    backward-send-forward-recv .....................: (382.26, 916.34)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.75, 4.95)
    grads-reduce-scatter ...........................: (8.13, 8.46)
    params-all-gather ..............................: (4.77, 4.95)
    optimizer-copy-to-main-grad ....................: (0.53, 0.66)
    optimizer-clip-main-grad .......................: (1.51, 1.52)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.87, 5.02)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (9.33, 9.45)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 23007.4 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.089739E+00 | loss scale: 1.0 | grad norm: 1.271 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (22926.32, 22973.97)
    forward-compute ................................: (8217.27, 9648.59)
    backward-compute ...............................: (11943.34, 12254.71)
    batch-generator ................................: (177.19, 238.71)
    forward-recv ...................................: (29.36, 29.61)
    forward-send ...................................: (0.82, 0.87)
    backward-recv ..................................: (31.27, 32.39)
    backward-send ..................................: (0.84, 0.87)
    forward-send-backward-recv .....................: (2452.45, 2694.47)
    backward-send-forward-recv .....................: (915.79, 1179.69)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.74, 4.97)
    grads-reduce-scatter ...........................: (8.12, 8.37)
    params-all-gather ..............................: (4.77, 4.93)
    optimizer-copy-to-main-grad ....................: (0.53, 0.83)
    optimizer-clip-main-grad .......................: (2.21, 2.24)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.87, 5.14)
    optimizer-copy-main-to-model-params ............: (1.69, 1.81)
    optimizer ......................................: (10.31, 10.43)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (464207): No such process
kill: (464213): No such process
kill: (464219): No such process
kill: (464225): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=2, pp=4, mbs=4
LOCAL_IP = 10.64.24.51
DP=2, MP=2, PP=4
[2024-02-11 22:57:21,653] torch.distributed.run: [WARNING] 
[2024-02-11 22:57:21,653] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 22:57:21,653] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-11 22:57:21,653] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 805617664
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 805617664
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 908910592
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 908910592
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...

 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.724 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.722 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.737 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.796 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.196 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.193 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.275 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.269 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (56.00, 600.57)
    train/valid/test-data-iterators-setup ..........: (0.02, 11433.46)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 16630.6 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 9.188653E+00 | loss scale: 1.0 | grad norm: 700.157 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 9] (after 10 iterations) memory (MB) | allocated: 9540.376953125 | max allocated: 34136.845703125 | reserved: 36378.0 | max reserved: 36378.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 9540.376953125 | max allocated: 34136.845703125 | reserved: 36502.0 | max reserved: 36502.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 10724.3544921875 | max allocated: 26707.6748046875 | reserved: 30716.0 | max reserved: 30716.0
[Rank 13] (after 10 iterations) memory (MB) | allocated: 10724.3544921875 | max allocated: 26707.6748046875 | reserved: 30972.0 | max reserved: 30972.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (15990.37, 16373.14)
    forward-compute ................................: (4883.93, 5912.23)
    backward-compute ...............................: (7940.85, 9158.72)
    batch-generator ................................: (154.74, 185.00)
    forward-recv ...................................: (251.01, 674.01)
    forward-send ...................................: (6.63, 256.62)
    backward-recv ..................................: (104.00, 315.94)
    backward-send ..................................: (2.65, 7.75)
    forward-send-backward-recv .....................: (2199.95, 2505.11)
    backward-send-forward-recv .....................: (257.39, 318.55)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.53)
    grads-reduce-scatter ...........................: (8.75, 222.47)
    params-all-gather ..............................: (4.15, 4.88)
    optimizer-copy-to-main-grad ....................: (0.29, 0.39)
    optimizer-clip-main-grad .......................: (6.47, 6.69)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.66, 5.47)
    optimizer-copy-main-to-model-params ............: (1.43, 1.67)
    optimizer ......................................: (14.76, 15.02)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 15655.5 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.553416E+00 | loss scale: 1.0 | grad norm: 6.934 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15235.00, 15617.12)
    forward-compute ................................: (4743.84, 5612.06)
    backward-compute ...............................: (7913.78, 9151.86)
    batch-generator ................................: (43.68, 57.67)
    forward-recv ...................................: (82.56, 229.84)
    forward-send ...................................: (2.39, 8.24)
    backward-recv ..................................: (104.52, 316.57)
    backward-send ..................................: (2.63, 7.77)
    forward-send-backward-recv .....................: (2008.48, 2256.83)
    backward-send-forward-recv .....................: (244.35, 299.48)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.75)
    grads-reduce-scatter ...........................: (7.61, 8.98)
    params-all-gather ..............................: (4.22, 4.87)
    optimizer-copy-to-main-grad ....................: (0.28, 0.37)
    optimizer-clip-main-grad .......................: (2.35, 2.56)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.53, 5.26)
    optimizer-copy-main-to-model-params ............: (1.42, 1.66)
    optimizer ......................................: (10.01, 10.25)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 15654.1 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.409774E+00 | loss scale: 1.0 | grad norm: 2.305 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15234.25, 15615.53)
    forward-compute ................................: (4738.14, 5616.96)
    backward-compute ...............................: (7896.22, 9137.86)
    batch-generator ................................: (44.63, 60.45)
    forward-recv ...................................: (82.87, 229.80)
    forward-send ...................................: (2.38, 7.73)
    backward-recv ..................................: (104.20, 316.58)
    backward-send ..................................: (2.61, 7.72)
    forward-send-backward-recv .....................: (1996.41, 2277.95)
    backward-send-forward-recv .....................: (244.41, 299.92)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.64)
    grads-reduce-scatter ...........................: (7.65, 9.02)
    params-all-gather ..............................: (4.16, 4.88)
    optimizer-copy-to-main-grad ....................: (0.28, 0.47)
    optimizer-clip-main-grad .......................: (2.39, 2.60)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.53, 5.26)
    optimizer-copy-main-to-model-params ............: (1.42, 1.68)
    optimizer ......................................: (10.16, 10.42)
Sun Feb 11 23:08:31 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   40C    P0             438W / 700W |  39842MiB / 81559MiB |     53%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   47C    P0             414W / 700W |  39718MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   48C    P0             392W / 700W |  39842MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0             391W / 700W |  39842MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   42C    P0             319W / 700W |  34142MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             312W / 700W |  34398MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             332W / 700W |  34410MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             359W / 700W |  34410MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 15731.3 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.146781E+00 | loss scale: 1.0 | grad norm: 1.093 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15221.54, 15603.10)
    forward-compute ................................: (4744.90, 5601.06)
    backward-compute ...............................: (7903.17, 9136.58)
    batch-generator ................................: (44.85, 58.64)
    forward-recv ...................................: (83.07, 229.84)
    forward-send ...................................: (2.39, 7.84)
    backward-recv ..................................: (104.40, 317.09)
    backward-send ..................................: (2.65, 7.74)
    forward-send-backward-recv .....................: (1989.97, 2251.58)
    backward-send-forward-recv .....................: (244.35, 299.26)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.80)
    grads-reduce-scatter ...........................: (7.74, 8.99)
    params-all-gather ..............................: (4.24, 4.85)
    optimizer-copy-to-main-grad ....................: (0.28, 0.44)
    optimizer-clip-main-grad .......................: (2.36, 3.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.54, 5.26)
    optimizer-copy-main-to-model-params ............: (1.42, 1.65)
    optimizer ......................................: (10.95, 11.18)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 15643.5 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.282398E+00 | loss scale: 1.0 | grad norm: 1.412 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15223.76, 15605.05)
    forward-compute ................................: (4744.06, 5612.06)
    backward-compute ...............................: (7908.73, 9140.55)
    batch-generator ................................: (43.88, 58.01)
    forward-recv ...................................: (82.87, 230.51)
    forward-send ...................................: (2.38, 8.03)
    backward-recv ..................................: (104.35, 315.85)
    backward-send ..................................: (2.63, 7.76)
    forward-send-backward-recv .....................: (1992.26, 2250.12)
    backward-send-forward-recv .....................: (244.09, 298.97)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.50)
    grads-reduce-scatter ...........................: (7.66, 9.10)
    params-all-gather ..............................: (4.22, 4.85)
    optimizer-copy-to-main-grad ....................: (0.28, 0.43)
    optimizer-clip-main-grad .......................: (2.36, 2.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.53, 5.26)
    optimizer-copy-main-to-model-params ............: (1.42, 1.65)
    optimizer ......................................: (10.07, 10.30)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 15642.9 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.155895E+00 | loss scale: 1.0 | grad norm: 3.626 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15222.92, 15603.83)
    forward-compute ................................: (4741.29, 5609.26)
    backward-compute ...............................: (7899.44, 9133.97)
    batch-generator ................................: (44.36, 58.61)
    forward-recv ...................................: (82.73, 229.68)
    forward-send ...................................: (2.38, 7.68)
    backward-recv ..................................: (104.28, 317.11)
    backward-send ..................................: (2.62, 7.68)
    forward-send-backward-recv .....................: (2005.57, 2262.17)
    backward-send-forward-recv .....................: (243.81, 298.33)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.63)
    grads-reduce-scatter ...........................: (7.65, 9.09)
    params-all-gather ..............................: (4.21, 4.87)
    optimizer-copy-to-main-grad ....................: (0.28, 0.41)
    optimizer-clip-main-grad .......................: (2.24, 2.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.53, 5.25)
    optimizer-copy-main-to-model-params ............: (1.42, 1.65)
    optimizer ......................................: (9.91, 10.14)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 15633.9 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.144507E+00 | loss scale: 1.0 | grad norm: 0.859 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15214.53, 15595.43)
    forward-compute ................................: (4743.23, 5609.29)
    backward-compute ...............................: (7909.81, 9129.62)
    batch-generator ................................: (43.63, 58.73)
    forward-recv ...................................: (82.58, 229.97)
    forward-send ...................................: (2.38, 7.91)
    backward-recv ..................................: (103.77, 315.77)
    backward-send ..................................: (2.61, 7.72)
    forward-send-backward-recv .....................: (1978.19, 2241.18)
    backward-send-forward-recv .....................: (243.85, 298.47)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.52)
    grads-reduce-scatter ...........................: (7.75, 9.05)
    params-all-gather ..............................: (4.16, 4.84)
    optimizer-copy-to-main-grad ....................: (0.28, 0.41)
    optimizer-clip-main-grad .......................: (2.11, 2.28)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.53, 5.25)
    optimizer-copy-main-to-model-params ............: (1.42, 1.65)
    optimizer ......................................: (9.76, 10.00)
Sun Feb 11 23:19:00 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   39C    P0             309W / 700W |  39842MiB / 81559MiB |     70%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   46C    P0             406W / 700W |  39718MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   47C    P0             362W / 700W |  39842MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0             397W / 700W |  39842MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   41C    P0             358W / 700W |  34142MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   48C    P0             389W / 700W |  34398MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   46C    P0             309W / 700W |  34410MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   41C    P0             300W / 700W |  34410MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 15966.0 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.162040E+00 | loss scale: 1.0 | grad norm: 0.721 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15457.55, 15838.62)
    forward-compute ................................: (4743.17, 5858.35)
    backward-compute ...............................: (7907.17, 9131.15)
    batch-generator ................................: (43.60, 57.96)
    forward-recv ...................................: (83.70, 419.01)
    forward-send ...................................: (2.38, 7.86)
    backward-recv ..................................: (104.37, 315.81)
    backward-send ..................................: (2.63, 7.76)
    forward-send-backward-recv .....................: (1993.73, 2454.56)
    backward-send-forward-recv .....................: (243.73, 516.40)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.76)
    grads-reduce-scatter ...........................: (7.56, 8.94)
    params-all-gather ..............................: (4.20, 4.85)
    optimizer-copy-to-main-grad ....................: (0.28, 0.40)
    optimizer-clip-main-grad .......................: (1.73, 1.85)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.53, 5.26)
    optimizer-copy-main-to-model-params ............: (1.42, 1.66)
    optimizer ......................................: (9.42, 9.65)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 15970.7 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.124474E+00 | loss scale: 1.0 | grad norm: 0.674 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15551.84, 15933.57)
    forward-compute ................................: (4738.61, 5919.11)
    backward-compute ...............................: (7897.53, 9123.90)
    batch-generator ................................: (44.68, 58.45)
    forward-recv ...................................: (82.70, 512.71)
    forward-send ...................................: (2.37, 8.00)
    backward-recv ..................................: (104.32, 315.58)
    backward-send ..................................: (2.63, 7.80)
    forward-send-backward-recv .....................: (1983.57, 2595.83)
    backward-send-forward-recv .....................: (244.09, 606.79)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.49)
    grads-reduce-scatter ...........................: (7.70, 8.92)
    params-all-gather ..............................: (4.17, 4.89)
    optimizer-copy-to-main-grad ....................: (0.28, 0.41)
    optimizer-clip-main-grad .......................: (1.49, 1.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.53, 5.25)
    optimizer-copy-main-to-model-params ............: (1.42, 1.65)
    optimizer ......................................: (9.03, 9.26)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 15627.9 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.087469E+00 | loss scale: 1.0 | grad norm: 0.915 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15209.13, 15590.01)
    forward-compute ................................: (4742.23, 5610.98)
    backward-compute ...............................: (7901.28, 9127.79)
    batch-generator ................................: (44.33, 58.40)
    forward-recv ...................................: (82.67, 229.80)
    forward-send ...................................: (2.38, 7.73)
    backward-recv ..................................: (104.51, 315.54)
    backward-send ..................................: (2.63, 7.72)
    forward-send-backward-recv .....................: (1984.46, 2246.64)
    backward-send-forward-recv .....................: (243.90, 299.18)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.75)
    grads-reduce-scatter ...........................: (7.75, 9.03)
    params-all-gather ..............................: (4.20, 4.88)
    optimizer-copy-to-main-grad ....................: (0.28, 0.41)
    optimizer-clip-main-grad .......................: (1.61, 1.69)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.53, 5.25)
    optimizer-copy-main-to-model-params ............: (1.42, 1.65)
    optimizer ......................................: (9.17, 9.39)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (465699): No such process
kill: (465705): No such process
kill: (465711): No such process
kill: (465717): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=2, pp=4, mbs=2
LOCAL_IP = 10.64.24.51
DP=2, MP=2, PP=4
[2024-02-11 23:26:30,180] torch.distributed.run: [WARNING] 
[2024-02-11 23:26:30,180] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 23:26:30,180] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-11 23:26:30,180] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 805617664
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 805617664
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 908910592
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 908910592
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.637 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.642 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.651 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.666 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.028 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.147 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.165 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.227 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (39.34, 576.90)
    train/valid/test-data-iterators-setup ..........: (0.02, 10195.38)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 17269.6 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 9.187567E+00 | loss scale: 1.0 | grad norm: 700.292 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 9] (after 10 iterations) memory (MB) | allocated: 9412.376953125 | max allocated: 21710.619140625 | reserved: 22914.0 | max reserved: 22914.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 9412.376953125 | max allocated: 21710.619140625 | reserved: 22918.0 | max reserved: 22918.0
[Rank 13] (after 10 iterations) memory (MB) | allocated: 10596.3544921875 | max allocated: 18588.0654296875 | reserved: 20190.0 | max reserved: 20190.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 10596.3544921875 | max allocated: 18588.0654296875 | reserved: 20210.0 | max reserved: 20210.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (16812.36, 17017.92)
    forward-compute ................................: (5126.78, 6215.17)
    backward-compute ...............................: (8510.37, 9703.76)
    batch-generator ................................: (192.25, 238.90)
    forward-recv ...................................: (204.48, 591.06)
    forward-send ...................................: (4.97, 278.12)
    backward-recv ..................................: (54.08, 162.02)
    backward-send ..................................: (1.38, 4.01)
    forward-send-backward-recv .....................: (2217.20, 2522.12)
    backward-send-forward-recv .....................: (276.85, 338.35)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.72)
    grads-reduce-scatter ...........................: (8.67, 215.82)
    params-all-gather ..............................: (4.20, 5.52)
    optimizer-copy-to-main-grad ....................: (0.29, 0.40)
    optimizer-clip-main-grad .......................: (5.83, 6.03)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.65, 5.51)
    optimizer-copy-main-to-model-params ............: (1.43, 1.77)
    optimizer ......................................: (14.13, 14.49)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 16251.6 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.553476E+00 | loss scale: 1.0 | grad norm: 7.115 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16007.28, 16212.70)
    forward-compute ................................: (4974.67, 5916.33)
    backward-compute ...............................: (8492.25, 9710.54)
    batch-generator ................................: (85.23, 109.90)
    forward-recv ...................................: (42.96, 120.38)
    forward-send ...................................: (1.27, 4.77)
    backward-recv ..................................: (54.67, 163.91)
    backward-send ..................................: (1.39, 3.98)
    forward-send-backward-recv .....................: (1937.79, 2230.72)
    backward-send-forward-recv .....................: (248.91, 307.93)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.72)
    grads-reduce-scatter ...........................: (7.75, 8.99)
    params-all-gather ..............................: (4.16, 4.86)
    optimizer-copy-to-main-grad ....................: (0.28, 0.37)
    optimizer-clip-main-grad .......................: (2.36, 2.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.26)
    optimizer-copy-main-to-model-params ............: (1.42, 1.64)
    optimizer ......................................: (10.06, 10.27)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 16245.9 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.410254E+00 | loss scale: 1.0 | grad norm: 2.294 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16002.03, 16207.20)
    forward-compute ................................: (4979.08, 5919.73)
    backward-compute ...............................: (8479.59, 9705.99)
    batch-generator ................................: (86.29, 117.05)
    forward-recv ...................................: (42.93, 120.46)
    forward-send ...................................: (1.27, 4.48)
    backward-recv ..................................: (54.93, 164.87)
    backward-send ..................................: (1.39, 4.00)
    forward-send-backward-recv .....................: (1937.93, 2231.66)
    backward-send-forward-recv .....................: (249.51, 304.54)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.52)
    grads-reduce-scatter ...........................: (7.75, 8.94)
    params-all-gather ..............................: (4.24, 4.85)
    optimizer-copy-to-main-grad ....................: (0.27, 0.39)
    optimizer-clip-main-grad .......................: (2.36, 2.56)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.24)
    optimizer-copy-main-to-model-params ............: (1.43, 1.64)
    optimizer ......................................: (10.08, 10.29)
Sun Feb 11 23:38:05 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   40C    P0             502W / 700W |  26390MiB / 81559MiB |     56%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   47C    P0             544W / 700W |  26386MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   48C    P0             479W / 700W |  26258MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0             458W / 700W |  26386MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   42C    P0             461W / 700W |  23636MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             420W / 700W |  23616MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             411W / 700W |  23500MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             479W / 700W |  23596MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 16591.4 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.147279E+00 | loss scale: 1.0 | grad norm: 1.147 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16260.19, 16465.17)
    forward-compute ................................: (4978.92, 5923.60)
    backward-compute ...............................: (8481.61, 9706.29)
    batch-generator ................................: (85.15, 115.20)
    forward-recv ...................................: (120.01, 356.68)
    forward-send ...................................: (1.27, 4.53)
    backward-recv ..................................: (54.46, 163.99)
    backward-send ..................................: (1.36, 3.99)
    forward-send-backward-recv .....................: (1919.51, 2214.44)
    backward-send-forward-recv .....................: (249.63, 540.55)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.71)
    grads-reduce-scatter ...........................: (7.70, 8.96)
    params-all-gather ..............................: (4.20, 4.87)
    optimizer-copy-to-main-grad ....................: (0.28, 0.37)
    optimizer-clip-main-grad .......................: (2.35, 2.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.25)
    optimizer-copy-main-to-model-params ............: (1.42, 1.64)
    optimizer ......................................: (9.99, 10.21)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 16229.7 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.281928E+00 | loss scale: 1.0 | grad norm: 1.579 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15985.05, 16190.34)
    forward-compute ................................: (4971.48, 5910.68)
    backward-compute ...............................: (8481.88, 9700.84)
    batch-generator ................................: (84.99, 113.32)
    forward-recv ...................................: (42.92, 120.23)
    forward-send ...................................: (1.27, 4.49)
    backward-recv ..................................: (54.80, 164.40)
    backward-send ..................................: (1.37, 3.99)
    forward-send-backward-recv .....................: (1923.36, 2220.20)
    backward-send-forward-recv .....................: (249.48, 306.15)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.64)
    grads-reduce-scatter ...........................: (7.59, 8.95)
    params-all-gather ..............................: (4.21, 4.87)
    optimizer-copy-to-main-grad ....................: (0.27, 0.38)
    optimizer-clip-main-grad .......................: (2.10, 2.26)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.54)
    optimizer-copy-main-to-model-params ............: (1.43, 1.64)
    optimizer ......................................: (10.03, 10.25)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 16227.2 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.153083E+00 | loss scale: 1.0 | grad norm: 1.454 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15983.47, 16188.58)
    forward-compute ................................: (4972.65, 5908.41)
    backward-compute ...............................: (8476.14, 9693.90)
    batch-generator ................................: (84.67, 116.85)
    forward-recv ...................................: (42.86, 120.30)
    forward-send ...................................: (1.27, 4.35)
    backward-recv ..................................: (54.50, 163.79)
    backward-send ..................................: (1.37, 4.00)
    forward-send-backward-recv .....................: (1924.69, 2223.35)
    backward-send-forward-recv .....................: (249.49, 305.79)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.61)
    grads-reduce-scatter ...........................: (7.62, 8.89)
    params-all-gather ..............................: (4.25, 4.85)
    optimizer-copy-to-main-grad ....................: (0.27, 0.37)
    optimizer-clip-main-grad .......................: (2.29, 2.47)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.24)
    optimizer-copy-main-to-model-params ............: (1.42, 1.64)
    optimizer ......................................: (9.91, 10.13)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 16560.2 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.141623E+00 | loss scale: 1.0 | grad norm: 1.115 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16315.97, 16520.87)
    forward-compute ................................: (4970.98, 6254.46)
    backward-compute ...............................: (8485.95, 9697.65)
    batch-generator ................................: (84.39, 119.05)
    forward-recv ...................................: (42.87, 120.26)
    forward-send ...................................: (1.27, 4.41)
    backward-recv ..................................: (54.31, 163.45)
    backward-send ..................................: (1.36, 3.98)
    forward-send-backward-recv .....................: (2255.32, 2549.78)
    backward-send-forward-recv .....................: (248.64, 305.39)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.59)
    grads-reduce-scatter ...........................: (7.76, 8.94)
    params-all-gather ..............................: (4.14, 4.88)
    optimizer-copy-to-main-grad ....................: (0.27, 0.37)
    optimizer-clip-main-grad .......................: (2.34, 2.54)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.23)
    optimizer-copy-main-to-model-params ............: (1.42, 2.25)
    optimizer ......................................: (9.99, 10.81)
Sun Feb 11 23:48:58 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   40C    P0             415W / 700W |  26390MiB / 81559MiB |     21%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   47C    P0             457W / 700W |  26386MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   48C    P0             422W / 700W |  26258MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0             459W / 700W |  26386MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   42C    P0             455W / 700W |  23636MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             435W / 700W |  23616MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             416W / 700W |  23500MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             408W / 700W |  23596MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 16299.9 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.160384E+00 | loss scale: 1.0 | grad norm: 1.337 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15967.15, 16172.04)
    forward-compute ................................: (4973.12, 5900.89)
    backward-compute ...............................: (8491.52, 9689.81)
    batch-generator ................................: (83.58, 106.96)
    forward-recv ...................................: (42.95, 120.25)
    forward-send ...................................: (1.27, 4.35)
    backward-recv ..................................: (54.37, 162.85)
    backward-send ..................................: (1.37, 3.99)
    forward-send-backward-recv .....................: (1908.90, 2193.29)
    backward-send-forward-recv .....................: (249.00, 307.35)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.65)
    grads-reduce-scatter ...........................: (7.66, 8.91)
    params-all-gather ..............................: (4.24, 4.86)
    optimizer-copy-to-main-grad ....................: (0.27, 0.37)
    optimizer-clip-main-grad .......................: (1.85, 1.97)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.24)
    optimizer-copy-main-to-model-params ............: (1.42, 1.64)
    optimizer ......................................: (9.41, 9.62)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 16209.6 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.125016E+00 | loss scale: 1.0 | grad norm: 0.839 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15966.30, 16171.45)
    forward-compute ................................: (4968.64, 5892.46)
    backward-compute ...............................: (8485.36, 9683.56)
    batch-generator ................................: (83.95, 106.52)
    forward-recv ...................................: (42.93, 120.21)
    forward-send ...................................: (1.27, 4.32)
    backward-recv ..................................: (54.47, 163.73)
    backward-send ..................................: (1.37, 3.98)
    forward-send-backward-recv .....................: (1906.17, 2202.14)
    backward-send-forward-recv .....................: (248.79, 305.43)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.60)
    grads-reduce-scatter ...........................: (7.81, 8.93)
    params-all-gather ..............................: (4.15, 4.82)
    optimizer-copy-to-main-grad ....................: (0.27, 0.37)
    optimizer-clip-main-grad .......................: (1.85, 1.97)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.24)
    optimizer-copy-main-to-model-params ............: (1.42, 1.64)
    optimizer ......................................: (9.46, 9.67)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 16202.9 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.091547E+00 | loss scale: 1.0 | grad norm: 1.295 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15959.42, 16164.54)
    forward-compute ................................: (4969.40, 5887.20)
    backward-compute ...............................: (8486.46, 9683.37)
    batch-generator ................................: (84.81, 106.73)
    forward-recv ...................................: (42.96, 120.27)
    forward-send ...................................: (1.26, 4.43)
    backward-recv ..................................: (54.19, 162.53)
    backward-send ..................................: (1.38, 3.96)
    forward-send-backward-recv .....................: (1905.28, 2195.44)
    backward-send-forward-recv .....................: (249.38, 307.79)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.72)
    grads-reduce-scatter ...........................: (7.80, 8.97)
    params-all-gather ..............................: (4.17, 4.87)
    optimizer-copy-to-main-grad ....................: (0.27, 0.37)
    optimizer-clip-main-grad .......................: (1.97, 2.11)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.24)
    optimizer-copy-main-to-model-params ............: (1.42, 1.64)
    optimizer ......................................: (9.55, 9.77)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (467493): No such process
kill: (467499): No such process
kill: (467505): No such process
kill: (467511): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=2, pp=4, mbs=1
LOCAL_IP = 10.64.24.51
DP=2, MP=2, PP=4
[2024-02-11 23:56:38,752] torch.distributed.run: [WARNING] 
[2024-02-11 23:56:38,752] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 23:56:38,752] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-11 23:56:38,752] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 805617664
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 805617664
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 908910592
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 908910592
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...

 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.735 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.739 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.773 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.993 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.016 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.021 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.238 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.246 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (36.35, 591.20)
    train/valid/test-data-iterators-setup ..........: (0.02, 10536.93)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 18336.7 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 9.188361E+00 | loss scale: 1.0 | grad norm: 700.310 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 9] (after 10 iterations) memory (MB) | allocated: 9348.376953125 | max allocated: 15241.47509765625 | reserved: 15806.0 | max reserved: 15806.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 9348.376953125 | max allocated: 15241.47509765625 | reserved: 15906.0 | max reserved: 15906.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 10532.3544921875 | max allocated: 14400.24560546875 | reserved: 15280.0 | max reserved: 15280.0
[Rank 13] (after 10 iterations) memory (MB) | allocated: 10532.3544921875 | max allocated: 14400.24560546875 | reserved: 15296.0 | max reserved: 15296.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (17970.94, 18083.15)
    forward-compute ................................: (5414.09, 6463.80)
    backward-compute ...............................: (9252.73, 10440.43)
    batch-generator ................................: (287.15, 509.72)
    forward-recv ...................................: (374.09, 717.30)
    forward-send ...................................: (4.70, 286.84)
    backward-recv ..................................: (27.94, 82.78)
    backward-send ..................................: (0.67, 1.92)
    forward-send-backward-recv .....................: (1960.09, 2424.63)
    backward-send-forward-recv .....................: (282.22, 368.53)
    layernorm-grads-all-reduce .....................: (0.02, 0.04)
    embedding-grads-all-reduce .....................: (0.02, 9.63)
    grads-reduce-scatter ...........................: (8.54, 218.23)
    params-all-gather ..............................: (4.22, 4.84)
    optimizer-copy-to-main-grad ....................: (0.30, 0.39)
    optimizer-clip-main-grad .......................: (5.31, 5.52)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.65, 5.56)
    optimizer-copy-main-to-model-params ............: (1.43, 1.63)
    optimizer ......................................: (13.71, 13.93)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 17093.8 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.553365E+00 | loss scale: 1.0 | grad norm: 7.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16941.67, 17053.90)
    forward-compute ................................: (5226.29, 6125.39)
    backward-compute ...............................: (9240.49, 10417.94)
    batch-generator ................................: (166.47, 219.01)
    forward-recv ...................................: (22.54, 64.27)
    forward-send ...................................: (0.72, 2.76)
    backward-recv ..................................: (27.71, 82.34)
    backward-send ..................................: (0.67, 1.89)
    forward-send-backward-recv .....................: (1616.44, 2137.70)
    backward-send-forward-recv .....................: (262.72, 324.38)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.68)
    grads-reduce-scatter ...........................: (7.71, 8.97)
    params-all-gather ..............................: (4.23, 4.91)
    optimizer-copy-to-main-grad ....................: (0.29, 0.43)
    optimizer-clip-main-grad .......................: (2.38, 2.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.53, 5.32)
    optimizer-copy-main-to-model-params ............: (1.43, 1.63)
    optimizer ......................................: (10.22, 10.43)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 17080.4 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.409992E+00 | loss scale: 1.0 | grad norm: 2.305 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16928.90, 17040.96)
    forward-compute ................................: (5222.72, 6126.11)
    backward-compute ...............................: (9233.54, 10403.16)
    batch-generator ................................: (166.76, 214.05)
    forward-recv ...................................: (22.60, 63.72)
    forward-send ...................................: (0.72, 2.97)
    backward-recv ..................................: (27.68, 82.99)
    backward-send ..................................: (0.67, 1.88)
    forward-send-backward-recv .....................: (1610.06, 2135.89)
    backward-send-forward-recv .....................: (261.79, 326.54)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.45)
    grads-reduce-scatter ...........................: (7.71, 8.87)
    params-all-gather ..............................: (4.17, 4.86)
    optimizer-copy-to-main-grad ....................: (0.30, 0.42)
    optimizer-clip-main-grad .......................: (2.37, 2.61)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.32)
    optimizer-copy-main-to-model-params ............: (1.43, 1.63)
    optimizer ......................................: (10.17, 10.38)
Mon Feb 12 00:08:54 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   40C    P0             455W / 700W |  19248MiB / 81559MiB |     77%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   47C    P0             475W / 700W |  19280MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   48C    P0             484W / 700W |  19280MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0             468W / 700W |  19408MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   42C    P0             542W / 700W |  18706MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   49C    P0             495W / 700W |  18722MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             484W / 700W |  18682MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             465W / 700W |  18698MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 17752.1 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.147214E+00 | loss scale: 1.0 | grad norm: 1.137 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (17512.50, 17624.56)
    forward-compute ................................: (5222.81, 6719.32)
    backward-compute ...............................: (9209.15, 10695.23)
    batch-generator ................................: (166.53, 212.11)
    forward-recv ...................................: (22.73, 63.89)
    forward-send ...................................: (0.72, 2.84)
    backward-recv ..................................: (27.48, 82.37)
    backward-send ..................................: (0.67, 1.89)
    forward-send-backward-recv .....................: (2172.84, 2724.82)
    backward-send-forward-recv .....................: (261.98, 325.72)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.70)
    grads-reduce-scatter ...........................: (7.71, 8.94)
    params-all-gather ..............................: (4.21, 4.86)
    optimizer-copy-to-main-grad ....................: (0.27, 0.40)
    optimizer-clip-main-grad .......................: (2.38, 2.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.30)
    optimizer-copy-main-to-model-params ............: (1.43, 1.67)
    optimizer ......................................: (10.12, 10.38)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 17062.5 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.281791E+00 | loss scale: 1.0 | grad norm: 1.451 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16911.17, 17023.34)
    forward-compute ................................: (5220.61, 6119.72)
    backward-compute ...............................: (9199.41, 10397.37)
    batch-generator ................................: (166.28, 212.97)
    forward-recv ...................................: (22.54, 63.89)
    forward-send ...................................: (0.73, 3.28)
    backward-recv ..................................: (27.59, 82.61)
    backward-send ..................................: (0.67, 1.89)
    forward-send-backward-recv .....................: (1552.75, 2128.58)
    backward-send-forward-recv .....................: (261.22, 325.34)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.46)
    grads-reduce-scatter ...........................: (7.77, 8.94)
    params-all-gather ..............................: (4.21, 4.87)
    optimizer-copy-to-main-grad ....................: (0.27, 0.41)
    optimizer-clip-main-grad .......................: (2.25, 2.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.25)
    optimizer-copy-main-to-model-params ............: (1.43, 1.63)
    optimizer ......................................: (9.93, 10.14)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 17060.0 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.152493E+00 | loss scale: 1.0 | grad norm: 1.243 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16908.22, 17020.38)
    forward-compute ................................: (5223.43, 6120.34)
    backward-compute ...............................: (9192.99, 10392.01)
    batch-generator ................................: (167.32, 212.45)
    forward-recv ...................................: (22.55, 63.63)
    forward-send ...................................: (0.73, 2.86)
    backward-recv ..................................: (27.92, 82.80)
    backward-send ..................................: (0.67, 1.88)
    forward-send-backward-recv .....................: (1569.83, 2130.65)
    backward-send-forward-recv .....................: (260.95, 328.37)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.71)
    grads-reduce-scatter ...........................: (7.62, 8.88)
    params-all-gather ..............................: (4.22, 4.86)
    optimizer-copy-to-main-grad ....................: (0.27, 0.41)
    optimizer-clip-main-grad .......................: (2.23, 2.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.26)
    optimizer-copy-main-to-model-params ............: (1.43, 1.63)
    optimizer ......................................: (9.93, 10.14)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 17057.6 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.140114E+00 | loss scale: 1.0 | grad norm: 1.157 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16905.67, 17018.03)
    forward-compute ................................: (5226.16, 6116.98)
    backward-compute ...............................: (9233.63, 10392.16)
    batch-generator ................................: (166.78, 215.04)
    forward-recv ...................................: (22.71, 63.70)
    forward-send ...................................: (0.72, 2.68)
    backward-recv ..................................: (27.58, 81.61)
    backward-send ..................................: (0.67, 1.89)
    forward-send-backward-recv .....................: (1604.82, 2101.64)
    backward-send-forward-recv .....................: (261.79, 326.71)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.43)
    grads-reduce-scatter ...........................: (7.68, 8.91)
    params-all-gather ..............................: (4.20, 4.85)
    optimizer-copy-to-main-grad ....................: (0.27, 0.40)
    optimizer-clip-main-grad .......................: (2.23, 2.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.25)
    optimizer-copy-main-to-model-params ............: (1.43, 1.63)
    optimizer ......................................: (9.92, 10.13)
Mon Feb 12 00:20:17 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   40C    P0             517W / 700W |  19248MiB / 81559MiB |     90%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   48C    P0             478W / 700W |  19280MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   49C    P0             483W / 700W |  19280MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0             465W / 700W |  19408MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   43C    P0             485W / 700W |  18706MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   50C    P0             464W / 700W |  18722MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             491W / 700W |  18682MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             491W / 700W |  18698MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 17144.3 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.160472E+00 | loss scale: 1.0 | grad norm: 1.618 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16905.45, 17017.56)
    forward-compute ................................: (5221.35, 6115.30)
    backward-compute ...............................: (9230.30, 10394.07)
    batch-generator ................................: (164.14, 213.56)
    forward-recv ...................................: (22.71, 63.86)
    forward-send ...................................: (0.72, 2.75)
    backward-recv ..................................: (27.62, 81.66)
    backward-send ..................................: (0.67, 1.99)
    forward-send-backward-recv .....................: (1607.05, 2117.67)
    backward-send-forward-recv .....................: (261.10, 328.11)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.68)
    grads-reduce-scatter ...........................: (7.74, 9.02)
    params-all-gather ..............................: (4.17, 4.87)
    optimizer-copy-to-main-grad ....................: (0.27, 0.41)
    optimizer-clip-main-grad .......................: (1.99, 2.14)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.53, 5.29)
    optimizer-copy-main-to-model-params ............: (1.43, 1.63)
    optimizer ......................................: (9.67, 9.88)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 17045.6 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.122391E+00 | loss scale: 1.0 | grad norm: 1.103 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16894.57, 17006.78)
    forward-compute ................................: (5223.37, 6111.56)
    backward-compute ...............................: (9242.35, 10387.34)
    batch-generator ................................: (164.95, 211.46)
    forward-recv ...................................: (22.54, 63.70)
    forward-send ...................................: (0.73, 2.66)
    backward-recv ..................................: (27.35, 81.85)
    backward-send ..................................: (0.67, 2.02)
    forward-send-backward-recv .....................: (1605.73, 2096.59)
    backward-send-forward-recv .....................: (261.34, 326.04)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.47)
    grads-reduce-scatter ...........................: (7.59, 8.93)
    params-all-gather ..............................: (4.17, 4.85)
    optimizer-copy-to-main-grad ....................: (0.27, 0.40)
    optimizer-clip-main-grad .......................: (1.49, 1.56)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.27)
    optimizer-copy-main-to-model-params ............: (1.43, 1.63)
    optimizer ......................................: (9.06, 9.26)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 17961.4 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.086411E+00 | loss scale: 1.0 | grad norm: 1.726 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (17810.03, 17922.07)
    forward-compute ................................: (5510.00, 6200.91)
    backward-compute ...............................: (9242.32, 10381.81)
    batch-generator ................................: (165.51, 204.58)
    forward-recv ...................................: (22.59, 337.53)
    forward-send ...................................: (0.72, 297.14)
    backward-recv ..................................: (27.81, 81.39)
    backward-send ..................................: (0.67, 1.91)
    forward-send-backward-recv .....................: (1888.96, 2459.21)
    backward-send-forward-recv .....................: (525.40, 1169.67)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.78)
    grads-reduce-scatter ...........................: (7.67, 9.02)
    params-all-gather ..............................: (4.23, 4.85)
    optimizer-copy-to-main-grad ....................: (0.28, 0.37)
    optimizer-clip-main-grad .......................: (1.63, 1.71)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.25)
    optimizer-copy-main-to-model-params ............: (1.43, 2.15)
    optimizer ......................................: (9.19, 9.92)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (468985): No such process
kill: (468991): No such process
kill: (468997): No such process
kill: (469003): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=8, pp=1, mbs=4
LOCAL_IP = 10.64.24.51
DP=2, MP=8, PP=1
[2024-02-12 00:28:23,423] torch.distributed.run: [WARNING] 
[2024-02-12 00:28:23,423] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 00:28:23,423] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 00:28:23,423] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.183 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.004 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (371.60, 410.52)
    train/valid/test-data-iterators-setup ..........: (0.02, 13450.72)
NCCL version 2.19.3+cuda12.2
benchmark/test_padding.sh.two.bak: line 138: 406688 Killed                  torchrun $DISTRIBUTED_ARGS pretrain_gpt.py $GPT_ARGS $DATA_ARGS $OUTPUT_ARGS --distributed-backend nccl
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (469919): No such process
kill: (469937): No such process
kill: (469955): No such process
kill: (469973): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=8, pp=1, mbs=2
LOCAL_IP = 10.64.24.51
DP=2, MP=8, PP=1
[2024-02-12 00:30:28,195] torch.distributed.run: [WARNING] 
[2024-02-12 00:30:28,195] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 00:30:28,195] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 00:30:28,195] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.106 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.898 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (380.79, 450.75)
    train/valid/test-data-iterators-setup ..........: (0.02, 10602.07)
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 27403.5 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 8.191365E+00 | loss scale: 1.0 | grad norm: 689.532 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (27322.22, 27324.01)
    forward-compute ................................: (12350.64, 12443.25)
    backward-compute ...............................: (14808.11, 14902.64)
    batch-generator ................................: (749.11, 819.72)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (38.46, 38.91)
    params-all-gather ..............................: (20.69, 21.05)
    optimizer-copy-to-main-grad ....................: (1.13, 1.37)
    optimizer-clip-main-grad .......................: (5.82, 5.83)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.21, 5.33)
    optimizer-copy-main-to-model-params ............: (2.09, 2.21)
    optimizer ......................................: (15.36, 15.48)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 26609.1 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.559623E+00 | loss scale: 1.0 | grad norm: 10.353 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (26531.47, 26533.23)
    forward-compute ................................: (11573.62, 11663.16)
    backward-compute ...............................: (14797.23, 14892.81)
    batch-generator ................................: (119.65, 186.94)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (38.19, 38.65)
    params-all-gather ..............................: (20.78, 21.07)
    optimizer-copy-to-main-grad ....................: (1.06, 1.39)
    optimizer-clip-main-grad .......................: (2.91, 2.93)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.95, 5.02)
    optimizer-copy-main-to-model-params ............: (2.09, 2.21)
    optimizer ......................................: (11.90, 12.02)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 26628.6 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.402935E+00 | loss scale: 1.0 | grad norm: 4.319 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (26549.18, 26551.01)
    forward-compute ................................: (11576.56, 11681.40)
    backward-compute ...............................: (14798.91, 14907.64)
    batch-generator ................................: (118.77, 188.66)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (38.07, 38.69)
    params-all-gather ..............................: (20.76, 21.03)
    optimizer-copy-to-main-grad ....................: (1.07, 1.36)
    optimizer-clip-main-grad .......................: (2.94, 4.07)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.95, 5.02)
    optimizer-copy-main-to-model-params ............: (2.09, 2.21)
    optimizer ......................................: (13.16, 13.28)
Mon Feb 12 00:48:59 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   39C    P0             466W / 700W |  29968MiB / 81559MiB |     41%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   46C    P0             507W / 700W |  30700MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   47C    P0             493W / 700W |  30628MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   40C    P0             454W / 700W |  30628MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   40C    P0             485W / 700W |  30592MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   46C    P0             479W / 700W |  30568MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   44C    P0             399W / 700W |  30524MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   40C    P0             445W / 700W |  30368MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 27247.7 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.141592E+00 | loss scale: 1.0 | grad norm: 1.702 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (27078.38, 27079.93)
    forward-compute ................................: (12108.43, 12208.92)
    backward-compute ...............................: (14797.03, 14904.30)
    batch-generator ................................: (120.93, 187.86)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.10, 39.03)
    params-all-gather ..............................: (20.84, 21.05)
    optimizer-copy-to-main-grad ....................: (1.05, 1.39)
    optimizer-clip-main-grad .......................: (2.94, 2.96)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.95, 5.01)
    optimizer-copy-main-to-model-params ............: (2.09, 2.21)
    optimizer ......................................: (11.94, 12.06)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 26627.6 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.283349E+00 | loss scale: 1.0 | grad norm: 1.368 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (26548.95, 26550.07)
    forward-compute ................................: (11580.54, 11681.65)
    backward-compute ...............................: (14794.02, 14901.32)
    batch-generator ................................: (119.24, 187.96)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.01, 38.81)
    params-all-gather ..............................: (20.81, 20.98)
    optimizer-copy-to-main-grad ....................: (1.06, 1.38)
    optimizer-clip-main-grad .......................: (2.95, 2.98)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.95, 5.02)
    optimizer-copy-main-to-model-params ............: (2.09, 2.24)
    optimizer ......................................: (12.12, 12.28)
benchmark/test_padding.sh.two.bak: line 138: 407459 Killed                  torchrun $DISTRIBUTED_ARGS pretrain_gpt.py $GPT_ARGS $DATA_ARGS $OUTPUT_ARGS --distributed-backend nccl
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (470898): No such process
kill: (470916): No such process
kill: (470934): No such process
kill: (470952): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=8, pp=1, mbs=1
LOCAL_IP = 10.64.24.51
DP=2, MP=8, PP=1
[2024-02-12 00:56:46,667] torch.distributed.run: [WARNING] 
[2024-02-12 00:56:46,667] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 00:56:46,667] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 00:56:46,667] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  6.974 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.164 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (371.54, 462.74)
    train/valid/test-data-iterators-setup ..........: (0.02, 12487.38)
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 31740.7 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 8.194667E+00 | loss scale: 1.0 | grad norm: 689.664 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (31658.94, 31659.43)
    forward-compute ................................: (14092.23, 14115.96)
    backward-compute ...............................: (17385.89, 17430.17)
    batch-generator ................................: (860.97, 924.11)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (38.60, 39.02)
    params-all-gather ..............................: (20.89, 21.02)
    optimizer-copy-to-main-grad ....................: (1.12, 1.37)
    optimizer-clip-main-grad .......................: (5.86, 5.88)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.18, 5.42)
    optimizer-copy-main-to-model-params ............: (2.09, 2.23)
    optimizer ......................................: (15.46, 15.61)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 31136.7 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.559050E+00 | loss scale: 1.0 | grad norm: 10.340 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (31059.66, 31059.93)
    forward-compute ................................: (13503.18, 13532.12)
    backward-compute ...............................: (17391.54, 17422.24)
    batch-generator ................................: (231.45, 272.51)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.37, 38.72)
    params-all-gather ..............................: (20.81, 21.02)
    optimizer-copy-to-main-grad ....................: (1.05, 1.27)
    optimizer-clip-main-grad .......................: (2.93, 2.95)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.94, 5.02)
    optimizer-copy-main-to-model-params ............: (2.09, 2.24)
    optimizer ......................................: (11.93, 12.08)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 31151.2 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.402930E+00 | loss scale: 1.0 | grad norm: 4.340 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (31072.94, 31074.36)
    forward-compute ................................: (13511.45, 13572.01)
    backward-compute ...............................: (17369.38, 17425.76)
    batch-generator ................................: (225.64, 272.36)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.27, 38.78)
    params-all-gather ..............................: (20.83, 21.04)
    optimizer-copy-to-main-grad ....................: (1.08, 1.24)
    optimizer-clip-main-grad .......................: (2.93, 2.99)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.94, 5.02)
    optimizer-copy-main-to-model-params ............: (2.09, 2.21)
    optimizer ......................................: (12.02, 12.18)
benchmark/test_padding.sh.two.bak: line 138: 408239 Killed                  torchrun $DISTRIBUTED_ARGS pretrain_gpt.py $GPT_ARGS $DATA_ARGS $OUTPUT_ARGS --distributed-backend nccl
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (471868): No such process
kill: (471886): No such process
kill: (471904): No such process
kill: (471922): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=1, pp=8, mbs=2
LOCAL_IP = 10.64.24.51
DP=2, MP=1, PP=8
[2024-02-12 01:16:40,659] torch.distributed.run: [WARNING] 
[2024-02-12 01:16:40,659] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 01:16:40,659] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 01:16:40,659] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 805519360
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 805519360
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 805519360
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 1011572736
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.712 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.784 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.785 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.796 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.847 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.865 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.926 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.037 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.040 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.185 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.163 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.219 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.184 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.237 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.208 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.295 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (30.48, 437.67)
    train/valid/test-data-iterators-setup ..........: (10111.39, 11420.36)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 16829.5 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 8.400044E+00 | loss scale: 1.0 | grad norm: 689.530 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 10] (after 10 iterations) memory (MB) | allocated: 9410.439453125 | max allocated: 23855.951171875 | reserved: 24994.0 | max reserved: 24994.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 9410.439453125 | max allocated: 19179.447265625 | reserved: 20194.0 | max reserved: 20194.0[Rank 8] (after 10 iterations) memory (MB) | allocated: 9410.439453125 | max allocated: 28532.455078125 | reserved: 29670.0 | max reserved: 29670.0

[Rank 14] (after 10 iterations) memory (MB) | allocated: 11770.4248046875 | max allocated: 20441.6318359375 | reserved: 22964.0 | max reserved: 22964.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (16144.94, 16555.51)
    forward-compute ................................: (4057.59, 5974.41)
    backward-compute ...............................: (7237.06, 9490.58)
    batch-generator ................................: (103.89, 176.07)
    forward-recv ...................................: (156.84, 571.09)
    forward-send ...................................: (3.60, 217.27)
    backward-recv ..................................: (60.80, 420.76)
    backward-send ..................................: (1.52, 10.11)
    forward-send-backward-recv .....................: (3759.86, 4083.29)
    backward-send-forward-recv .....................: (267.98, 355.57)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.19)
    grads-reduce-scatter ...........................: (9.83, 228.45)
    params-all-gather ..............................: (4.06, 5.17)
    optimizer-copy-to-main-grad ....................: (0.16, 0.25)
    optimizer-clip-main-grad .......................: (5.49, 5.83)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.62, 5.91)
    optimizer-copy-main-to-model-params ............: (1.32, 1.70)
    optimizer ......................................: (14.16, 14.57)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 16068.4 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.574514E+00 | loss scale: 1.0 | grad norm: 5.943 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15609.41, 16018.64)
    forward-compute ................................: (3985.54, 5766.75)
    backward-compute ...............................: (7214.81, 9438.96)
    batch-generator ................................: (89.23, 100.86)
    forward-recv ...................................: (40.04, 221.34)
    forward-send ...................................: (1.30, 12.51)
    backward-recv ..................................: (60.50, 416.68)
    backward-send ..................................: (1.54, 10.09)
    forward-send-backward-recv .....................: (3635.59, 3985.67)
    backward-send-forward-recv .....................: (251.85, 320.94)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.22)
    grads-reduce-scatter ...........................: (7.75, 10.01)
    params-all-gather ..............................: (4.09, 5.23)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (2.36, 2.71)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.75)
    optimizer-copy-main-to-model-params ............: (1.31, 1.68)
    optimizer ......................................: (10.39, 10.76)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 16074.3 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.405660E+00 | loss scale: 1.0 | grad norm: 2.227 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15617.21, 16025.06)
    forward-compute ................................: (3987.76, 5745.63)
    backward-compute ...............................: (7208.76, 9440.90)
    batch-generator ................................: (89.90, 109.65)
    forward-recv ...................................: (39.89, 221.49)
    forward-send ...................................: (1.32, 13.65)
    backward-recv ..................................: (60.66, 415.76)
    backward-send ..................................: (1.53, 10.41)
    forward-send-backward-recv .....................: (3656.39, 3989.26)
    backward-send-forward-recv .....................: (251.47, 322.85)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.09)
    grads-reduce-scatter ...........................: (7.60, 10.00)
    params-all-gather ..............................: (4.09, 5.15)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (2.47, 2.83)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.76)
    optimizer-copy-main-to-model-params ............: (1.31, 1.68)
    optimizer ......................................: (10.60, 10.96)
Mon Feb 12 01:28:05 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   39C    P0             353W / 700W |  32530MiB / 81559MiB |     79%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   46C    P0             374W / 700W |  32530MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   47C    P0             391W / 700W |  27846MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   40C    P0             317W / 700W |  27846MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   40C    P0             242W / 700W |  23046MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   46C    P0             352W / 700W |  23046MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             333W / 700W |  25910MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             297W / 700W |  25872MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 16154.3 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.143656E+00 | loss scale: 1.0 | grad norm: 1.211 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15606.06, 16015.44)
    forward-compute ................................: (3979.56, 5713.03)
    backward-compute ...............................: (7216.71, 9429.50)
    batch-generator ................................: (89.69, 105.14)
    forward-recv ...................................: (40.06, 221.56)
    forward-send ...................................: (1.33, 12.17)
    backward-recv ..................................: (60.94, 417.29)
    backward-send ..................................: (1.54, 10.43)
    forward-send-backward-recv .....................: (3631.84, 3981.03)
    backward-send-forward-recv .....................: (250.79, 319.18)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.34)
    grads-reduce-scatter ...........................: (7.71, 10.08)
    params-all-gather ..............................: (4.07, 5.19)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (2.35, 2.70)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.76)
    optimizer-copy-main-to-model-params ............: (1.31, 1.69)
    optimizer ......................................: (10.41, 11.36)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 16076.7 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.281481E+00 | loss scale: 1.0 | grad norm: 1.446 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15619.30, 16027.76)
    forward-compute ................................: (3985.92, 5717.32)
    backward-compute ...............................: (7215.72, 9434.01)
    batch-generator ................................: (90.17, 104.64)
    forward-recv ...................................: (39.74, 221.19)
    forward-send ...................................: (1.31, 12.57)
    backward-recv ..................................: (60.85, 417.64)
    backward-send ..................................: (1.53, 10.39)
    forward-send-backward-recv .....................: (3649.72, 3987.21)
    backward-send-forward-recv .....................: (251.72, 320.00)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.04)
    grads-reduce-scatter ...........................: (7.69, 9.98)
    params-all-gather ..............................: (4.02, 5.15)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (2.11, 2.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.75)
    optimizer-copy-main-to-model-params ............: (1.31, 1.68)
    optimizer ......................................: (10.07, 10.43)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 16084.3 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.152405E+00 | loss scale: 1.0 | grad norm: 1.722 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15626.04, 16034.62)
    forward-compute ................................: (3984.22, 5723.04)
    backward-compute ...............................: (7209.85, 9444.38)
    batch-generator ................................: (90.77, 100.38)
    forward-recv ...................................: (39.92, 221.19)
    forward-send ...................................: (1.30, 12.21)
    backward-recv ..................................: (60.81, 419.77)
    backward-send ..................................: (1.51, 10.09)
    forward-send-backward-recv .....................: (3651.52, 4000.35)
    backward-send-forward-recv .....................: (250.74, 319.60)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.15)
    grads-reduce-scatter ...........................: (7.68, 9.77)
    params-all-gather ..............................: (4.07, 5.12)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (1.98, 2.23)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.74)
    optimizer-copy-main-to-model-params ............: (1.31, 1.68)
    optimizer ......................................: (10.42, 10.79)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 16097.9 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.140520E+00 | loss scale: 1.0 | grad norm: 1.195 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15640.41, 16048.18)
    forward-compute ................................: (3994.46, 5757.82)
    backward-compute ...............................: (7203.92, 9457.78)
    batch-generator ................................: (91.21, 99.40)
    forward-recv ...................................: (39.89, 221.31)
    forward-send ...................................: (1.29, 12.68)
    backward-recv ..................................: (60.89, 420.83)
    backward-send ..................................: (1.53, 10.04)
    forward-send-backward-recv .....................: (3674.69, 4009.24)
    backward-send-forward-recv .....................: (251.22, 317.81)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.17)
    grads-reduce-scatter ...........................: (7.64, 10.00)
    params-all-gather ..............................: (4.05, 5.14)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (2.23, 2.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.74)
    optimizer-copy-main-to-model-params ............: (1.31, 1.68)
    optimizer ......................................: (10.26, 10.63)
Mon Feb 12 01:38:49 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   39C    P0             366W / 700W |  32530MiB / 81559MiB |     81%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   46C    P0             336W / 700W |  32530MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   47C    P0             411W / 700W |  27846MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   40C    P0             345W / 700W |  27846MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   40C    P0             269W / 700W |  23046MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   46C    P0             258W / 700W |  23046MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             300W / 700W |  25910MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             303W / 700W |  25872MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 16183.3 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.163010E+00 | loss scale: 1.0 | grad norm: 0.646 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (15636.78, 16044.45)
    forward-compute ................................: (4009.85, 5744.81)
    backward-compute ...............................: (7210.77, 9416.80)
    batch-generator ................................: (91.97, 99.04)
    forward-recv ...................................: (39.77, 221.50)
    forward-send ...................................: (1.29, 13.41)
    backward-recv ..................................: (60.34, 420.54)
    backward-send ..................................: (1.53, 10.08)
    forward-send-backward-recv .....................: (3701.28, 3980.86)
    backward-send-forward-recv .....................: (250.26, 317.74)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.19)
    grads-reduce-scatter ...........................: (7.73, 10.04)
    params-all-gather ..............................: (4.00, 5.17)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (2.10, 2.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.75)
    optimizer-copy-main-to-model-params ............: (1.31, 1.68)
    optimizer ......................................: (10.07, 10.44)
benchmark/test_padding.sh.two.bak: line 138: 409312 Killed                  torchrun $DISTRIBUTED_ARGS pretrain_gpt.py $GPT_ARGS $DATA_ARGS $OUTPUT_ARGS --distributed-backend nccl
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (474248): No such process
kill: (474266): No such process
kill: (474284): No such process
kill: (474302): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=1, pp=8, mbs=1
LOCAL_IP = 10.64.24.51
DP=2, MP=1, PP=8
[2024-02-12 01:40:18,900] torch.distributed.run: [WARNING] 
[2024-02-12 01:40:18,900] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 01:40:18,900] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 01:40:18,900] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 805519360
 > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 805519360
 > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 805519360
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 1011572736
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...


 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.644 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.656 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.790 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.842 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.842 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.870 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.941 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.967 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.184 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.185 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.087 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.106 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.225 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.238 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.227 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.280 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (31.47, 422.68)
    train/valid/test-data-iterators-setup ..........: (10089.63, 10742.39)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 17268.5 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 8.401793E+00 | loss scale: 1.0 | grad norm: 689.542 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 12] (after 10 iterations) memory (MB) | allocated: 9346.439453125 | max allocated: 14038.947265625 | reserved: 14492.0 | max reserved: 14492.0[Rank 8] (after 10 iterations) memory (MB) | allocated: 9346.439453125 | max allocated: 18459.455078125 | reserved: 18912.0 | max reserved: 18912.0

[Rank 10] (after 10 iterations) memory (MB) | allocated: 9346.439453125 | max allocated: 16249.201171875 | reserved: 16838.0 | max reserved: 16838.0
[Rank 14] (after 10 iterations) memory (MB) | allocated: 11706.4248046875 | max allocated: 15914.06201171875 | reserved: 16760.0 | max reserved: 16760.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (16786.34, 17004.82)
    forward-compute ................................: (4123.18, 6202.31)
    backward-compute ...............................: (7717.74, 9967.54)
    batch-generator ................................: (188.23, 211.75)
    forward-recv ...................................: (72.74, 382.97)
    forward-send ...................................: (5.25, 204.19)
    backward-recv ..................................: (31.49, 214.24)
    backward-send ..................................: (0.71, 4.84)
    forward-send-backward-recv .....................: (3975.79, 4322.15)
    backward-send-forward-recv .....................: (271.31, 357.66)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.35)
    grads-reduce-scatter ...........................: (9.83, 216.52)
    params-all-gather ..............................: (4.11, 5.18)
    optimizer-copy-to-main-grad ....................: (0.18, 0.25)
    optimizer-clip-main-grad .......................: (5.52, 5.87)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.63, 5.88)
    optimizer-copy-main-to-model-params ............: (1.32, 1.67)
    optimizer ......................................: (15.13, 15.54)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 16568.7 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.574442E+00 | loss scale: 1.0 | grad norm: 5.867 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16299.62, 16517.31)
    forward-compute ................................: (4069.76, 6032.96)
    backward-compute ...............................: (7652.69, 9958.04)
    batch-generator ................................: (170.26, 190.01)
    forward-recv ...................................: (20.42, 116.09)
    forward-send ...................................: (0.75, 8.01)
    backward-recv ..................................: (31.71, 214.35)
    backward-send ..................................: (0.71, 4.81)
    forward-send-backward-recv .....................: (3751.30, 4199.69)
    backward-send-forward-recv .....................: (255.68, 313.54)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.43)
    grads-reduce-scatter ...........................: (7.71, 9.85)
    params-all-gather ..............................: (4.09, 5.16)
    optimizer-copy-to-main-grad ....................: (0.15, 0.24)
    optimizer-clip-main-grad .......................: (2.36, 2.71)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.75)
    optimizer-copy-main-to-model-params ............: (1.32, 1.67)
    optimizer ......................................: (10.54, 10.89)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 16545.2 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.405567E+00 | loss scale: 1.0 | grad norm: 2.235 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16276.74, 16493.98)
    forward-compute ................................: (4069.81, 6011.14)
    backward-compute ...............................: (7659.44, 9949.25)
    batch-generator ................................: (169.97, 195.11)
    forward-recv ...................................: (20.40, 116.10)
    forward-send ...................................: (0.75, 7.91)
    backward-recv ..................................: (31.77, 213.37)
    backward-send ..................................: (0.71, 4.82)
    forward-send-backward-recv .....................: (3750.27, 4167.99)
    backward-send-forward-recv .....................: (256.96, 312.43)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.03)
    grads-reduce-scatter ...........................: (7.71, 9.88)
    params-all-gather ..............................: (4.07, 5.41)
    optimizer-copy-to-main-grad ....................: (0.15, 0.25)
    optimizer-clip-main-grad .......................: (2.35, 2.70)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.74)
    optimizer-copy-main-to-model-params ............: (1.32, 1.66)
    optimizer ......................................: (10.38, 10.72)
Mon Feb 12 01:52:01 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   40C    P0             433W / 700W |  21772MiB / 81559MiB |     30%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   47C    P0             463W / 700W |  21772MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   48C    P0             465W / 700W |  19690MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0             410W / 700W |  19690MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   40C    P0             355W / 700W |  17344MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   47C    P0             362W / 700W |  17344MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   48C    P0             447W / 700W |  19706MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   43C    P0             451W / 700W |  19686MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 16621.7 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.143359E+00 | loss scale: 1.0 | grad norm: 1.166 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16263.99, 16481.46)
    forward-compute ................................: (4070.82, 5984.30)
    backward-compute ...............................: (7666.08, 9943.96)
    batch-generator ................................: (170.45, 203.96)
    forward-recv ...................................: (20.37, 116.27)
    forward-send ...................................: (0.74, 8.24)
    backward-recv ..................................: (31.15, 212.52)
    backward-send ..................................: (0.70, 4.88)
    forward-send-backward-recv .....................: (3752.53, 4151.85)
    backward-send-forward-recv .....................: (256.94, 310.24)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.37)
    grads-reduce-scatter ...........................: (7.69, 9.84)
    params-all-gather ..............................: (4.10, 5.13)
    optimizer-copy-to-main-grad ....................: (0.15, 0.24)
    optimizer-clip-main-grad .......................: (2.34, 2.69)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.74)
    optimizer-copy-main-to-model-params ............: (1.32, 1.66)
    optimizer ......................................: (11.42, 11.77)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 16851.4 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.281062E+00 | loss scale: 1.0 | grad norm: 2.330 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16584.48, 16801.87)
    forward-compute ................................: (4068.18, 5975.25)
    backward-compute ...............................: (7667.25, 9940.67)
    batch-generator ................................: (169.56, 190.86)
    forward-recv ...................................: (84.59, 417.61)
    forward-send ...................................: (0.74, 7.64)
    backward-recv ..................................: (30.97, 211.01)
    backward-send ..................................: (0.72, 5.14)
    forward-send-backward-recv .....................: (3723.92, 4193.07)
    backward-send-forward-recv .....................: (254.73, 620.77)
    layernorm-grads-all-reduce .....................: (0.02, 0.04)
    embedding-grads-all-reduce .....................: (0.02, 17.97)
    grads-reduce-scatter ...........................: (7.67, 9.94)
    params-all-gather ..............................: (4.08, 5.13)
    optimizer-copy-to-main-grad ....................: (0.15, 0.24)
    optimizer-clip-main-grad .......................: (2.22, 2.54)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.73)
    optimizer-copy-main-to-model-params ............: (1.32, 1.67)
    optimizer ......................................: (10.21, 10.56)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 16519.8 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.153725E+00 | loss scale: 1.0 | grad norm: 1.230 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16252.09, 16469.57)
    forward-compute ................................: (4068.99, 5988.87)
    backward-compute ...............................: (7665.46, 9955.41)
    batch-generator ................................: (169.14, 187.97)
    forward-recv ...................................: (20.33, 115.90)
    forward-send ...................................: (0.74, 8.73)
    backward-recv ..................................: (30.96, 211.43)
    backward-send ..................................: (0.71, 4.81)
    forward-send-backward-recv .....................: (3726.61, 4141.02)
    backward-send-forward-recv .....................: (257.11, 316.36)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.18)
    grads-reduce-scatter ...........................: (7.72, 9.88)
    params-all-gather ..............................: (4.07, 5.10)
    optimizer-copy-to-main-grad ....................: (0.15, 0.24)
    optimizer-clip-main-grad .......................: (2.22, 2.54)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.73)
    optimizer-copy-main-to-model-params ............: (1.32, 1.66)
    optimizer ......................................: (10.21, 10.56)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 16503.7 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.145684E+00 | loss scale: 1.0 | grad norm: 0.966 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16236.68, 16454.09)
    forward-compute ................................: (4065.10, 6021.76)
    backward-compute ...............................: (7664.86, 9951.83)
    batch-generator ................................: (169.41, 191.78)
    forward-recv ...................................: (20.37, 116.14)
    forward-send ...................................: (0.75, 7.63)
    backward-recv ..................................: (31.06, 211.01)
    backward-send ..................................: (0.70, 4.84)
    forward-send-backward-recv .....................: (3717.35, 4131.06)
    backward-send-forward-recv .....................: (257.34, 317.71)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.26)
    grads-reduce-scatter ...........................: (7.76, 9.99)
    params-all-gather ..............................: (4.05, 5.12)
    optimizer-copy-to-main-grad ....................: (0.15, 0.24)
    optimizer-clip-main-grad .......................: (2.10, 2.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.72)
    optimizer-copy-main-to-model-params ............: (1.32, 1.66)
    optimizer ......................................: (10.06, 10.40)
Mon Feb 12 02:03:05 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   39C    P0             455W / 700W |  21772MiB / 81559MiB |     22%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   47C    P0             404W / 700W |  21772MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   48C    P0             452W / 700W |  19690MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0             401W / 700W |  19690MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   40C    P0             354W / 700W |  17344MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   47C    P0             434W / 700W |  17344MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   47C    P0             393W / 700W |  19706MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   42C    P0             412W / 700W |  19686MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 16602.7 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.166271E+00 | loss scale: 1.0 | grad norm: 0.597 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16247.98, 16465.57)
    forward-compute ................................: (4063.10, 6003.33)
    backward-compute ...............................: (7657.59, 9946.91)
    batch-generator ................................: (168.90, 188.57)
    forward-recv ...................................: (20.36, 116.14)
    forward-send ...................................: (0.74, 8.29)
    backward-recv ..................................: (31.62, 212.47)
    backward-send ..................................: (0.71, 4.85)
    forward-send-backward-recv .....................: (3709.35, 4147.09)
    backward-send-forward-recv .....................: (257.42, 310.02)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.31)
    grads-reduce-scatter ...........................: (7.65, 9.95)
    params-all-gather ..............................: (4.09, 5.09)
    optimizer-copy-to-main-grad ....................: (0.15, 0.25)
    optimizer-clip-main-grad .......................: (2.10, 2.37)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.72)
    optimizer-copy-main-to-model-params ............: (1.32, 1.66)
    optimizer ......................................: (10.06, 10.41)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 16504.5 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.126751E+00 | loss scale: 1.0 | grad norm: 2.992 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16237.15, 16454.98)
    forward-compute ................................: (4062.33, 6020.42)
    backward-compute ...............................: (7664.40, 9945.60)
    batch-generator ................................: (168.26, 188.20)
    forward-recv ...................................: (20.33, 116.13)
    forward-send ...................................: (0.74, 7.88)
    backward-recv ..................................: (31.17, 211.65)
    backward-send ..................................: (0.70, 4.90)
    forward-send-backward-recv .....................: (3725.29, 4133.51)
    backward-send-forward-recv .....................: (255.78, 309.22)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.03)
    grads-reduce-scatter ...........................: (7.70, 9.92)
    params-all-gather ..............................: (4.04, 5.17)
    optimizer-copy-to-main-grad ....................: (0.15, 0.25)
    optimizer-clip-main-grad .......................: (1.48, 1.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.72)
    optimizer-copy-main-to-model-params ............: (1.31, 1.66)
    optimizer ......................................: (9.28, 9.69)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 16765.2 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.092833E+00 | loss scale: 1.0 | grad norm: 3.663 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (16496.66, 16714.63)
    forward-compute ................................: (4061.57, 6109.41)
    backward-compute ...............................: (7695.92, 9929.98)
    batch-generator ................................: (167.37, 192.07)
    forward-recv ...................................: (20.40, 336.07)
    forward-send ...................................: (0.75, 274.44)
    backward-recv ..................................: (31.01, 211.61)
    backward-send ..................................: (0.71, 5.09)
    forward-send-backward-recv .....................: (3703.07, 4361.60)
    backward-send-forward-recv .....................: (254.78, 540.17)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.41)
    grads-reduce-scatter ...........................: (7.75, 9.95)
    params-all-gather ..............................: (4.08, 5.20)
    optimizer-copy-to-main-grad ....................: (0.15, 0.25)
    optimizer-clip-main-grad .......................: (2.12, 2.36)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 6.19)
    optimizer-copy-main-to-model-params ............: (1.32, 1.66)
    optimizer ......................................: (10.55, 10.90)
benchmark/test_padding.sh.two.bak: line 138: 411497 Killed                  torchrun $DISTRIBUTED_ARGS pretrain_gpt.py $GPT_ARGS $DATA_ARGS $OUTPUT_ARGS --distributed-backend nccl
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (476636): No such process
kill: (476654): No such process
kill: (476672): No such process
kill: (476690): No such process
10.64.24.50 kill done.

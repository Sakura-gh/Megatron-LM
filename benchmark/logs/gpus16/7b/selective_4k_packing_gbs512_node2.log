7b, 4k, gbs=512: dp=8, tp=2, pp=1, mbs=4
LOCAL_IP = 10.64.24.51
DP=8, MP=2, PP=1
[2024-02-12 09:37:04,009] torch.distributed.run: [WARNING] 
[2024-02-12 09:37:04,009] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 09:37:04,009] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 09:37:04,009] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.481 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.498 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.499 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.527 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.067 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.138 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.160 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.204 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (286.10, 317.42)
    train/valid/test-data-iterators-setup ..........: (0.02, 11099.32)
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 5284.7 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.093422E+01 | loss scale: 1.0 | grad norm: 6.494 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5029.79, 5071.50)
    forward-compute ................................: (3301.45, 3536.69)
    backward-compute ...............................: (1486.47, 1753.23)
    batch-generator ................................: (209.62, 237.43)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.73, 68.94)
    params-all-gather ..............................: (36.28, 36.37)
    optimizer-copy-to-main-grad ....................: (0.28, 0.44)
    optimizer-clip-main-grad .......................: (5.13, 5.16)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.92)
    optimizer-copy-main-to-model-params ............: (1.42, 1.50)
    optimizer ......................................: (12.50, 12.58)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 3910.0 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.089707E+01 | loss scale: 1.0 | grad norm: 25.671 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3701.67, 3747.69)
    forward-compute ................................: (2172.15, 2295.70)
    backward-compute ...............................: (1397.80, 1534.98)
    batch-generator ................................: (14.46, 23.88)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.42, 69.21)
    params-all-gather ..............................: (36.23, 36.48)
    optimizer-copy-to-main-grad ....................: (0.25, 0.42)
    optimizer-clip-main-grad .......................: (2.31, 2.34)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.67, 4.73)
    optimizer-copy-main-to-model-params ............: (1.42, 1.50)
    optimizer ......................................: (9.41, 9.49)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 4279.3 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.066085E+01 | loss scale: 1.0 | grad norm: 2.853 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4033.47, 4075.95)
    forward-compute ................................: (2389.91, 2483.63)
    backward-compute ...............................: (1564.50, 1651.84)
    batch-generator ................................: (13.98, 22.81)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-reduce-scatter ...........................: (68.40, 69.04)
    params-all-gather ..............................: (36.26, 36.34)
    optimizer-copy-to-main-grad ....................: (0.24, 0.42)
    optimizer-clip-main-grad .......................: (2.31, 2.34)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.66, 4.82)
    optimizer-copy-main-to-model-params ............: (1.42, 1.50)
    optimizer ......................................: (9.51, 9.59)
Mon Feb 12 09:40:43 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   35C    P0             279W / 700W |  70734MiB / 81559MiB |     85%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   42C    P0             207W / 700W |  70880MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   42C    P0             315W / 700W |  73398MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             301W / 700W |  73602MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             426W / 700W |  68190MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   43C    P0             427W / 700W |  68190MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   40C    P0             231W / 700W |  64582MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             230W / 700W |  64980MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 5007.5 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.059993E+01 | loss scale: 1.0 | grad norm: 1.151 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4705.15, 4741.23)
    forward-compute ................................: (3211.08, 3285.59)
    backward-compute ...............................: (1426.94, 1519.29)
    batch-generator ................................: (14.53, 22.86)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.42, 68.83)
    params-all-gather ..............................: (36.31, 36.38)
    optimizer-copy-to-main-grad ....................: (0.24, 0.39)
    optimizer-clip-main-grad .......................: (2.19, 2.22)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.67, 4.73)
    optimizer-copy-main-to-model-params ............: (1.42, 1.50)
    optimizer ......................................: (9.33, 9.40)
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 165, in pretrain
    iteration = train(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 699, in train
    train_step(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 416, in train_step
    losses_reduced = forward_backward_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 372, in forward_backward_no_pipelining
Traceback (most recent call last):
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 203, in <module>
    pretrain(train_dataset_provider,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 165, in pretrain
    iteration = train(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 699, in train
    output_tensor = forward_step(forward_step_func, data_iterator,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 228, in forward_step
    train_step(forward_step_func,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py", line 416, in train_step
    output_tensor, loss_func = forward_step_func(data_iterator, model)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 153, in forward_step
    losses_reduced = forward_backward_func(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 372, in forward_backward_no_pipelining
    output_tensor = model(tokens, position_ids, attention_mask,
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/distributed.py", line 58, in forward
    output_tensor = forward_step(forward_step_func, data_iterator,
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 228, in forward_step
    output_tensor, loss_func = forward_step_func(data_iterator, model)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/pretrain_gpt.py", line 153, in forward_step
    output_tensor = model(tokens, position_ids, attention_mask,
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/distributed.py", line 58, in forward
    return self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 183, in forward
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/module.py", line 183, in forward
    outputs = self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    outputs = self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 134, in forward
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 134, in forward
    return post_language_model_processing(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 28, in post_language_model_processing
    return post_language_model_processing(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/gpt_model.py", line 28, in post_language_model_processing
    output = parallel_lm_logits(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 36, in parallel_lm_logits
    output = parallel_lm_logits(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/model/language_model.py", line 36, in parallel_lm_logits
    logits_parallel = tensor_parallel.linear_with_grad_accumulation_and_async_allreduce(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/tensor_parallel/layers.py", line 414, in linear_with_grad_accumulation_and_async_allreduce
    logits_parallel = tensor_parallel.linear_with_grad_accumulation_and_async_allreduce(
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/tensor_parallel/layers.py", line 414, in linear_with_grad_accumulation_and_async_allreduce
    return LinearWithGradAccumulationAndAsyncCommunication.apply(*args)
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/autocast_mode.py", line 113, in decorate_fwd
    return fwd(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/tensor_parallel/layers.py", line 243, in forward
    return LinearWithGradAccumulationAndAsyncCommunication.apply(*args)
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/autocast_mode.py", line 113, in decorate_fwd
    return fwd(*args, **kwargs)
  File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/core/tensor_parallel/layers.py", line 243, in forward
    output = torch.matmul(total_input, weight.t())
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 788.00 MiB. GPU 2 has a total capacty of 79.11 GiB of which 75.50 MiB is free. Process 882310 has 79.02 GiB memory in use. Of the allocated memory 67.76 GiB is allocated by PyTorch, and 8.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    output = torch.matmul(total_input, weight.t())
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 788.00 MiB. GPU 3 has a total capacty of 79.11 GiB of which 689.50 MiB is free. Process 882311 has 78.42 GiB memory in use. Of the allocated memory 67.76 GiB is allocated by PyTorch, and 7.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-02-12 09:41:09,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 414090 closing signal SIGTERM
[2024-02-12 09:41:09,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 414091 closing signal SIGTERM
[2024-02-12 09:41:09,274] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 414092 closing signal SIGTERM
[2024-02-12 09:41:09,275] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 414094 closing signal SIGTERM
[2024-02-12 09:41:09,276] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 414095 closing signal SIGTERM
[2024-02-12 09:41:09,277] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 414096 closing signal SIGTERM
[2024-02-12 09:41:09,277] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 414097 closing signal SIGTERM
[2024-02-12 09:41:10,119] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 3 (pid: 414093) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_09:41:09
  host      : SYM206-GPU-A0205-P2-Node51
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 414093)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (480803): No such process
kill: (480826): No such process
kill: (480832): No such process
kill: (480838): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=8, tp=2, pp=1, mbs=2
LOCAL_IP = 10.64.24.51
DP=8, MP=2, PP=1
[2024-02-12 09:43:26,993] torch.distributed.run: [WARNING] 
[2024-02-12 09:43:26,993] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 09:43:26,993] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 09:43:26,993] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.286 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.340 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.404 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.580 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.941 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  4.976 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.045 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.090 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (298.13, 330.97)
    train/valid/test-data-iterators-setup ..........: (0.02, 11159.19)
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 6579.9 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.093421E+01 | loss scale: 1.0 | grad norm: 6.492 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6379.84, 6414.11)
    forward-compute ................................: (4125.15, 4374.08)
    backward-compute ...............................: (2006.48, 2273.34)
    batch-generator ................................: (228.49, 251.83)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.83, 69.68)
    params-all-gather ..............................: (36.25, 36.36)
    optimizer-copy-to-main-grad ....................: (0.25, 0.41)
    optimizer-clip-main-grad .......................: (5.35, 5.37)
    optimizer-count-zeros ..........................: (0.01, 0.02)
    optimizer-inner-step ...........................: (4.81, 4.94)
    optimizer-copy-main-to-model-params ............: (1.43, 1.51)
    optimizer ......................................: (12.67, 12.75)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 5760.0 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.089635E+01 | loss scale: 1.0 | grad norm: 25.578 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5593.64, 5611.46)
    forward-compute ................................: (3630.06, 3754.24)
    backward-compute ...............................: (1823.19, 1959.22)
    batch-generator ................................: (28.94, 34.24)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-reduce-scatter ...........................: (68.43, 68.52)
    params-all-gather ..............................: (36.25, 36.38)
    optimizer-copy-to-main-grad ....................: (0.24, 0.34)
    optimizer-clip-main-grad .......................: (2.41, 2.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.67, 4.78)
    optimizer-copy-main-to-model-params ............: (1.42, 1.50)
    optimizer ......................................: (9.44, 9.52)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 5417.8 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.066019E+01 | loss scale: 1.0 | grad norm: 2.748 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5235.53, 5264.63)
    forward-compute ................................: (3124.79, 3208.01)
    backward-compute ...............................: (2023.51, 2099.52)
    batch-generator ................................: (28.62, 35.42)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.43, 68.57)
    params-all-gather ..............................: (36.28, 36.39)
    optimizer-copy-to-main-grad ....................: (0.24, 0.34)
    optimizer-clip-main-grad .......................: (2.32, 2.35)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.66, 4.74)
    optimizer-copy-main-to-model-params ............: (1.42, 1.50)
    optimizer ......................................: (9.33, 9.41)
Mon Feb 12 09:47:59 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   37C    P0             412W / 700W |  56322MiB / 81559MiB |     76%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   43C    P0             447W / 700W |  56480MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   44C    P0             400W / 700W |  56892MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             411W / 700W |  57032MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   37C    P0             268W / 700W |  56690MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   42C    P0             277W / 700W |  56728MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   39C    P0             285W / 700W |  56516MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   36C    P0             237W / 700W |  56656MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 6113.4 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.059967E+01 | loss scale: 1.0 | grad norm: 1.143 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5865.24, 5891.01)
    forward-compute ................................: (3903.38, 3998.37)
    backward-compute ...............................: (1851.81, 1959.57)
    batch-generator ................................: (28.02, 34.45)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.43, 68.71)
    params-all-gather ..............................: (36.25, 36.36)
    optimizer-copy-to-main-grad ....................: (0.24, 0.33)
    optimizer-clip-main-grad .......................: (2.20, 2.22)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.66, 4.73)
    optimizer-copy-main-to-model-params ............: (1.42, 1.51)
    optimizer ......................................: (9.23, 9.31)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 5775.1 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.049138E+01 | loss scale: 1.0 | grad norm: 0.841 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5596.26, 5629.82)
    forward-compute ................................: (3400.17, 3577.00)
    backward-compute ...............................: (2008.49, 2193.79)
    batch-generator ................................: (28.29, 36.30)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-reduce-scatter ...........................: (68.42, 68.58)
    params-all-gather ..............................: (36.24, 36.37)
    optimizer-copy-to-main-grad ....................: (0.24, 0.33)
    optimizer-clip-main-grad .......................: (1.43, 1.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.65, 4.72)
    optimizer-copy-main-to-model-params ............: (1.42, 2.08)
    optimizer ......................................: (8.42, 9.08)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 6272.8 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049769E+01 | loss scale: 1.0 | grad norm: 0.619 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6076.61, 6101.43)
    forward-compute ................................: (4009.27, 4139.64)
    backward-compute ...............................: (1936.70, 2060.58)
    batch-generator ................................: (28.51, 36.40)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.41, 69.50)
    params-all-gather ..............................: (36.27, 36.37)
    optimizer-copy-to-main-grad ....................: (0.24, 0.34)
    optimizer-clip-main-grad .......................: (1.17, 1.19)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.65, 4.72)
    optimizer-copy-main-to-model-params ............: (1.42, 1.50)
    optimizer ......................................: (8.13, 8.21)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 5858.6 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047607E+01 | loss scale: 1.0 | grad norm: 0.502 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5682.57, 5708.10)
    forward-compute ................................: (3621.56, 3737.86)
    backward-compute ...............................: (1933.93, 2053.47)
    batch-generator ................................: (29.36, 35.60)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-reduce-scatter ...........................: (68.42, 68.70)
    params-all-gather ..............................: (36.22, 36.43)
    optimizer-copy-to-main-grad ....................: (0.24, 0.33)
    optimizer-clip-main-grad .......................: (1.05, 1.05)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.65, 4.73)
    optimizer-copy-main-to-model-params ............: (1.42, 1.50)
    optimizer ......................................: (8.01, 8.09)
Mon Feb 12 09:51:59 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   37C    P0             382W / 700W |  56322MiB / 81559MiB |     46%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   43C    P0             468W / 700W |  56480MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   44C    P0             422W / 700W |  56894MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             422W / 700W |  57034MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             325W / 700W |  56690MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   43C    P0             323W / 700W |  56730MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   41C    P0             403W / 700W |  56516MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             389W / 700W |  56656MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 6068.7 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044256E+01 | loss scale: 1.0 | grad norm: 1.040 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5804.14, 5827.20)
    forward-compute ................................: (3714.06, 3856.27)
    backward-compute ...............................: (1940.78, 2096.90)
    batch-generator ................................: (29.51, 34.28)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.42, 68.69)
    params-all-gather ..............................: (36.24, 36.35)
    optimizer-copy-to-main-grad ....................: (0.24, 0.33)
    optimizer-clip-main-grad .......................: (1.56, 1.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.66, 4.72)
    optimizer-copy-main-to-model-params ............: (1.42, 1.50)
    optimizer ......................................: (8.53, 8.63)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 6962.2 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.043090E+01 | loss scale: 1.0 | grad norm: 0.763 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6773.35, 6804.03)
    forward-compute ................................: (4687.70, 4879.82)
    backward-compute ...............................: (1907.95, 2090.13)
    batch-generator ................................: (29.02, 34.57)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.43, 68.56)
    params-all-gather ..............................: (36.30, 36.66)
    optimizer-copy-to-main-grad ....................: (0.24, 0.33)
    optimizer-clip-main-grad .......................: (1.94, 1.95)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.66, 4.74)
    optimizer-copy-main-to-model-params ............: (1.42, 1.50)
    optimizer ......................................: (8.95, 9.03)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 5343.5 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041918E+01 | loss scale: 1.0 | grad norm: 1.882 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5149.56, 5174.48)
    forward-compute ................................: (3067.71, 3226.99)
    backward-compute ...............................: (1906.29, 2091.57)
    batch-generator ................................: (29.08, 34.72)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (68.52, 68.81)
    params-all-gather ..............................: (36.24, 36.55)
    optimizer-copy-to-main-grad ....................: (0.24, 0.33)
    optimizer-clip-main-grad .......................: (2.20, 2.23)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.66, 4.71)
    optimizer-copy-main-to-model-params ............: (1.42, 1.50)
    optimizer ......................................: (9.18, 9.26)
[2024-02-12 09:59:08,768] torch.distributed.elastic.agent.server.api: [ERROR] Error waiting on exit barrier. Elapsed: 300.1067912578583 seconds
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (482261): No such process
kill: (482284): No such process
kill: (482290): No such process
kill: (482296): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=8, tp=1, pp=2, mbs=4
LOCAL_IP = 10.64.24.51
DP=8, MP=1, PP=2
[2024-02-12 10:01:15,608] torch.distributed.run: [WARNING] 
[2024-02-12 10:01:15,608] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 10:01:15,608] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 10:01:15,608] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 3428130816
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.400 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.400 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.508 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.472 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.524 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.608 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.787 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.792 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.050 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.155 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.099 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.140 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.166 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.327 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.147 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.300 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (868.00, 939.38)
    train/valid/test-data-iterators-setup ..........: (10850.23, 14671.75)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 6031.5 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.091120E+01 | loss scale: 1.0 | grad norm: 6.205 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 24593.8583984375 | max allocated: 52238.150390625 | reserved: 53826.0 | max reserved: 53826.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (5009.68, 5147.00)
    forward-compute ................................: (595.06, 3354.24)
    backward-compute ...............................: (1093.58, 1621.37)
    batch-generator ................................: (24.26, 82.17)
    forward-recv ...................................: (135.57, 163.44)
    forward-send ...................................: (3.09, 7.81)
    backward-recv ..................................: (153.94, 217.54)
    backward-send ..................................: (0.60, 9.43)
    forward-send-backward-recv .....................: (2917.52, 3153.06)
    backward-send-forward-recv .....................: (74.98, 151.54)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.54, 17.78)
    grads-reduce-scatter ...........................: (41.98, 679.41)
    params-all-gather ..............................: (22.46, 22.54)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (5.33, 6.05)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.89, 5.15)
    optimizer-copy-main-to-model-params ............: (1.36, 1.42)
    optimizer ......................................: (13.66, 13.75)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 4032.4 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.087027E+01 | loss scale: 1.0 | grad norm: 20.054 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3728.42, 3864.92)
    forward-compute ................................: (443.71, 2298.35)
    backward-compute ...............................: (1008.54, 1453.92)
    batch-generator ................................: (13.37, 15.03)
    forward-recv ...................................: (22.53, 47.71)
    forward-send ...................................: (0.54, 1.01)
    backward-recv ..................................: (130.46, 198.90)
    backward-send ..................................: (0.61, 16.23)
    forward-send-backward-recv .....................: (1990.74, 2192.70)
    backward-send-forward-recv .....................: (49.13, 134.37)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (17.37, 17.55)
    grads-reduce-scatter ...........................: (41.99, 42.46)
    params-all-gather ..............................: (22.42, 22.56)
    optimizer-copy-to-main-grad ....................: (0.13, 0.20)
    optimizer-clip-main-grad .......................: (2.28, 2.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.75, 4.84)
    optimizer-copy-main-to-model-params ............: (1.36, 1.42)
    optimizer ......................................: (9.11, 9.17)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 4357.2 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.062802E+01 | loss scale: 1.0 | grad norm: 1.861 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3986.46, 4118.30)
    forward-compute ................................: (519.27, 2372.33)
    backward-compute ...............................: (1147.76, 1556.53)
    batch-generator ................................: (13.34, 15.24)
    forward-recv ...................................: (27.57, 37.14)
    forward-send ...................................: (0.61, 0.77)
    backward-recv ..................................: (135.90, 187.75)
    backward-send ..................................: (0.61, 9.84)
    forward-send-backward-recv .....................: (2139.55, 2258.63)
    backward-send-forward-recv .....................: (92.03, 174.86)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (17.43, 17.56)
    grads-reduce-scatter ...........................: (41.95, 42.47)
    params-all-gather ..............................: (22.41, 22.61)
    optimizer-copy-to-main-grad ....................: (0.13, 0.20)
    optimizer-clip-main-grad .......................: (2.28, 2.31)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.83)
    optimizer-copy-main-to-model-params ............: (1.36, 1.42)
    optimizer ......................................: (9.11, 9.17)
Mon Feb 12 10:04:57 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   36C    P0             282W / 700W |  60164MiB / 81559MiB |     12%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   43C    P0             207W / 700W |  62932MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   43C    P0             302W / 700W |  62936MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             249W / 700W |  64506MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   36C    P0             196W / 700W |  61354MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   43C    P0             266W / 700W |  64308MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   42C    P0             357W / 700W |  59838MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             216W / 700W |  56356MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 4049.1 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.058510E+01 | loss scale: 1.0 | grad norm: 0.777 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3657.74, 3780.69)
    forward-compute ................................: (450.84, 2184.99)
    backward-compute ...............................: (1024.02, 1450.27)
    batch-generator ................................: (13.26, 14.97)
    forward-recv ...................................: (27.68, 41.30)
    forward-send ...................................: (0.63, 0.85)
    backward-recv ..................................: (111.26, 168.56)
    backward-send ..................................: (0.54, 17.34)
    forward-send-backward-recv .....................: (2013.50, 2100.40)
    backward-send-forward-recv .....................: (61.20, 136.15)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (17.40, 17.67)
    grads-reduce-scatter ...........................: (42.00, 42.35)
    params-all-gather ..............................: (22.42, 22.59)
    optimizer-copy-to-main-grad ....................: (0.13, 0.20)
    optimizer-clip-main-grad .......................: (1.75, 1.77)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.75, 4.83)
    optimizer-copy-main-to-model-params ............: (1.36, 1.47)
    optimizer ......................................: (8.56, 8.67)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 4442.9 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.048112E+01 | loss scale: 1.0 | grad norm: 1.396 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4094.14, 4264.97)
    forward-compute ................................: (529.27, 2515.98)
    backward-compute ...............................: (1168.73, 1628.84)
    batch-generator ................................: (13.10, 15.08)
    forward-recv ...................................: (29.42, 47.96)
    forward-send ...................................: (0.65, 0.95)
    backward-recv ..................................: (133.53, 217.04)
    backward-send ..................................: (0.74, 22.53)
    forward-send-backward-recv .....................: (2080.32, 2346.62)
    backward-send-forward-recv .....................: (64.32, 175.26)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.39, 17.62)
    grads-reduce-scatter ...........................: (41.93, 42.55)
    params-all-gather ..............................: (22.41, 22.55)
    optimizer-copy-to-main-grad ....................: (0.13, 0.22)
    optimizer-clip-main-grad .......................: (1.36, 1.37)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.75, 4.83)
    optimizer-copy-main-to-model-params ............: (1.36, 1.42)
    optimizer ......................................: (8.19, 8.25)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 5014.9 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049645E+01 | loss scale: 1.0 | grad norm: 1.349 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4700.39, 4823.57)
    forward-compute ................................: (756.85, 2935.25)
    backward-compute ...............................: (1059.53, 1524.05)
    batch-generator ................................: (13.15, 16.16)
    forward-recv ...................................: (32.23, 444.64)
    forward-send ...................................: (0.60, 1.08)
    backward-recv ..................................: (144.59, 208.32)
    backward-send ..................................: (0.62, 6.37)
    forward-send-backward-recv .....................: (2472.86, 2769.81)
    backward-send-forward-recv .....................: (65.41, 487.91)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.45, 17.73)
    grads-reduce-scatter ...........................: (41.99, 42.55)
    params-all-gather ..............................: (22.44, 22.58)
    optimizer-copy-to-main-grad ....................: (0.14, 0.28)
    optimizer-clip-main-grad .......................: (2.03, 2.05)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.75, 4.85)
    optimizer-copy-main-to-model-params ............: (1.36, 1.42)
    optimizer ......................................: (9.03, 9.09)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 3972.2 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047773E+01 | loss scale: 1.0 | grad norm: 1.130 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3667.87, 3796.55)
    forward-compute ................................: (470.34, 2210.02)
    backward-compute ...............................: (1054.43, 1560.14)
    batch-generator ................................: (12.74, 19.25)
    forward-recv ...................................: (25.65, 39.05)
    forward-send ...................................: (0.60, 1.26)
    backward-recv ..................................: (126.07, 169.62)
    backward-send ..................................: (0.59, 5.96)
    forward-send-backward-recv .....................: (1790.25, 2055.31)
    backward-send-forward-recv .....................: (49.07, 132.35)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.41, 17.61)
    grads-reduce-scatter ...........................: (41.97, 42.43)
    params-all-gather ..............................: (22.41, 22.59)
    optimizer-copy-to-main-grad ....................: (0.13, 0.24)
    optimizer-clip-main-grad .......................: (2.00, 2.03)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 5.31)
    optimizer-copy-main-to-model-params ............: (1.36, 1.42)
    optimizer ......................................: (9.40, 9.46)
Mon Feb 12 10:07:52 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   37C    P0             420W / 700W |  60164MiB / 81559MiB |     80%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   43C    P0             454W / 700W |  62932MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   45C    P0             487W / 700W |  62936MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             481W / 700W |  64506MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   37C    P0             371W / 700W |  61354MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   44C    P0             502W / 700W |  72296MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   41C    P0             349W / 700W |  62982MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             320W / 700W |  65298MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 4084.7 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044275E+01 | loss scale: 1.0 | grad norm: 1.193 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3664.15, 3800.39)
    forward-compute ................................: (495.78, 2168.48)
    backward-compute ...............................: (1092.02, 1571.74)
    batch-generator ................................: (12.78, 18.60)
    forward-recv ...................................: (24.82, 43.15)
    forward-send ...................................: (0.57, 0.90)
    backward-recv ..................................: (114.28, 187.61)
    backward-send ..................................: (0.65, 10.85)
    forward-send-backward-recv .....................: (1801.23, 1983.82)
    backward-send-forward-recv .....................: (40.71, 101.86)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (17.39, 18.23)
    grads-reduce-scatter ...........................: (42.03, 42.56)
    params-all-gather ..............................: (22.44, 22.60)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (1.38, 1.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.83)
    optimizer-copy-main-to-model-params ............: (1.36, 1.42)
    optimizer ......................................: (8.24, 8.31)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 5035.5 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.042812E+01 | loss scale: 1.0 | grad norm: 0.986 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4737.14, 4866.65)
    forward-compute ................................: (493.95, 3227.65)
    backward-compute ...............................: (1089.20, 1619.00)
    batch-generator ................................: (12.87, 17.76)
    forward-recv ...................................: (29.14, 44.88)
    forward-send ...................................: (0.67, 0.86)
    backward-recv ..................................: (124.39, 191.84)
    backward-send ..................................: (0.64, 6.25)
    forward-send-backward-recv .....................: (2806.82, 3085.82)
    backward-send-forward-recv .....................: (60.01, 165.38)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (17.42, 17.65)
    grads-reduce-scatter ...........................: (41.95, 42.38)
    params-all-gather ..............................: (22.43, 22.57)
    optimizer-copy-to-main-grad ....................: (0.14, 0.21)
    optimizer-clip-main-grad .......................: (1.37, 1.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.83)
    optimizer-copy-main-to-model-params ............: (1.36, 2.26)
    optimizer ......................................: (8.20, 9.09)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 3954.6 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041457E+01 | loss scale: 1.0 | grad norm: 0.908 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3626.38, 3745.01)
    forward-compute ................................: (492.94, 2115.08)
    backward-compute ...............................: (1097.84, 1565.68)
    batch-generator ................................: (13.04, 16.95)
    forward-recv ...................................: (23.06, 37.35)
    forward-send ...................................: (0.53, 1.28)
    backward-recv ..................................: (114.66, 187.66)
    backward-send ..................................: (0.71, 11.91)
    forward-send-backward-recv .....................: (1788.42, 1957.01)
    backward-send-forward-recv .....................: (45.13, 163.10)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (17.37, 17.87)
    grads-reduce-scatter ...........................: (42.03, 42.42)
    params-all-gather ..............................: (22.44, 22.61)
    optimizer-copy-to-main-grad ....................: (0.13, 0.21)
    optimizer-clip-main-grad .......................: (1.62, 1.64)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.82)
    optimizer-copy-main-to-model-params ............: (1.36, 1.42)
    optimizer ......................................: (8.43, 8.49)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (484571): No such process
kill: (484577): No such process
kill: (484583): No such process
kill: (484589): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=8, tp=1, pp=2, mbs=2
LOCAL_IP = 10.64.24.51
DP=8, MP=1, PP=2
[2024-02-12 10:11:38,970] torch.distributed.run: [WARNING] 
[2024-02-12 10:11:38,970] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 10:11:38,970] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 10:11:38,970] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 3428130816
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...

> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...


 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.271 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.293 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.298 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.384 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.418 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.411 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.416 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.660 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.988 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.014 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.024 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.082 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.226 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.195 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.218 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.085 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (855.10, 907.16)
    train/valid/test-data-iterators-setup ..........: (10644.62, 11367.69)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 7217.1 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.091124E+01 | loss scale: 1.0 | grad norm: 6.207 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 24591.2490234375 | max allocated: 43726.49609375 | reserved: 44464.0 | max reserved: 44464.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (6306.20, 6413.16)
    forward-compute ................................: (713.70, 4209.63)
    backward-compute ...............................: (1489.96, 2032.91)
    batch-generator ................................: (38.57, 96.74)
    forward-recv ...................................: (115.47, 142.38)
    forward-send ...................................: (2.45, 6.90)
    backward-recv ..................................: (128.29, 156.26)
    backward-send ..................................: (0.44, 12.08)
    forward-send-backward-recv .....................: (3752.80, 3991.64)
    backward-send-forward-recv .....................: (137.48, 246.06)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (17.50, 17.73)
    grads-reduce-scatter ...........................: (41.99, 677.27)
    params-all-gather ..............................: (22.45, 22.60)
    optimizer-copy-to-main-grad ....................: (0.14, 0.23)
    optimizer-clip-main-grad .......................: (5.34, 5.36)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.88, 4.98)
    optimizer-copy-main-to-model-params ............: (1.36, 1.49)
    optimizer ......................................: (12.71, 12.81)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 4834.7 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.086982E+01 | loss scale: 1.0 | grad norm: 20.045 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4609.63, 4685.77)
    forward-compute ................................: (526.76, 2809.56)
    backward-compute ...............................: (1338.95, 1774.79)
    batch-generator ................................: (26.00, 30.93)
    forward-recv ...................................: (13.02, 27.68)
    forward-send ...................................: (0.38, 0.66)
    backward-recv ..................................: (64.00, 109.67)
    backward-send ..................................: (0.40, 21.37)
    forward-send-backward-recv .....................: (2527.32, 2658.30)
    backward-send-forward-recv .....................: (102.25, 166.35)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (17.36, 17.56)
    grads-reduce-scatter ...........................: (42.02, 42.41)
    params-all-gather ..............................: (22.42, 22.56)
    optimizer-copy-to-main-grad ....................: (0.14, 0.20)
    optimizer-clip-main-grad .......................: (2.28, 2.31)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.83)
    optimizer-copy-main-to-model-params ............: (1.36, 1.41)
    optimizer ......................................: (9.11, 9.18)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 5703.1 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.062787E+01 | loss scale: 1.0 | grad norm: 1.862 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5444.82, 5537.82)
    forward-compute ................................: (891.67, 3443.77)
    backward-compute ...............................: (1501.23, 1909.14)
    batch-generator ................................: (25.66, 30.88)
    forward-recv ...................................: (12.55, 28.99)
    forward-send ...................................: (0.37, 0.63)
    backward-recv ..................................: (74.88, 156.47)
    backward-send ..................................: (0.41, 15.36)
    forward-send-backward-recv .....................: (2825.33, 2998.14)
    backward-send-forward-recv .....................: (142.46, 454.67)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (17.38, 17.60)
    grads-reduce-scatter ...........................: (41.93, 42.53)
    params-all-gather ..............................: (22.44, 22.56)
    optimizer-copy-to-main-grad ....................: (0.13, 0.21)
    optimizer-clip-main-grad .......................: (2.28, 2.31)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.81)
    optimizer-copy-main-to-model-params ............: (1.36, 1.52)
    optimizer ......................................: (9.12, 9.28)
Mon Feb 12 10:15:59 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   35C    P0             255W / 700W |  49230MiB / 81559MiB |     80%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   42C    P0             279W / 700W |  51282MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   43C    P0             234W / 700W |  49926MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0             294W / 700W |  49864MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   37C    P0             393W / 700W |  49840MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   43C    P0             331W / 700W |  51350MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   40C    P0             314W / 700W |  50352MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   36C    P0             236W / 700W |  49476MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 5014.0 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.058508E+01 | loss scale: 1.0 | grad norm: 0.780 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4715.48, 4798.31)
    forward-compute ................................: (558.70, 2862.74)
    backward-compute ...............................: (1387.20, 1779.18)
    batch-generator ................................: (25.56, 30.59)
    forward-recv ...................................: (16.29, 26.26)
    forward-send ...................................: (0.43, 0.58)
    backward-recv ..................................: (65.32, 89.88)
    backward-send ..................................: (0.40, 8.87)
    forward-send-backward-recv .....................: (2616.14, 2709.55)
    backward-send-forward-recv .....................: (104.34, 185.85)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (17.37, 17.62)
    grads-reduce-scatter ...........................: (41.98, 42.32)
    params-all-gather ..............................: (22.42, 22.57)
    optimizer-copy-to-main-grad ....................: (0.14, 0.21)
    optimizer-clip-main-grad .......................: (1.76, 1.77)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.81)
    optimizer-copy-main-to-model-params ............: (1.36, 1.42)
    optimizer ......................................: (8.58, 8.64)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 6266.4 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.048107E+01 | loss scale: 1.0 | grad norm: 1.334 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6031.40, 6130.13)
    forward-compute ................................: (628.00, 4021.35)
    backward-compute ...............................: (1539.67, 1973.61)
    batch-generator ................................: (25.99, 30.67)
    forward-recv ...................................: (19.77, 32.71)
    forward-send ...................................: (0.48, 0.70)
    backward-recv ..................................: (345.47, 400.84)
    backward-send ..................................: (0.46, 5.88)
    forward-send-backward-recv .....................: (3370.55, 3560.45)
    backward-send-forward-recv .....................: (131.13, 257.73)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (17.32, 17.58)
    grads-reduce-scatter ...........................: (41.99, 42.54)
    params-all-gather ..............................: (22.43, 22.57)
    optimizer-copy-to-main-grad ....................: (0.14, 0.21)
    optimizer-clip-main-grad .......................: (1.24, 1.25)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.81)
    optimizer-copy-main-to-model-params ............: (1.36, 1.47)
    optimizer ......................................: (8.08, 8.18)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 5892.5 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049354E+01 | loss scale: 1.0 | grad norm: 0.971 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5627.94, 5706.30)
    forward-compute ................................: (854.68, 3699.84)
    backward-compute ...............................: (1461.95, 1873.37)
    batch-generator ................................: (25.91, 31.04)
    forward-recv ...................................: (15.39, 295.25)
    forward-send ...................................: (0.41, 0.69)
    backward-recv ..................................: (77.37, 118.68)
    backward-send ..................................: (0.45, 7.45)
    forward-send-backward-recv .....................: (3084.76, 3285.17)
    backward-send-forward-recv .....................: (129.43, 410.68)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (17.32, 17.58)
    grads-reduce-scatter ...........................: (41.97, 42.53)
    params-all-gather ..............................: (22.43, 22.57)
    optimizer-copy-to-main-grad ....................: (0.14, 0.21)
    optimizer-clip-main-grad .......................: (1.23, 1.24)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.80)
    optimizer-copy-main-to-model-params ............: (1.36, 1.41)
    optimizer ......................................: (8.03, 8.09)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 4983.0 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047287E+01 | loss scale: 1.0 | grad norm: 0.608 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4744.58, 4829.26)
    forward-compute ................................: (578.24, 2851.60)
    backward-compute ...............................: (1445.53, 1883.12)
    batch-generator ................................: (25.90, 31.49)
    forward-recv ...................................: (14.61, 22.84)
    forward-send ...................................: (0.40, 0.53)
    backward-recv ..................................: (80.48, 117.35)
    backward-send ..................................: (0.48, 10.82)
    forward-send-backward-recv .....................: (2473.96, 2654.73)
    backward-send-forward-recv .....................: (105.82, 213.69)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (17.40, 17.59)
    grads-reduce-scatter ...........................: (41.97, 42.37)
    params-all-gather ..............................: (22.44, 22.59)
    optimizer-copy-to-main-grad ....................: (0.13, 0.21)
    optimizer-clip-main-grad .......................: (1.10, 1.10)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.75, 4.81)
    optimizer-copy-main-to-model-params ............: (1.36, 1.42)
    optimizer ......................................: (7.92, 7.98)
Mon Feb 12 10:19:42 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   37C    P0             445W / 700W |  49230MiB / 81559MiB |     41%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   43C    P0             348W / 700W |  51282MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   44C    P0             379W / 700W |  49926MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             354W / 700W |  49864MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   37C    P0             400W / 700W |  49840MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   43C    P0             453W / 700W |  51350MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   41C    P0             291W / 700W |  50352MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             408W / 700W |  51048MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 5131.3 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.043891E+01 | loss scale: 1.0 | grad norm: 1.283 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4812.89, 4890.91)
    forward-compute ................................: (598.44, 2900.01)
    backward-compute ...............................: (1480.81, 1910.50)
    batch-generator ................................: (25.81, 31.72)
    forward-recv ...................................: (14.39, 29.18)
    forward-send ...................................: (0.40, 0.71)
    backward-recv ..................................: (79.86, 119.78)
    backward-send ..................................: (0.47, 7.30)
    forward-send-backward-recv .....................: (2526.28, 2667.53)
    backward-send-forward-recv .....................: (120.64, 164.33)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (17.33, 17.66)
    grads-reduce-scatter ...........................: (41.93, 42.58)
    params-all-gather ..............................: (22.42, 22.54)
    optimizer-copy-to-main-grad ....................: (0.14, 0.21)
    optimizer-clip-main-grad .......................: (1.50, 1.51)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.80)
    optimizer-copy-main-to-model-params ............: (1.36, 1.42)
    optimizer ......................................: (8.31, 8.37)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 5851.3 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.042715E+01 | loss scale: 1.0 | grad norm: 1.303 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5595.02, 5689.75)
    forward-compute ................................: (578.60, 3721.85)
    backward-compute ...............................: (1454.40, 1907.18)
    batch-generator ................................: (25.67, 31.57)
    forward-recv ...................................: (15.22, 28.56)
    forward-send ...................................: (0.43, 0.73)
    backward-recv ..................................: (350.29, 387.09)
    backward-send ..................................: (0.49, 3.24)
    forward-send-backward-recv .....................: (2755.89, 3258.59)
    backward-send-forward-recv .....................: (88.74, 446.75)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (17.43, 17.68)
    grads-reduce-scatter ...........................: (41.98, 42.35)
    params-all-gather ..............................: (22.43, 22.57)
    optimizer-copy-to-main-grad ....................: (0.14, 0.21)
    optimizer-clip-main-grad .......................: (2.02, 2.04)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.82)
    optimizer-copy-main-to-model-params ............: (1.36, 1.41)
    optimizer ......................................: (8.85, 8.91)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 6807.2 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.042028E+01 | loss scale: 1.0 | grad norm: 1.725 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6534.74, 6626.96)
    forward-compute ................................: (668.24, 4664.84)
    backward-compute ...............................: (1435.12, 1905.70)
    batch-generator ................................: (25.64, 31.51)
    forward-recv ...................................: (14.09, 21.66)
    forward-send ...................................: (0.41, 0.52)
    backward-recv ..................................: (105.95, 131.64)
    backward-send ..................................: (0.47, 8.32)
    forward-send-backward-recv .....................: (3990.51, 4240.50)
    backward-send-forward-recv .....................: (103.90, 407.93)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (17.43, 17.76)
    grads-reduce-scatter ...........................: (42.01, 42.40)
    params-all-gather ..............................: (22.43, 22.65)
    optimizer-copy-to-main-grad ....................: (0.14, 0.20)
    optimizer-clip-main-grad .......................: (2.02, 2.04)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 5.42)
    optimizer-copy-main-to-model-params ............: (1.36, 1.95)
    optimizer ......................................: (9.50, 10.08)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (486864): No such process
kill: (486870): No such process
kill: (486876): No such process
kill: (486882): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=4, tp=2, pp=2, mbs=8
LOCAL_IP = 10.64.24.51
DP=4, MP=2, PP=2
[2024-02-12 10:24:06,455] torch.distributed.run: [WARNING] 
[2024-02-12 10:24:06,455] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 10:24:06,455] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 10:24:06,455] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1714528256
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1714528256
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...

Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.304 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.347 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.380 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.384 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.978 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.057 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.146 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.151 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (888.68, 926.09)
    train/valid/test-data-iterators-setup ..........: (0.02, 15150.91)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 5195.0 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.094822E+01 | loss scale: 1.0 | grad norm: 6.512 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 14802.4794921875 | max allocated: 38694.16845703125 | reserved: 46962.0 | max reserved: 46962.0
[Rank 9] (after 10 iterations) memory (MB) | allocated: 14802.1982421875 | max allocated: 38693.60595703125 | reserved: 45748.0 | max reserved: 45748.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (4529.24, 4658.30)
    forward-compute ................................: (911.88, 2604.94)
    backward-compute ...............................: (1139.71, 1664.49)
    batch-generator ................................: (184.12, 238.91)
    forward-recv ...................................: (302.83, 334.81)
    forward-send ...................................: (3.51, 11.87)
    backward-recv ..................................: (126.18, 187.19)
    backward-send ..................................: (1.02, 4.09)
    forward-send-backward-recv .....................: (2180.26, 2333.53)
    backward-send-forward-recv .....................: (89.65, 119.88)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.98, 9.27)
    grads-reduce-scatter ...........................: (19.15, 426.39)
    params-all-gather ..............................: (10.62, 11.27)
    optimizer-copy-to-main-grad ....................: (0.28, 0.60)
    optimizer-clip-main-grad .......................: (5.42, 5.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.91, 5.11)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (13.56, 13.65)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 3419.2 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.091010E+01 | loss scale: 1.0 | grad norm: 26.930 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3225.52, 3325.06)
    forward-compute ................................: (560.56, 1749.16)
    backward-compute ...............................: (1026.26, 1468.78)
    batch-generator ................................: (13.41, 17.49)
    forward-recv ...................................: (30.68, 50.37)
    forward-send ...................................: (0.89, 1.39)
    backward-recv ..................................: (116.61, 147.30)
    backward-send ..................................: (0.97, 5.29)
    forward-send-backward-recv .....................: (1432.52, 1553.50)
    backward-send-forward-recv .....................: (52.53, 112.67)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.87, 9.33)
    grads-reduce-scatter ...........................: (19.23, 19.58)
    params-all-gather ..............................: (10.61, 10.89)
    optimizer-copy-to-main-grad ....................: (0.26, 0.37)
    optimizer-clip-main-grad .......................: (2.40, 2.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.91)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (9.59, 9.67)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 3767.3 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.066511E+01 | loss scale: 1.0 | grad norm: 4.716 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3495.49, 3620.03)
    forward-compute ................................: (671.74, 1814.15)
    backward-compute ...............................: (1216.76, 1581.94)
    batch-generator ................................: (13.18, 17.38)
    forward-recv ...................................: (38.17, 45.09)
    forward-send ...................................: (1.05, 1.24)
    backward-recv ..................................: (114.57, 154.57)
    backward-send ..................................: (1.20, 4.60)
    forward-send-backward-recv .....................: (1536.15, 1585.53)
    backward-send-forward-recv .....................: (84.76, 137.64)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.88, 9.15)
    grads-reduce-scatter ...........................: (19.08, 19.66)
    params-all-gather ..............................: (10.63, 10.75)
    optimizer-copy-to-main-grad ....................: (0.26, 0.38)
    optimizer-clip-main-grad .......................: (2.41, 2.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.96)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (9.71, 9.80)
Mon Feb 12 10:27:24 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   38C    P0             371W / 700W |  53788MiB / 81559MiB |     90%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   45C    P0             367W / 700W |  55726MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   45C    P0             410W / 700W |  53310MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0             432W / 700W |  53310MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             375W / 700W |  59640MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   44C    P0             370W / 700W |  58064MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   44C    P0             384W / 700W |  54700MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             407W / 700W |  54700MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 3606.5 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.060306E+01 | loss scale: 1.0 | grad norm: 1.255 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3305.71, 3414.68)
    forward-compute ................................: (576.74, 1776.04)
    backward-compute ...............................: (1050.21, 1470.98)
    batch-generator ................................: (12.65, 16.26)
    forward-recv ...................................: (40.63, 45.76)
    forward-send ...................................: (1.11, 1.25)
    backward-recv ..................................: (99.91, 160.70)
    backward-send ..................................: (0.82, 8.31)
    forward-send-backward-recv .....................: (1426.52, 1617.98)
    backward-send-forward-recv .....................: (59.80, 215.75)
    layernorm-grads-all-reduce .....................: (0.01, 0.04)
    embedding-grads-all-reduce .....................: (8.85, 9.31)
    grads-reduce-scatter ...........................: (19.25, 19.60)
    params-all-gather ..............................: (10.60, 10.75)
    optimizer-copy-to-main-grad ....................: (0.26, 0.39)
    optimizer-clip-main-grad .......................: (2.40, 2.67)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 5.00)
    optimizer-copy-main-to-model-params ............: (1.47, 1.55)
    optimizer ......................................: (10.10, 10.18)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 3916.8 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.049366E+01 | loss scale: 1.0 | grad norm: 0.918 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3690.03, 3819.77)
    forward-compute ................................: (688.62, 2008.52)
    backward-compute ...............................: (1218.59, 1676.26)
    batch-generator ................................: (12.55, 18.23)
    forward-recv ...................................: (44.56, 55.28)
    forward-send ...................................: (1.21, 1.46)
    backward-recv ..................................: (117.64, 161.70)
    backward-send ..................................: (1.15, 17.48)
    forward-send-backward-recv .....................: (1563.85, 1723.81)
    backward-send-forward-recv .....................: (79.95, 146.38)
    layernorm-grads-all-reduce .....................: (0.02, 0.04)
    embedding-grads-all-reduce .....................: (8.87, 9.28)
    grads-reduce-scatter ...........................: (19.11, 19.63)
    params-all-gather ..............................: (10.62, 10.77)
    optimizer-copy-to-main-grad ....................: (0.26, 0.44)
    optimizer-clip-main-grad .......................: (1.38, 1.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 5.06)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (8.82, 8.91)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 3972.3 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.050042E+01 | loss scale: 1.0 | grad norm: 1.240 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3761.48, 3877.71)
    forward-compute ................................: (900.62, 1889.82)
    backward-compute ...............................: (1136.59, 1551.95)
    batch-generator ................................: (12.85, 17.33)
    forward-recv ...................................: (312.25, 381.99)
    forward-send ...................................: (1.07, 1.49)
    backward-recv ..................................: (100.33, 147.65)
    backward-send ..................................: (0.99, 4.74)
    forward-send-backward-recv .....................: (1519.32, 1660.98)
    backward-send-forward-recv .....................: (92.07, 160.13)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.87, 9.19)
    grads-reduce-scatter ...........................: (19.16, 19.67)
    params-all-gather ..............................: (10.60, 10.77)
    optimizer-copy-to-main-grad ....................: (0.26, 0.39)
    optimizer-clip-main-grad .......................: (1.87, 1.89)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.94)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (9.10, 9.19)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 3559.4 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047854E+01 | loss scale: 1.0 | grad norm: 0.894 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3361.63, 3463.55)
    forward-compute ................................: (621.17, 1771.13)
    backward-compute ...............................: (1132.94, 1575.10)
    batch-generator ................................: (12.33, 17.09)
    forward-recv ...................................: (36.17, 42.62)
    forward-send ...................................: (0.99, 1.18)
    backward-recv ..................................: (111.78, 139.43)
    backward-send ..................................: (1.08, 1.33)
    forward-send-backward-recv .....................: (1395.81, 1541.49)
    backward-send-forward-recv .....................: (78.71, 115.39)
    layernorm-grads-all-reduce .....................: (0.01, 0.03)
    embedding-grads-all-reduce .....................: (8.88, 9.21)
    grads-reduce-scatter ...........................: (19.21, 19.58)
    params-all-gather ..............................: (10.60, 10.76)
    optimizer-copy-to-main-grad ....................: (0.25, 0.38)
    optimizer-clip-main-grad .......................: (1.21, 1.21)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.92)
    optimizer-copy-main-to-model-params ............: (1.47, 1.57)
    optimizer ......................................: (8.44, 8.54)
Mon Feb 12 10:29:55 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   39C    P0             414W / 700W |  53788MiB / 81559MiB |     16%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   46C    P0             440W / 700W |  55726MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   47C    P0             442W / 700W |  53310MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   40C    P0             467W / 700W |  56462MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   40C    P0             468W / 700W |  62792MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   46C    P0             440W / 700W |  61216MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   44C    P0             414W / 700W |  61004MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             387W / 700W |  61004MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 3637.0 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044368E+01 | loss scale: 1.0 | grad norm: 1.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3348.15, 3453.45)
    forward-compute ................................: (649.32, 1737.68)
    backward-compute ...............................: (1177.51, 1597.39)
    batch-generator ................................: (12.44, 16.94)
    forward-recv ...................................: (35.86, 51.92)
    forward-send ...................................: (1.00, 1.39)
    backward-recv ..................................: (95.09, 127.09)
    backward-send ..................................: (1.05, 3.43)
    forward-send-backward-recv .....................: (1376.85, 1464.46)
    backward-send-forward-recv .....................: (57.28, 88.80)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.83, 9.28)
    grads-reduce-scatter ...........................: (19.14, 19.66)
    params-all-gather ..............................: (10.61, 10.76)
    optimizer-copy-to-main-grad ....................: (0.25, 0.40)
    optimizer-clip-main-grad .......................: (1.62, 1.63)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.94)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (8.84, 8.93)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 3806.0 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.042977E+01 | loss scale: 1.0 | grad norm: 0.618 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3609.25, 3724.26)
    forward-compute ................................: (656.34, 1966.83)
    backward-compute ...............................: (1178.18, 1621.66)
    batch-generator ................................: (12.71, 16.56)
    forward-recv ...................................: (42.80, 51.00)
    forward-send ...................................: (1.17, 1.38)
    backward-recv ..................................: (380.67, 401.26)
    backward-send ..................................: (1.00, 1.38)
    forward-send-backward-recv .....................: (1332.42, 1445.07)
    backward-send-forward-recv .....................: (71.08, 117.24)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.90, 9.16)
    grads-reduce-scatter ...........................: (19.19, 19.69)
    params-all-gather ..............................: (10.60, 10.75)
    optimizer-copy-to-main-grad ....................: (0.25, 0.37)
    optimizer-clip-main-grad .......................: (1.86, 1.88)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.90)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (9.02, 9.12)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 3505.3 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041463E+01 | loss scale: 1.0 | grad norm: 0.954 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3271.43, 3394.13)
    forward-compute ................................: (642.05, 1676.81)
    backward-compute ...............................: (1161.40, 1593.24)
    batch-generator ................................: (12.28, 16.76)
    forward-recv ...................................: (31.73, 41.12)
    forward-send ...................................: (0.89, 1.13)
    backward-recv ..................................: (106.59, 155.38)
    backward-send ..................................: (1.26, 3.55)
    forward-send-backward-recv .....................: (1301.90, 1405.66)
    backward-send-forward-recv .....................: (51.57, 119.05)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.86, 9.22)
    grads-reduce-scatter ...........................: (19.25, 19.59)
    params-all-gather ..............................: (10.61, 10.76)
    optimizer-copy-to-main-grad ....................: (0.25, 0.36)
    optimizer-clip-main-grad .......................: (1.45, 1.46)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.89)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (8.59, 8.68)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (488370): No such process
kill: (488376): No such process
kill: (488382): No such process
kill: (488388): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=4, tp=2, pp=2, mbs=4
LOCAL_IP = 10.64.24.51
DP=4, MP=2, PP=2
[2024-02-12 10:33:24,757] torch.distributed.run: [WARNING] 
[2024-02-12 10:33:24,757] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 10:33:24,757] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 10:33:24,757] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1714528256
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1714528256
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.331 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.353 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.807 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  7.455 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.066 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.156 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.418 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.235 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (842.35, 890.98)
    train/valid/test-data-iterators-setup ..........: (0.02, 13102.38)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 5945.2 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.094828E+01 | loss scale: 1.0 | grad norm: 6.511 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 14792.7294921875 | max allocated: 34716.1337890625 | reserved: 37824.0 | max reserved: 37824.0[Rank 9] (after 10 iterations) memory (MB) | allocated: 14792.7294921875 | max allocated: 34714.2744140625 | reserved: 37824.0 | max reserved: 37824.0

(min, max) time across ranks (ms):
    forward-backward ...............................: (5359.47, 5434.89)
    forward-compute ................................: (1038.41, 3106.32)
    backward-compute ...............................: (1415.85, 1886.40)
    batch-generator ................................: (207.82, 247.87)
    forward-recv ...................................: (284.21, 308.60)
    forward-send ...................................: (2.98, 6.64)
    backward-recv ..................................: (95.41, 119.97)
    backward-send ..................................: (1.65, 4.52)
    forward-send-backward-recv .....................: (2659.12, 2790.71)
    backward-send-forward-recv .....................: (175.33, 240.87)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (9.02, 9.37)
    grads-reduce-scatter ...........................: (19.22, 428.22)
    params-all-gather ..............................: (10.61, 10.83)
    optimizer-copy-to-main-grad ....................: (0.26, 0.48)
    optimizer-clip-main-grad .......................: (5.50, 5.54)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.91, 5.20)
    optimizer-copy-main-to-model-params ............: (1.47, 1.57)
    optimizer ......................................: (13.63, 13.76)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 4051.9 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.090995E+01 | loss scale: 1.0 | grad norm: 26.938 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3919.56, 3973.15)
    forward-compute ................................: (657.12, 2168.37)
    backward-compute ...............................: (1265.16, 1670.21)
    batch-generator ................................: (25.43, 29.69)
    forward-recv ...................................: (17.57, 32.77)
    forward-send ...................................: (0.53, 0.92)
    backward-recv ..................................: (52.25, 90.19)
    backward-send ..................................: (0.69, 12.97)
    forward-send-backward-recv .....................: (1828.46, 1934.36)
    backward-send-forward-recv .....................: (132.10, 187.58)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.89, 9.21)
    grads-reduce-scatter ...........................: (19.25, 19.55)
    params-all-gather ..............................: (10.63, 10.76)
    optimizer-copy-to-main-grad ....................: (0.24, 0.35)
    optimizer-clip-main-grad .......................: (2.41, 2.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.98)
    optimizer-copy-main-to-model-params ............: (1.47, 1.64)
    optimizer ......................................: (9.75, 9.92)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 5042.9 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.066518E+01 | loss scale: 1.0 | grad norm: 4.879 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4888.49, 4962.73)
    forward-compute ................................: (1046.42, 2841.92)
    backward-compute ...............................: (1461.26, 1805.81)
    batch-generator ................................: (25.63, 30.30)
    forward-recv ...................................: (23.26, 26.49)
    forward-send ...................................: (0.66, 0.74)
    backward-recv ..................................: (59.83, 101.06)
    backward-send ..................................: (4.27, 7.97)
    forward-send-backward-recv .....................: (2245.17, 2346.81)
    backward-send-forward-recv .....................: (214.39, 479.81)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.89, 9.16)
    grads-reduce-scatter ...........................: (19.14, 19.66)
    params-all-gather ..............................: (10.62, 10.75)
    optimizer-copy-to-main-grad ....................: (0.24, 0.34)
    optimizer-clip-main-grad .......................: (2.38, 2.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.94)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (9.64, 9.73)
Mon Feb 12 10:37:11 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   38C    P0             274W / 700W |  41498MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   44C    P0             240W / 700W |  43074MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   45C    P0             208W / 700W |  44680MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0             295W / 700W |  43994MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             406W / 700W |  43880MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   46C    P0             397W / 700W |  43880MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             282W / 700W |  43698MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             172W / 700W |  43698MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 4124.2 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.060308E+01 | loss scale: 1.0 | grad norm: 1.246 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3900.43, 3969.72)
    forward-compute ................................: (699.40, 2088.87)
    backward-compute ...............................: (1349.72, 1677.64)
    batch-generator ................................: (25.02, 29.64)
    forward-recv ...................................: (21.68, 29.12)
    forward-send ...................................: (0.63, 0.82)
    backward-recv ..................................: (52.01, 75.77)
    backward-send ..................................: (1.53, 7.34)
    forward-send-backward-recv .....................: (1764.84, 1828.11)
    backward-send-forward-recv .....................: (142.55, 193.47)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.84, 9.23)
    grads-reduce-scatter ...........................: (19.21, 19.49)
    params-all-gather ..............................: (10.62, 10.77)
    optimizer-copy-to-main-grad ....................: (0.24, 0.34)
    optimizer-clip-main-grad .......................: (2.38, 2.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.92)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (9.57, 9.65)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 4918.2 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.049371E+01 | loss scale: 1.0 | grad norm: 0.909 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4765.94, 4839.67)
    forward-compute ................................: (800.67, 2819.87)
    backward-compute ...............................: (1504.52, 1845.53)
    batch-generator ................................: (25.22, 29.52)
    forward-recv ...................................: (25.50, 32.40)
    forward-send ...................................: (0.73, 0.89)
    backward-recv ..................................: (49.02, 80.62)
    backward-send ..................................: (0.62, 6.50)
    forward-send-backward-recv .....................: (2347.84, 2452.18)
    backward-send-forward-recv .....................: (129.34, 276.21)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.87, 9.24)
    grads-reduce-scatter ...........................: (19.11, 19.53)
    params-all-gather ..............................: (10.62, 10.75)
    optimizer-copy-to-main-grad ....................: (0.24, 0.34)
    optimizer-clip-main-grad .......................: (1.33, 1.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.89)
    optimizer-copy-main-to-model-params ............: (1.47, 1.55)
    optimizer ......................................: (8.56, 8.65)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 5129.3 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.050037E+01 | loss scale: 1.0 | grad norm: 1.218 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4966.44, 5031.09)
    forward-compute ................................: (1024.85, 2801.36)
    backward-compute ...............................: (1395.20, 1758.87)
    batch-generator ................................: (25.69, 30.89)
    forward-recv ...................................: (22.13, 339.24)
    forward-send ...................................: (0.59, 1.00)
    backward-recv ..................................: (79.10, 350.57)
    backward-send ..................................: (0.74, 3.30)
    forward-send-backward-recv .....................: (2171.70, 2434.87)
    backward-send-forward-recv .....................: (143.57, 470.53)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.84, 9.27)
    grads-reduce-scatter ...........................: (19.14, 19.56)
    params-all-gather ..............................: (10.60, 10.74)
    optimizer-copy-to-main-grad ....................: (0.24, 0.34)
    optimizer-clip-main-grad .......................: (1.85, 1.88)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.89)
    optimizer-copy-main-to-model-params ............: (1.47, 1.55)
    optimizer ......................................: (9.05, 9.14)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 4128.7 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047843E+01 | loss scale: 1.0 | grad norm: 0.838 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3977.83, 4048.71)
    forward-compute ................................: (719.16, 2145.21)
    backward-compute ...............................: (1386.37, 1778.73)
    batch-generator ................................: (25.27, 29.85)
    forward-recv ...................................: (20.17, 27.41)
    forward-send ...................................: (0.61, 0.77)
    backward-recv ..................................: (63.83, 96.82)
    backward-send ..................................: (0.70, 2.74)
    forward-send-backward-recv .....................: (1673.65, 1815.24)
    backward-send-forward-recv .....................: (131.82, 227.16)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.84, 9.10)
    grads-reduce-scatter ...........................: (19.17, 19.61)
    params-all-gather ..............................: (10.61, 10.75)
    optimizer-copy-to-main-grad ....................: (0.24, 0.34)
    optimizer-clip-main-grad .......................: (1.20, 1.20)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.87)
    optimizer-copy-main-to-model-params ............: (1.47, 1.55)
    optimizer ......................................: (8.31, 8.40)
Mon Feb 12 10:40:16 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   39C    P0             366W / 700W |  41498MiB / 81559MiB |      9%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   46C    P0             521W / 700W |  43074MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   46C    P0             495W / 700W |  50988MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   40C    P0             427W / 700W |  51006MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             431W / 700W |  43880MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             411W / 700W |  43880MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   44C    P0             450W / 700W |  45274MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             432W / 700W |  45274MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 4267.7 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044379E+01 | loss scale: 1.0 | grad norm: 1.145 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4032.20, 4099.35)
    forward-compute ................................: (758.98, 2119.91)
    backward-compute ...............................: (1448.82, 1807.13)
    batch-generator ................................: (25.33, 30.42)
    forward-recv ...................................: (19.07, 30.08)
    forward-send ...................................: (0.56, 0.85)
    backward-recv ..................................: (58.67, 95.68)
    backward-send ..................................: (0.69, 6.94)
    forward-send-backward-recv .....................: (1712.62, 1766.45)
    backward-send-forward-recv .....................: (131.64, 183.69)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.85, 9.34)
    grads-reduce-scatter ...........................: (19.19, 19.63)
    params-all-gather ..............................: (10.60, 10.76)
    optimizer-copy-to-main-grad ....................: (0.24, 0.33)
    optimizer-clip-main-grad .......................: (1.58, 1.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.87)
    optimizer-copy-main-to-model-params ............: (1.47, 1.56)
    optimizer ......................................: (8.70, 8.79)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 4444.2 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.043016E+01 | loss scale: 1.0 | grad norm: 0.983 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4282.02, 4359.58)
    forward-compute ................................: (741.70, 2396.48)
    backward-compute ...............................: (1433.66, 1790.79)
    batch-generator ................................: (25.36, 29.96)
    forward-recv ...................................: (24.73, 27.51)
    forward-send ...................................: (0.70, 0.81)
    backward-recv ..................................: (74.02, 91.99)
    backward-send ..................................: (0.69, 0.95)
    forward-send-backward-recv .....................: (1971.85, 2080.57)
    backward-send-forward-recv .....................: (137.62, 173.44)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.84, 9.13)
    grads-reduce-scatter ...........................: (19.20, 19.52)
    params-all-gather ..............................: (10.62, 10.75)
    optimizer-copy-to-main-grad ....................: (0.24, 0.33)
    optimizer-clip-main-grad .......................: (1.84, 1.87)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.88)
    optimizer-copy-main-to-model-params ............: (1.47, 1.55)
    optimizer ......................................: (9.00, 9.22)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 4685.3 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041757E+01 | loss scale: 1.0 | grad norm: 0.685 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4526.60, 4591.22)
    forward-compute ................................: (999.19, 2406.05)
    backward-compute ...............................: (1398.08, 1761.06)
    batch-generator ................................: (25.37, 29.66)
    forward-recv ...................................: (20.70, 23.58)
    forward-send ...................................: (0.61, 0.69)
    backward-recv ..................................: (70.84, 99.26)
    backward-send ..................................: (0.80, 3.16)
    forward-send-backward-recv .....................: (1946.00, 2069.73)
    backward-send-forward-recv .....................: (403.50, 466.97)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.81, 9.19)
    grads-reduce-scatter ...........................: (19.17, 19.60)
    params-all-gather ..............................: (10.62, 10.76)
    optimizer-copy-to-main-grad ....................: (0.25, 0.34)
    optimizer-clip-main-grad .......................: (1.46, 1.47)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.88)
    optimizer-copy-main-to-model-params ............: (1.47, 1.55)
    optimizer ......................................: (8.60, 8.69)
[SYM206-GPU-A0205-P2-Node51:422665:0:422784] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x55a9c2410b4b)
malloc_consolidate(): invalid chunk size
benchmark/test_packing.sh.two.bak: line 139: 422584 Killed                  torchrun $DISTRIBUTED_ARGS pretrain_gpt.py $GPT_ARGS $DATA_ARGS $OUTPUT_ARGS --distributed-backend nccl
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (489876): No such process
kill: (489882): No such process
kill: (489888): No such process
kill: (489894): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=4, tp=2, pp=2, mbs=2
LOCAL_IP = 10.64.24.51
DP=4, MP=2, PP=2
[2024-02-12 10:49:02,536] torch.distributed.run: [WARNING] 
[2024-02-12 10:49:02,536] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 10:49:02,536] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 10:49:02,536] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1714528256
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1714528256
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.467 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.482 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.493 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.548 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.037 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.152 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.216 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.205 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (877.45, 930.56)
    train/valid/test-data-iterators-setup ..........: (0.02, 11122.17)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 7230.9 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.094822E+01 | loss scale: 1.0 | grad norm: 6.512 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 9] (after 10 iterations) memory (MB) | allocated: 14797.8388671875 | max allocated: 26641.19677734375 | reserved: 28728.0 | max reserved: 28728.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 14797.8388671875 | max allocated: 26641.19677734375 | reserved: 28720.0 | max reserved: 28720.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (6676.05, 6726.23)
    forward-compute ................................: (1316.13, 3847.19)
    backward-compute ...............................: (1963.80, 2357.93)
    batch-generator ................................: (252.76, 271.91)
    forward-recv ...................................: (282.31, 295.23)
    forward-send ...................................: (2.70, 7.46)
    backward-recv ..................................: (57.25, 72.83)
    backward-send ..................................: (0.53, 0.70)
    forward-send-backward-recv .....................: (3192.72, 3341.47)
    backward-send-forward-recv .....................: (301.70, 374.74)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.98, 9.18)
    grads-reduce-scatter ...........................: (19.20, 434.35)
    params-all-gather ..............................: (10.63, 10.76)
    optimizer-copy-to-main-grad ....................: (0.27, 0.41)
    optimizer-clip-main-grad .......................: (5.36, 5.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.93, 5.12)
    optimizer-copy-main-to-model-params ............: (1.47, 1.55)
    optimizer ......................................: (13.31, 13.43)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 5658.7 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.091043E+01 | loss scale: 1.0 | grad norm: 26.869 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5549.35, 5581.13)
    forward-compute ................................: (1269.25, 2925.77)
    backward-compute ...............................: (1858.08, 2125.80)
    batch-generator ................................: (51.31, 68.54)
    forward-recv ...................................: (13.05, 15.27)
    forward-send ...................................: (0.37, 0.45)
    backward-recv ..................................: (35.56, 55.78)
    backward-send ..................................: (0.47, 0.79)
    forward-send-backward-recv .....................: (2311.28, 2375.02)
    backward-send-forward-recv .....................: (488.30, 523.14)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.91, 9.11)
    grads-reduce-scatter ...........................: (19.21, 19.43)
    params-all-gather ..............................: (10.59, 10.76)
    optimizer-copy-to-main-grad ....................: (0.24, 0.34)
    optimizer-clip-main-grad .......................: (2.37, 2.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.92)
    optimizer-copy-main-to-model-params ............: (1.46, 1.55)
    optimizer ......................................: (9.58, 9.67)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 7667.1 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.066507E+01 | loss scale: 1.0 | grad norm: 4.706 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7558.18, 7600.17)
    forward-compute ................................: (1338.47, 4897.91)
    backward-compute ...............................: (2026.99, 2277.19)
    batch-generator ................................: (49.68, 61.34)
    forward-recv ...................................: (14.62, 19.68)
    forward-send ...................................: (0.40, 0.55)
    backward-recv ..................................: (42.29, 49.17)
    backward-send ..................................: (0.44, 9.29)
    forward-send-backward-recv .....................: (4040.15, 4161.00)
    backward-send-forward-recv .....................: (350.02, 595.20)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.90, 9.26)
    grads-reduce-scatter ...........................: (19.11, 19.50)
    params-all-gather ..............................: (10.61, 10.73)
    optimizer-copy-to-main-grad ....................: (0.25, 0.58)
    optimizer-clip-main-grad .......................: (2.39, 2.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.93)
    optimizer-copy-main-to-model-params ............: (1.46, 1.55)
    optimizer ......................................: (9.87, 9.96)
Mon Feb 12 10:53:55 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   37C    P0             417W / 700W |  34364MiB / 81559MiB |      6%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   44C    P0             389W / 700W |  34374MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   44C    P0             422W / 700W |  34696MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             400W / 700W |  34752MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             278W / 700W |  34142MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   44C    P0             294W / 700W |  34924MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   41C    P0             323W / 700W |  34912MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             231W / 700W |  34954MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 5338.3 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.060297E+01 | loss scale: 1.0 | grad norm: 1.238 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5132.40, 5180.10)
    forward-compute ................................: (996.05, 2764.46)
    backward-compute ...............................: (1860.92, 2150.09)
    batch-generator ................................: (50.11, 63.80)
    forward-recv ...................................: (14.52, 21.46)
    forward-send ...................................: (0.41, 0.58)
    backward-recv ..................................: (46.79, 55.35)
    backward-send ..................................: (0.44, 1.10)
    forward-send-backward-recv .....................: (2053.63, 2230.71)
    backward-send-forward-recv .....................: (234.64, 265.49)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.86, 9.11)
    grads-reduce-scatter ...........................: (19.21, 19.48)
    params-all-gather ..............................: (10.62, 10.76)
    optimizer-copy-to-main-grad ....................: (0.25, 0.36)
    optimizer-clip-main-grad .......................: (2.37, 2.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.93)
    optimizer-copy-main-to-model-params ............: (1.46, 1.58)
    optimizer ......................................: (9.56, 9.69)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 7426.7 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.049361E+01 | loss scale: 1.0 | grad norm: 0.906 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7313.45, 7358.63)
    forward-compute ................................: (1352.82, 4661.05)
    backward-compute ...............................: (2055.89, 2315.97)
    batch-generator ................................: (50.88, 60.14)
    forward-recv ...................................: (16.94, 18.88)
    forward-send ...................................: (0.47, 0.53)
    backward-recv ..................................: (40.07, 72.04)
    backward-send ..................................: (0.48, 10.00)
    forward-send-backward-recv .....................: (3749.13, 3865.77)
    backward-send-forward-recv .....................: (316.00, 556.23)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.80, 9.12)
    grads-reduce-scatter ...........................: (19.19, 19.56)
    params-all-gather ..............................: (10.61, 10.74)
    optimizer-copy-to-main-grad ....................: (0.25, 0.34)
    optimizer-clip-main-grad .......................: (1.33, 1.34)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.90)
    optimizer-copy-main-to-model-params ............: (1.46, 1.55)
    optimizer ......................................: (8.49, 8.59)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 5462.9 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049993E+01 | loss scale: 1.0 | grad norm: 1.097 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5347.76, 5387.86)
    forward-compute ................................: (1040.40, 2918.82)
    backward-compute ...............................: (1958.02, 2244.27)
    batch-generator ................................: (49.73, 58.89)
    forward-recv ...................................: (13.77, 17.26)
    forward-send ...................................: (0.37, 0.47)
    backward-recv ..................................: (42.39, 58.94)
    backward-send ..................................: (0.55, 9.58)
    forward-send-backward-recv .....................: (2184.42, 2309.67)
    backward-send-forward-recv .....................: (217.61, 335.00)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.82, 9.07)
    grads-reduce-scatter ...........................: (19.14, 19.55)
    params-all-gather ..............................: (10.64, 10.76)
    optimizer-copy-to-main-grad ....................: (0.25, 0.34)
    optimizer-clip-main-grad .......................: (1.72, 1.73)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.88)
    optimizer-copy-main-to-model-params ............: (1.46, 1.55)
    optimizer ......................................: (8.88, 8.96)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 7010.3 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047832E+01 | loss scale: 1.0 | grad norm: 0.914 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6895.95, 6935.34)
    forward-compute ................................: (1309.69, 4453.11)
    backward-compute ...............................: (1969.96, 2214.63)
    batch-generator ................................: (49.84, 58.61)
    forward-recv ...................................: (13.66, 19.49)
    forward-send ...................................: (0.37, 0.52)
    backward-recv ..................................: (31.53, 56.66)
    backward-send ..................................: (0.51, 3.26)
    forward-send-backward-recv .....................: (3497.86, 3599.95)
    backward-send-forward-recv .....................: (232.99, 564.70)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (8.92, 9.07)
    grads-reduce-scatter ...........................: (19.17, 19.55)
    params-all-gather ..............................: (10.60, 10.74)
    optimizer-copy-to-main-grad ....................: (0.25, 0.33)
    optimizer-clip-main-grad .......................: (1.19, 1.19)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.88)
    optimizer-copy-main-to-model-params ............: (1.46, 1.55)
    optimizer ......................................: (8.31, 8.52)
Mon Feb 12 10:58:17 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   37C    P0             433W / 700W |  34364MiB / 81559MiB |     38%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   44C    P0             364W / 700W |  34374MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   44C    P0             408W / 700W |  34696MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             418W / 700W |  34752MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             341W / 700W |  34930MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   44C    P0             398W / 700W |  34926MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   41C    P0             384W / 700W |  34918MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             398W / 700W |  34954MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 6324.3 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044194E+01 | loss scale: 1.0 | grad norm: 0.424 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6120.94, 6167.55)
    forward-compute ................................: (1325.44, 3618.53)
    backward-compute ...............................: (2001.49, 2255.32)
    batch-generator ................................: (49.07, 60.59)
    forward-recv ...................................: (14.62, 17.05)
    forward-send ...................................: (0.39, 0.50)
    backward-recv ..................................: (49.96, 60.01)
    backward-send ..................................: (0.51, 2.71)
    forward-send-backward-recv .....................: (2679.33, 2769.86)
    backward-send-forward-recv .....................: (271.64, 550.02)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.83, 9.33)
    grads-reduce-scatter ...........................: (19.14, 19.49)
    params-all-gather ..............................: (10.60, 10.76)
    optimizer-copy-to-main-grad ....................: (0.25, 0.38)
    optimizer-clip-main-grad .......................: (1.22, 1.23)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.91)
    optimizer-copy-main-to-model-params ............: (1.46, 1.58)
    optimizer ......................................: (8.47, 8.58)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 5438.4 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.042902E+01 | loss scale: 1.0 | grad norm: 0.727 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5310.21, 5357.78)
    forward-compute ................................: (1030.17, 2902.97)
    backward-compute ...............................: (1959.21, 2278.22)
    batch-generator ................................: (49.17, 61.06)
    forward-recv ...................................: (14.97, 20.88)
    forward-send ...................................: (0.43, 0.61)
    backward-recv ..................................: (38.33, 53.66)
    backward-send ..................................: (0.51, 4.91)
    forward-send-backward-recv .....................: (2098.47, 2303.97)
    backward-send-forward-recv .....................: (205.20, 345.47)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.88, 9.30)
    grads-reduce-scatter ...........................: (19.20, 19.52)
    params-all-gather ..............................: (10.61, 10.77)
    optimizer-copy-to-main-grad ....................: (0.25, 0.41)
    optimizer-clip-main-grad .......................: (1.77, 1.85)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 5.05)
    optimizer-copy-main-to-model-params ............: (1.46, 1.55)
    optimizer ......................................: (9.32, 9.41)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 7319.5 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041305E+01 | loss scale: 1.0 | grad norm: 1.088 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7193.26, 7240.03)
    forward-compute ................................: (1279.40, 4589.62)
    backward-compute ...............................: (1925.73, 2275.45)
    batch-generator ................................: (50.25, 75.39)
    forward-recv ...................................: (13.52, 17.98)
    forward-send ...................................: (0.38, 0.53)
    backward-recv ..................................: (40.61, 59.62)
    backward-send ..................................: (0.49, 6.90)
    forward-send-backward-recv .....................: (3726.68, 3953.94)
    backward-send-forward-recv .....................: (298.55, 536.14)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (8.90, 9.27)
    grads-reduce-scatter ...........................: (19.22, 19.55)
    params-all-gather ..............................: (10.62, 10.75)
    optimizer-copy-to-main-grad ....................: (0.25, 0.41)
    optimizer-clip-main-grad .......................: (1.37, 1.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.92)
    optimizer-copy-main-to-model-params ............: (1.46, 1.55)
    optimizer ......................................: (8.66, 8.74)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (491382): No such process
kill: (491388): No such process
kill: (491394): No such process
kill: (491400): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=4, tp=4, pp=1, mbs=8
LOCAL_IP = 10.64.24.51
DP=4, MP=4, PP=1
[2024-02-12 11:02:40,099] torch.distributed.run: [WARNING] 
[2024-02-12 11:02:40,099] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 11:02:40,099] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 11:02:40,099] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.163 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.220 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.069 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.029 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (283.66, 315.63)
    train/valid/test-data-iterators-setup ..........: (0.02, 11441.47)
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 4609.3 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.093380E+01 | loss scale: 1.0 | grad norm: 6.393 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4423.20, 4463.14)
    forward-compute ................................: (2624.56, 2779.17)
    backward-compute ...............................: (1635.04, 1831.10)
    batch-generator ................................: (407.96, 429.60)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (56.29, 56.43)
    params-all-gather ..............................: (29.60, 29.76)
    optimizer-copy-to-main-grad ....................: (0.51, 0.73)
    optimizer-clip-main-grad .......................: (5.39, 5.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.92, 5.07)
    optimizer-copy-main-to-model-params ............: (1.65, 1.75)
    optimizer ......................................: (13.37, 13.47)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 3497.2 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.089313E+01 | loss scale: 1.0 | grad norm: 27.668 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3349.67, 3369.66)
    forward-compute ................................: (1756.28, 1841.20)
    backward-compute ...............................: (1502.17, 1592.93)
    batch-generator ................................: (15.00, 21.83)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.87, 56.11)
    params-all-gather ..............................: (29.62, 29.70)
    optimizer-copy-to-main-grad ....................: (0.54, 0.72)
    optimizer-clip-main-grad .......................: (2.53, 2.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.85)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (10.29, 10.40)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 4175.5 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.064457E+01 | loss scale: 1.0 | grad norm: 1.820 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3982.31, 4018.77)
    forward-compute ................................: (2244.83, 2316.84)
    backward-compute ...............................: (1693.46, 1746.71)
    batch-generator ................................: (15.29, 23.29)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.86, 56.07)
    params-all-gather ..............................: (29.59, 29.71)
    optimizer-copy-to-main-grad ....................: (0.50, 0.84)
    optimizer-clip-main-grad .......................: (2.54, 2.56)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.88)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (10.42, 10.54)
Mon Feb 12 11:05:54 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   37C    P0             393W / 700W |  77792MiB / 81559MiB |     42%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   44C    P0             409W / 700W |  72276MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   44C    P0             410W / 700W |  72520MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             359W / 700W |  70834MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             481W / 700W |  65720MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             485W / 700W |  67300MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             375W / 700W |  72176MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             440W / 700W |  71470MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 3860.8 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.059652E+01 | loss scale: 1.0 | grad norm: 1.013 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3603.85, 3640.18)
    forward-compute ................................: (2018.83, 2088.71)
    backward-compute ...............................: (1532.91, 1608.94)
    batch-generator ................................: (14.81, 24.20)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.84, 56.49)
    params-all-gather ..............................: (29.63, 29.82)
    optimizer-copy-to-main-grad ....................: (0.50, 0.99)
    optimizer-clip-main-grad .......................: (2.45, 2.46)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.93)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (10.73, 10.83)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 3985.4 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.049139E+01 | loss scale: 1.0 | grad norm: 1.063 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3823.14, 3856.75)
    forward-compute ................................: (2023.39, 2105.98)
    backward-compute ...............................: (1720.82, 1825.09)
    batch-generator ................................: (14.73, 24.49)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.86, 56.05)
    params-all-gather ..............................: (29.60, 30.70)
    optimizer-copy-to-main-grad ....................: (0.51, 0.80)
    optimizer-clip-main-grad .......................: (2.03, 2.04)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.93)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (9.88, 9.98)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 3681.2 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.050094E+01 | loss scale: 1.0 | grad norm: 0.816 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3522.56, 3558.06)
    forward-compute ................................: (1832.44, 1915.54)
    backward-compute ...............................: (1604.77, 1695.21)
    batch-generator ................................: (14.70, 23.81)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.87, 56.07)
    params-all-gather ..............................: (29.61, 29.71)
    optimizer-copy-to-main-grad ....................: (0.51, 0.83)
    optimizer-clip-main-grad .......................: (1.27, 1.28)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.94)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (9.22, 9.32)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 4162.8 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.048156E+01 | loss scale: 1.0 | grad norm: 1.405 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4012.35, 4035.62)
    forward-compute ................................: (2274.41, 2406.89)
    backward-compute ...............................: (1606.62, 1737.00)
    batch-generator ................................: (14.75, 24.29)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.89, 57.45)
    params-all-gather ..............................: (29.60, 29.72)
    optimizer-copy-to-main-grad ....................: (0.55, 0.83)
    optimizer-clip-main-grad .......................: (1.78, 1.80)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.92)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (9.76, 9.87)
Mon Feb 12 11:08:30 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   39C    P0             560W / 700W |  69798MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   46C    P0             493W / 700W |  78622MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   47C    P0             516W / 700W |  72770MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   40C    P0             481W / 700W |  76400MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             380W / 700W |  71948MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             443W / 700W |  72108MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             360W / 700W |  70314MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             363W / 700W |  70550MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 3805.3 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044938E+01 | loss scale: 1.0 | grad norm: 1.316 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3566.55, 3587.94)
    forward-compute ................................: (1809.88, 1895.45)
    backward-compute ...............................: (1661.51, 1748.70)
    batch-generator ................................: (14.54, 24.51)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.85, 56.15)
    params-all-gather ..............................: (29.60, 29.70)
    optimizer-copy-to-main-grad ....................: (0.52, 0.82)
    optimizer-clip-main-grad .......................: (1.91, 1.92)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.90)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (9.71, 9.82)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 3672.8 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.043467E+01 | loss scale: 1.0 | grad norm: 1.440 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3521.52, 3555.61)
    forward-compute ................................: (1760.44, 1877.61)
    backward-compute ...............................: (1640.79, 1778.29)
    batch-generator ................................: (14.88, 23.94)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.85, 55.96)
    params-all-gather ..............................: (29.61, 29.73)
    optimizer-copy-to-main-grad ....................: (0.51, 0.78)
    optimizer-clip-main-grad .......................: (2.03, 2.05)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.76, 4.90)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (9.90, 10.01)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 4117.4 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.042254E+01 | loss scale: 1.0 | grad norm: 1.062 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3941.63, 3981.81)
    forward-compute ................................: (2223.89, 2309.40)
    backward-compute ...............................: (1634.77, 1749.76)
    batch-generator ................................: (15.80, 23.98)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.89, 56.00)
    params-all-gather ..............................: (29.61, 29.70)
    optimizer-copy-to-main-grad ....................: (0.51, 0.93)
    optimizer-clip-main-grad .......................: (2.17, 2.19)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.92)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (10.19, 10.30)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (492448): No such process
kill: (492454): No such process
kill: (492460): No such process
kill: (492466): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=4, tp=4, pp=1, mbs=4
LOCAL_IP = 10.64.24.51
DP=4, MP=4, PP=1
[2024-02-12 11:12:02,399] torch.distributed.run: [WARNING] 
[2024-02-12 11:12:02,399] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 11:12:02,399] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 11:12:02,399] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.169 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.227 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.951 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.033 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (279.58, 312.81)
    train/valid/test-data-iterators-setup ..........: (0.02, 10664.98)
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 5486.2 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.093369E+01 | loss scale: 1.0 | grad norm: 6.402 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5332.90, 5357.62)
    forward-compute ................................: (3181.47, 3342.02)
    backward-compute ...............................: (1975.02, 2160.68)
    batch-generator ................................: (420.95, 436.74)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (56.20, 57.05)
    params-all-gather ..............................: (29.60, 29.74)
    optimizer-copy-to-main-grad ....................: (0.55, 0.79)
    optimizer-clip-main-grad .......................: (5.41, 5.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.90, 5.08)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (13.49, 13.61)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 5322.7 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.089319E+01 | loss scale: 1.0 | grad norm: 27.585 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5203.79, 5207.67)
    forward-compute ................................: (3286.89, 3394.63)
    backward-compute ...............................: (1793.18, 1903.55)
    batch-generator ................................: (29.02, 37.20)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.83, 55.91)
    params-all-gather ..............................: (29.60, 29.74)
    optimizer-copy-to-main-grad ....................: (0.56, 0.71)
    optimizer-clip-main-grad .......................: (2.53, 2.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.88)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (10.36, 10.48)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 4949.4 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.064458E+01 | loss scale: 1.0 | grad norm: 1.817 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4813.16, 4835.91)
    forward-compute ................................: (2761.25, 2803.34)
    backward-compute ...............................: (2006.94, 2043.13)
    batch-generator ................................: (29.32, 40.84)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.84, 55.98)
    params-all-gather ..............................: (29.60, 29.80)
    optimizer-copy-to-main-grad ....................: (0.56, 0.70)
    optimizer-clip-main-grad .......................: (2.53, 2.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.90)
    optimizer-copy-main-to-model-params ............: (1.64, 1.76)
    optimizer ......................................: (10.32, 10.45)
Mon Feb 12 11:16:04 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   38C    P0             477W / 700W |  54370MiB / 81559MiB |     19%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   45C    P0             459W / 700W |  54624MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   46C    P0             436W / 700W |  54762MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0             455W / 700W |  54318MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             219W / 700W |  63144MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   44C    P0             303W / 700W |  63056MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   42C    P0             191W / 700W |  62884MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             214W / 700W |  62724MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 5157.8 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.059660E+01 | loss scale: 1.0 | grad norm: 1.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4935.69, 4956.07)
    forward-compute ................................: (3031.92, 3063.95)
    backward-compute ...............................: (1851.14, 1908.18)
    batch-generator ................................: (29.49, 40.28)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.83, 55.99)
    params-all-gather ..............................: (29.58, 29.69)
    optimizer-copy-to-main-grad ....................: (0.53, 0.71)
    optimizer-clip-main-grad .......................: (2.42, 2.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.89)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (10.18, 10.29)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 5817.4 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.049150E+01 | loss scale: 1.0 | grad norm: 1.120 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5678.87, 5698.45)
    forward-compute ................................: (3546.55, 3640.41)
    backward-compute ...............................: (2041.56, 2127.50)
    batch-generator ................................: (29.78, 38.80)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.83, 56.02)
    params-all-gather ..............................: (29.58, 29.72)
    optimizer-copy-to-main-grad ....................: (0.53, 0.78)
    optimizer-clip-main-grad .......................: (1.90, 1.91)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.89)
    optimizer-copy-main-to-model-params ............: (1.63, 1.75)
    optimizer ......................................: (9.79, 9.91)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 4784.4 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.050098E+01 | loss scale: 1.0 | grad norm: 0.745 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4644.88, 4658.29)
    forward-compute ................................: (2634.47, 2738.66)
    backward-compute ...............................: (1903.17, 1995.35)
    batch-generator ................................: (29.76, 42.02)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.85, 55.96)
    params-all-gather ..............................: (29.59, 29.70)
    optimizer-copy-to-main-grad ....................: (0.51, 0.70)
    optimizer-clip-main-grad .......................: (1.25, 1.27)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.89)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (9.04, 9.15)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 5159.5 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047983E+01 | loss scale: 1.0 | grad norm: 0.677 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5027.71, 5043.93)
    forward-compute ................................: (2996.65, 3112.56)
    backward-compute ...............................: (1904.12, 2016.28)
    batch-generator ................................: (29.17, 38.94)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.85, 55.90)
    params-all-gather ..............................: (29.61, 29.72)
    optimizer-copy-to-main-grad ....................: (0.53, 0.71)
    optimizer-clip-main-grad .......................: (1.24, 1.25)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.89)
    optimizer-copy-main-to-model-params ............: (1.64, 1.76)
    optimizer ......................................: (9.04, 9.16)
Mon Feb 12 11:19:30 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   37C    P0             431W / 700W |  54370MiB / 81559MiB |     59%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   43C    P0             403W / 700W |  54624MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   44C    P0             440W / 700W |  54762MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             348W / 700W |  54318MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             354W / 700W |  68878MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   44C    P0             381W / 700W |  68790MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   42C    P0             366W / 700W |  68618MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             303W / 700W |  68458MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 4826.3 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044587E+01 | loss scale: 1.0 | grad norm: 0.976 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4603.12, 4618.65)
    forward-compute ................................: (2518.51, 2614.95)
    backward-compute ...............................: (1973.22, 2083.10)
    batch-generator ................................: (30.00, 39.45)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.83, 56.02)
    params-all-gather ..............................: (29.62, 29.74)
    optimizer-copy-to-main-grad ....................: (0.51, 0.73)
    optimizer-clip-main-grad .......................: (1.52, 1.53)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.91)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (9.34, 9.45)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 5253.6 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.043578E+01 | loss scale: 1.0 | grad norm: 1.054 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5111.90, 5133.14)
    forward-compute ................................: (3051.60, 3191.95)
    backward-compute ...............................: (1924.65, 2063.86)
    batch-generator ................................: (29.67, 36.91)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.85, 55.91)
    params-all-gather ..............................: (29.60, 29.72)
    optimizer-copy-to-main-grad ....................: (0.52, 0.74)
    optimizer-clip-main-grad .......................: (2.41, 2.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.88)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (10.32, 10.43)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 4906.3 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.042069E+01 | loss scale: 1.0 | grad norm: 0.696 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4770.05, 4781.82)
    forward-compute ................................: (2743.70, 2829.56)
    backward-compute ...............................: (1908.09, 2010.55)
    batch-generator ................................: (30.24, 40.10)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.85, 55.92)
    params-all-gather ..............................: (29.58, 29.69)
    optimizer-copy-to-main-grad ....................: (0.51, 0.69)
    optimizer-clip-main-grad .......................: (1.76, 1.77)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.89)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (9.48, 9.60)
[2024-02-12 11:22:58,200] torch.distributed.elastic.agent.server.api: [ERROR] Error waiting on exit barrier. Elapsed: 100.25190925598145 seconds
benchmark/test_packing.sh.two.bak: line 139: 426401 Killed                  torchrun $DISTRIBUTED_ARGS pretrain_gpt.py $GPT_ARGS $DATA_ARGS $OUTPUT_ARGS --distributed-backend nccl
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (493514): No such process
kill: (493520): No such process
kill: (493526): No such process
kill: (493532): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=4, tp=4, pp=1, mbs=2
LOCAL_IP = 10.64.24.51
DP=4, MP=4, PP=1
[2024-02-12 11:23:35,050] torch.distributed.run: [WARNING] 
[2024-02-12 11:23:35,050] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 11:23:35,050] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 11:23:35,050] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.068 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.174 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.920 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.110 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (288.58, 303.51)
    train/valid/test-data-iterators-setup ..........: (0.02, 13858.62)
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 7996.9 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.093367E+01 | loss scale: 1.0 | grad norm: 6.401 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7861.84, 7873.39)
    forward-compute ................................: (4995.51, 5142.63)
    backward-compute ...............................: (2688.70, 2844.80)
    batch-generator ................................: (470.68, 505.02)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (56.30, 56.37)
    params-all-gather ..............................: (29.59, 29.69)
    optimizer-copy-to-main-grad ....................: (0.53, 0.72)
    optimizer-clip-main-grad .......................: (5.43, 5.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.95, 5.11)
    optimizer-copy-main-to-model-params ............: (1.64, 1.74)
    optimizer ......................................: (13.43, 13.53)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 6796.5 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.089325E+01 | loss scale: 1.0 | grad norm: 27.631 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6679.17, 6682.55)
    forward-compute ................................: (4071.81, 4130.74)
    backward-compute ...............................: (2517.79, 2576.30)
    batch-generator ................................: (76.70, 101.56)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.84, 55.99)
    params-all-gather ..............................: (29.60, 29.71)
    optimizer-copy-to-main-grad ....................: (0.48, 0.72)
    optimizer-clip-main-grad .......................: (2.54, 2.56)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.87)
    optimizer-copy-main-to-model-params ............: (1.64, 1.74)
    optimizer ......................................: (10.33, 10.43)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 7350.2 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.064472E+01 | loss scale: 1.0 | grad norm: 1.815 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7236.05, 7241.31)
    forward-compute ................................: (4440.66, 4499.00)
    backward-compute ...............................: (2706.58, 2765.29)
    batch-generator ................................: (76.55, 95.94)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.83, 56.08)
    params-all-gather ..............................: (29.61, 29.73)
    optimizer-copy-to-main-grad ....................: (0.48, 0.73)
    optimizer-clip-main-grad .......................: (2.51, 2.53)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.92)
    optimizer-copy-main-to-model-params ............: (1.64, 1.75)
    optimizer ......................................: (10.35, 10.47)
Mon Feb 12 11:29:02 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   36C    P0             285W / 700W |  39958MiB / 81559MiB |     52%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   43C    P0             317W / 700W |  40612MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   44C    P0             295W / 700W |  39998MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             376W / 700W |  39928MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   37C    P0             325W / 700W |  43736MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   42C    P0             281W / 700W |  43646MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   40C    P0             310W / 700W |  43724MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             299W / 700W |  43894MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 7041.8 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.059649E+01 | loss scale: 1.0 | grad norm: 1.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6828.33, 6843.10)
    forward-compute ................................: (4186.98, 4277.80)
    backward-compute ...............................: (2517.03, 2620.09)
    batch-generator ................................: (74.34, 101.09)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.82, 55.88)
    params-all-gather ..............................: (29.59, 29.73)
    optimizer-copy-to-main-grad ....................: (0.48, 0.68)
    optimizer-clip-main-grad .......................: (2.43, 2.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.86)
    optimizer-copy-main-to-model-params ............: (1.64, 1.74)
    optimizer ......................................: (10.27, 10.37)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 7476.5 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.049143E+01 | loss scale: 1.0 | grad norm: 1.115 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7356.23, 7365.01)
    forward-compute ................................: (4516.91, 4581.94)
    backward-compute ...............................: (2741.14, 2812.65)
    batch-generator ................................: (75.63, 95.69)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.83, 57.93)
    params-all-gather ..............................: (29.61, 29.73)
    optimizer-copy-to-main-grad ....................: (0.48, 0.68)
    optimizer-clip-main-grad .......................: (1.89, 1.91)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.84)
    optimizer-copy-main-to-model-params ............: (1.64, 1.74)
    optimizer ......................................: (9.60, 9.71)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 7938.3 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.050094E+01 | loss scale: 1.0 | grad norm: 0.747 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7818.43, 7830.27)
    forward-compute ................................: (5059.29, 5157.15)
    backward-compute ...............................: (2629.51, 2735.03)
    batch-generator ................................: (76.81, 97.89)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.84, 56.12)
    params-all-gather ..............................: (29.63, 30.20)
    optimizer-copy-to-main-grad ....................: (0.50, 0.67)
    optimizer-clip-main-grad .......................: (1.26, 1.27)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.86)
    optimizer-copy-main-to-model-params ............: (1.64, 1.74)
    optimizer ......................................: (8.99, 9.11)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 7337.6 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.048032E+01 | loss scale: 1.0 | grad norm: 0.715 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7216.54, 7225.29)
    forward-compute ................................: (4498.51, 4535.83)
    backward-compute ...............................: (2652.76, 2689.70)
    batch-generator ................................: (74.45, 94.26)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.83, 55.87)
    params-all-gather ..............................: (29.62, 29.79)
    optimizer-copy-to-main-grad ....................: (0.49, 0.71)
    optimizer-clip-main-grad .......................: (1.39, 1.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.86)
    optimizer-copy-main-to-model-params ............: (1.64, 1.74)
    optimizer ......................................: (9.24, 9.35)
Mon Feb 12 11:34:02 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   36C    P0             323W / 700W |  39972MiB / 81559MiB |     66%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   42C    P0             376W / 700W |  40620MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   43C    P0             376W / 700W |  39998MiB / 81559MiB |     97%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             355W / 700W |  39928MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   37C    P0             351W / 700W |  43736MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   42C    P0             339W / 700W |  43648MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   40C    P0             370W / 700W |  43724MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             374W / 700W |  43894MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 7250.0 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.045009E+01 | loss scale: 1.0 | grad norm: 1.620 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7040.10, 7050.12)
    forward-compute ................................: (4266.45, 4335.83)
    backward-compute ...............................: (2671.71, 2740.93)
    batch-generator ................................: (71.69, 96.67)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.84, 55.93)
    params-all-gather ..............................: (29.61, 29.82)
    optimizer-copy-to-main-grad ....................: (0.48, 0.68)
    optimizer-clip-main-grad .......................: (2.42, 2.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.79, 4.84)
    optimizer-copy-main-to-model-params ............: (1.64, 1.74)
    optimizer ......................................: (10.19, 10.29)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 7158.7 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.043716E+01 | loss scale: 1.0 | grad norm: 0.834 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7033.55, 7045.52)
    forward-compute ................................: (4223.20, 4383.10)
    backward-compute ...............................: (2623.36, 2777.79)
    batch-generator ................................: (74.16, 94.22)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (55.85, 55.88)
    params-all-gather ..............................: (29.58, 29.73)
    optimizer-copy-to-main-grad ....................: (0.49, 0.66)
    optimizer-clip-main-grad .......................: (2.30, 2.32)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.78, 4.86)
    optimizer-copy-main-to-model-params ............: (1.64, 1.74)
    optimizer ......................................: (10.02, 10.16)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 7092.0 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.042265E+01 | loss scale: 1.0 | grad norm: 0.725 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6970.04, 6979.75)
    forward-compute ................................: (4165.27, 4344.19)
    backward-compute ...............................: (2592.56, 2779.93)
    batch-generator ................................: (75.47, 94.57)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-reduce-scatter ...........................: (55.84, 56.02)
    params-all-gather ..............................: (29.62, 29.73)
    optimizer-copy-to-main-grad ....................: (0.48, 0.68)
    optimizer-clip-main-grad .......................: (1.78, 1.79)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.77, 4.84)
    optimizer-copy-main-to-model-params ............: (1.64, 1.74)
    optimizer ......................................: (9.50, 9.61)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (494580): No such process
kill: (494586): No such process
kill: (494592): No such process
kill: (494598): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=4, tp=1, pp=4, mbs=8
LOCAL_IP = 10.64.24.51
DP=4, MP=1, PP=4
[2024-02-12 11:38:37,704] torch.distributed.run: [WARNING] 
[2024-02-12 11:38:37,704] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 11:38:37,704] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 11:38:37,704] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1611038720
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1817092096
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...

Loading exists cache end, time cost:  4.843 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.843 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.846 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.894 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.894 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.861 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.878 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.849 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.068 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.073 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.101 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.304 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.345 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.374 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.717 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.416 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (44.78, 618.29)
    train/valid/test-data-iterators-setup ..........: (10102.82, 11699.36)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 5269.5 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.092161E+01 | loss scale: 1.0 | grad norm: 6.396 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 12] (after 10 iterations) memory (MB) | allocated: 15682.8544921875 | max allocated: 43877.85205078125 | reserved: 48018.0 | max reserved: 48018.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 13913.314453125 | max allocated: 34988.0478515625 | reserved: 39660.0 | max reserved: 39660.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (4389.36, 4681.31)
    forward-compute ................................: (518.61, 2342.99)
    backward-compute ...............................: (929.81, 1866.78)
    batch-generator ................................: (35.53, 51.20)
    forward-recv ...................................: (113.51, 288.20)
    forward-send ...................................: (4.36, 135.87)
    backward-recv ..................................: (148.52, 547.00)
    backward-send ..................................: (1.02, 37.98)
    forward-send-backward-recv .....................: (2156.32, 2393.14)
    backward-send-forward-recv .....................: (39.29, 180.97)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 17.99)
    grads-reduce-scatter ...........................: (20.41, 422.41)
    params-all-gather ..............................: (9.73, 11.02)
    optimizer-copy-to-main-grad ....................: (0.14, 0.24)
    optimizer-clip-main-grad .......................: (5.41, 5.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.61, 5.27)
    optimizer-copy-main-to-model-params ............: (1.32, 1.52)
    optimizer ......................................: (13.31, 13.52)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 3948.0 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.089149E+01 | loss scale: 1.0 | grad norm: 24.432 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3584.22, 3823.64)
    forward-compute ................................: (425.25, 1879.75)
    backward-compute ...............................: (842.59, 1691.36)
    batch-generator ................................: (12.72, 14.98)
    forward-recv ...................................: (39.79, 114.63)
    forward-send ...................................: (0.97, 37.43)
    backward-recv ..................................: (137.65, 493.32)
    backward-send ..................................: (0.99, 33.47)
    forward-send-backward-recv .....................: (1818.70, 2017.33)
    backward-send-forward-recv .....................: (26.19, 143.34)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.99)
    grads-reduce-scatter ...........................: (18.05, 20.72)
    params-all-gather ..............................: (9.70, 11.01)
    optimizer-copy-to-main-grad ....................: (0.13, 0.21)
    optimizer-clip-main-grad .......................: (2.25, 2.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.15)
    optimizer-copy-main-to-model-params ............: (1.31, 1.52)
    optimizer ......................................: (9.49, 9.70)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 4305.0 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.065848E+01 | loss scale: 1.0 | grad norm: 5.613 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3819.07, 4091.55)
    forward-compute ................................: (505.44, 1949.52)
    backward-compute ...............................: (989.89, 1792.49)
    batch-generator ................................: (12.79, 16.19)
    forward-recv ...................................: (43.39, 103.47)
    forward-send ...................................: (1.09, 27.50)
    backward-recv ..................................: (144.40, 564.96)
    backward-send ..................................: (0.99, 39.91)
    forward-send-backward-recv .....................: (1885.01, 2024.70)
    backward-send-forward-recv .....................: (30.22, 209.00)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 17.89)
    grads-reduce-scatter ...........................: (18.12, 20.70)
    params-all-gather ..............................: (9.71, 11.00)
    optimizer-copy-to-main-grad ....................: (0.13, 0.25)
    optimizer-clip-main-grad .......................: (2.26, 2.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.15)
    optimizer-copy-main-to-model-params ............: (1.31, 1.52)
    optimizer ......................................: (9.54, 9.75)
Mon Feb 12 11:42:07 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   33C    P0             209W / 700W |  52556MiB / 81559MiB |     33%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   39C    P0             222W / 700W |  60098MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   39C    P0             311W / 700W |  56492MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   35C    P0             287W / 700W |  46644MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             228W / 700W |  57492MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             323W / 700W |  53862MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             179W / 700W |  57124MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             287W / 700W |  53730MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 4043.4 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.060086E+01 | loss scale: 1.0 | grad norm: 1.162 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3579.52, 3820.85)
    forward-compute ................................: (437.23, 1834.76)
    backward-compute ...............................: (862.05, 1698.17)
    batch-generator ................................: (12.76, 17.02)
    forward-recv ...................................: (40.48, 103.14)
    forward-send ...................................: (1.12, 31.23)
    backward-recv ..................................: (131.68, 473.46)
    backward-send ..................................: (0.84, 31.05)
    forward-send-backward-recv .....................: (1811.51, 2011.42)
    backward-send-forward-recv .....................: (26.17, 183.60)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.00)
    grads-reduce-scatter ...........................: (18.10, 20.78)
    params-all-gather ..............................: (9.72, 11.02)
    optimizer-copy-to-main-grad ....................: (0.13, 0.24)
    optimizer-clip-main-grad .......................: (2.25, 2.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.14)
    optimizer-copy-main-to-model-params ............: (1.31, 1.52)
    optimizer ......................................: (9.68, 9.89)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 4437.6 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.049325E+01 | loss scale: 1.0 | grad norm: 0.970 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3985.95, 4276.92)
    forward-compute ................................: (503.25, 2072.40)
    backward-compute ...............................: (992.33, 1869.47)
    batch-generator ................................: (12.71, 17.42)
    forward-recv ...................................: (43.61, 126.25)
    forward-send ...................................: (1.22, 31.39)
    backward-recv ..................................: (151.72, 594.18)
    backward-send ..................................: (1.16, 33.95)
    forward-send-backward-recv .....................: (1909.28, 2210.63)
    backward-send-forward-recv .....................: (41.13, 208.58)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.80)
    grads-reduce-scatter ...........................: (18.09, 20.74)
    params-all-gather ..............................: (9.71, 11.18)
    optimizer-copy-to-main-grad ....................: (0.13, 0.24)
    optimizer-clip-main-grad .......................: (1.75, 2.01)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.13)
    optimizer-copy-main-to-model-params ............: (1.31, 1.52)
    optimizer ......................................: (9.10, 9.31)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 4169.6 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049773E+01 | loss scale: 1.0 | grad norm: 1.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3772.15, 4025.90)
    forward-compute ................................: (462.75, 1980.76)
    backward-compute ...............................: (918.26, 1766.48)
    batch-generator ................................: (12.54, 16.65)
    forward-recv ...................................: (41.03, 127.39)
    forward-send ...................................: (1.10, 42.74)
    backward-recv ..................................: (125.38, 513.38)
    backward-send ..................................: (0.99, 37.84)
    forward-send-backward-recv .....................: (1890.65, 2108.85)
    backward-send-forward-recv .....................: (31.43, 174.60)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.12)
    grads-reduce-scatter ...........................: (18.04, 20.78)
    params-all-gather ..............................: (9.71, 11.05)
    optimizer-copy-to-main-grad ....................: (0.13, 0.23)
    optimizer-clip-main-grad .......................: (1.25, 1.29)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.13)
    optimizer-copy-main-to-model-params ............: (1.31, 1.52)
    optimizer ......................................: (8.42, 8.63)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 4378.6 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047593E+01 | loss scale: 1.0 | grad norm: 0.832 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4012.71, 4251.72)
    forward-compute ................................: (467.32, 2198.59)
    backward-compute ...............................: (921.31, 1774.09)
    batch-generator ................................: (12.56, 16.73)
    forward-recv ...................................: (36.72, 418.80)
    forward-send ...................................: (1.02, 368.80)
    backward-recv ..................................: (140.28, 537.13)
    backward-send ..................................: (1.06, 27.58)
    forward-send-backward-recv .....................: (1910.67, 2382.94)
    backward-send-forward-recv .....................: (37.70, 374.18)
    layernorm-grads-all-reduce .....................: (0.01, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 17.98)
    grads-reduce-scatter ...........................: (18.14, 20.76)
    params-all-gather ..............................: (9.71, 11.04)
    optimizer-copy-to-main-grad ....................: (0.13, 0.24)
    optimizer-clip-main-grad .......................: (1.28, 1.32)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.17)
    optimizer-copy-main-to-model-params ............: (1.31, 1.53)
    optimizer ......................................: (8.48, 8.70)
Mon Feb 12 11:45:01 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   33C    P0             316W / 700W |  52556MiB / 81559MiB |      7%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   39C    P0             351W / 700W |  62558MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   39C    P0             303W / 700W |  61452MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   34C    P0             286W / 700W |  56202MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             383W / 700W |  57492MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             369W / 700W |  53862MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             389W / 700W |  63412MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             358W / 700W |  60018MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 4409.1 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044027E+01 | loss scale: 1.0 | grad norm: 0.689 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3953.83, 4199.31)
    forward-compute ................................: (491.86, 2021.80)
    backward-compute ...............................: (950.93, 1805.06)
    batch-generator ................................: (12.60, 16.34)
    forward-recv ...................................: (40.69, 371.55)
    forward-send ...................................: (1.07, 298.24)
    backward-recv ..................................: (128.51, 493.82)
    backward-send ..................................: (1.02, 24.42)
    forward-send-backward-recv .....................: (1796.80, 2181.19)
    backward-send-forward-recv .....................: (33.67, 278.33)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.08)
    grads-reduce-scatter ...........................: (17.99, 20.79)
    params-all-gather ..............................: (9.72, 11.03)
    optimizer-copy-to-main-grad ....................: (0.14, 0.23)
    optimizer-clip-main-grad .......................: (1.01, 1.02)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.47, 5.14)
    optimizer-copy-main-to-model-params ............: (1.31, 1.51)
    optimizer ......................................: (8.12, 8.32)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 4357.0 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.042803E+01 | loss scale: 1.0 | grad norm: 0.824 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3998.52, 4251.53)
    forward-compute ................................: (485.66, 1912.57)
    backward-compute ...............................: (956.30, 1830.65)
    batch-generator ................................: (12.50, 15.54)
    forward-recv ...................................: (44.00, 117.68)
    forward-send ...................................: (1.13, 36.28)
    backward-recv ..................................: (130.61, 529.92)
    backward-send ..................................: (1.01, 32.15)
    forward-send-backward-recv .....................: (1795.33, 1987.54)
    backward-send-forward-recv .....................: (247.04, 445.39)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.94)
    grads-reduce-scatter ...........................: (18.08, 20.69)
    params-all-gather ..............................: (9.70, 11.03)
    optimizer-copy-to-main-grad ....................: (0.13, 0.22)
    optimizer-clip-main-grad .......................: (1.62, 1.71)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.13)
    optimizer-copy-main-to-model-params ............: (1.31, 1.53)
    optimizer ......................................: (8.76, 8.98)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 4265.6 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041879E+01 | loss scale: 1.0 | grad norm: 1.132 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3839.16, 4096.66)
    forward-compute ................................: (477.24, 2017.57)
    backward-compute ...............................: (939.15, 1775.08)
    batch-generator ................................: (12.55, 16.01)
    forward-recv ...................................: (35.35, 93.95)
    forward-send ...................................: (0.94, 20.40)
    backward-recv ..................................: (138.03, 468.88)
    backward-send ..................................: (0.97, 37.59)
    forward-send-backward-recv .....................: (1663.47, 2185.26)
    backward-send-forward-recv .....................: (25.79, 366.56)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.97)
    grads-reduce-scatter ...........................: (18.15, 20.77)
    params-all-gather ..............................: (9.72, 11.03)
    optimizer-copy-to-main-grad ....................: (0.13, 0.23)
    optimizer-clip-main-grad .......................: (1.87, 1.99)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.13)
    optimizer-copy-main-to-model-params ............: (1.31, 1.52)
    optimizer ......................................: (9.14, 9.35)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (496858): No such process
kill: (496864): No such process
kill: (496870): No such process
kill: (496876): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=4, tp=1, pp=4, mbs=4
LOCAL_IP = 10.64.24.51
DP=4, MP=1, PP=4
[2024-02-12 11:48:46,087] torch.distributed.run: [WARNING] 
[2024-02-12 11:48:46,087] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 11:48:46,087] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 11:48:46,087] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1611038720
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1817092096
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...

> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.840 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.835 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.837 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.843 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.851 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.969 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.144 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.800 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.069 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.119 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.268 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.302 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.329 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.252 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.239 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.323 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (38.49, 591.18)
    train/valid/test-data-iterators-setup ..........: (10201.28, 11460.71)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 5711.9 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.092182E+01 | loss scale: 1.0 | grad norm: 6.397 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 13904.072265625 | max allocated: 29018.88623046875 | reserved: 33042.0 | max reserved: 33042.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 15673.5576171875 | max allocated: 35937.0595703125 | reserved: 38592.0 | max reserved: 38592.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (4993.57, 5179.11)
    forward-compute ................................: (567.51, 2728.03)
    backward-compute ...............................: (1113.06, 1992.80)
    batch-generator ................................: (42.54, 63.92)
    forward-recv ...................................: (90.68, 248.58)
    forward-send ...................................: (3.68, 140.62)
    backward-recv ..................................: (92.06, 338.58)
    backward-send ..................................: (0.81, 30.16)
    forward-send-backward-recv .....................: (2521.36, 2832.77)
    backward-send-forward-recv .....................: (127.75, 356.95)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.88)
    grads-reduce-scatter ...........................: (20.45, 415.96)
    params-all-gather ..............................: (9.72, 11.63)
    optimizer-copy-to-main-grad ....................: (0.14, 0.23)
    optimizer-clip-main-grad .......................: (5.33, 5.60)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.62, 5.29)
    optimizer-copy-main-to-model-params ............: (1.32, 1.52)
    optimizer ......................................: (13.27, 13.84)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 4370.0 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.089271E+01 | loss scale: 1.0 | grad norm: 24.592 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4124.12, 4263.99)
    forward-compute ................................: (459.17, 2282.07)
    backward-compute ...............................: (987.20, 1800.10)
    batch-generator ................................: (23.60, 28.32)
    forward-recv ...................................: (17.39, 73.92)
    forward-send ...................................: (0.56, 27.63)
    backward-recv ..................................: (62.48, 319.40)
    backward-send ..................................: (0.64, 36.25)
    forward-send-backward-recv .....................: (2087.88, 2417.51)
    backward-send-forward-recv .....................: (73.64, 326.59)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.10)
    grads-reduce-scatter ...........................: (18.09, 20.75)
    params-all-gather ..............................: (9.72, 11.00)
    optimizer-copy-to-main-grad ....................: (0.13, 0.21)
    optimizer-clip-main-grad .......................: (2.24, 2.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.14)
    optimizer-copy-main-to-model-params ............: (1.31, 1.55)
    optimizer ......................................: (9.45, 9.69)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 4725.1 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.065914E+01 | loss scale: 1.0 | grad norm: 5.461 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4421.48, 4593.25)
    forward-compute ................................: (549.37, 2386.58)
    backward-compute ...............................: (1157.18, 1906.97)
    batch-generator ................................: (24.19, 28.96)
    forward-recv ...................................: (21.88, 58.34)
    forward-send ...................................: (0.66, 17.95)
    backward-recv ..................................: (70.86, 317.77)
    backward-send ..................................: (1.83, 27.05)
    forward-send-backward-recv .....................: (2217.24, 2472.96)
    backward-send-forward-recv .....................: (107.48, 336.67)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.75)
    grads-reduce-scatter ...........................: (18.13, 20.70)
    params-all-gather ..............................: (9.70, 11.02)
    optimizer-copy-to-main-grad ....................: (0.13, 0.21)
    optimizer-clip-main-grad .......................: (2.24, 2.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.16)
    optimizer-copy-main-to-model-params ............: (1.31, 1.51)
    optimizer ......................................: (9.57, 9.78)
Mon Feb 12 11:52:33 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   33C    P0             192W / 700W |  37788MiB / 81559MiB |     52%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   39C    P0             243W / 700W |  39070MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   40C    P0             340W / 700W |  43924MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   34C    P0             233W / 700W |  39268MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             240W / 700W |  41778MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             264W / 700W |  41292MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             312W / 700W |  41404MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             221W / 700W |  44304MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 4653.4 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.060075E+01 | loss scale: 1.0 | grad norm: 1.153 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4313.72, 4465.98)
    forward-compute ................................: (495.57, 2378.15)
    backward-compute ...............................: (1060.37, 1794.72)
    batch-generator ................................: (23.78, 31.52)
    forward-recv ...................................: (22.45, 315.31)
    forward-send ...................................: (0.59, 23.22)
    backward-recv ..................................: (51.74, 288.28)
    backward-send ..................................: (0.58, 33.74)
    forward-send-backward-recv .....................: (2146.08, 2575.05)
    backward-send-forward-recv .....................: (110.78, 390.66)
    layernorm-grads-all-reduce .....................: (0.01, 0.05)
    embedding-grads-all-reduce .....................: (0.02, 18.05)
    grads-reduce-scatter ...........................: (18.05, 20.68)
    params-all-gather ..............................: (9.70, 11.03)
    optimizer-copy-to-main-grad ....................: (0.13, 0.29)
    optimizer-clip-main-grad .......................: (2.29, 2.47)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.20)
    optimizer-copy-main-to-model-params ............: (1.31, 1.52)
    optimizer ......................................: (9.67, 9.89)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 4838.9 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.049300E+01 | loss scale: 1.0 | grad norm: 0.935 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4545.26, 4722.47)
    forward-compute ................................: (564.41, 2506.13)
    backward-compute ...............................: (1189.57, 1954.39)
    batch-generator ................................: (23.60, 31.53)
    forward-recv ...................................: (26.75, 72.97)
    forward-send ...................................: (0.67, 22.38)
    backward-recv ..................................: (60.63, 322.98)
    backward-send ..................................: (0.62, 33.64)
    forward-send-backward-recv .....................: (2249.04, 2525.12)
    backward-send-forward-recv .....................: (91.12, 361.39)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.00)
    grads-reduce-scatter ...........................: (18.11, 20.75)
    params-all-gather ..............................: (9.71, 11.02)
    optimizer-copy-to-main-grad ....................: (0.13, 0.27)
    optimizer-clip-main-grad .......................: (1.77, 1.88)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.59)
    optimizer-copy-main-to-model-params ............: (1.32, 1.52)
    optimizer ......................................: (9.56, 9.77)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 4877.9 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049759E+01 | loss scale: 1.0 | grad norm: 0.840 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4591.94, 4737.60)
    forward-compute ................................: (523.70, 2515.68)
    backward-compute ...............................: (1098.67, 1864.27)
    batch-generator ................................: (23.64, 30.72)
    forward-recv ...................................: (22.04, 365.55)
    forward-send ...................................: (0.51, 321.81)
    backward-recv ..................................: (88.06, 314.22)
    backward-send ..................................: (0.72, 17.94)
    forward-send-backward-recv .....................: (2173.72, 2570.23)
    backward-send-forward-recv .....................: (128.19, 441.50)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.94)
    grads-reduce-scatter ...........................: (18.00, 20.74)
    params-all-gather ..............................: (9.71, 11.01)
    optimizer-copy-to-main-grad ....................: (0.13, 0.26)
    optimizer-clip-main-grad .......................: (1.14, 1.18)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.15)
    optimizer-copy-main-to-model-params ............: (1.31, 1.51)
    optimizer ......................................: (8.30, 8.50)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 5223.8 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047613E+01 | loss scale: 1.0 | grad norm: 0.880 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4963.23, 5115.58)
    forward-compute ................................: (520.13, 2690.34)
    backward-compute ...............................: (1107.23, 1882.66)
    batch-generator ................................: (23.54, 29.64)
    forward-recv ...................................: (20.83, 406.24)
    forward-send ...................................: (0.55, 19.29)
    backward-recv ..................................: (71.72, 277.05)
    backward-send ..................................: (0.71, 17.37)
    forward-send-backward-recv .....................: (2496.51, 3108.13)
    backward-send-forward-recv .....................: (192.86, 575.63)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.83)
    grads-reduce-scatter ...........................: (18.10, 20.67)
    params-all-gather ..............................: (9.70, 11.04)
    optimizer-copy-to-main-grad ....................: (0.13, 0.25)
    optimizer-clip-main-grad .......................: (1.39, 1.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.15)
    optimizer-copy-main-to-model-params ............: (1.31, 1.51)
    optimizer ......................................: (8.54, 8.74)
Mon Feb 12 11:55:48 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   34C    P0             343W / 700W |  38080MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   39C    P0             360W / 700W |  42142MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   40C    P0             480W / 700W |  43924MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   35C    P0             225W / 700W |  43970MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             438W / 700W |  41778MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   46C    P0             371W / 700W |  47580MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             470W / 700W |  41404MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             425W / 700W |  44304MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 4489.6 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.043969E+01 | loss scale: 1.0 | grad norm: 0.537 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4137.27, 4307.50)
    forward-compute ................................: (546.61, 2152.10)
    backward-compute ...............................: (1154.14, 1921.41)
    batch-generator ................................: (23.66, 29.51)
    forward-recv ...................................: (26.67, 67.81)
    forward-send ...................................: (0.51, 18.44)
    backward-recv ..................................: (66.90, 271.32)
    backward-send ..................................: (0.65, 28.11)
    forward-send-backward-recv .....................: (2009.84, 2200.11)
    backward-send-forward-recv .....................: (72.48, 252.24)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.94)
    grads-reduce-scatter ...........................: (18.01, 20.72)
    params-all-gather ..............................: (9.72, 11.02)
    optimizer-copy-to-main-grad ....................: (0.13, 0.25)
    optimizer-clip-main-grad .......................: (1.01, 1.02)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.47, 5.14)
    optimizer-copy-main-to-model-params ............: (1.31, 1.51)
    optimizer ......................................: (8.18, 8.39)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 4426.6 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.042637E+01 | loss scale: 1.0 | grad norm: 0.766 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4148.21, 4312.57)
    forward-compute ................................: (528.53, 2162.09)
    backward-compute ...............................: (1122.95, 1903.67)
    batch-generator ................................: (23.73, 30.41)
    forward-recv ...................................: (23.59, 60.08)
    forward-send ...................................: (0.69, 18.44)
    backward-recv ..................................: (84.12, 294.26)
    backward-send ..................................: (0.69, 15.60)
    forward-send-backward-recv .....................: (1947.96, 2284.74)
    backward-send-forward-recv .....................: (66.12, 327.71)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.79)
    grads-reduce-scatter ...........................: (18.06, 20.68)
    params-all-gather ..............................: (9.70, 11.03)
    optimizer-copy-to-main-grad ....................: (0.13, 0.26)
    optimizer-clip-main-grad .......................: (1.51, 1.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.15)
    optimizer-copy-main-to-model-params ............: (1.31, 1.51)
    optimizer ......................................: (8.68, 8.87)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 5201.0 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041072E+01 | loss scale: 1.0 | grad norm: 0.484 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4908.63, 5057.11)
    forward-compute ................................: (518.84, 2936.43)
    backward-compute ...............................: (1098.24, 1870.39)
    batch-generator ................................: (23.85, 30.81)
    forward-recv ...................................: (18.14, 52.14)
    forward-send ...................................: (0.57, 17.14)
    backward-recv ..................................: (88.61, 308.56)
    backward-send ..................................: (0.75, 17.58)
    forward-send-backward-recv .....................: (2621.32, 3030.26)
    backward-send-forward-recv .....................: (66.26, 361.66)
    layernorm-grads-all-reduce .....................: (0.01, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 17.98)
    grads-reduce-scatter ...........................: (18.10, 20.76)
    params-all-gather ..............................: (9.72, 11.05)
    optimizer-copy-to-main-grad ....................: (0.13, 0.26)
    optimizer-clip-main-grad .......................: (1.27, 1.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.15)
    optimizer-copy-main-to-model-params ............: (1.31, 1.52)
    optimizer ......................................: (8.40, 8.61)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (499136): No such process
kill: (499142): No such process
kill: (499148): No such process
kill: (499154): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=4, tp=1, pp=4, mbs=2
LOCAL_IP = 10.64.24.51
DP=4, MP=1, PP=4
[2024-02-12 11:59:38,502] torch.distributed.run: [WARNING] 
[2024-02-12 11:59:38,502] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 11:59:38,502] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 11:59:38,502] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1611038720
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1817092096
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.830 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.870 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.875 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.889 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.905 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.922 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.127 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.449 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.120 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.148 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.211 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.249 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.303 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.303 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.263 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.873 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (47.74, 577.21)
    train/valid/test-data-iterators-setup ..........: (10156.99, 11659.25)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 6797.5 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.092175E+01 | loss scale: 1.0 | grad norm: 6.394 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 13908.986328125 | max allocated: 24432.4130859375 | reserved: 27580.0 | max reserved: 27580.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 15678.4951171875 | max allocated: 27264.73583984375 | reserved: 29198.0 | max reserved: 29198.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (6170.75, 6295.70)
    forward-compute ................................: (714.08, 3423.16)
    backward-compute ...............................: (1482.12, 2372.07)
    batch-generator ................................: (67.41, 85.59)
    forward-recv ...................................: (79.66, 215.53)
    forward-send ...................................: (3.89, 123.24)
    backward-recv ..................................: (50.29, 231.99)
    backward-send ..................................: (0.50, 20.49)
    forward-send-backward-recv .....................: (3035.03, 3600.94)
    backward-send-forward-recv .....................: (290.31, 593.05)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.74)
    grads-reduce-scatter ...........................: (20.37, 406.00)
    params-all-gather ..............................: (9.70, 11.03)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (5.36, 5.53)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.61, 5.26)
    optimizer-copy-main-to-model-params ............: (1.32, 1.51)
    optimizer ......................................: (13.10, 13.31)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 5163.1 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.089230E+01 | loss scale: 1.0 | grad norm: 24.481 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4970.27, 5062.48)
    forward-compute ................................: (607.82, 2611.62)
    backward-compute ...............................: (1400.91, 2168.15)
    batch-generator ................................: (46.07, 51.21)
    forward-recv ...................................: (13.30, 32.40)
    forward-send ...................................: (0.31, 9.05)
    backward-recv ..................................: (30.42, 200.33)
    backward-send ..................................: (0.44, 27.10)
    forward-send-backward-recv .....................: (2388.82, 2733.30)
    backward-send-forward-recv .....................: (213.14, 431.25)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.89)
    grads-reduce-scatter ...........................: (18.15, 20.65)
    params-all-gather ..............................: (9.74, 11.03)
    optimizer-copy-to-main-grad ....................: (0.13, 0.21)
    optimizer-clip-main-grad .......................: (2.24, 2.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.12)
    optimizer-copy-main-to-model-params ............: (1.31, 1.51)
    optimizer ......................................: (9.43, 9.62)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 5790.7 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.065884E+01 | loss scale: 1.0 | grad norm: 5.506 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5577.78, 5678.30)
    forward-compute ................................: (679.25, 2965.78)
    backward-compute ...............................: (1539.77, 2285.07)
    batch-generator ................................: (46.41, 50.24)
    forward-recv ...................................: (12.78, 40.36)
    forward-send ...................................: (0.35, 13.33)
    backward-recv ..................................: (34.71, 215.75)
    backward-send ..................................: (0.45, 26.70)
    forward-send-backward-recv .....................: (2665.69, 3134.35)
    backward-send-forward-recv .....................: (318.93, 541.98)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.81)
    grads-reduce-scatter ...........................: (18.14, 20.68)
    params-all-gather ..............................: (9.71, 11.03)
    optimizer-copy-to-main-grad ....................: (0.13, 0.21)
    optimizer-clip-main-grad .......................: (2.24, 2.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.12)
    optimizer-copy-main-to-model-params ............: (1.31, 1.51)
    optimizer ......................................: (9.45, 9.66)
Mon Feb 12 12:04:06 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   33C    P0             339W / 700W |  31962MiB / 81559MiB |     47%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   39C    P0             327W / 700W |  33560MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   39C    P0             224W / 700W |  31318MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   34C    P0             274W / 700W |  32578MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             408W / 700W |  32384MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   44C    P0             402W / 700W |  32684MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   42C    P0             234W / 700W |  33586MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             215W / 700W |  33336MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 5682.0 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.060078E+01 | loss scale: 1.0 | grad norm: 1.159 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5399.02, 5501.45)
    forward-compute ................................: (617.43, 2912.46)
    backward-compute ...............................: (1412.56, 2178.27)
    batch-generator ................................: (45.57, 50.96)
    forward-recv ...................................: (14.05, 43.29)
    forward-send ...................................: (0.36, 9.74)
    backward-recv ..................................: (46.28, 149.05)
    backward-send ..................................: (0.41, 12.62)
    forward-send-backward-recv .....................: (2582.69, 3110.47)
    backward-send-forward-recv .....................: (283.50, 649.67)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.92)
    grads-reduce-scatter ...........................: (18.10, 20.75)
    params-all-gather ..............................: (9.73, 11.01)
    optimizer-copy-to-main-grad ....................: (0.13, 0.21)
    optimizer-clip-main-grad .......................: (2.24, 2.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.12)
    optimizer-copy-main-to-model-params ............: (1.31, 1.51)
    optimizer ......................................: (9.42, 9.62)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 6658.9 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.049302E+01 | loss scale: 1.0 | grad norm: 0.947 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6460.63, 6568.74)
    forward-compute ................................: (704.95, 3683.97)
    backward-compute ...............................: (1557.19, 2331.05)
    batch-generator ................................: (46.01, 49.84)
    forward-recv ...................................: (15.05, 323.80)
    forward-send ...................................: (0.44, 300.97)
    backward-recv ..................................: (43.76, 166.12)
    backward-send ..................................: (0.48, 28.19)
    forward-send-backward-recv .....................: (3289.23, 3901.40)
    backward-send-forward-recv .....................: (331.83, 773.96)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.81)
    grads-reduce-scatter ...........................: (18.09, 20.66)
    params-all-gather ..............................: (9.71, 11.01)
    optimizer-copy-to-main-grad ....................: (0.13, 0.21)
    optimizer-clip-main-grad .......................: (1.75, 1.97)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.13)
    optimizer-copy-main-to-model-params ............: (1.31, 1.51)
    optimizer ......................................: (9.06, 9.25)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 6366.3 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049763E+01 | loss scale: 1.0 | grad norm: 0.966 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6153.68, 6245.08)
    forward-compute ................................: (663.41, 3636.08)
    backward-compute ...............................: (1496.89, 2276.62)
    batch-generator ................................: (46.19, 50.62)
    forward-recv ...................................: (11.73, 34.56)
    forward-send ...................................: (0.32, 9.24)
    backward-recv ..................................: (43.52, 170.50)
    backward-send ..................................: (0.53, 21.36)
    forward-send-backward-recv .....................: (3023.39, 3812.94)
    backward-send-forward-recv .....................: (260.17, 799.99)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.96)
    grads-reduce-scatter ...........................: (18.03, 20.76)
    params-all-gather ..............................: (9.72, 11.00)
    optimizer-copy-to-main-grad ....................: (0.13, 0.21)
    optimizer-clip-main-grad .......................: (1.12, 1.20)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.11)
    optimizer-copy-main-to-model-params ............: (1.31, 1.50)
    optimizer ......................................: (8.24, 8.44)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 5634.0 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047604E+01 | loss scale: 1.0 | grad norm: 0.958 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5437.41, 5531.75)
    forward-compute ................................: (655.76, 3008.54)
    backward-compute ...............................: (1493.82, 2227.92)
    batch-generator ................................: (46.16, 60.11)
    forward-recv ...................................: (12.87, 38.21)
    forward-send ...................................: (0.33, 13.76)
    backward-recv ..................................: (38.76, 186.53)
    backward-send ..................................: (0.57, 14.91)
    forward-send-backward-recv .....................: (2687.59, 3091.79)
    backward-send-forward-recv .....................: (191.34, 440.67)
    layernorm-grads-all-reduce .....................: (0.01, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 17.92)
    grads-reduce-scatter ...........................: (18.13, 20.67)
    params-all-gather ..............................: (9.72, 11.00)
    optimizer-copy-to-main-grad ....................: (0.13, 0.22)
    optimizer-clip-main-grad .......................: (1.38, 1.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.16)
    optimizer-copy-main-to-model-params ............: (1.31, 1.51)
    optimizer ......................................: (8.53, 8.72)
Mon Feb 12 12:08:18 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   34C    P0             381W / 700W |  31962MiB / 81559MiB |     79%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   39C    P0             441W / 700W |  33560MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   39C    P0             378W / 700W |  31320MiB / 81559MiB |     97%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   35C    P0             307W / 700W |  32578MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             478W / 700W |  32384MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   44C    P0             429W / 700W |  32684MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   42C    P0             426W / 700W |  33586MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             471W / 700W |  33336MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 6528.7 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044013E+01 | loss scale: 1.0 | grad norm: 0.606 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6241.04, 6353.87)
    forward-compute ................................: (671.63, 3455.74)
    backward-compute ...............................: (1520.04, 2261.30)
    batch-generator ................................: (46.45, 58.63)
    forward-recv ...................................: (15.71, 34.70)
    forward-send ...................................: (0.34, 9.64)
    backward-recv ..................................: (44.67, 181.27)
    backward-send ..................................: (0.42, 18.53)
    forward-send-backward-recv .....................: (3071.83, 3859.58)
    backward-send-forward-recv .....................: (309.56, 679.21)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.07)
    grads-reduce-scatter ...........................: (18.04, 20.69)
    params-all-gather ..............................: (9.74, 10.99)
    optimizer-copy-to-main-grad ....................: (0.13, 0.21)
    optimizer-clip-main-grad .......................: (1.00, 1.00)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.13)
    optimizer-copy-main-to-model-params ............: (1.31, 1.51)
    optimizer ......................................: (8.05, 8.24)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 5465.1 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.042490E+01 | loss scale: 1.0 | grad norm: 0.520 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5243.94, 5351.62)
    forward-compute ................................: (665.48, 2850.14)
    backward-compute ...............................: (1465.38, 2286.36)
    batch-generator ................................: (45.95, 57.72)
    forward-recv ...................................: (15.35, 44.70)
    forward-send ...................................: (0.38, 11.93)
    backward-recv ..................................: (44.46, 177.30)
    backward-send ..................................: (0.48, 12.74)
    forward-send-backward-recv .....................: (2324.29, 2895.89)
    backward-send-forward-recv .....................: (165.53, 541.39)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.84)
    grads-reduce-scatter ...........................: (18.12, 20.66)
    params-all-gather ..............................: (9.72, 11.00)
    optimizer-copy-to-main-grad ....................: (0.13, 0.21)
    optimizer-clip-main-grad .......................: (1.13, 1.15)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.13)
    optimizer-copy-main-to-model-params ............: (1.31, 1.51)
    optimizer ......................................: (8.20, 8.49)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 5386.9 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041340E+01 | loss scale: 1.0 | grad norm: 1.208 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5161.56, 5265.49)
    forward-compute ................................: (651.25, 2746.50)
    backward-compute ...............................: (1466.42, 2289.45)
    batch-generator ................................: (45.97, 57.55)
    forward-recv ...................................: (13.09, 36.80)
    forward-send ...................................: (0.33, 6.83)
    backward-recv ..................................: (52.92, 188.25)
    backward-send ..................................: (0.46, 17.02)
    forward-send-backward-recv .....................: (2346.04, 2835.02)
    backward-send-forward-recv .....................: (213.25, 439.22)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.94)
    grads-reduce-scatter ...........................: (18.17, 20.70)
    params-all-gather ..............................: (9.74, 10.99)
    optimizer-copy-to-main-grad ....................: (0.13, 0.21)
    optimizer-clip-main-grad .......................: (1.75, 1.85)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.13)
    optimizer-copy-main-to-model-params ............: (1.32, 1.51)
    optimizer ......................................: (8.90, 9.14)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (501414): No such process
kill: (501420): No such process
kill: (501426): No such process
kill: (501432): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=4, pp=2, mbs=16
LOCAL_IP = 10.64.24.51
DP=2, MP=4, PP=2
[2024-02-12 12:12:21,007] torch.distributed.run: [WARNING] 
[2024-02-12 12:12:21,007] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 12:12:21,007] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 12:12:21,007] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 857726976
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.246 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.303 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.079 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.049 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (862.83, 917.93)
    train/valid/test-data-iterators-setup ..........: (0.02, 12432.62)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 5085.5 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.095858E+01 | loss scale: 1.0 | grad norm: 6.529 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 11] (after 10 iterations) memory (MB) | allocated: 10016.7880859375 | max allocated: 36854.029296875 | reserved: 47124.0 | max reserved: 47124.0
[Rank 10] (after 10 iterations) memory (MB) | allocated: 10016.7880859375 | max allocated: 36853.48828125 | reserved: 46872.0 | max reserved: 46872.0[Rank 8] (after 10 iterations) memory (MB) | allocated: 10016.7880859375 | max allocated: 36854.5703125 | reserved: 46796.0 | max reserved: 46796.0

[Rank 9] (after 10 iterations) memory (MB) | allocated: 10016.7880859375 | max allocated: 36855.111328125 | reserved: 46880.0 | max reserved: 46880.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (4696.13, 4801.45)
    forward-compute ................................: (1347.84, 2289.13)
    backward-compute ...............................: (1371.97, 1827.91)
    batch-generator ................................: (405.27, 421.88)
    forward-recv ...................................: (508.88, 517.89)
    forward-send ...................................: (3.99, 4.84)
    backward-recv ..................................: (123.38, 165.69)
    backward-send ..................................: (1.97, 2.29)
    forward-send-backward-recv .....................: (1841.57, 1882.49)
    backward-send-forward-recv .....................: (127.98, 133.09)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.71, 5.02)
    grads-reduce-scatter ...........................: (8.20, 225.23)
    params-all-gather ..............................: (4.79, 4.96)
    optimizer-copy-to-main-grad ....................: (0.51, 0.68)
    optimizer-clip-main-grad .......................: (5.47, 5.51)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.00, 5.30)
    optimizer-copy-main-to-model-params ............: (1.70, 1.81)
    optimizer ......................................: (13.97, 14.10)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 3284.2 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.090965E+01 | loss scale: 1.0 | grad norm: 26.368 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3148.17, 3229.51)
    forward-compute ................................: (759.75, 1423.56)
    backward-compute ...............................: (1185.73, 1613.52)
    batch-generator ................................: (13.78, 17.54)
    forward-recv ...................................: (46.14, 54.09)
    forward-send ...................................: (1.67, 2.00)
    backward-recv ..................................: (96.84, 124.01)
    backward-send ..................................: (1.72, 1.86)
    forward-send-backward-recv .....................: (1117.67, 1138.70)
    backward-send-forward-recv .....................: (89.89, 94.11)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (4.58, 5.14)
    grads-reduce-scatter ...........................: (8.25, 8.44)
    params-all-gather ..............................: (4.76, 4.97)
    optimizer-copy-to-main-grad ....................: (0.47, 0.61)
    optimizer-clip-main-grad .......................: (2.53, 2.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.84, 5.01)
    optimizer-copy-main-to-model-params ............: (1.69, 1.81)
    optimizer ......................................: (10.28, 10.40)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 3624.9 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.067615E+01 | loss scale: 1.0 | grad norm: 3.399 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3460.37, 3563.03)
    forward-compute ................................: (875.94, 1544.91)
    backward-compute ...............................: (1361.58, 1757.96)
    batch-generator ................................: (14.02, 17.57)
    forward-recv ...................................: (55.06, 56.07)
    forward-send ...................................: (1.95, 2.05)
    backward-recv ..................................: (124.42, 126.54)
    backward-send ..................................: (2.03, 2.27)
    forward-send-backward-recv .....................: (1162.91, 1172.41)
    backward-send-forward-recv .....................: (95.46, 128.89)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.62, 4.92)
    grads-reduce-scatter ...........................: (8.17, 8.56)
    params-all-gather ..............................: (4.76, 4.96)
    optimizer-copy-to-main-grad ....................: (0.48, 0.63)
    optimizer-clip-main-grad .......................: (2.53, 2.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.02)
    optimizer-copy-main-to-model-params ............: (1.69, 1.81)
    optimizer ......................................: (10.31, 10.43)
Mon Feb 12 12:15:30 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   39C    P0             418W / 700W |  56806MiB / 81559MiB |     46%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   46C    P0             515W / 700W |  56938MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   47C    P0             473W / 700W |  56930MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   40C    P0             469W / 700W |  56942MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   40C    P0             476W / 700W |  60070MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   47C    P0             419W / 700W |  60264MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   44C    P0             437W / 700W |  60210MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   40C    P0             437W / 700W |  59958MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 3479.1 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.060551E+01 | loss scale: 1.0 | grad norm: 0.919 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3238.46, 3339.83)
    forward-compute ................................: (775.10, 1490.24)
    backward-compute ...............................: (1203.43, 1610.79)
    batch-generator ................................: (14.06, 17.30)
    forward-recv ...................................: (59.77, 68.50)
    forward-send ...................................: (1.95, 2.13)
    backward-recv ..................................: (115.68, 127.80)
    backward-send ..................................: (1.63, 2.05)
    forward-send-backward-recv .....................: (1166.28, 1201.56)
    backward-send-forward-recv .....................: (82.81, 146.42)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.58, 5.07)
    grads-reduce-scatter ...........................: (8.11, 8.50)
    params-all-gather ..............................: (4.80, 4.97)
    optimizer-copy-to-main-grad ....................: (0.47, 0.64)
    optimizer-clip-main-grad .......................: (2.28, 2.31)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.83, 5.02)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (10.07, 10.19)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 3767.1 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.049715E+01 | loss scale: 1.0 | grad norm: 0.921 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3600.46, 3726.10)
    forward-compute ................................: (904.81, 1613.81)
    backward-compute ...............................: (1406.09, 1803.17)
    batch-generator ................................: (14.41, 16.88)
    forward-recv ...................................: (61.98, 66.38)
    forward-send ...................................: (2.20, 2.40)
    backward-recv ..................................: (111.49, 146.80)
    backward-send ..................................: (2.46, 9.01)
    forward-send-backward-recv .....................: (1225.41, 1249.26)
    backward-send-forward-recv .....................: (129.22, 131.79)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.61, 4.91)
    grads-reduce-scatter ...........................: (8.07, 8.56)
    params-all-gather ..............................: (4.77, 4.98)
    optimizer-copy-to-main-grad ....................: (0.48, 0.62)
    optimizer-clip-main-grad .......................: (2.31, 2.32)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.84, 5.03)
    optimizer-copy-main-to-model-params ............: (1.69, 1.81)
    optimizer ......................................: (10.10, 10.23)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 4126.2 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.050357E+01 | loss scale: 1.0 | grad norm: 0.865 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3982.05, 4085.39)
    forward-compute ................................: (1081.77, 1889.39)
    backward-compute ...............................: (1275.14, 1723.73)
    batch-generator ................................: (14.46, 16.64)
    forward-recv ...................................: (56.31, 63.95)
    forward-send ...................................: (1.98, 2.31)
    backward-recv ..................................: (110.51, 133.87)
    backward-send ..................................: (1.76, 2.16)
    forward-send-backward-recv .....................: (1439.08, 1555.81)
    backward-send-forward-recv .....................: (368.51, 485.28)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (4.59, 5.16)
    grads-reduce-scatter ...........................: (8.21, 8.51)
    params-all-gather ..............................: (4.78, 4.98)
    optimizer-copy-to-main-grad ....................: (0.47, 0.64)
    optimizer-clip-main-grad .......................: (1.38, 1.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.84, 5.07)
    optimizer-copy-main-to-model-params ............: (1.69, 1.81)
    optimizer ......................................: (9.22, 9.34)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 3470.7 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.048202E+01 | loss scale: 1.0 | grad norm: 0.589 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3326.50, 3425.63)
    forward-compute ................................: (832.32, 1506.40)
    backward-compute ...............................: (1305.24, 1705.51)
    batch-generator ................................: (14.82, 16.92)
    forward-recv ...................................: (48.23, 51.71)
    forward-send ...................................: (1.73, 1.88)
    backward-recv ..................................: (105.45, 122.10)
    backward-send ..................................: (1.80, 2.20)
    forward-send-backward-recv .....................: (1104.59, 1158.27)
    backward-send-forward-recv .....................: (88.66, 122.03)
    layernorm-grads-all-reduce .....................: (0.01, 0.03)
    embedding-grads-all-reduce .....................: (4.59, 4.96)
    grads-reduce-scatter ...........................: (8.13, 8.53)
    params-all-gather ..............................: (4.82, 4.98)
    optimizer-copy-to-main-grad ....................: (0.47, 0.63)
    optimizer-clip-main-grad .......................: (1.24, 1.25)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.83, 5.06)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (9.06, 9.17)
Mon Feb 12 12:18:00 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   40C    P0             516W / 700W |  59974MiB / 81559MiB |     33%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   47C    P0             502W / 700W |  60106MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   48C    P0             527W / 700W |  60098MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0             502W / 700W |  60110MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   40C    P0             511W / 700W |  60072MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   47C    P0             449W / 700W |  60266MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   45C    P0             473W / 700W |  60212MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   40C    P0             460W / 700W |  59960MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 3613.5 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044683E+01 | loss scale: 1.0 | grad norm: 0.926 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3383.43, 3477.81)
    forward-compute ................................: (864.39, 1533.57)
    backward-compute ...............................: (1360.88, 1745.58)
    batch-generator ................................: (13.84, 16.76)
    forward-recv ...................................: (53.13, 63.97)
    forward-send ...................................: (1.88, 2.27)
    backward-recv ..................................: (89.51, 109.82)
    backward-send ..................................: (1.84, 2.08)
    forward-send-backward-recv .....................: (1129.97, 1135.12)
    backward-send-forward-recv .....................: (66.27, 85.82)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.59, 5.18)
    grads-reduce-scatter ...........................: (8.11, 8.56)
    params-all-gather ..............................: (4.75, 4.99)
    optimizer-copy-to-main-grad ....................: (0.45, 0.62)
    optimizer-clip-main-grad .......................: (1.36, 1.37)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.83, 5.05)
    optimizer-copy-main-to-model-params ............: (1.69, 1.81)
    optimizer ......................................: (9.17, 9.30)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 3801.2 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.043484E+01 | loss scale: 1.0 | grad norm: 0.744 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3644.63, 3762.93)
    forward-compute ................................: (841.80, 1799.42)
    backward-compute ...............................: (1316.21, 1759.61)
    batch-generator ................................: (14.00, 16.93)
    forward-recv ...................................: (60.33, 62.10)
    forward-send ...................................: (2.11, 2.22)
    backward-recv ..................................: (98.53, 117.95)
    backward-send ..................................: (1.61, 2.26)
    forward-send-backward-recv .....................: (1370.36, 1447.81)
    backward-send-forward-recv .....................: (92.39, 106.06)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (4.59, 4.95)
    grads-reduce-scatter ...........................: (8.13, 8.49)
    params-all-gather ..............................: (4.78, 4.95)
    optimizer-copy-to-main-grad ....................: (0.46, 0.61)
    optimizer-clip-main-grad .......................: (2.02, 2.04)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.83, 5.05)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (9.87, 9.98)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 3455.4 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041988E+01 | loss scale: 1.0 | grad norm: 0.668 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3291.35, 3402.17)
    forward-compute ................................: (826.68, 1492.25)
    backward-compute ...............................: (1304.96, 1737.05)
    batch-generator ................................: (13.83, 16.64)
    forward-recv ...................................: (47.81, 49.02)
    forward-send ...................................: (1.71, 1.80)
    backward-recv ..................................: (118.57, 135.49)
    backward-send ..................................: (1.81, 2.29)
    forward-send-backward-recv .....................: (1039.78, 1110.32)
    backward-send-forward-recv .....................: (68.07, 97.19)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (4.56, 5.05)
    grads-reduce-scatter ...........................: (8.23, 8.45)
    params-all-gather ..............................: (4.81, 4.98)
    optimizer-copy-to-main-grad ....................: (0.46, 0.61)
    optimizer-clip-main-grad .......................: (1.88, 1.90)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.83, 5.07)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (9.70, 9.82)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (502528): No such process
kill: (502534): No such process
kill: (502540): No such process
kill: (502546): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=4, pp=2, mbs=8
LOCAL_IP = 10.64.24.51
DP=2, MP=4, PP=2
[2024-02-12 12:21:28,332] torch.distributed.run: [WARNING] 
[2024-02-12 12:21:28,332] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 12:21:28,332] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 12:21:28,332] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 857726976
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.239 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.284 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.008 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.063 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (862.18, 911.94)
    train/valid/test-data-iterators-setup ..........: (0.02, 10784.56)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 5685.0 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.095853E+01 | loss scale: 1.0 | grad norm: 6.531 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 9919.6787109375 | max allocated: 28492.20751953125 | reserved: 33960.0 | max reserved: 33960.0[Rank 9] (after 10 iterations) memory (MB) | allocated: 9919.6787109375 | max allocated: 28491.43408203125 | reserved: 33954.0 | max reserved: 33954.0

[Rank 11] (after 10 iterations) memory (MB) | allocated: 9919.6787109375 | max allocated: 28491.43408203125 | reserved: 34466.0 | max reserved: 34466.0
[Rank 10] (after 10 iterations) memory (MB) | allocated: 9919.6787109375 | max allocated: 28491.43408203125 | reserved: 34096.0 | max reserved: 34096.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (5360.84, 5427.23)
    forward-compute ................................: (1414.57, 2763.46)
    backward-compute ...............................: (1540.64, 2022.15)
    batch-generator ................................: (412.68, 442.05)
    forward-recv ...................................: (475.17, 476.27)
    forward-send ...................................: (2.98, 3.94)
    backward-recv ..................................: (83.87, 110.43)
    backward-send ..................................: (1.41, 5.36)
    forward-send-backward-recv .....................: (2118.77, 2327.93)
    backward-send-forward-recv .....................: (215.09, 316.59)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.64, 4.98)
    grads-reduce-scatter ...........................: (8.15, 219.65)
    params-all-gather ..............................: (4.81, 4.91)
    optimizer-copy-to-main-grad ....................: (0.51, 0.68)
    optimizer-clip-main-grad .......................: (5.50, 5.54)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.00, 5.21)
    optimizer-copy-main-to-model-params ............: (1.69, 1.79)
    optimizer ......................................: (13.87, 13.99)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 3767.1 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.090984E+01 | loss scale: 1.0 | grad norm: 26.443 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3673.13, 3722.25)
    forward-compute ................................: (837.67, 1767.51)
    backward-compute ...............................: (1367.16, 1784.32)
    batch-generator ................................: (25.80, 31.52)
    forward-recv ...................................: (32.60, 44.39)
    forward-send ...................................: (0.88, 1.35)
    backward-recv ..................................: (55.15, 70.68)
    backward-send ..................................: (0.99, 5.09)
    forward-send-backward-recv .....................: (1323.35, 1425.31)
    backward-send-forward-recv .....................: (137.01, 269.73)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.60, 4.90)
    grads-reduce-scatter ...........................: (8.18, 8.50)
    params-all-gather ..............................: (4.80, 4.94)
    optimizer-copy-to-main-grad ....................: (0.50, 0.65)
    optimizer-clip-main-grad .......................: (2.55, 2.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.00)
    optimizer-copy-main-to-model-params ............: (1.69, 1.78)
    optimizer ......................................: (10.35, 10.45)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 4400.9 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.067648E+01 | loss scale: 1.0 | grad norm: 3.386 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4285.20, 4354.60)
    forward-compute ................................: (1219.34, 1864.10)
    backward-compute ...............................: (1542.61, 1927.76)
    batch-generator ................................: (26.39, 31.39)
    forward-recv ...................................: (29.82, 31.50)
    forward-send ...................................: (1.10, 1.16)
    backward-recv ..................................: (73.43, 82.89)
    backward-send ..................................: (3.24, 7.87)
    forward-send-backward-recv .....................: (1443.17, 1476.48)
    backward-send-forward-recv .....................: (463.31, 496.38)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (4.61, 4.95)
    grads-reduce-scatter ...........................: (8.09, 8.55)
    params-all-gather ..............................: (4.80, 4.93)
    optimizer-copy-to-main-grad ....................: (0.48, 0.67)
    optimizer-clip-main-grad .......................: (2.58, 2.62)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.01)
    optimizer-copy-main-to-model-params ............: (1.69, 1.79)
    optimizer ......................................: (10.43, 10.53)
Mon Feb 12 12:24:58 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   39C    P0             419W / 700W |  43178MiB / 81559MiB |     91%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   46C    P0             393W / 700W |  43220MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   47C    P0             394W / 700W |  42570MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   40C    P0             349W / 700W |  42700MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   40C    P0             418W / 700W |  38934MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   47C    P0             506W / 700W |  38982MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   44C    P0             404W / 700W |  37398MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   40C    P0             382W / 700W |  38742MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 3816.8 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.060543E+01 | loss scale: 1.0 | grad norm: 0.919 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3631.57, 3691.96)
    forward-compute ................................: (876.60, 1639.93)
    backward-compute ...............................: (1439.78, 1770.10)
    batch-generator ................................: (26.67, 31.81)
    forward-recv ...................................: (29.59, 32.76)
    forward-send ...................................: (1.09, 1.22)
    backward-recv ..................................: (45.20, 52.84)
    backward-send ..................................: (1.14, 5.09)
    forward-send-backward-recv .....................: (1290.18, 1299.14)
    backward-send-forward-recv .....................: (189.39, 199.11)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.62, 4.85)
    grads-reduce-scatter ...........................: (8.10, 8.45)
    params-all-gather ..............................: (4.79, 4.93)
    optimizer-copy-to-main-grad ....................: (0.49, 0.66)
    optimizer-clip-main-grad .......................: (2.33, 2.36)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.03)
    optimizer-copy-main-to-model-params ............: (1.69, 1.79)
    optimizer ......................................: (10.17, 10.27)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 4749.7 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.049698E+01 | loss scale: 1.0 | grad norm: 0.861 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4657.76, 4705.74)
    forward-compute ................................: (997.36, 2427.68)
    backward-compute ...............................: (1626.41, 1994.42)
    batch-generator ................................: (26.78, 31.30)
    forward-recv ...................................: (32.87, 39.17)
    forward-send ...................................: (1.18, 1.42)
    backward-recv ..................................: (47.47, 63.28)
    backward-send ..................................: (1.71, 3.51)
    forward-send-backward-recv .....................: (1962.63, 2007.10)
    backward-send-forward-recv .....................: (205.20, 241.63)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.59, 4.78)
    grads-reduce-scatter ...........................: (8.17, 8.47)
    params-all-gather ..............................: (4.78, 4.94)
    optimizer-copy-to-main-grad ....................: (0.49, 0.78)
    optimizer-clip-main-grad .......................: (1.40, 1.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.84, 5.04)
    optimizer-copy-main-to-model-params ............: (1.69, 1.79)
    optimizer ......................................: (9.41, 9.51)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 4496.3 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.050343E+01 | loss scale: 1.0 | grad norm: 0.807 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4381.46, 4449.62)
    forward-compute ................................: (1206.66, 2028.73)
    backward-compute ...............................: (1526.81, 1867.12)
    batch-generator ................................: (25.94, 32.34)
    forward-recv ...................................: (25.42, 36.62)
    forward-send ...................................: (0.93, 1.34)
    backward-recv ..................................: (71.93, 75.69)
    backward-send ..................................: (1.11, 1.38)
    forward-send-backward-recv .....................: (1594.01, 1602.41)
    backward-send-forward-recv .....................: (442.13, 487.85)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.58, 5.04)
    grads-reduce-scatter ...........................: (8.03, 8.55)
    params-all-gather ..............................: (4.81, 4.95)
    optimizer-copy-to-main-grad ....................: (0.49, 0.70)
    optimizer-clip-main-grad .......................: (1.40, 1.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.84, 5.08)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (9.30, 9.41)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 3901.7 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.048189E+01 | loss scale: 1.0 | grad norm: 0.658 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3793.43, 3854.67)
    forward-compute ................................: (897.29, 1808.81)
    backward-compute ...............................: (1483.38, 1911.24)
    batch-generator ................................: (26.02, 32.62)
    forward-recv ...................................: (26.59, 31.48)
    forward-send ...................................: (0.98, 1.20)
    backward-recv ..................................: (67.13, 75.78)
    backward-send ..................................: (1.15, 1.34)
    forward-send-backward-recv .....................: (1208.19, 1373.33)
    backward-send-forward-recv .....................: (133.67, 220.43)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.61, 5.06)
    grads-reduce-scatter ...........................: (8.06, 8.49)
    params-all-gather ..............................: (4.80, 4.93)
    optimizer-copy-to-main-grad ....................: (0.48, 0.77)
    optimizer-clip-main-grad .......................: (1.28, 1.29)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.03)
    optimizer-copy-main-to-model-params ............: (1.69, 1.79)
    optimizer ......................................: (9.36, 9.47)
Mon Feb 12 12:27:49 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   40C    P0             485W / 700W |  44762MiB / 81559MiB |     13%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   47C    P0             525W / 700W |  46388MiB / 81559MiB |     97%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   48C    P0             522W / 700W |  45738MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0             400W / 700W |  45868MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   40C    P0             457W / 700W |  44542MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   47C    P0             469W / 700W |  44672MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   45C    P0             454W / 700W |  44262MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   40C    P0             441W / 700W |  44270MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 4036.0 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044597E+01 | loss scale: 1.0 | grad norm: 0.595 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3850.97, 3905.76)
    forward-compute ................................: (966.39, 1720.32)
    backward-compute ...............................: (1579.41, 1915.08)
    batch-generator ................................: (26.02, 32.20)
    forward-recv ...................................: (26.38, 32.71)
    forward-send ...................................: (0.97, 1.21)
    backward-recv ..................................: (53.09, 65.54)
    backward-send ..................................: (1.02, 1.82)
    forward-send-backward-recv .....................: (1269.68, 1277.97)
    backward-send-forward-recv .....................: (198.06, 200.23)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.59, 5.11)
    grads-reduce-scatter ...........................: (8.16, 8.47)
    params-all-gather ..............................: (4.75, 4.96)
    optimizer-copy-to-main-grad ....................: (0.49, 0.67)
    optimizer-clip-main-grad .......................: (1.29, 1.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.84, 5.02)
    optimizer-copy-main-to-model-params ............: (1.69, 1.79)
    optimizer ......................................: (9.15, 9.25)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 4707.9 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.043019E+01 | loss scale: 1.0 | grad norm: 0.825 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4603.53, 4663.65)
    forward-compute ................................: (942.15, 2530.00)
    backward-compute ...............................: (1552.52, 1907.24)
    batch-generator ................................: (25.84, 33.65)
    forward-recv ...................................: (32.63, 33.55)
    forward-send ...................................: (1.19, 1.23)
    backward-recv ..................................: (71.64, 77.82)
    backward-send ..................................: (1.22, 1.36)
    forward-send-backward-recv .....................: (1761.31, 2078.92)
    backward-send-forward-recv .....................: (162.71, 462.19)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.57, 4.88)
    grads-reduce-scatter ...........................: (8.18, 8.43)
    params-all-gather ..............................: (4.75, 4.93)
    optimizer-copy-to-main-grad ....................: (0.49, 0.68)
    optimizer-clip-main-grad .......................: (1.28, 1.29)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.04)
    optimizer-copy-main-to-model-params ............: (1.69, 1.79)
    optimizer ......................................: (9.15, 9.25)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 4159.8 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.042224E+01 | loss scale: 1.0 | grad norm: 1.496 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4044.51, 4111.86)
    forward-compute ................................: (933.96, 1996.57)
    backward-compute ...............................: (1528.10, 1884.80)
    batch-generator ................................: (26.21, 36.88)
    forward-recv ...................................: (27.96, 304.39)
    forward-send ...................................: (1.02, 1.08)
    backward-recv ..................................: (66.46, 93.83)
    backward-send ..................................: (1.21, 2.65)
    forward-send-backward-recv .....................: (1207.69, 1527.52)
    backward-send-forward-recv .....................: (157.13, 175.82)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.57, 5.04)
    grads-reduce-scatter ...........................: (8.13, 8.46)
    params-all-gather ..............................: (4.81, 4.94)
    optimizer-copy-to-main-grad ....................: (0.49, 0.74)
    optimizer-clip-main-grad .......................: (2.47, 2.51)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.86, 5.08)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (10.58, 10.70)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (503642): No such process
kill: (503648): No such process
kill: (503654): No such process
kill: (503660): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=4, pp=2, mbs=4
LOCAL_IP = 10.64.24.51
DP=2, MP=4, PP=2
[2024-02-12 12:31:35,674] torch.distributed.run: [WARNING] 
[2024-02-12 12:31:35,674] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 12:31:35,674] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 12:31:35,674] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 857726976
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.189 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.219 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.063 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.131 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (865.48, 909.87)
    train/valid/test-data-iterators-setup ..........: (0.02, 10728.44)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 6667.8 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.095859E+01 | loss scale: 1.0 | grad norm: 6.537 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 9] (after 10 iterations) memory (MB) | allocated: 9930.2255859375 | max allocated: 23546.16748046875 | reserved: 26502.0 | max reserved: 26502.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 9930.2255859375 | max allocated: 23547.01513671875 | reserved: 26376.0 | max reserved: 26376.0
[Rank 10] (after 10 iterations) memory (MB) | allocated: 9930.3193359375 | max allocated: 23546.26123046875 | reserved: 26380.0 | max reserved: 26380.0[Rank 11] (after 10 iterations) memory (MB) | allocated: 9930.8544921875 | max allocated: 23546.16748046875 | reserved: 26332.0 | max reserved: 26332.0

(min, max) time across ranks (ms):
    forward-backward ...............................: (6356.05, 6394.73)
    forward-compute ................................: (1685.99, 3185.45)
    backward-compute ...............................: (1973.29, 2302.52)
    batch-generator ................................: (454.30, 631.45)
    forward-recv ...................................: (478.19, 481.90)
    forward-send ...................................: (2.46, 3.17)
    backward-recv ..................................: (44.41, 59.15)
    backward-send ..................................: (0.84, 0.97)
    forward-send-backward-recv .....................: (2601.69, 2645.09)
    backward-send-forward-recv .....................: (416.95, 421.42)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.72, 4.98)
    grads-reduce-scatter ...........................: (8.10, 230.28)
    params-all-gather ..............................: (4.78, 4.98)
    optimizer-copy-to-main-grad ....................: (0.53, 0.85)
    optimizer-clip-main-grad .......................: (5.75, 5.79)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.00, 5.31)
    optimizer-copy-main-to-model-params ............: (1.69, 1.81)
    optimizer ......................................: (14.46, 14.60)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 4807.2 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.090929E+01 | loss scale: 1.0 | grad norm: 26.360 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4737.93, 4767.56)
    forward-compute ................................: (1394.69, 2062.71)
    backward-compute ...............................: (1804.17, 2080.48)
    batch-generator ................................: (49.35, 68.50)
    forward-recv ...................................: (14.69, 16.53)
    forward-send ...................................: (0.52, 0.61)
    backward-recv ..................................: (31.92, 44.20)
    backward-send ..................................: (0.66, 0.75)
    forward-send-backward-recv .....................: (1456.44, 1485.17)
    backward-send-forward-recv .....................: (565.70, 591.29)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.63, 4.85)
    grads-reduce-scatter ...........................: (8.15, 8.43)
    params-all-gather ..............................: (4.79, 4.94)
    optimizer-copy-to-main-grad ....................: (0.52, 0.76)
    optimizer-clip-main-grad .......................: (2.63, 2.82)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.86, 5.12)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (10.85, 10.96)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 6112.7 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.067606E+01 | loss scale: 1.0 | grad norm: 3.398 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6045.54, 6074.72)
    forward-compute ................................: (1511.33, 3114.78)
    backward-compute ...............................: (2013.83, 2239.54)
    batch-generator ................................: (51.76, 69.61)
    forward-recv ...................................: (19.04, 19.26)
    forward-send ...................................: (0.66, 0.73)
    backward-recv ..................................: (27.47, 33.05)
    backward-send ..................................: (4.02, 4.25)
    forward-send-backward-recv .....................: (2469.82, 2488.01)
    backward-send-forward-recv .....................: (654.61, 677.73)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.65, 4.78)
    grads-reduce-scatter ...........................: (8.17, 8.47)
    params-all-gather ..............................: (4.78, 4.98)
    optimizer-copy-to-main-grad ....................: (0.51, 0.76)
    optimizer-clip-main-grad .......................: (2.69, 2.73)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.86, 5.08)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (10.67, 10.78)
Mon Feb 12 12:35:51 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   38C    P0             424W / 700W |  32426MiB / 81559MiB |     50%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   45C    P0             409W / 700W |  32600MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   46C    P0             427W / 700W |  32478MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0             343W / 700W |  32190MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             240W / 700W |  32778MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             337W / 700W |  32684MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   42C    P0             217W / 700W |  32734MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             256W / 700W |  32592MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 4703.5 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.060549E+01 | loss scale: 1.0 | grad norm: 0.917 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4538.26, 4573.98)
    forward-compute ................................: (1140.03, 2136.61)
    backward-compute ...............................: (1853.77, 2096.85)
    batch-generator ................................: (48.90, 69.58)
    forward-recv ...................................: (16.96, 20.90)
    forward-send ...................................: (0.61, 0.80)
    backward-recv ..................................: (47.50, 48.15)
    backward-send ..................................: (0.68, 0.78)
    forward-send-backward-recv .....................: (1425.35, 1502.42)
    backward-send-forward-recv .....................: (299.55, 331.08)
    layernorm-grads-all-reduce .....................: (0.02, 0.04)
    embedding-grads-all-reduce .....................: (4.61, 4.79)
    grads-reduce-scatter ...........................: (8.13, 8.47)
    params-all-gather ..............................: (4.77, 4.96)
    optimizer-copy-to-main-grad ....................: (0.51, 0.74)
    optimizer-clip-main-grad .......................: (2.34, 2.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.05)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (10.37, 10.49)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 6167.3 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.049708E+01 | loss scale: 1.0 | grad norm: 0.881 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6090.57, 6124.33)
    forward-compute ................................: (1537.60, 3124.15)
    backward-compute ...............................: (2069.01, 2287.60)
    batch-generator ................................: (50.45, 69.54)
    forward-recv ...................................: (19.74, 20.72)
    forward-send ...................................: (0.70, 0.78)
    backward-recv ..................................: (26.84, 55.45)
    backward-send ..................................: (0.72, 4.64)
    forward-send-backward-recv .....................: (2388.34, 2463.40)
    backward-send-forward-recv .....................: (637.50, 658.19)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.60, 4.84)
    grads-reduce-scatter ...........................: (8.15, 8.54)
    params-all-gather ..............................: (4.77, 4.92)
    optimizer-copy-to-main-grad ....................: (0.51, 0.69)
    optimizer-clip-main-grad .......................: (1.41, 1.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.84, 5.08)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (9.31, 9.42)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 4828.6 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.050353E+01 | loss scale: 1.0 | grad norm: 0.857 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4753.92, 4787.59)
    forward-compute ................................: (1195.63, 2228.80)
    backward-compute ...............................: (1933.60, 2187.38)
    batch-generator ................................: (50.18, 68.54)
    forward-recv ...................................: (15.58, 17.90)
    forward-send ...................................: (0.54, 0.66)
    backward-recv ..................................: (33.29, 34.83)
    backward-send ..................................: (1.13, 4.65)
    forward-send-backward-recv .....................: (1485.28, 1598.55)
    backward-send-forward-recv .....................: (336.36, 367.72)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.59, 4.95)
    grads-reduce-scatter ...........................: (8.11, 8.38)
    params-all-gather ..............................: (4.80, 4.92)
    optimizer-copy-to-main-grad ....................: (0.51, 0.72)
    optimizer-clip-main-grad .......................: (1.43, 1.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.11)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (9.41, 9.52)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 5832.7 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.048227E+01 | loss scale: 1.0 | grad norm: 0.799 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5759.33, 5793.37)
    forward-compute ................................: (1480.96, 2949.35)
    backward-compute ...............................: (1961.01, 2174.75)
    batch-generator ................................: (48.99, 71.27)
    forward-recv ...................................: (19.20, 20.88)
    forward-send ...................................: (0.63, 0.76)
    backward-recv ..................................: (24.29, 43.60)
    backward-send ..................................: (0.75, 2.19)
    forward-send-backward-recv .....................: (2226.67, 2286.37)
    backward-send-forward-recv .....................: (591.76, 634.36)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.62, 4.83)
    grads-reduce-scatter ...........................: (8.12, 8.45)
    params-all-gather ..............................: (4.81, 4.94)
    optimizer-copy-to-main-grad ....................: (0.51, 0.76)
    optimizer-clip-main-grad .......................: (1.29, 1.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.07)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (9.46, 9.57)
Mon Feb 12 12:39:34 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   38C    P0             476W / 700W |  32426MiB / 81559MiB |     86%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   45C    P0             447W / 700W |  32600MiB / 81559MiB |     97%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   46C    P0             463W / 700W |  32478MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0             447W / 700W |  32190MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             417W / 700W |  36966MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             454W / 700W |  37096MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             384W / 700W |  37082MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             367W / 700W |  36684MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 5419.1 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044779E+01 | loss scale: 1.0 | grad norm: 0.565 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5248.84, 5289.42)
    forward-compute ................................: (1495.42, 2420.51)
    backward-compute ...............................: (1993.22, 2222.42)
    batch-generator ................................: (50.56, 69.13)
    forward-recv ...................................: (16.16, 18.57)
    forward-send ...................................: (0.55, 0.71)
    backward-recv ..................................: (38.39, 45.94)
    backward-send ..................................: (0.66, 1.22)
    forward-send-backward-recv .....................: (1695.58, 1716.70)
    backward-send-forward-recv .....................: (580.43, 607.46)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.63, 4.80)
    grads-reduce-scatter ...........................: (8.20, 8.44)
    params-all-gather ..............................: (4.77, 4.97)
    optimizer-copy-to-main-grad ....................: (0.51, 0.78)
    optimizer-clip-main-grad .......................: (1.66, 1.69)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.86, 5.07)
    optimizer-copy-main-to-model-params ............: (1.69, 1.79)
    optimizer ......................................: (9.68, 9.79)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 4828.6 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.043305E+01 | loss scale: 1.0 | grad norm: 0.716 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4744.47, 4792.52)
    forward-compute ................................: (1209.63, 2237.44)
    backward-compute ...............................: (1982.74, 2219.66)
    batch-generator ................................: (50.08, 69.21)
    forward-recv ...................................: (20.42, 21.04)
    forward-send ...................................: (0.71, 0.77)
    backward-recv ..................................: (36.58, 40.14)
    backward-send ..................................: (0.94, 2.94)
    forward-send-backward-recv .....................: (1457.82, 1538.78)
    backward-send-forward-recv .....................: (284.56, 369.87)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.61, 4.80)
    grads-reduce-scatter ...........................: (8.19, 8.43)
    params-all-gather ..............................: (4.78, 4.96)
    optimizer-copy-to-main-grad ....................: (0.51, 0.71)
    optimizer-clip-main-grad .......................: (1.99, 2.02)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.09)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (9.95, 10.06)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 5545.0 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041949E+01 | loss scale: 1.0 | grad norm: 1.752 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5467.91, 5503.95)
    forward-compute ................................: (1458.97, 2896.80)
    backward-compute ...............................: (1943.82, 2214.45)
    batch-generator ................................: (49.08, 66.88)
    forward-recv ...................................: (17.58, 17.88)
    forward-send ...................................: (0.61, 0.64)
    backward-recv ..................................: (31.32, 36.48)
    backward-send ..................................: (0.73, 2.18)
    forward-send-backward-recv .....................: (1961.33, 2038.42)
    backward-send-forward-recv .....................: (317.73, 583.10)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.59, 4.82)
    grads-reduce-scatter ...........................: (8.18, 8.40)
    params-all-gather ..............................: (4.74, 4.98)
    optimizer-copy-to-main-grad ....................: (0.51, 0.68)
    optimizer-clip-main-grad .......................: (1.92, 1.95)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.03)
    optimizer-copy-main-to-model-params ............: (1.69, 1.79)
    optimizer ......................................: (9.79, 9.90)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (504756): No such process
kill: (504762): No such process
kill: (504768): No such process
kill: (504774): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=4, pp=2, mbs=2
LOCAL_IP = 10.64.24.51
DP=2, MP=4, PP=2
[2024-02-12 12:43:33,152] torch.distributed.run: [WARNING] 
[2024-02-12 12:43:33,152] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 12:43:33,152] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 12:43:33,152] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 857726976
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 857726976
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 857726976
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.188 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.251 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.925 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.091 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (860.16, 918.12)
    train/valid/test-data-iterators-setup ..........: (0.02, 10749.04)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 8484.1 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.095864E+01 | loss scale: 1.0 | grad norm: 6.531 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 9] (after 10 iterations) memory (MB) | allocated: 9896.9599609375 | max allocated: 19221.32373046875 | reserved: 21696.0 | max reserved: 21696.0
[Rank 10] (after 10 iterations) memory (MB) | allocated: 9897.0380859375 | max allocated: 19221.32373046875 | reserved: 21700.0 | max reserved: 21700.0
[Rank 11] (after 10 iterations) memory (MB) | allocated: 9896.9599609375 | max allocated: 19221.32373046875 | reserved: 21764.0 | max reserved: 21764.0[Rank 8] (after 10 iterations) memory (MB) | allocated: 9896.9599609375 | max allocated: 19221.32373046875 | reserved: 21752.0 | max reserved: 21752.0

(min, max) time across ranks (ms):
    forward-backward ...............................: (8202.28, 8235.66)
    forward-compute ................................: (2636.92, 4120.67)
    backward-compute ...............................: (2846.08, 2996.78)
    batch-generator ................................: (503.65, 555.09)
    forward-recv ...................................: (467.07, 479.28)
    forward-send ...................................: (2.56, 9.51)
    backward-recv ..................................: (30.93, 35.12)
    backward-send ..................................: (1.85, 2.41)
    forward-send-backward-recv .....................: (2573.56, 2676.79)
    backward-send-forward-recv .....................: (585.47, 867.41)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.65, 4.90)
    grads-reduce-scatter ...........................: (8.19, 217.63)
    params-all-gather ..............................: (4.82, 4.93)
    optimizer-copy-to-main-grad ....................: (0.54, 0.67)
    optimizer-clip-main-grad .......................: (5.50, 5.54)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.01, 5.25)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (13.91, 14.05)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 7129.2 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.090949E+01 | loss scale: 1.0 | grad norm: 26.468 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7067.81, 7091.03)
    forward-compute ................................: (2108.74, 3474.94)
    backward-compute ...............................: (2721.66, 2852.04)
    batch-generator ................................: (113.22, 163.21)
    forward-recv ...................................: (13.05, 14.03)
    forward-send ...................................: (0.37, 0.42)
    backward-recv ..................................: (22.66, 23.73)
    backward-send ..................................: (1.22, 1.78)
    forward-send-backward-recv .....................: (2016.03, 2192.52)
    backward-send-forward-recv .....................: (720.49, 752.66)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.65, 4.90)
    grads-reduce-scatter ...........................: (8.08, 8.36)
    params-all-gather ..............................: (4.81, 4.95)
    optimizer-copy-to-main-grad ....................: (0.52, 0.74)
    optimizer-clip-main-grad .......................: (2.61, 2.64)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.86, 5.03)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (10.60, 10.71)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 8277.3 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.067601E+01 | loss scale: 1.0 | grad norm: 3.257 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8218.10, 8242.09)
    forward-compute ................................: (2448.21, 4202.41)
    backward-compute ...............................: (2865.67, 3030.36)
    batch-generator ................................: (112.30, 155.86)
    forward-recv ...................................: (15.17, 16.74)
    forward-send ...................................: (0.40, 0.56)
    backward-recv ..................................: (19.99, 22.93)
    backward-send ..................................: (0.48, 0.60)
    forward-send-backward-recv .....................: (2540.82, 2860.93)
    backward-send-forward-recv .....................: (979.25, 1052.56)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.65, 4.97)
    grads-reduce-scatter ...........................: (8.18, 8.34)
    params-all-gather ..............................: (4.82, 4.98)
    optimizer-copy-to-main-grad ....................: (0.50, 0.82)
    optimizer-clip-main-grad .......................: (2.68, 2.72)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.05)
    optimizer-copy-main-to-model-params ............: (1.69, 1.86)
    optimizer ......................................: (10.81, 10.99)
Mon Feb 12 12:49:20 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   36C    P0             377W / 700W |  25426MiB / 81559MiB |     95%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   43C    P0             394W / 700W |  25418MiB / 81559MiB |     97%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   43C    P0             415W / 700W |  25422MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             295W / 700W |  25246MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   37C    P0             370W / 700W |  25720MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   42C    P0             338W / 700W |  25798MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   40C    P0             322W / 700W |  25756MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             330W / 700W |  25592MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 7604.4 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.060526E+01 | loss scale: 1.0 | grad norm: 0.910 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7450.68, 7483.31)
    forward-compute ................................: (2124.29, 3850.36)
    backward-compute ...............................: (2704.78, 2837.34)
    batch-generator ................................: (117.28, 146.28)
    forward-recv ...................................: (13.94, 17.12)
    forward-send ...................................: (0.42, 0.57)
    backward-recv ..................................: (17.31, 24.71)
    backward-send ..................................: (1.36, 2.30)
    forward-send-backward-recv .....................: (2372.12, 2578.29)
    backward-send-forward-recv .....................: (717.62, 835.62)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.62, 4.94)
    grads-reduce-scatter ...........................: (8.10, 8.43)
    params-all-gather ..............................: (4.81, 4.95)
    optimizer-copy-to-main-grad ....................: (0.50, 0.68)
    optimizer-clip-main-grad .......................: (2.36, 2.52)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.02)
    optimizer-copy-main-to-model-params ............: (1.69, 1.79)
    optimizer ......................................: (10.38, 10.49)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 8048.2 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.049684E+01 | loss scale: 1.0 | grad norm: 0.760 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7987.75, 8015.73)
    forward-compute ................................: (2223.28, 4033.64)
    backward-compute ...............................: (2936.39, 3047.40)
    batch-generator ................................: (114.99, 142.88)
    forward-recv ...................................: (15.07, 15.68)
    forward-send ...................................: (0.48, 0.50)
    backward-recv ..................................: (23.62, 24.19)
    backward-send ..................................: (0.51, 0.64)
    forward-send-backward-recv .....................: (2672.01, 2782.28)
    backward-send-forward-recv .....................: (862.74, 907.11)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.65, 4.77)
    grads-reduce-scatter ...........................: (8.08, 8.36)
    params-all-gather ..............................: (4.82, 4.92)
    optimizer-copy-to-main-grad ....................: (0.51, 0.68)
    optimizer-clip-main-grad .......................: (1.29, 1.30)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.02)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (9.16, 9.27)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 7716.3 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.050348E+01 | loss scale: 1.0 | grad norm: 1.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7654.15, 7680.68)
    forward-compute ................................: (2165.17, 3911.04)
    backward-compute ...............................: (2819.64, 2930.48)
    batch-generator ................................: (116.48, 136.45)
    forward-recv ...................................: (12.35, 14.91)
    forward-send ...................................: (0.36, 0.45)
    backward-recv ..................................: (23.54, 26.32)
    backward-send ..................................: (0.52, 0.66)
    forward-send-backward-recv .....................: (2503.13, 2623.06)
    backward-send-forward-recv .....................: (761.43, 853.05)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.64, 4.87)
    grads-reduce-scatter ...........................: (8.10, 8.48)
    params-all-gather ..............................: (4.78, 4.97)
    optimizer-copy-to-main-grad ....................: (0.50, 0.64)
    optimizer-clip-main-grad .......................: (1.42, 1.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.03)
    optimizer-copy-main-to-model-params ............: (1.69, 1.79)
    optimizer ......................................: (9.28, 9.38)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 7733.8 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.048254E+01 | loss scale: 1.0 | grad norm: 0.847 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7672.04, 7699.53)
    forward-compute ................................: (2407.63, 3704.57)
    backward-compute ...............................: (2810.25, 2914.71)
    batch-generator ................................: (114.95, 139.86)
    forward-recv ...................................: (13.49, 15.92)
    forward-send ...................................: (0.39, 0.85)
    backward-recv ..................................: (22.49, 23.75)
    backward-send ..................................: (1.09, 2.47)
    forward-send-backward-recv .....................: (2194.39, 2411.79)
    backward-send-forward-recv .....................: (998.02, 1140.75)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.64, 4.81)
    grads-reduce-scatter ...........................: (8.17, 8.98)
    params-all-gather ..............................: (4.75, 4.94)
    optimizer-copy-to-main-grad ....................: (0.51, 0.64)
    optimizer-clip-main-grad .......................: (1.55, 1.56)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.06)
    optimizer-copy-main-to-model-params ............: (1.69, 1.79)
    optimizer ......................................: (9.42, 9.53)
Mon Feb 12 12:54:33 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   37C    P0             401W / 700W |  25426MiB / 81559MiB |     71%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   43C    P0             382W / 700W |  25418MiB / 81559MiB |     96%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   44C    P0             363W / 700W |  25422MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             350W / 700W |  25246MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   37C    P0             393W / 700W |  25720MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   43C    P0             398W / 700W |  25798MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   41C    P0             371W / 700W |  25758MiB / 81559MiB |     95%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             364W / 700W |  25592MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 7836.0 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044667E+01 | loss scale: 1.0 | grad norm: 0.582 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7685.09, 7714.22)
    forward-compute ................................: (2148.47, 3976.05)
    backward-compute ...............................: (2820.88, 2977.23)
    batch-generator ................................: (113.56, 137.54)
    forward-recv ...................................: (13.35, 14.11)
    forward-send ...................................: (0.38, 0.43)
    backward-recv ..................................: (27.23, 29.44)
    backward-send ..................................: (0.57, 1.10)
    forward-send-backward-recv .....................: (2422.95, 2666.87)
    backward-send-forward-recv .....................: (726.77, 863.10)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.62, 4.89)
    grads-reduce-scatter ...........................: (8.04, 8.50)
    params-all-gather ..............................: (4.82, 4.94)
    optimizer-copy-to-main-grad ....................: (0.49, 0.63)
    optimizer-clip-main-grad .......................: (1.30, 1.32)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.06)
    optimizer-copy-main-to-model-params ............: (1.69, 1.79)
    optimizer ......................................: (9.14, 9.24)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 7719.7 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.043271E+01 | loss scale: 1.0 | grad norm: 1.248 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7650.27, 7678.96)
    forward-compute ................................: (2166.96, 3932.89)
    backward-compute ...............................: (2825.42, 2940.66)
    batch-generator ................................: (113.58, 142.17)
    forward-recv ...................................: (14.41, 16.00)
    forward-send ...................................: (0.47, 0.50)
    backward-recv ..................................: (25.33, 37.11)
    backward-send ..................................: (0.54, 0.69)
    forward-send-backward-recv .....................: (2440.69, 2599.82)
    backward-send-forward-recv .....................: (730.27, 873.05)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (4.62, 4.79)
    grads-reduce-scatter ...........................: (8.18, 8.44)
    params-all-gather ..............................: (4.82, 4.94)
    optimizer-copy-to-main-grad ....................: (0.50, 0.64)
    optimizer-clip-main-grad .......................: (1.80, 1.82)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.02)
    optimizer-copy-main-to-model-params ............: (1.69, 1.79)
    optimizer ......................................: (9.64, 9.75)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 8256.5 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.042085E+01 | loss scale: 1.0 | grad norm: 3.101 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (8193.81, 8221.17)
    forward-compute ................................: (2443.91, 4242.12)
    backward-compute ...............................: (2808.95, 2916.16)
    batch-generator ................................: (115.05, 141.20)
    forward-recv ...................................: (14.03, 14.18)
    forward-send ...................................: (0.41, 0.48)
    backward-recv ..................................: (23.90, 37.10)
    backward-send ..................................: (0.60, 3.32)
    forward-send-backward-recv .....................: (2686.62, 2882.14)
    backward-send-forward-recv .....................: (987.00, 1193.63)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (4.60, 4.88)
    grads-reduce-scatter ...........................: (8.04, 8.45)
    params-all-gather ..............................: (4.83, 4.96)
    optimizer-copy-to-main-grad ....................: (0.50, 0.64)
    optimizer-clip-main-grad .......................: (2.06, 2.09)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.85, 5.15)
    optimizer-copy-main-to-model-params ............: (1.69, 1.80)
    optimizer ......................................: (10.04, 10.15)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (505870): No such process
kill: (505876): No such process
kill: (505882): No such process
kill: (505888): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=2, pp=4, mbs=16
LOCAL_IP = 10.64.24.51
DP=2, MP=2, PP=4
[2024-02-12 12:59:30,770] torch.distributed.run: [WARNING] 
[2024-02-12 12:59:30,770] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 12:59:30,770] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 12:59:30,770] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 805617664
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 805617664
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 908910592
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 908910592
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.528 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.729 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.734 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  6.067 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.165 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.151 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.179 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.469 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (36.35, 574.12)
    train/valid/test-data-iterators-setup ..........: (0.02, 11900.22)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 5054.7 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.094830E+01 | loss scale: 1.0 | grad norm: 6.522 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 9] (after 10 iterations) memory (MB) | allocated: 9419.646484375 | max allocated: 31801.8408203125 | reserved: 44330.0 | max reserved: 44330.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 9420.185546875 | max allocated: 31802.5634765625 | reserved: 44076.0 | max reserved: 44076.0
[Rank 13] (after 10 iterations) memory (MB) | allocated: 10603.2763671875 | max allocated: 39605.23046875 | reserved: 48180.0 | max reserved: 48180.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 10603.2763671875 | max allocated: 39605.23046875 | reserved: 47994.0 | max reserved: 47994.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (4508.86, 4751.43)
    forward-compute ................................: (777.42, 1936.38)
    backward-compute ...............................: (1066.10, 1965.32)
    batch-generator ................................: (113.81, 148.57)
    forward-recv ...................................: (236.23, 612.35)
    forward-send ...................................: (4.49, 289.22)
    backward-recv ..................................: (148.52, 442.22)
    backward-send ..................................: (1.70, 11.00)
    forward-send-backward-recv .....................: (1902.57, 1978.70)
    backward-send-forward-recv .....................: (46.57, 156.94)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.51)
    grads-reduce-scatter ...........................: (8.77, 221.41)
    params-all-gather ..............................: (4.19, 4.87)
    optimizer-copy-to-main-grad ....................: (0.27, 0.36)
    optimizer-clip-main-grad .......................: (5.33, 5.53)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.64, 5.49)
    optimizer-copy-main-to-model-params ............: (1.43, 1.64)
    optimizer ......................................: (13.53, 13.76)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 3726.1 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.094228E+01 | loss scale: 1.0 | grad norm: 30.844 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3459.87, 3654.15)
    forward-compute ................................: (548.32, 1536.41)
    backward-compute ...............................: (931.93, 1793.35)
    batch-generator ................................: (13.75, 16.57)
    forward-recv ...................................: (49.02, 117.29)
    forward-send ...................................: (1.37, 29.09)
    backward-recv ..................................: (131.32, 408.75)
    backward-send ..................................: (1.49, 15.83)
    forward-send-backward-recv .....................: (1606.21, 1697.24)
    backward-send-forward-recv .....................: (39.76, 115.22)
    layernorm-grads-all-reduce .....................: (0.01, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.55)
    grads-reduce-scatter ...........................: (7.69, 8.96)
    params-all-gather ..............................: (4.20, 4.86)
    optimizer-copy-to-main-grad ....................: (0.26, 0.35)
    optimizer-clip-main-grad .......................: (2.35, 2.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.51, 5.28)
    optimizer-copy-main-to-model-params ............: (1.42, 1.63)
    optimizer ......................................: (10.00, 10.21)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 4027.7 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.068894E+01 | loss scale: 1.0 | grad norm: 3.055 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3706.54, 3948.33)
    forward-compute ................................: (627.14, 1623.42)
    backward-compute ...............................: (1065.02, 1915.23)
    batch-generator ................................: (13.68, 17.04)
    forward-recv ...................................: (57.38, 122.54)
    forward-send ...................................: (1.59, 27.78)
    backward-recv ..................................: (154.20, 466.94)
    backward-send ..................................: (1.75, 14.49)
    forward-send-backward-recv .....................: (1612.87, 1702.93)
    backward-send-forward-recv .....................: (48.50, 152.27)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.34)
    grads-reduce-scatter ...........................: (7.68, 9.01)
    params-all-gather ..............................: (4.14, 4.87)
    optimizer-copy-to-main-grad ....................: (0.26, 0.35)
    optimizer-clip-main-grad .......................: (2.35, 2.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.53, 5.28)
    optimizer-copy-main-to-model-params ............: (1.42, 1.65)
    optimizer ......................................: (10.03, 10.25)
Mon Feb 12 13:02:51 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   34C    P0             289W / 700W |  65646MiB / 81559MiB |     69%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   39C    P0             283W / 700W |  65498MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   40C    P0             255W / 700W |  56320MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   35C    P0             324W / 700W |  56676MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             344W / 700W |  57724MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   46C    P0             369W / 700W |  57910MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   44C    P0             427W / 700W |  56742MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   40C    P0             339W / 700W |  56918MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 3875.0 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.060303E+01 | loss scale: 1.0 | grad norm: 0.760 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3499.23, 3724.60)
    forward-compute ................................: (555.60, 1563.64)
    backward-compute ...............................: (945.65, 1794.73)
    batch-generator ................................: (13.73, 16.58)
    forward-recv ...................................: (47.49, 124.50)
    forward-send ...................................: (1.58, 28.46)
    backward-recv ..................................: (131.57, 439.64)
    backward-send ..................................: (1.43, 14.64)
    forward-send-backward-recv .....................: (1601.58, 1745.46)
    backward-send-forward-recv .....................: (37.70, 138.86)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.54)
    grads-reduce-scatter ...........................: (7.58, 9.05)
    params-all-gather ..............................: (4.20, 4.85)
    optimizer-copy-to-main-grad ....................: (0.26, 0.34)
    optimizer-clip-main-grad .......................: (2.21, 2.49)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.24)
    optimizer-copy-main-to-model-params ............: (1.42, 1.63)
    optimizer ......................................: (9.90, 10.11)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 4148.2 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.049273E+01 | loss scale: 1.0 | grad norm: 0.931 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3805.27, 4097.15)
    forward-compute ................................: (657.60, 1664.13)
    backward-compute ...............................: (1112.49, 1958.43)
    batch-generator ................................: (14.25, 17.11)
    forward-recv ...................................: (57.12, 145.76)
    forward-send ...................................: (1.77, 31.37)
    backward-recv ..................................: (129.72, 515.62)
    backward-send ..................................: (2.15, 30.36)
    forward-send-backward-recv .....................: (1609.20, 1752.57)
    backward-send-forward-recv .....................: (54.50, 168.47)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.26)
    grads-reduce-scatter ...........................: (7.70, 9.05)
    params-all-gather ..............................: (4.21, 4.88)
    optimizer-copy-to-main-grad ....................: (0.26, 0.35)
    optimizer-clip-main-grad .......................: (1.22, 1.25)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.51, 5.24)
    optimizer-copy-main-to-model-params ............: (1.42, 1.64)
    optimizer ......................................: (8.65, 8.87)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 3913.8 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049893E+01 | loss scale: 1.0 | grad norm: 0.898 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3629.11, 3864.91)
    forward-compute ................................: (589.49, 1623.64)
    backward-compute ...............................: (999.23, 1878.83)
    batch-generator ................................: (14.17, 16.49)
    forward-recv ...................................: (48.96, 141.86)
    forward-send ...................................: (1.62, 35.63)
    backward-recv ..................................: (133.01, 458.92)
    backward-send ..................................: (1.54, 30.21)
    forward-send-backward-recv .....................: (1616.13, 1731.43)
    backward-send-forward-recv .....................: (40.18, 145.50)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.51)
    grads-reduce-scatter ...........................: (7.75, 9.08)
    params-all-gather ..............................: (4.17, 5.79)
    optimizer-copy-to-main-grad ....................: (0.25, 0.34)
    optimizer-clip-main-grad .......................: (1.22, 1.24)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.51, 5.24)
    optimizer-copy-main-to-model-params ............: (1.42, 1.63)
    optimizer ......................................: (8.63, 8.84)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 3877.3 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047739E+01 | loss scale: 1.0 | grad norm: 0.662 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3596.29, 3819.30)
    forward-compute ................................: (606.38, 1607.43)
    backward-compute ...............................: (1029.09, 1875.28)
    batch-generator ................................: (13.96, 16.52)
    forward-recv ...................................: (48.58, 113.61)
    forward-send ...................................: (1.43, 21.53)
    backward-recv ..................................: (137.92, 505.31)
    backward-send ..................................: (1.59, 17.19)
    forward-send-backward-recv .....................: (1580.93, 1714.08)
    backward-send-forward-recv .....................: (40.52, 124.32)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.31)
    grads-reduce-scatter ...........................: (7.70, 8.97)
    params-all-gather ..............................: (4.18, 4.89)
    optimizer-copy-to-main-grad ....................: (0.26, 0.34)
    optimizer-clip-main-grad .......................: (1.22, 1.24)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.51, 5.22)
    optimizer-copy-main-to-model-params ............: (1.42, 1.63)
    optimizer ......................................: (8.63, 8.84)
Mon Feb 12 13:05:31 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   34C    P0             333W / 700W |  69586MiB / 81559MiB |     95%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   40C    P0             378W / 700W |  69438MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   40C    P0             310W / 700W |  56320MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   35C    P0             307W / 700W |  56676MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   40C    P0             417W / 700W |  57724MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   47C    P0             464W / 700W |  57910MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   45C    P0             440W / 700W |  56742MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   40C    P0             465W / 700W |  56918MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 4047.3 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044246E+01 | loss scale: 1.0 | grad norm: 1.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3679.73, 3899.29)
    forward-compute ................................: (625.04, 1625.97)
    backward-compute ...............................: (1056.57, 1899.17)
    batch-generator ................................: (13.80, 16.56)
    forward-recv ...................................: (52.17, 141.68)
    forward-send ...................................: (1.55, 30.38)
    backward-recv ..................................: (121.80, 471.10)
    backward-send ..................................: (1.61, 9.32)
    forward-send-backward-recv .....................: (1598.67, 1747.86)
    backward-send-forward-recv .....................: (40.92, 106.96)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.50)
    grads-reduce-scatter ...........................: (7.71, 9.02)
    params-all-gather ..............................: (4.23, 4.87)
    optimizer-copy-to-main-grad ....................: (0.26, 0.34)
    optimizer-clip-main-grad .......................: (1.46, 1.53)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.23)
    optimizer-copy-main-to-model-params ............: (1.42, 1.64)
    optimizer ......................................: (8.91, 9.13)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 4180.9 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.042848E+01 | loss scale: 1.0 | grad norm: 0.610 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3887.03, 4135.88)
    forward-compute ................................: (601.29, 1625.54)
    backward-compute ...............................: (1021.33, 1911.64)
    batch-generator ................................: (13.60, 17.45)
    forward-recv ...................................: (54.96, 137.64)
    forward-send ...................................: (1.73, 35.00)
    backward-recv ..................................: (131.36, 428.74)
    backward-send ..................................: (1.44, 15.65)
    forward-send-backward-recv .....................: (1547.93, 1740.39)
    backward-send-forward-recv .....................: (266.38, 397.60)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.34)
    grads-reduce-scatter ...........................: (7.72, 8.99)
    params-all-gather ..............................: (4.19, 4.87)
    optimizer-copy-to-main-grad ....................: (0.26, 0.35)
    optimizer-clip-main-grad .......................: (1.47, 1.53)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.23)
    optimizer-copy-main-to-model-params ............: (1.42, 1.63)
    optimizer ......................................: (8.98, 9.19)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 4129.2 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041644E+01 | loss scale: 1.0 | grad norm: 1.472 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3813.48, 4058.29)
    forward-compute ................................: (604.19, 1565.29)
    backward-compute ...............................: (1011.46, 1890.18)
    batch-generator ................................: (13.71, 17.03)
    forward-recv ...................................: (45.15, 107.70)
    forward-send ...................................: (1.38, 16.68)
    backward-recv ..................................: (146.78, 433.30)
    backward-send ..................................: (1.60, 10.76)
    forward-send-backward-recv .....................: (1534.42, 1914.47)
    backward-send-forward-recv .....................: (65.84, 347.73)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.51)
    grads-reduce-scatter ...........................: (7.61, 9.00)
    params-all-gather ..............................: (4.16, 4.88)
    optimizer-copy-to-main-grad ....................: (0.26, 0.34)
    optimizer-clip-main-grad .......................: (1.84, 2.55)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.51, 5.30)
    optimizer-copy-main-to-model-params ............: (1.42, 1.64)
    optimizer ......................................: (10.14, 10.36)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (507362): No such process
kill: (507368): No such process
kill: (507374): No such process
kill: (507380): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=2, pp=4, mbs=8
LOCAL_IP = 10.64.24.51
DP=2, MP=2, PP=4
[2024-02-12 13:09:08,101] torch.distributed.run: [WARNING] 
[2024-02-12 13:09:08,101] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 13:09:08,101] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 13:09:08,101] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 805617664
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 805617664
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 908910592
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 908910592
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.792 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.812 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.817 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.863 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.982 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.148 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.112 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.965 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (37.28, 567.51)
    train/valid/test-data-iterators-setup ..........: (0.02, 11434.72)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 5243.8 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.094801E+01 | loss scale: 1.0 | grad norm: 6.515 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 9] (after 10 iterations) memory (MB) | allocated: 9322.189453125 | max allocated: 24889.8935546875 | reserved: 33636.0 | max reserved: 33636.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 9322.189453125 | max allocated: 24889.8857421875 | reserved: 33658.0 | max reserved: 33658.0
[Rank 13] (after 10 iterations) memory (MB) | allocated: 10506.1669921875 | max allocated: 28564.91064453125 | reserved: 31742.0 | max reserved: 31742.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 10506.1669921875 | max allocated: 28564.91064453125 | reserved: 31742.0 | max reserved: 31742.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (4807.87, 4965.39)
    forward-compute ................................: (817.82, 2213.63)
    backward-compute ...............................: (1163.72, 2036.39)
    batch-generator ................................: (136.66, 157.02)
    forward-recv ...................................: (190.51, 530.00)
    forward-send ...................................: (5.20, 320.63)
    backward-recv ..................................: (82.59, 290.13)
    backward-send ..................................: (1.20, 19.76)
    forward-send-backward-recv .....................: (1932.14, 2244.16)
    backward-send-forward-recv .....................: (130.68, 296.87)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.39)
    grads-reduce-scatter ...........................: (8.54, 222.19)
    params-all-gather ..............................: (4.22, 4.87)
    optimizer-copy-to-main-grad ....................: (0.27, 0.38)
    optimizer-clip-main-grad .......................: (5.66, 5.87)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.65, 5.80)
    optimizer-copy-main-to-model-params ............: (1.43, 1.63)
    optimizer ......................................: (14.33, 14.56)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 3854.1 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.094205E+01 | loss scale: 1.0 | grad norm: 30.782 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3677.68, 3797.85)
    forward-compute ................................: (581.65, 1772.04)
    backward-compute ...............................: (1028.64, 1842.42)
    batch-generator ................................: (23.89, 28.27)
    forward-recv ...................................: (27.71, 78.01)
    forward-send ...................................: (0.73, 23.81)
    backward-recv ..................................: (64.05, 230.27)
    backward-send ..................................: (0.89, 25.99)
    forward-send-backward-recv .....................: (1604.62, 1832.67)
    backward-send-forward-recv .....................: (60.40, 264.03)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.53)
    grads-reduce-scatter ...........................: (7.71, 9.13)
    params-all-gather ..............................: (4.21, 4.89)
    optimizer-copy-to-main-grad ....................: (0.26, 0.36)
    optimizer-clip-main-grad .......................: (2.33, 2.54)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.30)
    optimizer-copy-main-to-model-params ............: (1.42, 1.62)
    optimizer ......................................: (10.01, 10.21)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 4196.5 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.068886E+01 | loss scale: 1.0 | grad norm: 3.086 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3960.47, 4128.76)
    forward-compute ................................: (687.44, 1850.18)
    backward-compute ...............................: (1206.94, 1959.99)
    batch-generator ................................: (23.77, 28.25)
    forward-recv ...................................: (26.81, 69.03)
    forward-send ...................................: (0.94, 19.04)
    backward-recv ..................................: (64.31, 293.18)
    backward-send ..................................: (1.17, 23.34)
    forward-send-backward-recv .....................: (1655.19, 1844.24)
    backward-send-forward-recv .....................: (91.60, 280.56)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.25)
    grads-reduce-scatter ...........................: (7.76, 9.00)
    params-all-gather ..............................: (4.20, 4.85)
    optimizer-copy-to-main-grad ....................: (0.26, 0.38)
    optimizer-clip-main-grad .......................: (2.36, 2.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.51, 5.27)
    optimizer-copy-main-to-model-params ............: (1.42, 1.63)
    optimizer ......................................: (10.04, 10.25)
Mon Feb 12 13:12:35 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   34C    P0             272W / 700W |  42722MiB / 81559MiB |     35%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   39C    P0             222W / 700W |  42700MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   40C    P0             371W / 700W |  42194MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   35C    P0             215W / 700W |  42796MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             373W / 700W |  38320MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   46C    P0             276W / 700W |  38320MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   45C    P0             430W / 700W |  37566MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   40C    P0             372W / 700W |  37566MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 4189.1 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.060302E+01 | loss scale: 1.0 | grad norm: 0.758 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3917.14, 4053.20)
    forward-compute ................................: (613.04, 1937.60)
    backward-compute ...............................: (1097.10, 1828.19)
    batch-generator ................................: (22.91, 30.37)
    forward-recv ...................................: (29.66, 322.34)
    forward-send ...................................: (0.91, 21.84)
    backward-recv ..................................: (57.55, 250.89)
    backward-send ..................................: (1.01, 18.68)
    forward-send-backward-recv .....................: (1582.66, 2032.81)
    backward-send-forward-recv .....................: (89.59, 360.36)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.52)
    grads-reduce-scatter ...........................: (7.69, 8.99)
    params-all-gather ..............................: (4.19, 4.81)
    optimizer-copy-to-main-grad ....................: (0.26, 0.36)
    optimizer-clip-main-grad .......................: (2.20, 2.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.52, 5.26)
    optimizer-copy-main-to-model-params ............: (1.42, 1.62)
    optimizer ......................................: (9.83, 10.04)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 4314.4 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.049270E+01 | loss scale: 1.0 | grad norm: 0.949 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4112.40, 4252.88)
    forward-compute ................................: (701.64, 1911.32)
    backward-compute ...............................: (1243.81, 2017.74)
    batch-generator ................................: (22.49, 30.22)
    forward-recv ...................................: (30.35, 86.51)
    forward-send ...................................: (0.99, 22.12)
    backward-recv ..................................: (59.88, 253.34)
    backward-send ..................................: (0.99, 23.97)
    forward-send-backward-recv .....................: (1682.68, 1944.97)
    backward-send-forward-recv .....................: (135.70, 297.41)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.28)
    grads-reduce-scatter ...........................: (7.62, 8.97)
    params-all-gather ..............................: (4.15, 4.86)
    optimizer-copy-to-main-grad ....................: (0.26, 0.36)
    optimizer-clip-main-grad .......................: (1.22, 1.24)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.51, 5.24)
    optimizer-copy-main-to-model-params ............: (1.42, 1.62)
    optimizer ......................................: (8.65, 8.85)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 4408.5 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049893E+01 | loss scale: 1.0 | grad norm: 0.862 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4201.04, 4345.10)
    forward-compute ................................: (653.57, 2089.98)
    backward-compute ...............................: (1166.70, 1909.15)
    batch-generator ................................: (22.88, 30.14)
    forward-recv ...................................: (36.93, 413.05)
    forward-send ...................................: (0.78, 20.08)
    backward-recv ..................................: (75.82, 242.06)
    backward-send ..................................: (0.97, 13.17)
    forward-send-backward-recv .....................: (1686.03, 2108.20)
    backward-send-forward-recv .....................: (112.61, 458.15)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.37)
    grads-reduce-scatter ...........................: (7.68, 9.02)
    params-all-gather ..............................: (4.23, 4.84)
    optimizer-copy-to-main-grad ....................: (0.26, 0.35)
    optimizer-clip-main-grad .......................: (1.22, 1.24)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.25)
    optimizer-copy-main-to-model-params ............: (1.42, 1.63)
    optimizer ......................................: (8.66, 8.87)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 4285.2 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047729E+01 | loss scale: 1.0 | grad norm: 0.741 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4087.14, 4227.82)
    forward-compute ................................: (645.42, 1798.09)
    backward-compute ...............................: (1134.71, 1943.99)
    batch-generator ................................: (22.67, 29.61)
    forward-recv ...................................: (26.12, 67.43)
    forward-send ...................................: (0.84, 17.68)
    backward-recv ..................................: (79.63, 226.99)
    backward-send ..................................: (1.04, 12.52)
    forward-send-backward-recv .....................: (1529.30, 2095.04)
    backward-send-forward-recv .....................: (175.13, 554.74)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.24)
    grads-reduce-scatter ...........................: (7.73, 8.97)
    params-all-gather ..............................: (4.12, 4.85)
    optimizer-copy-to-main-grad ....................: (0.26, 0.36)
    optimizer-clip-main-grad .......................: (1.21, 1.24)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.25)
    optimizer-copy-main-to-model-params ............: (1.42, 1.63)
    optimizer ......................................: (8.67, 8.88)
Mon Feb 12 13:15:26 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   35C    P0             477W / 700W |  46062MiB / 81559MiB |     81%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   41C    P0             199W / 700W |  46040MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   41C    P0             411W / 700W |  47626MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0             231W / 700W |  47660MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   41C    P0             549W / 700W |  41472MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   48C    P0             432W / 700W |  41472MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   46C    P0             482W / 700W |  40718MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   41C    P0             504W / 700W |  40718MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 4081.7 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044240E+01 | loss scale: 1.0 | grad norm: 1.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3812.06, 3938.13)
    forward-compute ................................: (675.67, 1733.18)
    backward-compute ...............................: (1203.68, 1947.39)
    batch-generator ................................: (23.22, 28.31)
    forward-recv ...................................: (31.50, 72.03)
    forward-send ...................................: (0.83, 13.51)
    backward-recv ..................................: (68.01, 215.75)
    backward-send ..................................: (0.91, 12.43)
    forward-send-backward-recv .....................: (1584.00, 1718.79)
    backward-send-forward-recv .....................: (82.06, 230.58)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.50)
    grads-reduce-scatter ...........................: (7.68, 8.92)
    params-all-gather ..............................: (4.18, 4.87)
    optimizer-copy-to-main-grad ....................: (0.26, 0.35)
    optimizer-clip-main-grad .......................: (1.46, 1.53)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.25)
    optimizer-copy-main-to-model-params ............: (1.42, 1.62)
    optimizer ......................................: (8.93, 9.13)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 4016.0 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.043283E+01 | loss scale: 1.0 | grad norm: 2.425 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (3821.53, 3956.98)
    forward-compute ................................: (665.37, 1740.58)
    backward-compute ...............................: (1186.92, 1942.97)
    batch-generator ................................: (23.33, 28.12)
    forward-recv ...................................: (31.95, 72.89)
    forward-send ...................................: (0.99, 21.31)
    backward-recv ..................................: (80.82, 222.19)
    backward-send ..................................: (1.05, 8.58)
    forward-send-backward-recv .....................: (1560.73, 1788.89)
    backward-send-forward-recv .....................: (92.20, 238.61)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.28)
    grads-reduce-scatter ...........................: (7.66, 8.90)
    params-all-gather ..............................: (4.24, 4.84)
    optimizer-copy-to-main-grad ....................: (0.26, 0.35)
    optimizer-clip-main-grad .......................: (1.95, 2.10)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.24)
    optimizer-copy-main-to-model-params ............: (1.42, 1.63)
    optimizer ......................................: (9.49, 9.70)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 5269.1 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.042221E+01 | loss scale: 1.0 | grad norm: 0.866 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5056.34, 5209.53)
    forward-compute ................................: (667.79, 2735.54)
    backward-compute ...............................: (1170.58, 1918.47)
    batch-generator ................................: (22.55, 29.03)
    forward-recv ...................................: (22.20, 61.37)
    forward-send ...................................: (0.86, 13.42)
    backward-recv ..................................: (79.86, 271.31)
    backward-send ..................................: (1.04, 9.92)
    forward-send-backward-recv .....................: (2421.46, 3006.25)
    backward-send-forward-recv .....................: (182.76, 540.81)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.61)
    grads-reduce-scatter ...........................: (7.77, 9.04)
    params-all-gather ..............................: (4.23, 4.86)
    optimizer-copy-to-main-grad ....................: (0.26, 0.35)
    optimizer-clip-main-grad .......................: (1.71, 1.82)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.51, 5.24)
    optimizer-copy-main-to-model-params ............: (1.42, 1.63)
    optimizer ......................................: (9.22, 9.43)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (508854): No such process
kill: (508860): No such process
kill: (508866): No such process
kill: (508872): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=2, pp=4, mbs=4
LOCAL_IP = 10.64.24.51
DP=2, MP=2, PP=4
[2024-02-12 13:19:15,441] torch.distributed.run: [WARNING] 
[2024-02-12 13:19:15,441] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 13:19:15,441] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 13:19:15,441] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 805617664
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 805617664
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 908910592
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 908910592
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.481 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.707 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.713 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.780 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.144 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  4.972 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.061 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.128 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (36.70, 575.45)
    train/valid/test-data-iterators-setup ..........: (0.02, 10228.81)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 5899.0 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.094810E+01 | loss scale: 1.0 | grad norm: 6.518 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 9] (after 10 iterations) memory (MB) | allocated: 9332.736328125 | max allocated: 19033.091796875 | reserved: 26140.0 | max reserved: 26140.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 9332.736328125 | max allocated: 19033.283203125 | reserved: 26416.0 | max reserved: 26416.0
[Rank 13] (after 10 iterations) memory (MB) | allocated: 10517.5263671875 | max allocated: 22512.67529296875 | reserved: 24942.0 | max reserved: 24942.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 10517.0263671875 | max allocated: 22512.17529296875 | reserved: 24942.0 | max reserved: 24942.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (5530.35, 5623.20)
    forward-compute ................................: (945.42, 2500.86)
    backward-compute ...............................: (1458.82, 2228.50)
    batch-generator ................................: (155.20, 182.14)
    forward-recv ...................................: (194.20, 501.89)
    forward-send ...................................: (4.84, 311.56)
    backward-recv ..................................: (40.04, 179.90)
    backward-send ..................................: (0.70, 13.85)
    forward-send-backward-recv .....................: (2120.19, 2570.60)
    backward-send-forward-recv .....................: (357.43, 546.82)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.41)
    grads-reduce-scatter ...........................: (8.54, 220.19)
    params-all-gather ..............................: (4.16, 4.81)
    optimizer-copy-to-main-grad ....................: (0.27, 0.41)
    optimizer-clip-main-grad .......................: (5.35, 5.56)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.64, 5.55)
    optimizer-copy-main-to-model-params ............: (1.42, 1.63)
    optimizer ......................................: (13.72, 13.93)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 4459.0 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.094250E+01 | loss scale: 1.0 | grad norm: 30.823 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4331.01, 4405.33)
    forward-compute ................................: (748.86, 2026.22)
    backward-compute ...............................: (1382.43, 2052.53)
    batch-generator ................................: (43.83, 55.05)
    forward-recv ...................................: (18.19, 37.74)
    forward-send ...................................: (0.41, 10.60)
    backward-recv ..................................: (31.43, 156.16)
    backward-send ..................................: (0.57, 15.42)
    forward-send-backward-recv .....................: (1698.09, 1988.67)
    backward-send-forward-recv .....................: (240.91, 416.73)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.23)
    grads-reduce-scatter ...........................: (7.59, 8.95)
    params-all-gather ..............................: (4.12, 4.81)
    optimizer-copy-to-main-grad ....................: (0.27, 0.37)
    optimizer-clip-main-grad .......................: (2.35, 2.56)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.51, 5.31)
    optimizer-copy-main-to-model-params ............: (1.42, 1.62)
    optimizer ......................................: (10.27, 10.48)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 4884.1 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.068902E+01 | loss scale: 1.0 | grad norm: 3.050 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4748.87, 4833.49)
    forward-compute ................................: (829.20, 2189.41)
    backward-compute ...............................: (1524.61, 2178.25)
    batch-generator ................................: (43.71, 54.54)
    forward-recv ...................................: (18.18, 40.20)
    forward-send ...................................: (0.54, 9.37)
    backward-recv ..................................: (26.78, 139.01)
    backward-send ..................................: (2.03, 19.17)
    forward-send-backward-recv .....................: (1790.20, 2206.00)
    backward-send-forward-recv .....................: (335.01, 521.40)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.10)
    grads-reduce-scatter ...........................: (7.70, 8.92)
    params-all-gather ..............................: (4.19, 4.83)
    optimizer-copy-to-main-grad ....................: (0.27, 0.35)
    optimizer-clip-main-grad .......................: (2.33, 2.54)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.51, 5.29)
    optimizer-copy-main-to-model-params ............: (1.42, 1.62)
    optimizer ......................................: (9.99, 10.19)
Mon Feb 12 13:23:10 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   33C    P0             355W / 700W |  34470MiB / 81559MiB |     37%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   39C    P0             271W / 700W |  34106MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   39C    P0             208W / 700W |  32074MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   34C    P0             259W / 700W |  32188MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             403W / 700W |  29944MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             390W / 700W |  29944MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   42C    P0             275W / 700W |  29700MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             259W / 700W |  29700MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 5071.0 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.060298E+01 | loss scale: 1.0 | grad norm: 0.759 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4849.57, 4933.38)
    forward-compute ................................: (762.85, 2258.04)
    backward-compute ...............................: (1397.66, 2048.93)
    batch-generator ................................: (44.42, 55.77)
    forward-recv ...................................: (16.73, 44.20)
    forward-send ...................................: (0.49, 11.92)
    backward-recv ..................................: (43.34, 131.79)
    backward-send ..................................: (0.57, 9.32)
    forward-send-backward-recv .....................: (1892.56, 2240.64)
    backward-send-forward-recv .....................: (503.38, 661.73)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.16)
    grads-reduce-scatter ...........................: (7.72, 8.90)
    params-all-gather ..............................: (4.19, 4.81)
    optimizer-copy-to-main-grad ....................: (0.27, 0.35)
    optimizer-clip-main-grad .......................: (2.21, 2.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.51, 5.29)
    optimizer-copy-main-to-model-params ............: (1.42, 1.62)
    optimizer ......................................: (9.84, 10.05)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 6007.7 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.049270E+01 | loss scale: 1.0 | grad norm: 0.948 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5873.32, 5958.50)
    forward-compute ................................: (863.13, 3054.90)
    backward-compute ...............................: (1569.93, 2220.26)
    batch-generator ................................: (45.38, 54.55)
    forward-recv ...................................: (20.78, 306.11)
    forward-send ...................................: (0.58, 277.44)
    backward-recv ..................................: (33.64, 133.30)
    backward-send ..................................: (0.63, 26.41)
    forward-send-backward-recv .....................: (2538.90, 3282.27)
    backward-send-forward-recv .....................: (353.50, 695.41)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.16)
    grads-reduce-scatter ...........................: (7.62, 8.91)
    params-all-gather ..............................: (4.18, 4.86)
    optimizer-copy-to-main-grad ....................: (0.27, 0.36)
    optimizer-clip-main-grad .......................: (1.23, 1.25)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.26)
    optimizer-copy-main-to-model-params ............: (1.42, 1.62)
    optimizer ......................................: (8.69, 8.89)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 5224.4 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049887E+01 | loss scale: 1.0 | grad norm: 0.852 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5084.88, 5168.45)
    forward-compute ................................: (795.04, 2408.97)
    backward-compute ...............................: (1472.05, 2139.39)
    batch-generator ................................: (45.80, 54.67)
    forward-recv ...................................: (17.75, 38.84)
    forward-send ...................................: (0.44, 8.40)
    backward-recv ..................................: (39.66, 140.54)
    backward-send ..................................: (0.60, 12.07)
    forward-send-backward-recv .....................: (1953.29, 2634.79)
    backward-send-forward-recv .....................: (319.68, 583.40)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.27)
    grads-reduce-scatter ...........................: (7.55, 9.02)
    params-all-gather ..............................: (4.16, 4.85)
    optimizer-copy-to-main-grad ....................: (0.27, 0.35)
    optimizer-clip-main-grad .......................: (1.23, 1.25)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.24)
    optimizer-copy-main-to-model-params ............: (1.42, 1.63)
    optimizer ......................................: (8.66, 8.86)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 4527.2 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047735E+01 | loss scale: 1.0 | grad norm: 0.787 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4396.91, 4473.56)
    forward-compute ................................: (801.69, 2024.27)
    backward-compute ...............................: (1471.93, 2130.88)
    batch-generator ................................: (46.09, 56.27)
    forward-recv ...................................: (16.02, 44.95)
    forward-send ...................................: (0.50, 14.97)
    backward-recv ..................................: (31.06, 131.94)
    backward-send ..................................: (0.57, 10.89)
    forward-send-backward-recv .....................: (1588.12, 1931.63)
    backward-send-forward-recv .....................: (211.91, 441.00)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.19)
    grads-reduce-scatter ...........................: (7.64, 8.90)
    params-all-gather ..............................: (4.17, 4.86)
    optimizer-copy-to-main-grad ....................: (0.27, 0.35)
    optimizer-clip-main-grad .......................: (1.22, 1.25)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.22)
    optimizer-copy-main-to-model-params ............: (1.42, 1.62)
    optimizer ......................................: (8.64, 8.84)
Mon Feb 12 13:26:34 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   35C    P0             383W / 700W |  34470MiB / 81559MiB |     75%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   41C    P0             335W / 700W |  34106MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   41C    P0             422W / 700W |  37578MiB / 81559MiB |     94%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0             325W / 700W |  37564MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   40C    P0             462W / 700W |  29944MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   47C    P0             464W / 700W |  29944MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   45C    P0             472W / 700W |  32852MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   40C    P0             407W / 700W |  32852MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 4686.6 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044251E+01 | loss scale: 1.0 | grad norm: 0.939 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4460.34, 4553.78)
    forward-compute ................................: (814.83, 2064.82)
    backward-compute ...............................: (1486.37, 2167.29)
    batch-generator ................................: (46.07, 56.10)
    forward-recv ...................................: (19.27, 38.65)
    forward-send ...................................: (0.44, 6.66)
    backward-recv ..................................: (38.17, 137.92)
    backward-send ..................................: (0.55, 13.93)
    forward-send-backward-recv .....................: (1624.78, 1959.48)
    backward-send-forward-recv .....................: (202.80, 420.65)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.42)
    grads-reduce-scatter ...........................: (7.62, 8.89)
    params-all-gather ..............................: (4.18, 4.83)
    optimizer-copy-to-main-grad ....................: (0.27, 0.35)
    optimizer-clip-main-grad .......................: (1.36, 1.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.51, 5.22)
    optimizer-copy-main-to-model-params ............: (1.42, 1.63)
    optimizer ......................................: (8.80, 9.00)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 4622.2 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.042865E+01 | loss scale: 1.0 | grad norm: 0.662 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4472.11, 4575.45)
    forward-compute ................................: (811.60, 2084.38)
    backward-compute ...............................: (1490.53, 2152.83)
    batch-generator ................................: (45.56, 56.16)
    forward-recv ...................................: (18.92, 45.71)
    forward-send ...................................: (0.57, 8.60)
    backward-recv ..................................: (40.12, 147.72)
    backward-send ..................................: (0.53, 7.13)
    forward-send-backward-recv .....................: (1624.17, 1978.13)
    backward-send-forward-recv .....................: (195.74, 420.13)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.21)
    grads-reduce-scatter ...........................: (7.68, 8.77)
    params-all-gather ..............................: (4.21, 4.84)
    optimizer-copy-to-main-grad ....................: (0.27, 0.35)
    optimizer-clip-main-grad .......................: (1.35, 1.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.51, 5.23)
    optimizer-copy-main-to-model-params ............: (1.42, 1.62)
    optimizer ......................................: (8.88, 9.08)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 4560.0 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041687E+01 | loss scale: 1.0 | grad norm: 1.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4417.94, 4507.33)
    forward-compute ................................: (796.66, 2059.03)
    backward-compute ...............................: (1463.46, 2148.46)
    batch-generator ................................: (46.24, 56.54)
    forward-recv ...................................: (15.18, 38.66)
    forward-send ...................................: (0.49, 10.34)
    backward-recv ..................................: (38.00, 139.76)
    backward-send ..................................: (0.59, 11.72)
    forward-send-backward-recv .....................: (1634.65, 1961.35)
    backward-send-forward-recv .....................: (201.85, 381.41)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.25)
    grads-reduce-scatter ...........................: (7.78, 8.90)
    params-all-gather ..............................: (4.23, 4.82)
    optimizer-copy-to-main-grad ....................: (0.27, 0.35)
    optimizer-clip-main-grad .......................: (1.97, 2.11)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.51, 5.23)
    optimizer-copy-main-to-model-params ............: (1.42, 1.63)
    optimizer ......................................: (9.51, 9.71)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (510346): No such process
kill: (510352): No such process
kill: (510358): No such process
kill: (510364): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=2, pp=4, mbs=2
LOCAL_IP = 10.64.24.51
DP=2, MP=2, PP=4
[2024-02-12 13:30:22,859] torch.distributed.run: [WARNING] 
[2024-02-12 13:30:22,859] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 13:30:22,859] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 13:30:22,859] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 805617664
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 805617664
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 908910592
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 908910592
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.632 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.659 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.694 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.461 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.111 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.165 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.197 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.223 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (38.50, 595.12)
    train/valid/test-data-iterators-setup ..........: (0.02, 11004.58)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 7368.7 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.094805E+01 | loss scale: 1.0 | grad norm: 6.516 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 9] (after 10 iterations) memory (MB) | allocated: 9300.916015625 | max allocated: 17221.9794921875 | reserved: 20196.0 | max reserved: 20196.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 9300.197265625 | max allocated: 17223.8466796875 | reserved: 19898.0 | max reserved: 19898.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 10483.2294921875 | max allocated: 18226.58935546875 | reserved: 19906.0 | max reserved: 19906.0
[Rank 13] (after 10 iterations) memory (MB) | allocated: 10483.2294921875 | max allocated: 18226.66748046875 | reserved: 19856.0 | max reserved: 19856.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (7035.98, 7110.82)
    forward-compute ................................: (1335.02, 3135.45)
    backward-compute ...............................: (2062.65, 2728.37)
    batch-generator ................................: (200.22, 233.70)
    forward-recv ...................................: (184.24, 489.81)
    forward-send ...................................: (4.24, 297.79)
    backward-recv ..................................: (26.80, 117.13)
    backward-send ..................................: (1.05, 11.20)
    forward-send-backward-recv .....................: (2334.41, 3162.34)
    backward-send-forward-recv .....................: (558.89, 828.26)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.34)
    grads-reduce-scatter ...........................: (8.54, 216.37)
    params-all-gather ..............................: (4.17, 4.84)
    optimizer-copy-to-main-grad ....................: (0.27, 0.36)
    optimizer-clip-main-grad .......................: (5.40, 5.61)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.65, 5.49)
    optimizer-copy-main-to-model-params ............: (1.42, 1.63)
    optimizer ......................................: (13.66, 13.89)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 5729.8 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.094264E+01 | loss scale: 1.0 | grad norm: 30.937 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5626.75, 5681.61)
    forward-compute ................................: (1151.15, 2592.21)
    backward-compute ...............................: (1983.08, 2553.58)
    batch-generator ................................: (90.92, 112.98)
    forward-recv ...................................: (13.41, 27.10)
    forward-send ...................................: (0.29, 2.56)
    backward-recv ..................................: (21.64, 73.11)
    backward-send ..................................: (0.40, 4.31)
    forward-send-backward-recv .....................: (1745.66, 2306.10)
    backward-send-forward-recv .....................: (435.43, 606.43)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.21)
    grads-reduce-scatter ...........................: (7.68, 8.85)
    params-all-gather ..............................: (4.19, 4.85)
    optimizer-copy-to-main-grad ....................: (0.26, 0.35)
    optimizer-clip-main-grad .......................: (2.36, 2.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.51, 5.33)
    optimizer-copy-main-to-model-params ............: (1.42, 1.63)
    optimizer ......................................: (10.11, 10.33)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 7187.5 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.068933E+01 | loss scale: 1.0 | grad norm: 3.073 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7088.38, 7142.87)
    forward-compute ................................: (1208.39, 3116.22)
    backward-compute ...............................: (2107.77, 2681.81)
    batch-generator ................................: (91.53, 128.11)
    forward-recv ...................................: (27.89, 387.02)
    forward-send ...................................: (0.32, 8.01)
    backward-recv ..................................: (21.50, 79.98)
    backward-send ..................................: (0.36, 9.88)
    forward-send-backward-recv .....................: (2233.20, 3158.42)
    backward-send-forward-recv .....................: (503.73, 1261.56)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 9.23)
    grads-reduce-scatter ...........................: (7.67, 8.94)
    params-all-gather ..............................: (4.16, 4.84)
    optimizer-copy-to-main-grad ....................: (0.27, 0.38)
    optimizer-clip-main-grad .......................: (2.37, 2.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.51, 5.33)
    optimizer-copy-main-to-model-params ............: (1.42, 1.62)
    optimizer ......................................: (10.12, 10.32)
Mon Feb 12 13:35:20 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   34C    P0             310W / 700W |  23422MiB / 81559MiB |     35%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   39C    P0             287W / 700W |  23670MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   40C    P0             245W / 700W |  25654MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   35C    P0             324W / 700W |  25608MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             348W / 700W |  23332MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             408W / 700W |  23282MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   42C    P0             354W / 700W |  23026MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   38C    P0             361W / 700W |  23028MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 6206.5 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.060297E+01 | loss scale: 1.0 | grad norm: 0.758 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6010.06, 6079.78)
    forward-compute ................................: (1153.69, 2897.79)
    backward-compute ...............................: (1997.76, 2556.92)
    batch-generator ................................: (90.19, 124.56)
    forward-recv ...................................: (13.38, 35.87)
    forward-send ...................................: (0.34, 7.28)
    backward-recv ..................................: (21.14, 81.08)
    backward-send ..................................: (0.66, 5.03)
    forward-send-backward-recv .....................: (1770.47, 2624.48)
    backward-send-forward-recv .....................: (442.56, 805.14)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.20)
    grads-reduce-scatter ...........................: (7.71, 8.81)
    params-all-gather ..............................: (4.17, 4.84)
    optimizer-copy-to-main-grad ....................: (0.26, 0.36)
    optimizer-clip-main-grad .......................: (2.22, 2.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.27)
    optimizer-copy-main-to-model-params ............: (1.42, 1.62)
    optimizer ......................................: (9.85, 10.06)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 6620.0 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.049271E+01 | loss scale: 1.0 | grad norm: 0.931 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6517.96, 6583.15)
    forward-compute ................................: (1239.79, 3190.96)
    backward-compute ...............................: (2156.20, 2744.10)
    batch-generator ................................: (91.76, 124.70)
    forward-recv ...................................: (15.02, 31.32)
    forward-send ...................................: (0.39, 6.47)
    backward-recv ..................................: (22.17, 78.80)
    backward-send ..................................: (0.38, 8.15)
    forward-send-backward-recv .....................: (1917.21, 2912.86)
    backward-send-forward-recv .....................: (517.57, 977.31)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.16)
    grads-reduce-scatter ...........................: (7.68, 8.89)
    params-all-gather ..............................: (4.16, 4.83)
    optimizer-copy-to-main-grad ....................: (0.27, 0.36)
    optimizer-clip-main-grad .......................: (1.24, 1.26)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.51, 5.56)
    optimizer-copy-main-to-model-params ............: (1.42, 1.62)
    optimizer ......................................: (9.45, 9.65)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 6062.2 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049884E+01 | loss scale: 1.0 | grad norm: 0.854 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5960.82, 6022.04)
    forward-compute ................................: (1193.54, 2794.45)
    backward-compute ...............................: (2076.61, 2639.50)
    batch-generator ................................: (90.55, 126.25)
    forward-recv ...................................: (11.78, 29.35)
    forward-send ...................................: (0.29, 5.16)
    backward-recv ..................................: (22.93, 83.21)
    backward-send ..................................: (0.43, 10.38)
    forward-send-backward-recv .....................: (1847.30, 2497.20)
    backward-send-forward-recv .....................: (457.00, 739.07)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.21)
    grads-reduce-scatter ...........................: (7.63, 8.93)
    params-all-gather ..............................: (4.21, 4.85)
    optimizer-copy-to-main-grad ....................: (0.26, 0.37)
    optimizer-clip-main-grad .......................: (1.11, 1.12)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.24)
    optimizer-copy-main-to-model-params ............: (1.42, 1.62)
    optimizer ......................................: (8.58, 8.79)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 5958.7 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047741E+01 | loss scale: 1.0 | grad norm: 0.779 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5854.36, 5919.82)
    forward-compute ................................: (1183.79, 2727.66)
    backward-compute ...............................: (2079.43, 2627.61)
    batch-generator ................................: (89.85, 119.17)
    forward-recv ...................................: (12.06, 33.57)
    forward-send ...................................: (0.30, 5.30)
    backward-recv ..................................: (20.19, 81.85)
    backward-send ..................................: (0.39, 9.81)
    forward-send-backward-recv .....................: (1834.61, 2349.12)
    backward-send-forward-recv .....................: (445.16, 656.80)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.08)
    grads-reduce-scatter ...........................: (7.58, 8.76)
    params-all-gather ..............................: (4.13, 4.85)
    optimizer-copy-to-main-grad ....................: (0.27, 0.37)
    optimizer-clip-main-grad .......................: (1.23, 1.26)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.23)
    optimizer-copy-main-to-model-params ............: (1.42, 1.62)
    optimizer ......................................: (8.69, 8.89)
Mon Feb 12 13:39:28 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   35C    P0             304W / 700W |  23682MiB / 81559MiB |     46%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   40C    P0             319W / 700W |  23890MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   41C    P0             297W / 700W |  25654MiB / 81559MiB |     92%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0             365W / 700W |  25608MiB / 81559MiB |     93%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   40C    P0             412W / 700W |  23334MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   46C    P0             434W / 700W |  23284MiB / 81559MiB |     97%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   44C    P0             422W / 700W |  23026MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             396W / 700W |  23028MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 6143.9 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044312E+01 | loss scale: 1.0 | grad norm: 0.891 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5946.00, 6013.33)
    forward-compute ................................: (1187.18, 2773.70)
    backward-compute ...............................: (2095.57, 2678.72)
    batch-generator ................................: (89.82, 108.99)
    forward-recv ...................................: (13.18, 26.71)
    forward-send ...................................: (0.30, 3.60)
    backward-recv ..................................: (26.34, 88.59)
    backward-send ..................................: (0.42, 3.81)
    forward-send-backward-recv .....................: (1783.73, 2407.32)
    backward-send-forward-recv .....................: (461.29, 664.55)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.29)
    grads-reduce-scatter ...........................: (7.63, 8.89)
    params-all-gather ..............................: (4.20, 4.80)
    optimizer-copy-to-main-grad ....................: (0.27, 0.36)
    optimizer-clip-main-grad .......................: (1.46, 1.51)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.23)
    optimizer-copy-main-to-model-params ............: (1.42, 1.62)
    optimizer ......................................: (8.95, 9.15)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 6049.8 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.043058E+01 | loss scale: 1.0 | grad norm: 0.682 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5932.16, 5994.15)
    forward-compute ................................: (1193.99, 2726.24)
    backward-compute ...............................: (2102.29, 2650.32)
    batch-generator ................................: (90.13, 108.11)
    forward-recv ...................................: (13.31, 31.25)
    forward-send ...................................: (0.37, 5.49)
    backward-recv ..................................: (22.60, 93.35)
    backward-send ..................................: (0.50, 10.75)
    forward-send-backward-recv .....................: (1820.29, 2397.34)
    backward-send-forward-recv .....................: (475.53, 659.33)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.17)
    grads-reduce-scatter ...........................: (7.69, 8.97)
    params-all-gather ..............................: (4.17, 4.88)
    optimizer-copy-to-main-grad ....................: (0.27, 0.37)
    optimizer-clip-main-grad .......................: (1.84, 1.97)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.51, 5.25)
    optimizer-copy-main-to-model-params ............: (1.42, 1.63)
    optimizer ......................................: (9.42, 9.63)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 5981.5 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041585E+01 | loss scale: 1.0 | grad norm: 0.584 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5866.97, 5929.17)
    forward-compute ................................: (1179.97, 2713.56)
    backward-compute ...............................: (2084.54, 2635.59)
    batch-generator ................................: (90.56, 108.65)
    forward-recv ...................................: (11.34, 28.88)
    forward-send ...................................: (0.34, 4.35)
    backward-recv ..................................: (19.85, 103.24)
    backward-send ..................................: (0.48, 13.56)
    forward-send-backward-recv .....................: (1804.03, 2374.33)
    backward-send-forward-recv .....................: (453.19, 632.56)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 9.14)
    grads-reduce-scatter ...........................: (7.75, 8.95)
    params-all-gather ..............................: (4.18, 4.85)
    optimizer-copy-to-main-grad ....................: (0.27, 0.36)
    optimizer-clip-main-grad .......................: (1.47, 1.53)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.51, 5.22)
    optimizer-copy-main-to-model-params ............: (1.42, 1.64)
    optimizer ......................................: (9.01, 9.23)
terminate called after throwing an instance of 'c10::Error'
  what():  CUDA driver error: unknown error
Exception raised from _hasPrimaryContext at /opt/pytorch/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp:67 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f392cdb0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f392cd6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: <unknown function> + 0xd55cef (0x7f3892a14cef in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10::cuda::MaybeSetDevice(int) + 0xc (0x7f392dbd0bac in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0xd4e4a7 (0x7f3892a0d4a7 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xe44cb8 (0x7f3892b03cb8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xd520ea (0x7f3892a110ea in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #7: c10d::ProcessGroupNCCL::WorkNCCL::~WorkNCCL() + 0x127 (0x7f3892ad6f57 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #8: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x22a (0x7f3892ad908a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #9: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x80 (0x7f3892ad92f0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #10: <unknown function> + 0xdc253 (0x7f38918b0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #11: <unknown function> + 0x94ac3 (0x7f392e36fac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #12: <unknown function> + 0x126a40 (0x7f392e401a40 in /usr/lib/x86_64-linux-gnu/libc.so.6)

[2024-02-12 13:41:33,522] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 443353 closing signal SIGTERM
[2024-02-12 13:41:33,522] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 443355 closing signal SIGTERM
[2024-02-12 13:41:33,523] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 443357 closing signal SIGTERM
[2024-02-12 13:41:33,523] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 443359 closing signal SIGTERM
[2024-02-12 13:41:34,538] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 7 (pid: 443360) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
pretrain_gpt.py FAILED
-------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_13:41:33
  host      : SYM206-GPU-A0205-P2-Node51
  rank      : 15 (local_rank: 7)
  exitcode  : -6 (pid: 443360)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 443360
=======================================================
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (511821): No such process
kill: (511844): No such process
kill: (511850): No such process
kill: (511856): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=8, pp=1, mbs=8
LOCAL_IP = 10.64.24.51
DP=2, MP=8, PP=1
[2024-02-12 13:43:51,415] torch.distributed.run: [WARNING] 
[2024-02-12 13:43:51,415] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 13:43:51,415] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 13:43:51,415] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.537 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.147 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (384.72, 455.81)
    train/valid/test-data-iterators-setup ..........: (0.02, 11039.13)
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 5741.2 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.094753E+01 | loss scale: 1.0 | grad norm: 6.613 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5638.61, 5656.57)
    forward-compute ................................: (3062.49, 3236.17)
    backward-compute ...............................: (2386.42, 2577.49)
    batch-generator ................................: (658.01, 679.25)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.44, 38.98)
    params-all-gather ..............................: (20.79, 21.02)
    optimizer-copy-to-main-grad ....................: (1.06, 1.32)
    optimizer-clip-main-grad .......................: (6.17, 6.24)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.18, 5.32)
    optimizer-copy-main-to-model-params ............: (2.12, 2.21)
    optimizer ......................................: (16.93, 17.09)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 4713.4 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.088009E+01 | loss scale: 1.0 | grad norm: 22.064 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4622.85, 4628.16)
    forward-compute ................................: (2347.02, 2446.88)
    backward-compute ...............................: (2159.65, 2264.80)
    batch-generator ................................: (31.35, 48.88)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.33, 38.76)
    params-all-gather ..............................: (20.77, 20.95)
    optimizer-copy-to-main-grad ....................: (1.06, 1.29)
    optimizer-clip-main-grad .......................: (3.23, 3.36)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.95, 5.02)
    optimizer-copy-main-to-model-params ............: (2.12, 2.22)
    optimizer ......................................: (13.33, 13.49)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 4934.8 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.065376E+01 | loss scale: 1.0 | grad norm: 2.403 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4826.66, 4849.83)
    forward-compute ................................: (2373.27, 2402.87)
    backward-compute ...............................: (2415.91, 2438.08)
    batch-generator ................................: (32.28, 48.23)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.15, 38.70)
    params-all-gather ..............................: (20.88, 21.00)
    optimizer-copy-to-main-grad ....................: (1.05, 1.37)
    optimizer-clip-main-grad .......................: (3.18, 3.26)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.94, 5.03)
    optimizer-copy-main-to-model-params ............: (2.12, 2.21)
    optimizer ......................................: (13.67, 13.95)
Mon Feb 12 13:47:49 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   38C    P0             431W / 700W |  51216MiB / 81559MiB |     20%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   45C    P0             417W / 700W |  51910MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   46C    P0             436W / 700W |  51736MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0             441W / 700W |  51780MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             440W / 700W |  51968MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   45C    P0             425W / 700W |  51786MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             438W / 700W |  51658MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             400W / 700W |  51698MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 5038.0 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.060882E+01 | loss scale: 1.0 | grad norm: 0.984 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4847.65, 4871.96)
    forward-compute ................................: (2604.08, 2631.96)
    backward-compute ...............................: (2199.09, 2251.63)
    batch-generator ................................: (32.01, 49.91)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.23, 38.98)
    params-all-gather ..............................: (20.86, 21.04)
    optimizer-copy-to-main-grad ....................: (1.05, 1.33)
    optimizer-clip-main-grad .......................: (3.00, 3.14)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.94, 5.03)
    optimizer-copy-main-to-model-params ............: (2.12, 2.23)
    optimizer ......................................: (13.18, 13.30)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 5280.0 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.050312E+01 | loss scale: 1.0 | grad norm: 0.847 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5193.13, 5195.74)
    forward-compute ................................: (2620.47, 2723.62)
    backward-compute ...............................: (2452.91, 2558.85)
    batch-generator ................................: (33.67, 49.82)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.09, 38.61)
    params-all-gather ..............................: (20.82, 21.02)
    optimizer-copy-to-main-grad ....................: (1.06, 1.34)
    optimizer-clip-main-grad .......................: (2.28, 2.36)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.94, 5.00)
    optimizer-copy-main-to-model-params ............: (2.12, 2.21)
    optimizer ......................................: (12.29, 12.47)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 4994.0 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.051120E+01 | loss scale: 1.0 | grad norm: 0.688 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4893.71, 4907.14)
    forward-compute ................................: (2494.80, 2577.92)
    backward-compute ...............................: (2312.69, 2383.62)
    batch-generator ................................: (32.77, 49.36)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.34, 38.70)
    params-all-gather ..............................: (20.83, 21.00)
    optimizer-copy-to-main-grad ....................: (1.05, 1.39)
    optimizer-clip-main-grad .......................: (1.93, 2.00)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.93, 4.99)
    optimizer-copy-main-to-model-params ............: (2.12, 2.21)
    optimizer ......................................: (12.25, 12.42)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 5192.6 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.049118E+01 | loss scale: 1.0 | grad norm: 0.773 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5097.83, 5105.85)
    forward-compute ................................: (2683.26, 2775.56)
    backward-compute ...............................: (2283.47, 2406.63)
    batch-generator ................................: (34.20, 49.14)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.24, 38.62)
    params-all-gather ..............................: (20.87, 21.04)
    optimizer-copy-to-main-grad ....................: (1.04, 1.31)
    optimizer-clip-main-grad .......................: (2.03, 2.18)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.92, 4.99)
    optimizer-copy-main-to-model-params ............: (2.12, 2.21)
    optimizer ......................................: (12.71, 12.95)
Mon Feb 12 13:51:12 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   39C    P0             369W / 700W |  61678MiB / 81559MiB |     10%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   45C    P0             435W / 700W |  62366MiB / 81559MiB |     97%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   46C    P0             410W / 700W |  62200MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   40C    P0             444W / 700W |  62320MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   39C    P0             436W / 700W |  62346MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   46C    P0             389W / 700W |  62250MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             358W / 700W |  62366MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             417W / 700W |  62322MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 4833.7 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.045625E+01 | loss scale: 1.0 | grad norm: 0.943 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4650.18, 4659.53)
    forward-compute ................................: (2199.81, 2245.00)
    backward-compute ...............................: (2389.07, 2443.47)
    batch-generator ................................: (32.68, 49.46)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.02, 38.64)
    params-all-gather ..............................: (20.81, 20.95)
    optimizer-copy-to-main-grad ....................: (1.05, 1.31)
    optimizer-clip-main-grad .......................: (2.28, 2.47)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.93, 5.00)
    optimizer-copy-main-to-model-params ............: (2.12, 2.21)
    optimizer ......................................: (12.67, 12.77)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 5244.9 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.044159E+01 | loss scale: 1.0 | grad norm: 1.554 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5153.67, 5158.13)
    forward-compute ................................: (2698.86, 2786.23)
    backward-compute ...............................: (2355.74, 2438.81)
    batch-generator ................................: (34.09, 49.18)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.22, 38.58)
    params-all-gather ..............................: (20.93, 21.08)
    optimizer-copy-to-main-grad ....................: (1.05, 1.33)
    optimizer-clip-main-grad .......................: (2.59, 2.63)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.92, 5.00)
    optimizer-copy-main-to-model-params ............: (2.12, 2.21)
    optimizer ......................................: (12.78, 13.01)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 4946.9 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.042684E+01 | loss scale: 1.0 | grad norm: 0.542 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4848.83, 4858.18)
    forward-compute ................................: (2465.42, 2467.74)
    backward-compute ...............................: (2341.81, 2376.88)
    batch-generator ................................: (33.60, 49.55)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.28, 38.88)
    params-all-gather ..............................: (20.86, 20.99)
    optimizer-copy-to-main-grad ....................: (1.06, 1.32)
    optimizer-clip-main-grad .......................: (2.46, 2.62)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.93, 4.99)
    optimizer-copy-main-to-model-params ............: (2.12, 2.21)
    optimizer ......................................: (12.65, 12.79)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (512711): No such process
kill: (512717): No such process
kill: (512723): No such process
kill: (512729): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=8, pp=1, mbs=4
LOCAL_IP = 10.64.24.51
DP=2, MP=8, PP=1
[2024-02-12 13:55:09,835] torch.distributed.run: [WARNING] 
[2024-02-12 13:55:09,835] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 13:55:09,835] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 13:55:09,835] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.204 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.995 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (362.31, 475.90)
    train/valid/test-data-iterators-setup ..........: (0.02, 10528.24)
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 7572.5 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.094760E+01 | loss scale: 1.0 | grad norm: 6.604 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7477.10, 7480.01)
    forward-compute ................................: (4351.31, 4403.38)
    backward-compute ...............................: (3017.40, 3095.29)
    batch-generator ................................: (703.63, 733.63)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.03, 0.03)
    grads-reduce-scatter ...........................: (38.59, 39.03)
    params-all-gather ..............................: (20.85, 20.99)
    optimizer-copy-to-main-grad ....................: (1.08, 1.48)
    optimizer-clip-main-grad .......................: (5.96, 6.03)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.16, 5.44)
    optimizer-copy-main-to-model-params ............: (2.10, 2.26)
    optimizer ......................................: (16.91, 17.10)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 6241.2 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.088069E+01 | loss scale: 1.0 | grad norm: 21.887 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6158.13, 6159.03)
    forward-compute ................................: (3308.84, 3340.50)
    backward-compute ...............................: (2784.24, 2818.08)
    batch-generator ................................: (71.94, 91.12)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.25, 38.72)
    params-all-gather ..............................: (20.76, 21.13)
    optimizer-copy-to-main-grad ....................: (1.08, 1.28)
    optimizer-clip-main-grad .......................: (3.10, 3.17)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.93, 5.05)
    optimizer-copy-main-to-model-params ............: (2.09, 2.26)
    optimizer ......................................: (12.78, 12.97)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 6715.4 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.065362E+01 | loss scale: 1.0 | grad norm: 2.357 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6632.36, 6633.59)
    forward-compute ................................: (3571.75, 3578.52)
    backward-compute ...............................: (3020.63, 3029.51)
    batch-generator ................................: (72.27, 86.28)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.41, 38.62)
    params-all-gather ..............................: (20.82, 21.15)
    optimizer-copy-to-main-grad ....................: (1.07, 1.25)
    optimizer-clip-main-grad .......................: (3.07, 3.19)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.93, 5.03)
    optimizer-copy-main-to-model-params ............: (2.09, 2.21)
    optimizer ......................................: (12.73, 12.91)
Mon Feb 12 14:00:10 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   37C    P0             327W / 700W |  43076MiB / 81559MiB |     88%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   43C    P0             275W / 700W |  43460MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   44C    P0             244W / 700W |  43614MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             332W / 700W |  43364MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             301W / 700W |  43434MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   43C    P0             275W / 700W |  43332MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   41C    P0             228W / 700W |  43358MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             290W / 700W |  43060MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 6368.5 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.060871E+01 | loss scale: 1.0 | grad norm: 0.975 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6192.44, 6196.45)
    forward-compute ................................: (3320.51, 3358.15)
    backward-compute ...............................: (2800.54, 2843.70)
    batch-generator ................................: (71.48, 92.84)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.24, 38.91)
    params-all-gather ..............................: (20.65, 20.96)
    optimizer-copy-to-main-grad ....................: (1.04, 1.22)
    optimizer-clip-main-grad .......................: (2.97, 3.01)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.93, 5.02)
    optimizer-copy-main-to-model-params ............: (2.09, 2.31)
    optimizer ......................................: (12.66, 12.81)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 7105.4 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.050303E+01 | loss scale: 1.0 | grad norm: 0.835 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7018.58, 7019.40)
    forward-compute ................................: (3886.50, 3890.92)
    backward-compute ...............................: (3093.46, 3101.91)
    batch-generator ................................: (70.85, 87.23)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.17, 38.65)
    params-all-gather ..............................: (20.76, 21.07)
    optimizer-copy-to-main-grad ....................: (1.04, 1.23)
    optimizer-clip-main-grad .......................: (2.31, 2.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.92, 5.02)
    optimizer-copy-main-to-model-params ............: (2.09, 2.26)
    optimizer ......................................: (12.32, 12.60)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 6779.3 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.051102E+01 | loss scale: 1.0 | grad norm: 0.674 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6696.72, 6697.23)
    forward-compute ................................: (3692.79, 3756.16)
    backward-compute ...............................: (2905.93, 2973.07)
    batch-generator ................................: (72.73, 87.11)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (37.90, 38.70)
    params-all-gather ..............................: (20.74, 20.98)
    optimizer-copy-to-main-grad ....................: (1.06, 1.22)
    optimizer-clip-main-grad .......................: (1.84, 1.91)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.92, 5.00)
    optimizer-copy-main-to-model-params ............: (2.09, 3.21)
    optimizer ......................................: (11.47, 12.62)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 6424.5 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.049122E+01 | loss scale: 1.0 | grad norm: 0.636 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6339.46, 6343.81)
    forward-compute ................................: (3350.24, 3382.07)
    backward-compute ...............................: (2927.28, 2958.14)
    batch-generator ................................: (70.05, 82.70)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.04, 38.45)
    params-all-gather ..............................: (20.75, 21.10)
    optimizer-copy-to-main-grad ....................: (1.06, 1.20)
    optimizer-clip-main-grad .......................: (2.01, 2.08)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.92, 5.00)
    optimizer-copy-main-to-model-params ............: (2.09, 2.21)
    optimizer ......................................: (11.79, 12.03)
Mon Feb 12 14:04:39 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   37C    P0             394W / 700W |  55076MiB / 81559MiB |     52%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   43C    P0             389W / 700W |  53896MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   44C    P0             391W / 700W |  54656MiB / 81559MiB |     97%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             363W / 700W |  54712MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             374W / 700W |  55166MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   43C    P0             358W / 700W |  54608MiB / 81559MiB |     93%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   41C    P0             349W / 700W |  55218MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             374W / 700W |  54324MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 6626.3 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.045669E+01 | loss scale: 1.0 | grad norm: 1.031 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6444.25, 6453.36)
    forward-compute ................................: (3410.51, 3427.29)
    backward-compute ...............................: (2982.67, 3011.66)
    batch-generator ................................: (68.59, 85.09)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.11, 38.69)
    params-all-gather ..............................: (20.76, 20.95)
    optimizer-copy-to-main-grad ....................: (1.05, 1.20)
    optimizer-clip-main-grad .......................: (2.40, 2.49)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.92, 4.99)
    optimizer-copy-main-to-model-params ............: (2.09, 2.22)
    optimizer ......................................: (11.94, 12.06)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 6548.9 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.044180E+01 | loss scale: 1.0 | grad norm: 1.807 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6457.35, 6469.06)
    forward-compute ................................: (3420.31, 3477.15)
    backward-compute ...............................: (2957.78, 3005.58)
    batch-generator ................................: (69.93, 83.87)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.18, 38.67)
    params-all-gather ..............................: (20.79, 21.12)
    optimizer-copy-to-main-grad ....................: (1.07, 1.23)
    optimizer-clip-main-grad .......................: (2.42, 2.46)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.92, 5.00)
    optimizer-copy-main-to-model-params ............: (2.09, 2.21)
    optimizer ......................................: (12.34, 12.50)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 6446.2 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.042827E+01 | loss scale: 1.0 | grad norm: 0.864 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6361.16, 6364.02)
    forward-compute ................................: (3329.16, 3411.62)
    backward-compute ...............................: (2918.18, 3000.74)
    batch-generator ................................: (71.23, 83.09)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.19, 38.89)
    params-all-gather ..............................: (20.78, 20.96)
    optimizer-copy-to-main-grad ....................: (1.06, 1.23)
    optimizer-clip-main-grad .......................: (2.50, 2.60)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.93, 4.99)
    optimizer-copy-main-to-model-params ............: (2.10, 2.21)
    optimizer ......................................: (12.23, 12.33)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (513584): No such process
kill: (513590): No such process
kill: (513596): No such process
kill: (513602): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=8, pp=1, mbs=2
LOCAL_IP = 10.64.24.51
DP=2, MP=8, PP=1
[2024-02-12 14:09:02,381] torch.distributed.run: [WARNING] 
[2024-02-12 14:09:02,381] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 14:09:02,381] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 14:09:02,381] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  5.734 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.975 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (386.43, 449.49)
    train/valid/test-data-iterators-setup ..........: (0.02, 11032.18)
NCCL version 2.19.3+cuda12.2
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 10599.5 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.094759E+01 | loss scale: 1.0 | grad norm: 6.606 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10508.81, 10515.04)
    forward-compute ................................: (5804.09, 5835.53)
    backward-compute ...............................: (4604.42, 4644.96)
    batch-generator ................................: (810.74, 867.20)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.04)
    grads-reduce-scatter ...........................: (38.62, 39.24)
    params-all-gather ..............................: (20.78, 21.08)
    optimizer-copy-to-main-grad ....................: (1.13, 1.30)
    optimizer-clip-main-grad .......................: (5.92, 5.96)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (5.17, 5.38)
    optimizer-copy-main-to-model-params ............: (2.09, 2.26)
    optimizer ......................................: (16.47, 16.57)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 9908.3 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.088065E+01 | loss scale: 1.0 | grad norm: 21.976 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (9826.39, 9827.02)
    forward-compute ................................: (5276.88, 5323.02)
    backward-compute ...............................: (4432.72, 4483.15)
    batch-generator ................................: (196.54, 229.80)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.04)
    grads-reduce-scatter ...........................: (38.33, 38.64)
    params-all-gather ..............................: (20.69, 21.13)
    optimizer-copy-to-main-grad ....................: (1.10, 1.30)
    optimizer-clip-main-grad .......................: (3.01, 3.10)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.94, 5.03)
    optimizer-copy-main-to-model-params ............: (2.09, 2.21)
    optimizer ......................................: (12.56, 12.67)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 10587.5 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.065361E+01 | loss scale: 1.0 | grad norm: 2.348 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10507.78, 10510.18)
    forward-compute ................................: (5789.98, 5801.46)
    backward-compute ...............................: (4637.20, 4654.86)
    batch-generator ................................: (180.62, 221.86)
    layernorm-grads-all-reduce .....................: (0.02, 0.04)
    embedding-grads-all-reduce .....................: (0.02, 0.05)
    grads-reduce-scatter ...........................: (38.19, 38.68)
    params-all-gather ..............................: (20.77, 20.98)
    optimizer-copy-to-main-grad ....................: (1.08, 1.26)
    optimizer-clip-main-grad .......................: (3.03, 3.05)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.94, 5.00)
    optimizer-copy-main-to-model-params ............: (2.09, 2.25)
    optimizer ......................................: (12.31, 12.54)
Mon Feb 12 14:16:28 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   35C    P0             275W / 700W |  31938MiB / 81559MiB |     44%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   41C    P0             314W / 700W |  32114MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   42C    P0             334W / 700W |  32008MiB / 81559MiB |     96%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0             303W / 700W |  31896MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   36C    P0             296W / 700W |  32144MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   41C    P0             298W / 700W |  31960MiB / 81559MiB |     97%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   39C    P0             289W / 700W |  31936MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   35C    P0             307W / 700W |  31924MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 10325.3 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.060864E+01 | loss scale: 1.0 | grad norm: 0.972 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10152.92, 10158.18)
    forward-compute ................................: (5594.05, 5604.86)
    backward-compute ...............................: (4479.21, 4499.88)
    batch-generator ................................: (194.59, 229.30)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.04)
    grads-reduce-scatter ...........................: (38.28, 38.59)
    params-all-gather ..............................: (20.76, 20.95)
    optimizer-copy-to-main-grad ....................: (1.09, 1.24)
    optimizer-clip-main-grad .......................: (2.90, 2.97)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.93, 5.00)
    optimizer-copy-main-to-model-params ............: (2.09, 2.26)
    optimizer ......................................: (12.16, 12.33)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 10486.2 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.050298E+01 | loss scale: 1.0 | grad norm: 0.833 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10404.65, 10409.48)
    forward-compute ................................: (5602.63, 5634.16)
    backward-compute ...............................: (4700.39, 4741.84)
    batch-generator ................................: (186.17, 222.69)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.04)
    grads-reduce-scatter ...........................: (38.06, 38.72)
    params-all-gather ..............................: (20.71, 21.07)
    optimizer-copy-to-main-grad ....................: (1.07, 1.24)
    optimizer-clip-main-grad .......................: (2.11, 2.17)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.93, 4.99)
    optimizer-copy-main-to-model-params ............: (2.09, 2.23)
    optimizer ......................................: (11.27, 11.39)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 10440.5 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.051100E+01 | loss scale: 1.0 | grad norm: 0.697 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10358.90, 10363.68)
    forward-compute ................................: (5694.31, 5705.18)
    backward-compute ...............................: (4583.16, 4603.91)
    batch-generator ................................: (189.98, 241.40)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.04)
    grads-reduce-scatter ...........................: (38.32, 38.69)
    params-all-gather ..............................: (20.76, 21.00)
    optimizer-copy-to-main-grad ....................: (1.08, 1.23)
    optimizer-clip-main-grad .......................: (1.83, 1.87)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.93, 4.99)
    optimizer-copy-main-to-model-params ............: (2.09, 2.21)
    optimizer ......................................: (11.16, 11.28)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 10413.5 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.049079E+01 | loss scale: 1.0 | grad norm: 0.860 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10329.53, 10333.20)
    forward-compute ................................: (5656.15, 5716.13)
    backward-compute ...............................: (4548.14, 4609.80)
    batch-generator ................................: (175.60, 222.44)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.18, 38.71)
    params-all-gather ..............................: (20.77, 22.36)
    optimizer-copy-to-main-grad ....................: (1.07, 1.32)
    optimizer-clip-main-grad .......................: (1.97, 2.00)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.93, 4.99)
    optimizer-copy-main-to-model-params ............: (2.09, 2.21)
    optimizer ......................................: (11.85, 12.01)
Mon Feb 12 14:23:25 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   35C    P0             303W / 700W |  31942MiB / 81559MiB |     56%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   42C    P0             323W / 700W |  32114MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   43C    P0             328W / 700W |  32008MiB / 81559MiB |     91%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             293W / 700W |  31896MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   36C    P0             315W / 700W |  32144MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   42C    P0             317W / 700W |  31960MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   39C    P0             296W / 700W |  31936MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   36C    P0             275W / 700W |  31924MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 10324.7 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.045645E+01 | loss scale: 1.0 | grad norm: 0.695 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10151.24, 10156.52)
    forward-compute ................................: (5420.29, 5509.25)
    backward-compute ...............................: (4578.67, 4667.46)
    batch-generator ................................: (161.06, 226.60)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.04)
    grads-reduce-scatter ...........................: (38.17, 38.75)
    params-all-gather ..............................: (20.75, 20.94)
    optimizer-copy-to-main-grad ....................: (1.10, 1.24)
    optimizer-clip-main-grad .......................: (2.33, 2.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.93, 5.00)
    optimizer-copy-main-to-model-params ............: (2.09, 2.21)
    optimizer ......................................: (12.03, 12.38)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 10249.6 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.044025E+01 | loss scale: 1.0 | grad norm: 0.622 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10164.52, 10168.29)
    forward-compute ................................: (5459.23, 5486.72)
    backward-compute ...............................: (4614.05, 4641.66)
    batch-generator ................................: (161.86, 221.34)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.03, 0.04)
    grads-reduce-scatter ...........................: (38.16, 38.55)
    params-all-gather ..............................: (20.75, 20.97)
    optimizer-copy-to-main-grad ....................: (1.07, 1.24)
    optimizer-clip-main-grad .......................: (1.82, 1.88)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.93, 5.00)
    optimizer-copy-main-to-model-params ............: (2.09, 2.36)
    optimizer ......................................: (11.39, 11.70)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 10674.6 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.042656E+01 | loss scale: 1.0 | grad norm: 1.402 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (10592.28, 10598.95)
    forward-compute ................................: (5898.03, 5968.43)
    backward-compute ...............................: (4562.53, 4630.33)
    batch-generator ................................: (163.56, 223.49)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-reduce-scatter ...........................: (38.13, 38.59)
    params-all-gather ..............................: (20.72, 21.06)
    optimizer-copy-to-main-grad ....................: (1.08, 1.25)
    optimizer-clip-main-grad .......................: (2.40, 2.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.93, 4.99)
    optimizer-copy-main-to-model-params ............: (2.09, 2.22)
    optimizer ......................................: (11.49, 11.62)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (514457): No such process
kill: (514463): No such process
kill: (514469): No such process
kill: (514475): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=1, pp=8, mbs=16
LOCAL_IP = 10.64.24.51
DP=2, MP=1, PP=8
[2024-02-12 14:29:10,313] torch.distributed.run: [WARNING] 
[2024-02-12 14:29:10,313] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 14:29:10,313] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 14:29:10,313] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 805519360
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 805519360
 > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 805519360
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 1011572736
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...

> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.825 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.842 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.873 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.868 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.874 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.876 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.918 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.488 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  5.025 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.051 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.255 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.189 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.248 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.253 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.272 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.389 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (35.23, 425.45)
    train/valid/test-data-iterators-setup ..........: (10117.29, 11475.09)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 6234.0 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.094183E+01 | loss scale: 1.0 | grad norm: 6.441 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 10] (after 10 iterations) memory (MB) | allocated: 9417.361328125 | max allocated: 34816.03173828125 | reserved: 38510.0 | max reserved: 38510.0[Rank 12] (after 10 iterations) memory (MB) | allocated: 9417.634765625 | max allocated: 34816.03173828125 | reserved: 31844.0 | max reserved: 34818.0

[Rank 8] (after 10 iterations) memory (MB) | allocated: 9417.361328125 | max allocated: 37828.26953125 | reserved: 45390.0 | max reserved: 45390.0
[Rank 14] (after 10 iterations) memory (MB) | allocated: 11777.3466796875 | max allocated: 53620.556640625 | reserved: 56766.0 | max reserved: 56766.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (5409.80, 5892.49)
    forward-compute ................................: (529.94, 2324.78)
    backward-compute ...............................: (908.49, 2606.00)
    batch-generator ................................: (27.11, 40.05)
    forward-recv ...................................: (107.44, 518.27)
    forward-send ...................................: (4.64, 307.21)
    backward-recv ..................................: (231.84, 1669.82)
    backward-send ..................................: (1.83, 62.68)
    forward-send-backward-recv .....................: (2289.01, 3211.57)
    backward-send-forward-recv .....................: (42.96, 118.39)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.11)
    grads-reduce-scatter ...........................: (9.75, 215.52)
    params-all-gather ..............................: (4.08, 5.15)
    optimizer-copy-to-main-grad ....................: (0.17, 0.22)
    optimizer-clip-main-grad .......................: (5.48, 5.83)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.62, 5.88)
    optimizer-copy-main-to-model-params ............: (1.32, 1.67)
    optimizer ......................................: (14.09, 14.48)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 5219.1 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.088146E+01 | loss scale: 1.0 | grad norm: 25.514 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4728.05, 5119.13)
    forward-compute ................................: (426.81, 2073.46)
    backward-compute ...............................: (784.55, 2421.31)
    batch-generator ................................: (13.39, 17.45)
    forward-recv ...................................: (51.19, 219.35)
    forward-send ...................................: (1.50, 84.73)
    backward-recv ..................................: (215.52, 1644.94)
    backward-send ..................................: (1.63, 60.88)
    forward-send-backward-recv .....................: (2053.15, 3061.51)
    backward-send-forward-recv .....................: (36.71, 61.15)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.17)
    grads-reduce-scatter ...........................: (7.70, 10.19)
    params-all-gather ..............................: (4.10, 5.16)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (2.33, 2.68)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.73)
    optimizer-copy-main-to-model-params ............: (1.31, 1.68)
    optimizer ......................................: (10.32, 10.69)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 5546.9 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.064538E+01 | loss scale: 1.0 | grad norm: 1.738 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4950.24, 5431.01)
    forward-compute ................................: (490.56, 2161.51)
    backward-compute ...............................: (906.41, 2555.60)
    batch-generator ................................: (13.51, 18.26)
    forward-recv ...................................: (58.58, 236.06)
    forward-send ...................................: (1.69, 101.32)
    backward-recv ..................................: (237.20, 1551.18)
    backward-send ..................................: (1.88, 47.38)
    forward-send-backward-recv .....................: (2235.53, 3115.97)
    backward-send-forward-recv .....................: (40.04, 130.62)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.95)
    grads-reduce-scatter ...........................: (7.69, 9.88)
    params-all-gather ..............................: (4.05, 5.12)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (2.33, 2.68)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.73)
    optimizer-copy-main-to-model-params ............: (1.31, 1.68)
    optimizer ......................................: (10.32, 10.69)
Mon Feb 12 14:33:28 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   30C    P0             124W / 700W |  65262MiB / 81559MiB |     59%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   36C    P0             263W / 700W |  57720MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   35C    P0             196W / 700W |  60182MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   32C    P0             176W / 700W |  45734MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   31C    P0             219W / 700W |  51836MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   35C    P0             159W / 700W |  40230MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   43C    P0             205W / 700W |  72288MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             236W / 700W |  71802MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 5391.7 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.059204E+01 | loss scale: 1.0 | grad norm: 0.992 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4776.80, 5213.91)
    forward-compute ................................: (434.68, 2100.74)
    backward-compute ...............................: (791.94, 2436.02)
    batch-generator ................................: (13.52, 18.01)
    forward-recv ...................................: (47.87, 227.08)
    forward-send ...................................: (1.70, 81.32)
    backward-recv ..................................: (214.32, 1555.20)
    backward-send ..................................: (1.50, 65.66)
    forward-send-backward-recv .....................: (2183.03, 3112.19)
    backward-send-forward-recv .....................: (34.59, 97.76)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.20)
    grads-reduce-scatter ...........................: (7.73, 9.91)
    params-all-gather ..............................: (4.06, 5.10)
    optimizer-copy-to-main-grad ....................: (0.13, 0.22)
    optimizer-clip-main-grad .......................: (2.20, 2.52)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.50, 5.73)
    optimizer-copy-main-to-model-params ............: (1.31, 1.67)
    optimizer ......................................: (10.16, 10.51)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 5704.3 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.048650E+01 | loss scale: 1.0 | grad norm: 0.883 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5057.50, 5631.41)
    forward-compute ................................: (515.96, 2197.92)
    backward-compute ...............................: (943.71, 2592.90)
    batch-generator ................................: (13.75, 19.46)
    forward-recv ...................................: (58.28, 268.77)
    forward-send ...................................: (1.89, 102.83)
    backward-recv ..................................: (214.47, 1757.37)
    backward-send ..................................: (1.79, 74.78)
    forward-send-backward-recv .....................: (2209.10, 3120.89)
    backward-send-forward-recv .....................: (40.73, 108.71)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.01)
    grads-reduce-scatter ...........................: (7.71, 9.96)
    params-all-gather ..............................: (4.09, 5.11)
    optimizer-copy-to-main-grad ....................: (0.14, 0.25)
    optimizer-clip-main-grad .......................: (1.62, 1.77)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.84)
    optimizer-copy-main-to-model-params ............: (1.31, 1.70)
    optimizer ......................................: (9.62, 10.06)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 5431.9 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049655E+01 | loss scale: 1.0 | grad norm: 1.268 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4897.81, 5373.76)
    forward-compute ................................: (461.28, 2170.52)
    backward-compute ...............................: (845.95, 2529.83)
    batch-generator ................................: (13.68, 17.64)
    forward-recv ...................................: (54.48, 260.16)
    forward-send ...................................: (1.77, 107.72)
    backward-recv ..................................: (232.36, 1668.18)
    backward-send ..................................: (1.65, 87.92)
    forward-send-backward-recv .....................: (2144.11, 3118.26)
    backward-send-forward-recv .....................: (38.38, 73.04)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.02)
    grads-reduce-scatter ...........................: (7.59, 9.87)
    params-all-gather ..............................: (4.06, 5.13)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (1.84, 2.05)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.75)
    optimizer-copy-main-to-model-params ............: (1.31, 1.67)
    optimizer ......................................: (9.71, 10.06)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 5364.1 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047555E+01 | loss scale: 1.0 | grad norm: 0.741 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4848.06, 5279.63)
    forward-compute ................................: (475.26, 2102.56)
    backward-compute ...............................: (857.23, 2509.27)
    batch-generator ................................: (13.46, 16.91)
    forward-recv ...................................: (53.07, 217.62)
    forward-send ...................................: (1.53, 74.22)
    backward-recv ..................................: (229.36, 1681.36)
    backward-send ..................................: (1.68, 64.17)
    forward-send-backward-recv .....................: (2115.83, 3100.23)
    backward-send-forward-recv .....................: (38.37, 70.52)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.93)
    grads-reduce-scatter ...........................: (7.72, 9.94)
    params-all-gather ..............................: (4.09, 5.15)
    optimizer-copy-to-main-grad ....................: (0.14, 0.21)
    optimizer-clip-main-grad .......................: (1.22, 1.25)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.75)
    optimizer-copy-main-to-model-params ............: (1.31, 1.67)
    optimizer ......................................: (8.93, 9.29)
Mon Feb 12 14:37:11 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   31C    P0             166W / 700W |  69978MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   36C    P0             234W / 700W |  57720MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   36C    P0             201W / 700W |  64898MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   32C    P0             152W / 700W |  46470MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   32C    P0             215W / 700W |  56552MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   36C    P0             150W / 700W |  40230MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   44C    P0             282W / 700W |  72288MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             306W / 700W |  71802MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 5749.8 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044053E+01 | loss scale: 1.0 | grad norm: 0.685 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5143.64, 5577.23)
    forward-compute ................................: (489.25, 2198.67)
    backward-compute ...............................: (896.91, 2545.57)
    batch-generator ................................: (13.58, 17.13)
    forward-recv ...................................: (51.06, 477.43)
    forward-send ...................................: (1.66, 376.32)
    backward-recv ..................................: (207.03, 1617.57)
    backward-send ..................................: (1.70, 30.95)
    forward-send-backward-recv .....................: (2098.01, 3091.54)
    backward-send-forward-recv .....................: (51.31, 280.33)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.19)
    grads-reduce-scatter ...........................: (7.67, 9.76)
    params-all-gather ..............................: (4.06, 5.17)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (1.34, 1.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.75)
    optimizer-copy-main-to-model-params ............: (1.31, 1.70)
    optimizer ......................................: (9.07, 9.45)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 5444.9 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.042657E+01 | loss scale: 1.0 | grad norm: 0.540 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4910.40, 5381.50)
    forward-compute ................................: (470.13, 2142.74)
    backward-compute ...............................: (862.92, 2553.07)
    batch-generator ................................: (13.43, 16.54)
    forward-recv ...................................: (55.34, 251.28)
    forward-send ...................................: (1.87, 94.90)
    backward-recv ..................................: (218.69, 1545.54)
    backward-send ..................................: (1.55, 34.85)
    forward-send-backward-recv .....................: (2189.04, 3090.73)
    backward-send-forward-recv .....................: (36.75, 95.00)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.00)
    grads-reduce-scatter ...........................: (7.72, 9.80)
    params-all-gather ..............................: (4.05, 5.13)
    optimizer-copy-to-main-grad ....................: (0.15, 0.22)
    optimizer-clip-main-grad .......................: (1.84, 2.05)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.74)
    optimizer-copy-main-to-model-params ............: (1.31, 1.67)
    optimizer ......................................: (9.72, 10.69)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 5385.8 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041219E+01 | loss scale: 1.0 | grad norm: 0.912 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4807.02, 5285.71)
    forward-compute ................................: (467.25, 2097.02)
    backward-compute ...............................: (848.02, 2539.07)
    batch-generator ................................: (13.51, 17.19)
    forward-recv ...................................: (49.42, 227.29)
    forward-send ...................................: (1.49, 90.93)
    backward-recv ..................................: (228.29, 1656.21)
    backward-send ..................................: (1.69, 48.53)
    forward-send-backward-recv .....................: (2069.32, 3072.00)
    backward-send-forward-recv .....................: (33.68, 83.94)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.25)
    grads-reduce-scatter ...........................: (7.66, 9.92)
    params-all-gather ..............................: (4.07, 5.15)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (1.47, 1.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.74)
    optimizer-copy-main-to-model-params ............: (1.31, 1.67)
    optimizer ......................................: (9.24, 9.60)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (516729): No such process
kill: (516735): No such process
kill: (516741): No such process
kill: (516747): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=1, pp=8, mbs=8
LOCAL_IP = 10.64.24.51
DP=2, MP=1, PP=8
[2024-02-12 14:41:13,787] torch.distributed.run: [WARNING] 
[2024-02-12 14:41:13,787] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 14:41:13,787] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 14:41:13,787] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 805519360
 > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 805519360
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 805519360
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 1011572736
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...

> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.621 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.787 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.790 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.795 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.799 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.814 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.822 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.955 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.913 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.035 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.069 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.104 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.184 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.227 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.255 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.124 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (37.31, 437.92)
    train/valid/test-data-iterators-setup ..........: (9833.66, 11541.84)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 6208.5 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.094191E+01 | loss scale: 1.0 | grad norm: 6.440 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 10] (after 10 iterations) memory (MB) | allocated: 9320.251953125 | max allocated: 27917.83837890625 | reserved: 32288.0 | max reserved: 32288.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 9320.306640625 | max allocated: 21774.8662109375 | reserved: 24710.0 | max reserved: 24710.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 9321.169921875 | max allocated: 31240.28271484375 | reserved: 36464.0 | max reserved: 36464.0
[Rank 14] (after 10 iterations) memory (MB) | allocated: 11680.2373046875 | max allocated: 35361.80322265625 | reserved: 40532.0 | max reserved: 40532.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (5584.17, 5898.95)
    forward-compute ................................: (551.72, 2618.39)
    backward-compute ...............................: (974.96, 2633.70)
    batch-generator ................................: (40.09, 50.17)
    forward-recv ...................................: (100.04, 408.11)
    forward-send ...................................: (3.75, 266.96)
    backward-recv ..................................: (127.21, 904.18)
    backward-send ..................................: (1.13, 61.62)
    forward-send-backward-recv .....................: (2784.04, 3489.78)
    backward-send-forward-recv .....................: (57.09, 303.96)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.14)
    grads-reduce-scatter ...........................: (9.82, 220.99)
    params-all-gather ..............................: (4.05, 5.32)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (5.52, 5.88)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.60, 5.94)
    optimizer-copy-main-to-model-params ............: (1.32, 1.68)
    optimizer ......................................: (14.10, 14.48)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 5136.5 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.088149E+01 | loss scale: 1.0 | grad norm: 25.478 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4813.93, 5060.14)
    forward-compute ................................: (450.51, 2303.36)
    backward-compute ...............................: (855.86, 2456.25)
    batch-generator ................................: (23.91, 30.78)
    forward-recv ...................................: (29.49, 142.60)
    forward-send ...................................: (0.87, 74.19)
    backward-recv ..................................: (108.30, 785.16)
    backward-send ..................................: (0.99, 59.43)
    forward-send-backward-recv .....................: (2639.91, 3246.32)
    backward-send-forward-recv .....................: (49.14, 243.26)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.14)
    grads-reduce-scatter ...........................: (7.67, 10.02)
    params-all-gather ..............................: (4.09, 5.11)
    optimizer-copy-to-main-grad ....................: (0.15, 0.21)
    optimizer-clip-main-grad .......................: (2.34, 2.69)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.72)
    optimizer-copy-main-to-model-params ............: (1.31, 1.66)
    optimizer ......................................: (10.31, 10.66)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 5509.6 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.064538E+01 | loss scale: 1.0 | grad norm: 1.766 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5080.44, 5403.07)
    forward-compute ................................: (526.18, 2389.96)
    backward-compute ...............................: (995.04, 2554.77)
    batch-generator ................................: (24.15, 32.92)
    forward-recv ...................................: (33.46, 137.93)
    forward-send ...................................: (1.09, 63.73)
    backward-recv ..................................: (104.85, 870.14)
    backward-send ..................................: (1.02, 55.22)
    forward-send-backward-recv .....................: (2669.02, 3251.59)
    backward-send-forward-recv .....................: (54.56, 301.97)
    layernorm-grads-all-reduce .....................: (0.01, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 17.97)
    grads-reduce-scatter ...........................: (7.68, 10.26)
    params-all-gather ..............................: (4.06, 5.13)
    optimizer-copy-to-main-grad ....................: (0.15, 0.22)
    optimizer-clip-main-grad .......................: (2.35, 2.70)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.95)
    optimizer-copy-main-to-model-params ............: (1.31, 1.68)
    optimizer ......................................: (10.58, 10.95)
Mon Feb 12 14:45:28 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   31C    P0             135W / 700W |  43352MiB / 81559MiB |      3%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   37C    P0             189W / 700W |  44682MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   36C    P0             144W / 700W |  38836MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   33C    P0             266W / 700W |  37348MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   32C    P0             178W / 700W |  31258MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   36C    P0             240W / 700W |  31952MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   44C    P0             197W / 700W |  43478MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   40C    P0             283W / 700W |  42992MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 5228.1 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.059183E+01 | loss scale: 1.0 | grad norm: 1.017 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4815.07, 5075.78)
    forward-compute ................................: (473.58, 2244.09)
    backward-compute ...............................: (899.08, 2452.34)
    batch-generator ................................: (24.02, 28.12)
    forward-recv ...................................: (28.83, 133.80)
    forward-send ...................................: (1.09, 62.94)
    backward-recv ..................................: (102.30, 818.77)
    backward-send ..................................: (0.84, 49.48)
    forward-send-backward-recv .....................: (2631.27, 3146.54)
    backward-send-forward-recv .....................: (51.10, 232.45)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.19)
    grads-reduce-scatter ...........................: (7.66, 9.94)
    params-all-gather ..............................: (4.11, 5.35)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (2.34, 2.69)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.73)
    optimizer-copy-main-to-model-params ............: (1.31, 1.66)
    optimizer ......................................: (10.32, 10.67)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 5543.7 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.048640E+01 | loss scale: 1.0 | grad norm: 0.896 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5157.74, 5447.33)
    forward-compute ................................: (544.42, 2424.84)
    backward-compute ...............................: (1030.72, 2623.72)
    batch-generator ................................: (23.57, 29.46)
    forward-recv ...................................: (33.47, 158.03)
    forward-send ...................................: (1.19, 66.14)
    backward-recv ..................................: (104.48, 827.14)
    backward-send ..................................: (1.07, 56.27)
    forward-send-backward-recv .....................: (2635.07, 3235.66)
    backward-send-forward-recv .....................: (61.36, 262.10)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.90)
    grads-reduce-scatter ...........................: (7.67, 10.07)
    params-all-gather ..............................: (4.07, 5.16)
    optimizer-copy-to-main-grad ....................: (0.15, 0.21)
    optimizer-clip-main-grad .......................: (1.58, 1.73)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.74)
    optimizer-copy-main-to-model-params ............: (1.31, 1.68)
    optimizer ......................................: (9.36, 9.73)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 5359.7 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049664E+01 | loss scale: 1.0 | grad norm: 1.317 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (4980.01, 5250.42)
    forward-compute ................................: (505.47, 2360.69)
    backward-compute ...............................: (958.17, 2519.60)
    batch-generator ................................: (23.96, 29.29)
    forward-recv ...................................: (29.67, 160.04)
    forward-send ...................................: (0.96, 82.46)
    backward-recv ..................................: (120.53, 807.13)
    backward-send ..................................: (1.07, 38.99)
    forward-send-backward-recv .....................: (2686.70, 3230.36)
    backward-send-forward-recv .....................: (59.29, 258.32)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.06)
    grads-reduce-scatter ...........................: (7.67, 9.98)
    params-all-gather ..............................: (4.08, 5.14)
    optimizer-copy-to-main-grad ....................: (0.14, 0.23)
    optimizer-clip-main-grad .......................: (1.84, 2.05)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.73)
    optimizer-copy-main-to-model-params ............: (1.31, 1.67)
    optimizer ......................................: (9.90, 10.27)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 5928.8 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047596E+01 | loss scale: 1.0 | grad norm: 0.815 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5576.10, 5847.61)
    forward-compute ................................: (491.99, 2691.92)
    backward-compute ...............................: (932.14, 2502.55)
    batch-generator ................................: (23.97, 29.48)
    forward-recv ...................................: (27.35, 828.64)
    forward-send ...................................: (0.99, 340.93)
    backward-recv ..................................: (122.61, 775.39)
    backward-send ..................................: (1.10, 40.80)
    forward-send-backward-recv .....................: (2513.67, 3612.28)
    backward-send-forward-recv .....................: (135.23, 828.41)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.92)
    grads-reduce-scatter ...........................: (7.64, 10.00)
    params-all-gather ..............................: (4.08, 5.13)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (1.47, 1.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.73)
    optimizer-copy-main-to-model-params ............: (1.31, 1.67)
    optimizer ......................................: (9.23, 9.60)
Mon Feb 12 14:49:13 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   32C    P0             231W / 700W |  46452MiB / 81559MiB |     20%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   37C    P0             312W / 700W |  50192MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   37C    P0             222W / 700W |  41936MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   33C    P0             277W / 700W |  42856MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   33C    P0             253W / 700W |  34358MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   36C    P0             240W / 700W |  37462MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   44C    P0             390W / 700W |  43478MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   39C    P0             413W / 700W |  49280MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 5652.2 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044208E+01 | loss scale: 1.0 | grad norm: 0.830 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5244.10, 5484.77)
    forward-compute ................................: (524.24, 2574.89)
    backward-compute ...............................: (991.92, 2556.86)
    batch-generator ................................: (23.97, 29.41)
    forward-recv ...................................: (33.89, 384.63)
    forward-send ...................................: (1.00, 63.09)
    backward-recv ..................................: (112.13, 812.19)
    backward-send ..................................: (1.02, 32.25)
    forward-send-backward-recv .....................: (2521.68, 3409.76)
    backward-send-forward-recv .....................: (58.20, 530.67)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.13)
    grads-reduce-scatter ...........................: (7.66, 9.99)
    params-all-gather ..............................: (4.08, 5.10)
    optimizer-copy-to-main-grad ....................: (0.15, 0.22)
    optimizer-clip-main-grad .......................: (1.59, 1.73)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.74)
    optimizer-copy-main-to-model-params ............: (1.31, 1.66)
    optimizer ......................................: (9.37, 9.72)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 5603.6 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.042916E+01 | loss scale: 1.0 | grad norm: 0.924 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5258.70, 5520.00)
    forward-compute ................................: (516.74, 2291.74)
    backward-compute ...............................: (976.92, 2553.06)
    batch-generator ................................: (23.98, 29.24)
    forward-recv ...................................: (31.61, 135.76)
    forward-send ...................................: (1.13, 58.80)
    backward-recv ..................................: (122.35, 796.88)
    backward-send ..................................: (1.18, 41.86)
    forward-send-backward-recv .....................: (2685.33, 3186.88)
    backward-send-forward-recv .....................: (156.59, 512.69)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.93)
    grads-reduce-scatter ...........................: (7.55, 9.96)
    params-all-gather ..............................: (4.06, 5.19)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (1.59, 1.73)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.73)
    optimizer-copy-main-to-model-params ............: (1.31, 1.68)
    optimizer ......................................: (9.39, 9.77)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 5915.6 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041280E+01 | loss scale: 1.0 | grad norm: 0.768 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5529.48, 5825.56)
    forward-compute ................................: (508.02, 2634.84)
    backward-compute ...............................: (966.10, 2507.01)
    batch-generator ................................: (23.79, 29.22)
    forward-recv ...................................: (29.90, 508.43)
    forward-send ...................................: (1.03, 464.54)
    backward-recv ..................................: (119.85, 861.91)
    backward-send ..................................: (1.18, 43.34)
    forward-send-backward-recv .....................: (2877.85, 3524.50)
    backward-send-forward-recv .....................: (126.05, 488.98)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.24)
    grads-reduce-scatter ...........................: (7.70, 10.04)
    params-all-gather ..............................: (4.04, 5.12)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (1.22, 1.26)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.73)
    optimizer-copy-main-to-model-params ............: (1.31, 1.67)
    optimizer ......................................: (9.06, 9.42)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (519001): No such process
kill: (519007): No such process
kill: (519013): No such process
kill: (519019): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=1, pp=8, mbs=4
LOCAL_IP = 10.64.24.51
DP=2, MP=1, PP=8
[2024-02-12 14:53:26,264] torch.distributed.run: [WARNING] 
[2024-02-12 14:53:26,264] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 14:53:26,264] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 14:53:26,264] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 805519360
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 805519360
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 805519360
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 1011572736
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.638 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.835 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.834 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.836 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.830 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.876 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.052 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  5.102 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.946 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.109 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.130 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.285 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.336 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.302 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.210 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.375 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (30.72, 431.96)
    train/valid/test-data-iterators-setup ..........: (9880.07, 11577.22)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 6303.6 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.094175E+01 | loss scale: 1.0 | grad norm: 6.437 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 10 iterations) memory (MB) | allocated: 9330.798828125 | max allocated: 22069.330078125 | reserved: 26392.0 | max reserved: 26392.0
[Rank 10] (after 10 iterations) memory (MB) | allocated: 9330.798828125 | max allocated: 17923.69921875 | reserved: 23366.0 | max reserved: 23366.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 9330.798828125 | max allocated: 17059.7021484375 | reserved: 22494.0 | max reserved: 22494.0
[Rank 14] (after 10 iterations) memory (MB) | allocated: 11690.7841796875 | max allocated: 25820.15771484375 | reserved: 27958.0 | max reserved: 27958.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (5805.42, 6002.70)
    forward-compute ................................: (628.67, 2677.73)
    backward-compute ...............................: (1168.06, 2697.88)
    batch-generator ................................: (58.93, 73.57)
    forward-recv ...................................: (72.22, 351.20)
    forward-send ...................................: (3.55, 266.19)
    backward-recv ..................................: (58.80, 453.88)
    backward-send ..................................: (0.75, 44.49)
    forward-send-backward-recv .....................: (2880.44, 3288.87)
    backward-send-forward-recv .....................: (118.27, 601.05)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.06)
    grads-reduce-scatter ...........................: (9.79, 216.35)
    params-all-gather ..............................: (4.08, 5.19)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (5.52, 5.93)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.60, 5.89)
    optimizer-copy-main-to-model-params ............: (1.32, 1.67)
    optimizer ......................................: (14.12, 14.50)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 5287.1 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.088200E+01 | loss scale: 1.0 | grad norm: 25.539 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5053.53, 5208.63)
    forward-compute ................................: (543.37, 2401.86)
    backward-compute ...............................: (1087.05, 2551.34)
    batch-generator ................................: (45.53, 51.71)
    forward-recv ...................................: (20.24, 88.62)
    forward-send ...................................: (0.57, 48.25)
    backward-recv ..................................: (51.99, 450.78)
    backward-send ..................................: (0.60, 49.69)
    forward-send-backward-recv .....................: (2722.78, 3037.70)
    backward-send-forward-recv .....................: (97.26, 433.14)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.11)
    grads-reduce-scatter ...........................: (7.60, 9.85)
    params-all-gather ..............................: (4.07, 5.13)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (2.33, 2.68)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.49, 5.73)
    optimizer-copy-main-to-model-params ............: (1.31, 1.66)
    optimizer ......................................: (10.33, 10.68)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 5616.7 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.064597E+01 | loss scale: 1.0 | grad norm: 1.797 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5358.22, 5542.23)
    forward-compute ................................: (615.13, 2528.26)
    backward-compute ...............................: (1211.27, 2647.19)
    batch-generator ................................: (45.54, 50.25)
    forward-recv ...................................: (18.23, 78.92)
    forward-send ...................................: (0.72, 40.79)
    backward-recv ..................................: (44.97, 458.69)
    backward-send ..................................: (0.56, 43.69)
    forward-send-backward-recv .....................: (2716.77, 3113.89)
    backward-send-forward-recv .....................: (113.78, 542.29)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.89)
    grads-reduce-scatter ...........................: (7.72, 10.04)
    params-all-gather ..............................: (4.04, 5.16)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (2.40, 2.75)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.75)
    optimizer-copy-main-to-model-params ............: (1.31, 1.66)
    optimizer ......................................: (10.40, 10.76)
Mon Feb 12 14:57:45 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   32C    P0             238W / 700W |  34420MiB / 81559MiB |     23%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   37C    P0             179W / 700W |  28828MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   37C    P0             259W / 700W |  31386MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   33C    P0             187W / 700W |  26636MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   33C    P0             154W / 700W |  30514MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   37C    P0             185W / 700W |  24222MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   45C    P0             300W / 700W |  30904MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   40C    P0             224W / 700W |  30418MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 5388.9 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.059192E+01 | loss scale: 1.0 | grad norm: 1.021 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5065.83, 5226.47)
    forward-compute ................................: (556.39, 2380.61)
    backward-compute ...............................: (1103.64, 2518.48)
    batch-generator ................................: (45.42, 50.03)
    forward-recv ...................................: (19.05, 91.04)
    forward-send ...................................: (0.68, 53.42)
    backward-recv ..................................: (60.64, 419.33)
    backward-send ..................................: (0.63, 27.54)
    forward-send-backward-recv .....................: (2665.69, 3089.33)
    backward-send-forward-recv .....................: (85.61, 439.40)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.23)
    grads-reduce-scatter ...........................: (7.71, 9.74)
    params-all-gather ..............................: (4.06, 5.19)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (2.33, 2.68)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.74)
    optimizer-copy-main-to-model-params ............: (1.31, 1.66)
    optimizer ......................................: (10.32, 10.67)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 6552.5 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.048645E+01 | loss scale: 1.0 | grad norm: 0.888 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6290.07, 6479.06)
    forward-compute ................................: (636.27, 3051.71)
    backward-compute ...............................: (1249.57, 2683.08)
    batch-generator ................................: (45.71, 49.92)
    forward-recv ...................................: (23.48, 602.26)
    forward-send ...................................: (0.77, 577.50)
    backward-recv ..................................: (48.85, 423.43)
    backward-send ..................................: (0.59, 53.71)
    forward-send-backward-recv .....................: (2832.05, 3634.73)
    backward-send-forward-recv .....................: (324.88, 1235.93)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.92)
    grads-reduce-scatter ...........................: (7.63, 9.83)
    params-all-gather ..............................: (4.07, 5.17)
    optimizer-copy-to-main-grad ....................: (0.14, 0.23)
    optimizer-clip-main-grad .......................: (1.59, 1.73)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.73)
    optimizer-copy-main-to-model-params ............: (1.31, 1.66)
    optimizer ......................................: (9.38, 9.73)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 5696.9 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049682E+01 | loss scale: 1.0 | grad norm: 1.401 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5448.53, 5611.40)
    forward-compute ................................: (586.77, 2620.34)
    backward-compute ...............................: (1159.80, 2609.84)
    batch-generator ................................: (45.31, 50.06)
    forward-recv ...................................: (22.82, 291.90)
    forward-send ...................................: (0.65, 42.26)
    backward-recv ..................................: (54.20, 413.07)
    backward-send ..................................: (0.65, 28.88)
    forward-send-backward-recv .....................: (2772.14, 3262.81)
    backward-send-forward-recv .....................: (135.00, 700.92)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.94)
    grads-reduce-scatter ...........................: (7.75, 9.96)
    params-all-gather ..............................: (4.04, 5.24)
    optimizer-copy-to-main-grad ....................: (0.14, 0.23)
    optimizer-clip-main-grad .......................: (1.84, 2.10)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.73)
    optimizer-copy-main-to-model-params ............: (1.31, 1.66)
    optimizer ......................................: (9.77, 10.12)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 5630.9 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047753E+01 | loss scale: 1.0 | grad norm: 1.383 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5407.06, 5551.93)
    forward-compute ................................: (592.94, 2400.87)
    backward-compute ...............................: (1172.74, 2602.35)
    batch-generator ................................: (45.54, 54.08)
    forward-recv ...................................: (18.83, 362.47)
    forward-send ...................................: (0.70, 331.85)
    backward-recv ..................................: (49.88, 396.69)
    backward-send ..................................: (0.66, 23.08)
    forward-send-backward-recv .....................: (2652.90, 3059.60)
    backward-send-forward-recv .....................: (276.76, 581.29)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.92)
    grads-reduce-scatter ...........................: (7.74, 10.05)
    params-all-gather ..............................: (4.05, 5.15)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (1.84, 2.05)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.74)
    optimizer-copy-main-to-model-params ............: (1.31, 1.66)
    optimizer ......................................: (9.70, 10.05)
Mon Feb 12 15:01:53 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   31C    P0             198W / 700W |  34420MiB / 81559MiB |     37%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   36C    P0             218W / 700W |  30878MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   36C    P0             233W / 700W |  31386MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   32C    P0             241W / 700W |  28686MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   32C    P0             223W / 700W |  30514MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   36C    P0             231W / 700W |  26270MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   42C    P0             427W / 700W |  30904MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   37C    P0             429W / 700W |  33562MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 6924.9 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044350E+01 | loss scale: 1.0 | grad norm: 0.983 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6592.77, 6779.99)
    forward-compute ................................: (604.60, 3297.84)
    backward-compute ...............................: (1197.41, 2635.81)
    batch-generator ................................: (45.70, 52.82)
    forward-recv ...................................: (20.10, 78.38)
    forward-send ...................................: (0.64, 35.01)
    backward-recv ..................................: (59.54, 452.29)
    backward-send ..................................: (0.59, 36.65)
    forward-send-backward-recv .....................: (3757.36, 4320.72)
    backward-send-forward-recv .....................: (284.14, 736.00)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.11)
    grads-reduce-scatter ...........................: (7.65, 9.86)
    params-all-gather ..............................: (4.05, 5.16)
    optimizer-copy-to-main-grad ....................: (0.14, 0.23)
    optimizer-clip-main-grad .......................: (1.84, 2.05)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.75)
    optimizer-copy-main-to-model-params ............: (1.31, 1.66)
    optimizer ......................................: (9.72, 10.07)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 5981.6 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.042907E+01 | loss scale: 1.0 | grad norm: 0.689 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5723.48, 5915.34)
    forward-compute ................................: (599.98, 2692.00)
    backward-compute ...............................: (1183.99, 2610.70)
    batch-generator ................................: (45.16, 49.83)
    forward-recv ...................................: (18.98, 81.02)
    forward-send ...................................: (0.75, 42.10)
    backward-recv ..................................: (59.20, 433.35)
    backward-send ..................................: (0.60, 29.73)
    forward-send-backward-recv .....................: (3214.26, 3517.65)
    backward-send-forward-recv .....................: (145.81, 457.80)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.82)
    grads-reduce-scatter ...........................: (7.70, 9.95)
    params-all-gather ..............................: (4.03, 5.18)
    optimizer-copy-to-main-grad ....................: (0.14, 0.23)
    optimizer-clip-main-grad .......................: (1.46, 1.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.74)
    optimizer-copy-main-to-model-params ............: (1.31, 1.66)
    optimizer ......................................: (9.23, 9.57)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 5395.8 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041447E+01 | loss scale: 1.0 | grad norm: 0.493 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5141.55, 5319.37)
    forward-compute ................................: (588.05, 2406.32)
    backward-compute ...............................: (1157.13, 2586.22)
    batch-generator ................................: (44.96, 48.90)
    forward-recv ...................................: (16.35, 73.50)
    forward-send ...................................: (0.70, 36.61)
    backward-recv ..................................: (56.28, 439.44)
    backward-send ..................................: (0.66, 33.19)
    forward-send-backward-recv .....................: (2696.71, 3132.38)
    backward-send-forward-recv .....................: (75.19, 434.06)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.09)
    grads-reduce-scatter ...........................: (7.63, 10.10)
    params-all-gather ..............................: (4.08, 5.11)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (1.59, 1.73)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.73)
    optimizer-copy-main-to-model-params ............: (1.31, 1.66)
    optimizer ......................................: (9.37, 9.72)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (521273): No such process
kill: (521279): No such process
kill: (521285): No such process
kill: (521291): No such process
10.64.24.50 kill done.
7b, 4k, gbs=512: dp=2, tp=1, pp=8, mbs=2
LOCAL_IP = 10.64.24.51
DP=2, MP=1, PP=8
[2024-02-12 15:06:03,715] torch.distributed.run: [WARNING] 
[2024-02-12 15:06:03,715] torch.distributed.run: [WARNING] *****************************************
[2024-02-12 15:06:03,715] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-12 15:06:03,715] torch.distributed.run: [WARNING] *****************************************
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/initialize.py:277: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 805519360
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 805519360
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 805519360
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 1011572736
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/megatron/optimizer/distrib_optimizer.py:412: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = grad_buffer.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...

> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.527 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.847 sLoading exists cache end, time cost:  4.848 s

Cutting or padding data to max_seq_len + 1 = 4097 begin ...Cutting or padding data to max_seq_len + 1 = 4097 begin ...

Loading exists cache end, time cost:  4.862 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.887 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.888 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  4.937 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Loading exists cache end, time cost:  6.415 s
Cutting or padding data to max_seq_len + 1 = 4097 begin ...
Cutting or padding data end, time cost:  4.935 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.174 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.201 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.258 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.269 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.290 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.239 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  5.408 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (28.55, 442.34)
    train/valid/test-data-iterators-setup ..........: (9767.60, 12158.46)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 7374.2 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.094185E+01 | loss scale: 1.0 | grad norm: 6.441 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 10] (after 10 iterations) memory (MB) | allocated: 9297.595703125 | max allocated: 17579.29833984375 | reserved: 21408.0 | max reserved: 21408.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 9297.314453125 | max allocated: 15658.4345703125 | reserved: 19210.0 | max reserved: 19210.0
[Rank 8] (after 10 iterations) memory (MB) | allocated: 9297.392578125 | max allocated: 19339.90234375 | reserved: 22146.0 | max reserved: 22146.0
[Rank 14] (after 10 iterations) memory (MB) | allocated: 11657.2998046875 | max allocated: 20080.15771484375 | reserved: 23262.0 | max reserved: 23262.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (6952.91, 7099.46)
    forward-compute ................................: (822.73, 3130.26)
    backward-compute ...............................: (1558.36, 3095.25)
    batch-generator ................................: (101.91, 116.74)
    forward-recv ...................................: (64.81, 308.09)
    forward-send ...................................: (4.31, 240.47)
    backward-recv ..................................: (34.29, 296.57)
    backward-send ..................................: (0.42, 34.32)
    forward-send-backward-recv .....................: (3002.13, 3921.25)
    backward-send-forward-recv .....................: (399.95, 1130.05)
    layernorm-grads-all-reduce .....................: (0.02, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.99)
    grads-reduce-scatter ...........................: (9.84, 212.68)
    params-all-gather ..............................: (4.05, 5.13)
    optimizer-copy-to-main-grad ....................: (0.16, 0.24)
    optimizer-clip-main-grad .......................: (5.38, 5.74)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.61, 5.90)
    optimizer-copy-main-to-model-params ............: (1.32, 1.67)
    optimizer ......................................: (13.98, 14.36)
 iteration       20/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 6021.6 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.088135E+01 | loss scale: 1.0 | grad norm: 25.526 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (5841.78, 5959.94)
    forward-compute ................................: (751.65, 2657.01)
    backward-compute ...............................: (1516.41, 2945.17)
    batch-generator ................................: (87.25, 94.57)
    forward-recv ...................................: (13.11, 50.67)
    forward-send ...................................: (0.39, 22.29)
    backward-recv ..................................: (27.91, 237.05)
    backward-send ..................................: (0.38, 32.63)
    forward-send-backward-recv .....................: (2597.16, 3211.22)
    backward-send-forward-recv .....................: (213.34, 764.17)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.91)
    grads-reduce-scatter ...........................: (7.65, 9.83)
    params-all-gather ..............................: (4.03, 5.12)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (2.55, 2.90)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.73)
    optimizer-copy-main-to-model-params ............: (1.31, 1.66)
    optimizer ......................................: (10.59, 10.94)
 iteration       30/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 7210.1 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.064546E+01 | loss scale: 1.0 | grad norm: 1.757 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7017.41, 7149.33)
    forward-compute ................................: (810.81, 3256.24)
    backward-compute ...............................: (1622.67, 3049.00)
    batch-generator ................................: (87.01, 94.16)
    forward-recv ...................................: (13.60, 61.44)
    forward-send ...................................: (0.39, 33.29)
    backward-recv ..................................: (26.77, 256.41)
    backward-send ..................................: (0.32, 48.75)
    forward-send-backward-recv .....................: (3042.86, 3869.91)
    backward-send-forward-recv .....................: (633.31, 1367.27)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 17.78)
    grads-reduce-scatter ...........................: (7.67, 9.85)
    params-all-gather ..............................: (4.07, 5.16)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (2.34, 2.70)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.73)
    optimizer-copy-main-to-model-params ............: (1.31, 2.12)
    optimizer ......................................: (10.34, 11.15)
Mon Feb 12 15:11:09 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   32C    P0             212W / 700W |  25174MiB / 81559MiB |     15%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   37C    P0             287W / 700W |  25610MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   38C    P0             233W / 700W |  24262MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   33C    P0             240W / 700W |  24222MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   33C    P0             243W / 700W |  22064MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   37C    P0             227W / 700W |  22926MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   44C    P0             353W / 700W |  26208MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   40C    P0             362W / 700W |  24152MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       40/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 6488.5 | learning rate: 1.875E-06 | global batch size:   512 | lm loss: 1.059189E+01 | loss scale: 1.0 | grad norm: 1.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6219.93, 6350.00)
    forward-compute ................................: (752.10, 2937.56)
    backward-compute ...............................: (1502.93, 2935.84)
    batch-generator ................................: (86.47, 93.19)
    forward-recv ...................................: (13.61, 308.14)
    forward-send ...................................: (0.41, 293.00)
    backward-recv ..................................: (27.14, 232.82)
    backward-send ..................................: (0.32, 23.18)
    forward-send-backward-recv .....................: (2741.85, 3613.41)
    backward-send-forward-recv .....................: (265.83, 994.89)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.12)
    grads-reduce-scatter ...........................: (7.69, 9.79)
    params-all-gather ..............................: (4.06, 5.14)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (2.34, 2.69)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.80)
    optimizer-copy-main-to-model-params ............: (1.31, 1.66)
    optimizer ......................................: (10.41, 10.76)
 iteration       50/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 7620.4 | learning rate: 2.344E-06 | global batch size:   512 | lm loss: 1.048644E+01 | loss scale: 1.0 | grad norm: 0.894 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7439.69, 7565.21)
    forward-compute ................................: (825.14, 3506.65)
    backward-compute ...............................: (1648.60, 3108.77)
    batch-generator ................................: (87.00, 94.03)
    forward-recv ...................................: (14.08, 324.31)
    forward-send ...................................: (0.46, 299.63)
    backward-recv ..................................: (29.63, 205.03)
    backward-send ..................................: (0.36, 31.13)
    forward-send-backward-recv .....................: (3230.91, 4596.55)
    backward-send-forward-recv .....................: (476.28, 1547.19)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.00)
    grads-reduce-scatter ...........................: (7.72, 9.86)
    params-all-gather ..............................: (4.08, 5.12)
    optimizer-copy-to-main-grad ....................: (0.14, 0.23)
    optimizer-clip-main-grad .......................: (1.61, 1.75)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.72)
    optimizer-copy-main-to-model-params ............: (1.31, 1.66)
    optimizer ......................................: (9.38, 9.73)
 iteration       60/     100 | consumed samples:        30720 | elapsed time per iteration (ms): 8011.8 | learning rate: 2.812E-06 | global batch size:   512 | lm loss: 1.049678E+01 | loss scale: 1.0 | grad norm: 1.356 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (7819.53, 7933.62)
    forward-compute ................................: (788.76, 4010.64)
    backward-compute ...............................: (1568.73, 3024.63)
    batch-generator ................................: (86.71, 97.34)
    forward-recv ...................................: (14.54, 51.87)
    forward-send ...................................: (0.37, 23.34)
    backward-recv ..................................: (29.99, 234.40)
    backward-send ..................................: (0.40, 31.23)
    forward-send-backward-recv .....................: (3495.01, 5086.04)
    backward-send-forward-recv .....................: (430.08, 1655.36)
    layernorm-grads-all-reduce .....................: (0.01, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 18.04)
    grads-reduce-scatter ...........................: (7.73, 9.91)
    params-all-gather ..............................: (4.08, 5.10)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (1.85, 2.07)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.72)
    optimizer-copy-main-to-model-params ............: (1.31, 1.66)
    optimizer ......................................: (9.90, 10.25)
 iteration       70/     100 | consumed samples:        35840 | elapsed time per iteration (ms): 6758.2 | learning rate: 3.281E-06 | global batch size:   512 | lm loss: 1.047659E+01 | loss scale: 1.0 | grad norm: 0.913 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6577.10, 6700.74)
    forward-compute ................................: (788.33, 3051.40)
    backward-compute ...............................: (1588.73, 3021.38)
    batch-generator ................................: (86.80, 100.25)
    forward-recv ...................................: (11.88, 54.12)
    forward-send ...................................: (0.36, 21.53)
    backward-recv ..................................: (29.35, 250.24)
    backward-send ..................................: (0.36, 19.04)
    forward-send-backward-recv .....................: (3220.22, 3834.69)
    backward-send-forward-recv .....................: (424.15, 756.83)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 17.84)
    grads-reduce-scatter ...........................: (7.60, 9.85)
    params-all-gather ..............................: (4.07, 5.13)
    optimizer-copy-to-main-grad ....................: (0.14, 0.23)
    optimizer-clip-main-grad .......................: (1.73, 1.90)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.71)
    optimizer-copy-main-to-model-params ............: (1.31, 1.66)
    optimizer ......................................: (9.54, 9.89)
Mon Feb 12 15:15:57 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
| N/A   33C    P0             298W / 700W |  25174MiB / 81559MiB |     16%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
| N/A   38C    P0             295W / 700W |  25610MiB / 81559MiB |     95%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
| N/A   39C    P0             330W / 700W |  24262MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   34C    P0             304W / 700W |  24222MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
| N/A   34C    P0             223W / 700W |  22064MiB / 81559MiB |     98%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
| N/A   38C    P0             289W / 700W |  22926MiB / 81559MiB |     99%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
| N/A   46C    P0             438W / 700W |  26208MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
| N/A   41C    P0             418W / 700W |  24152MiB / 81559MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
 iteration       80/     100 | consumed samples:        40960 | elapsed time per iteration (ms): 6372.5 | learning rate: 3.750E-06 | global batch size:   512 | lm loss: 1.044055E+01 | loss scale: 1.0 | grad norm: 0.821 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6100.19, 6229.17)
    forward-compute ................................: (798.12, 2855.34)
    backward-compute ...............................: (1599.00, 3042.78)
    batch-generator ................................: (86.52, 98.84)
    forward-recv ...................................: (11.62, 55.05)
    forward-send ...................................: (0.33, 24.79)
    backward-recv ..................................: (34.29, 215.17)
    backward-send ..................................: (0.39, 19.56)
    forward-send-backward-recv .....................: (2663.42, 3389.44)
    backward-send-forward-recv .....................: (179.94, 788.55)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.14)
    grads-reduce-scatter ...........................: (7.70, 9.97)
    params-all-gather ..............................: (4.10, 5.15)
    optimizer-copy-to-main-grad ....................: (0.15, 0.22)
    optimizer-clip-main-grad .......................: (1.36, 1.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.72)
    optimizer-copy-main-to-model-params ............: (1.31, 1.66)
    optimizer ......................................: (9.07, 9.42)
 iteration       90/     100 | consumed samples:        46080 | elapsed time per iteration (ms): 6326.0 | learning rate: 4.219E-06 | global batch size:   512 | lm loss: 1.042712E+01 | loss scale: 1.0 | grad norm: 2.143 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6120.51, 6242.75)
    forward-compute ................................: (798.66, 2782.98)
    backward-compute ...............................: (1595.96, 3037.50)
    batch-generator ................................: (86.72, 93.52)
    forward-recv ...................................: (12.18, 58.71)
    forward-send ...................................: (0.46, 28.95)
    backward-recv ..................................: (29.70, 243.59)
    backward-send ..................................: (0.42, 23.24)
    forward-send-backward-recv .....................: (2740.62, 3347.52)
    backward-send-forward-recv .....................: (229.58, 804.99)
    layernorm-grads-all-reduce .....................: (0.02, 0.03)
    embedding-grads-all-reduce .....................: (0.02, 17.81)
    grads-reduce-scatter ...........................: (7.68, 9.86)
    params-all-gather ..............................: (4.09, 5.17)
    optimizer-copy-to-main-grad ....................: (0.14, 0.22)
    optimizer-clip-main-grad .......................: (1.60, 1.74)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.71)
    optimizer-copy-main-to-model-params ............: (1.31, 1.66)
    optimizer ......................................: (9.37, 10.39)
 iteration      100/     100 | consumed samples:        51200 | elapsed time per iteration (ms): 6255.1 | learning rate: 4.687E-06 | global batch size:   512 | lm loss: 1.041884E+01 | loss scale: 1.0 | grad norm: 1.188 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (6043.62, 6171.78)
    forward-compute ................................: (787.63, 2740.33)
    backward-compute ...............................: (1568.55, 3019.61)
    batch-generator ................................: (87.02, 94.13)
    forward-recv ...................................: (10.89, 48.05)
    forward-send ...................................: (0.40, 18.52)
    backward-recv ..................................: (26.46, 254.72)
    backward-send ..................................: (0.48, 29.11)
    forward-send-backward-recv .....................: (2729.21, 3312.76)
    backward-send-forward-recv .....................: (215.79, 748.14)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 18.10)
    grads-reduce-scatter ...........................: (7.74, 9.87)
    params-all-gather ..............................: (4.05, 5.12)
    optimizer-copy-to-main-grad ....................: (0.15, 0.23)
    optimizer-clip-main-grad .......................: (1.85, 2.06)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (4.48, 5.71)
    optimizer-copy-main-to-model-params ............: (1.31, 1.66)
    optimizer ......................................: (9.68, 10.04)
Kill on 10.64.24.50 via /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/kill.sh
*** Kill processes on 10.64.24.50
kill: (523545): No such process
kill: (523551): No such process
kill: (523557): No such process
kill: (523563): No such process
10.64.24.50 kill done.

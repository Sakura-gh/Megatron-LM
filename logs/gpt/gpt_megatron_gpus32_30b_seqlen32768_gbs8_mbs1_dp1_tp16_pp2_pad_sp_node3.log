[2024-03-13 13:16:55,532] torch.distributed.run: [WARNING] 
[2024-03-13 13:16:55,532] torch.distributed.run: [WARNING] *****************************************
[2024-03-13 13:16:55,532] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-13 13:16:55,532] torch.distributed.run: [WARNING] *****************************************
Zarr-based strategies will not be registered because of missing packages
Zarr-based strategies will not be registered because of missing packages
Zarr-based strategies will not be registered because of missing packages
Zarr-based strategies will not be registered because of missing packages
Zarr-based strategies will not be registered because of missing packages
Zarr-based strategies will not be registered because of missing packages
Zarr-based strategies will not be registered because of missing packages
Zarr-based strategies will not be registered because of missing packages
INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (14, 1): 1024252914
 > number of parameters on (tensor, pipeline) model parallel rank (10, 1): 1024252914
 > number of parameters on (tensor, pipeline) model parallel rank (9, 1): 1024252914
 > number of parameters on (tensor, pipeline) model parallel rank (11, 1): 1024252914
 > number of parameters on (tensor, pipeline) model parallel rank (13, 1): 1024252914
 > number of parameters on (tensor, pipeline) model parallel rank (15, 1): 1024252914
 > number of parameters on (tensor, pipeline) model parallel rank (12, 1): 1024252914
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (8, 1): 1024252914
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (2811.49, 3183.87)
    train/valid/test-data-iterators-setup ..........: (0.02, 41506.39)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
 iteration       10/      32 | consumed samples:           80 | elapsed time per iteration (ms): 22041.0 | learning rate: 4.687E-07 | global batch size:     8 | lm loss: 6.537831E+00 | loss scale: 1.0 | grad norm: 49.787 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 30] (after 10 iterations) memory (MB) | allocated: 18178.94140625 | max allocated: 33462.552734375 | reserved: 66718.0 | max reserved: 66718.0
[Rank 29] (after 10 iterations) memory (MB) | allocated: 18178.7490234375 | max allocated: 33468.4228515625 | reserved: 66696.0 | max reserved: 66696.0[Rank 24] (after 10 iterations) memory (MB) | allocated: 18179.619140625 | max allocated: 33469.35498046875 | reserved: 66700.0 | max reserved: 66700.0[Rank 28] (after 10 iterations) memory (MB) | allocated: 18179.619140625 | max allocated: 33469.42333984375 | reserved: 67318.0 | max reserved: 67318.0


[Rank 27] (after 10 iterations) memory (MB) | allocated: 18178.7490234375 | max allocated: 33464.48291015625 | reserved: 66732.0 | max reserved: 66732.0
[Rank 31] (after 10 iterations) memory (MB) | allocated: 18178.7490234375 | max allocated: 33468.5439453125 | reserved: 66720.0 | max reserved: 66720.0
[Rank 26] (after 10 iterations) memory (MB) | allocated: 18178.7490234375 | max allocated: 33470.5419921875 | reserved: 65852.0 | max reserved: 65852.0
[Rank 25] (after 10 iterations) memory (MB) | allocated: 18178.7490234375 | max allocated: 33460.42333984375 | reserved: 66312.0 | max reserved: 66312.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (20325.75, 21817.08)
    forward-compute ................................: (6748.78, 7429.50)
    backward-compute ...............................: (9841.38, 12053.68)
    batch-generator ................................: (636.93, 956.29)
    forward-recv ...................................: (1521.18, 1521.67)
    forward-send ...................................: (2.66, 3.20)
    backward-recv ..................................: (432.36, 470.19)
    backward-send ..................................: (0.73, 0.81)
    forward-send-backward-recv .....................: (1960.05, 1964.49)
    backward-send-forward-recv .....................: (2193.04, 2200.85)
    layernorm-grads-all-reduce .....................: (2.44, 3.01)
    embedding-grads-all-reduce .....................: (2.11, 2.19)
    all-grads-sync .................................: (57.75, 69.37)
    params-all-gather ..............................: (5.54, 6.53)
    optimizer-copy-to-main-grad ....................: (2.05, 2.28)
    optimizer-clip-main-grad .......................: (11.73, 13.02)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.31, 14.89)
    optimizer-copy-main-to-model-params ............: (4.63, 5.36)
    optimizer ......................................: (42.01, 42.96)
 iteration       20/      32 | consumed samples:          160 | elapsed time per iteration (ms): 20334.8 | learning rate: 9.375E-07 | global batch size:     8 | lm loss: 3.249572E-01 | loss scale: 1.0 | grad norm: 4.653 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (18789.70, 20281.35)
    forward-compute ................................: (6026.20, 6712.00)
    backward-compute ...............................: (9844.13, 11959.21)
    batch-generator ................................: (9.15, 214.02)
    forward-recv ...................................: (749.27, 749.36)
    forward-send ...................................: (0.71, 0.74)
    backward-recv ..................................: (439.47, 469.93)
    backward-send ..................................: (0.72, 0.80)
    forward-send-backward-recv .....................: (1241.98, 1243.31)
    backward-send-forward-recv .....................: (2157.91, 2163.32)
    layernorm-grads-all-reduce .....................: (2.46, 2.99)
    embedding-grads-all-reduce .....................: (2.08, 2.16)
    all-grads-sync .................................: (2.81, 3.47)
    params-all-gather ..............................: (5.52, 6.49)
    optimizer-copy-to-main-grad ....................: (2.00, 2.30)
    optimizer-clip-main-grad .......................: (8.97, 10.25)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (11.86, 14.37)
    optimizer-copy-main-to-model-params ............: (4.63, 5.36)
    optimizer ......................................: (39.08, 40.04)
 iteration       30/      32 | consumed samples:          240 | elapsed time per iteration (ms): 20336.9 | learning rate: 1.406E-06 | global batch size:     8 | lm loss: 3.946434E-01 | loss scale: 1.0 | grad norm: 4.013 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (18794.66, 20285.24)
    forward-compute ................................: (6022.03, 6617.25)
    backward-compute ...............................: (9839.25, 12087.77)
    batch-generator ................................: (9.19, 223.20)
    forward-recv ...................................: (749.58, 749.68)
    forward-send ...................................: (0.72, 0.74)
    backward-recv ..................................: (426.32, 477.15)
    backward-send ..................................: (0.71, 0.84)
    forward-send-backward-recv .....................: (1241.84, 1242.56)
    backward-send-forward-recv .....................: (2168.78, 2174.91)
    layernorm-grads-all-reduce .....................: (2.32, 3.03)
    embedding-grads-all-reduce .....................: (2.07, 2.14)
    all-grads-sync .................................: (2.80, 3.47)
    params-all-gather ..............................: (5.50, 6.45)
    optimizer-copy-to-main-grad ....................: (1.99, 2.36)
    optimizer-clip-main-grad .......................: (7.96, 8.99)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (11.86, 14.36)
    optimizer-copy-main-to-model-params ............: (4.62, 5.35)
    optimizer ......................................: (37.21, 38.15)

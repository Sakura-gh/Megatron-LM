[2024-03-11 18:35:09,898] torch.distributed.run: [WARNING] 
[2024-03-11 18:35:09,898] torch.distributed.run: [WARNING] *****************************************
[2024-03-11 18:35:09,898] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-11 18:35:09,898] torch.distributed.run: [WARNING] *****************************************
Zarr-based strategies will not be registered because of missing packages
Zarr-based strategies will not be registered because of missing packages
Zarr-based strategies will not be registered because of missing packages
Zarr-based strategies will not be registered because of missing packages
Zarr-based strategies will not be registered because of missing packages
Zarr-based strategies will not be registered because of missing packages
Zarr-based strategies will not be registered because of missing packages
Zarr-based strategies will not be registered because of missing packages
INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 786828800
 > number of parameters on (tensor, pipeline) model parallel rank (3, 2): 786828800
 > number of parameters on (tensor, pipeline) model parallel rank (2, 2): 786828800
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 786828800
INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (786828800 elements):
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
 > number of parameters on (tensor, pipeline) model parallel rank (3, 3): 851719680
 > number of parameters on (tensor, pipeline) model parallel rank (2, 3): 851719680
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 851719680
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 851719680
INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (851719680 elements):
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
> building GPT2BPETokenizer tokenizer ...
> building GPT2BPETokenizer tokenizer ...
INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
Loading exists cache end, time cost:  4.885 s
Cutting or padding data to max_seq_len + 1 = 16385 begin ...
Loading exists cache end, time cost:  4.909 s
Cutting or padding data to max_seq_len + 1 = 16385 begin ...
Cutting or padding data end, time cost:  18.615 s
consumed_train_samples = 0, dataloader_type = single
Cutting or padding data end, time cost:  18.701 s
consumed_train_samples = 0, dataloader_type = single
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (52.88, 1307.75)
    train/valid/test-data-iterators-setup ..........: (0.02, 24156.76)
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
NCCL version 2.19.3+cuda12.2
 iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 16161.5 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.086542E+01 | loss scale: 1.0 | grad norm: 9.419 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 9] (after 10 iterations) memory (MB) | allocated: 13933.9169921875 | max allocated: 32521.77978515625 | reserved: 49214.0 | max reserved: 49214.0[Rank 11] (after 10 iterations) memory (MB) | allocated: 13935.8076171875 | max allocated: 32521.24755859375 | reserved: 49590.0 | max reserved: 49590.0[Rank 8] (after 10 iterations) memory (MB) | allocated: 13933.9169921875 | max allocated: 32520.69921875 | reserved: 49850.0 | max reserved: 49850.0[Rank 10] (after 10 iterations) memory (MB) | allocated: 13936.5029296875 | max allocated: 32518.96435546875 | reserved: 49434.0 | max reserved: 49434.0



[Rank 13] (after 10 iterations) memory (MB) | allocated: 15039.1484375 | max allocated: 38588.98876953125 | reserved: 41722.0 | max reserved: 41722.0
[Rank 14] (after 10 iterations) memory (MB) | allocated: 15039.1484375 | max allocated: 38589.88720703125 | reserved: 41722.0 | max reserved: 41722.0
[Rank 12] (after 10 iterations) memory (MB) | allocated: 15039.85546875 | max allocated: 38589.77392578125 | reserved: 41722.0 | max reserved: 41722.0[Rank 15] (after 10 iterations) memory (MB) | allocated: 15039.1484375 | max allocated: 38589.13720703125 | reserved: 41722.0 | max reserved: 41722.0

(min, max) time across ranks (ms):
    forward-backward ...............................: (15811.02, 15961.34)
    forward-compute ................................: (2717.52, 6021.27)
    backward-compute ...............................: (3543.08, 7489.08)
    batch-generator ................................: (345.47, 379.34)
    forward-recv ...................................: (330.14, 895.85)
    forward-send ...................................: (6.41, 556.83)
    backward-recv ..................................: (67.09, 318.36)
    backward-send ..................................: (8.29, 38.60)
    forward-send-backward-recv .....................: (7062.15, 8144.38)
    backward-send-forward-recv .....................: (1316.56, 1770.15)
    layernorm-grads-all-reduce .....................: (0.98, 1.14)
    embedding-grads-all-reduce .....................: (0.02, 5.84)
    all-grads-sync .................................: (60.73, 69.08)
    params-all-gather ..............................: (2.64, 3.09)
    optimizer-copy-to-main-grad ....................: (0.69, 0.77)
    optimizer-clip-main-grad .......................: (7.46, 7.93)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.04, 10.96)
    optimizer-copy-main-to-model-params ............: (2.86, 3.34)
    optimizer ......................................: (26.17, 26.62)
 iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 13244.2 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.082763E+01 | loss scale: 1.0 | grad norm: 5.940 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (13138.15, 13208.72)
    forward-compute ................................: (2157.18, 5414.83)
    backward-compute ...............................: (3012.63, 6944.35)
    batch-generator ................................: (102.24, 120.10)
    forward-recv ...................................: (31.04, 54.04)
    forward-send ...................................: (0.37, 14.83)
    backward-recv ..................................: (62.95, 197.44)
    backward-send ..................................: (0.57, 14.06)
    forward-send-backward-recv .....................: (6898.27, 7272.24)
    backward-send-forward-recv .....................: (697.84, 923.76)
    layernorm-grads-all-reduce .....................: (0.93, 1.02)
    embedding-grads-all-reduce .....................: (0.02, 5.74)
    all-grads-sync .................................: (2.12, 2.51)
    params-all-gather ..............................: (2.63, 3.08)
    optimizer-copy-to-main-grad ....................: (0.66, 0.74)
    optimizer-clip-main-grad .......................: (4.60, 5.06)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.78, 10.44)
    optimizer-copy-main-to-model-params ............: (2.86, 3.33)
    optimizer ......................................: (22.72, 23.18)
 iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 14163.9 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.076530E+01 | loss scale: 1.0 | grad norm: 2.473 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (14063.22, 14128.42)
    forward-compute ................................: (2374.29, 5625.87)
    backward-compute ...............................: (3412.23, 7344.57)
    batch-generator ................................: (103.79, 121.93)
    forward-recv ...................................: (33.25, 62.22)
    forward-send ...................................: (0.42, 22.76)
    backward-recv ..................................: (61.13, 172.51)
    backward-send ..................................: (4.80, 19.94)
    forward-send-backward-recv .....................: (6661.66, 7529.94)
    backward-send-forward-recv .....................: (993.20, 1462.44)
    layernorm-grads-all-reduce .....................: (0.93, 1.01)
    embedding-grads-all-reduce .....................: (0.02, 5.77)
    all-grads-sync .................................: (2.12, 2.51)
    params-all-gather ..............................: (2.64, 3.16)
    optimizer-copy-to-main-grad ....................: (0.67, 0.74)
    optimizer-clip-main-grad .......................: (4.59, 5.06)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (8.79, 10.45)
    optimizer-copy-main-to-model-params ............: (2.86, 3.33)
    optimizer ......................................: (22.71, 23.24)
rank 8: {'packing_seq_len': {'128': 0, '256': 2, '512': 87, '1024': 757, '2048': 1555, '4096': 1162, '8192': 359, '16384': 133, '32768': 39, '>32k': 2}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}

10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 7b model, gpu_num=16, seq_len = 16384, DP=1, MP=8, PP=2, GBS=512, MBS=8
10.64.24.49: use sequence-packing
10.64.24.49: use sequence parallel
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 7b model, gpu_num=16, seq_len = 16384, DP=1, MP=8, PP=2, GBS=512, MBS=8
10.64.24.50: use sequence-packing
10.64.24.50: use sequence parallel
10.64.24.50: [2024-03-11 17:42:33,393] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 17:42:33,393] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 17:42:33,393] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 17:42:33,393] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 17:42:34,424] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 17:42:34,424] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 17:42:34,424] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 17:42:34,424] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 1, context-parallel size: 1 tensor-model-parallel size: 8, pipeline-model-parallel size: 2 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 1
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 32
10.64.24.49:   encoder_seq_length .............................. 16384
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 16384
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 4096
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 16384
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 8
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 32
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 32
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 2
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 16384
10.64.24.49:   sequence_packing ................................ True
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 8
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 2
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ True
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 64
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 943 dummy tokens (new size: 51200)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: > initialized tensor model parallel with size 8
10.64.24.49: > initialized pipeline model parallel with size 2
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.061 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.186 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 11.302
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 17:43:03 
10.64.24.49: building GPT model ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (6, 0): 496427008
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (7, 1): 429326336
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (7, 0): 496427008
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (6, 1): 429326336
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 429326336
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (4, 1): 429326336
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (5, 1): 429326336
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (4, 0): 496427008
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (5, 0): 496427008
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 429326336
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 496427008
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 496427008
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (429326336 elements):
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 429326336
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 496427008
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 429326336
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 496427008
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (496427008 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 17:43:04 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache end, time cost:  4.839 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Loading exists cache end, time cost:  4.855 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Cutting or padding data end, time cost:  18.409 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  18.658 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 17:43:28 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (1366.04, 1580.70)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 23960.19)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 17:43:28 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 17:44:34 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             317W / 700W |  72854MiB / 81559MiB |      6%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             268W / 700W |  73522MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             277W / 700W |  72924MiB / 81559MiB |     96%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             282W / 700W |  72680MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             283W / 700W |  73554MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             313W / 700W |  73290MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             247W / 700W |  73224MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             270W / 700W |  72470MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 9817.1 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.086747E+01 | loss scale: 1.0 | grad norm: 15.255 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: [Rank 9] (after 10 iterations) memory (MB) | allocated: 7740.3779296875 | max allocated: 27541.0185546875 | reserved: 38504.0 | max reserved: 38504.0
10.64.24.49: [Rank 6] (after 10 iterations) memory (MB) | allocated: 8883.994140625 | max allocated: 22252.13330078125 | reserved: 23418.0 | max reserved: 77730.0
10.64.24.50: [Rank 13] (after 10 iterations) memory (MB) | allocated: 7741.2294921875 | max allocated: 27541.7685546875 | reserved: 38770.0 | max reserved: 38770.0[Rank 11] (after 10 iterations) memory (MB) | allocated: 7740.4951171875 | max allocated: 27541.2548828125 | reserved: 38262.0 | max reserved: 38262.0[Rank 12] (after 10 iterations) memory (MB) | allocated: 7740.3779296875 | max allocated: 27541.2998046875 | reserved: 38250.0 | max reserved: 38250.0
10.64.24.49: [Rank 4] (after 10 iterations) memory (MB) | allocated: 8883.994140625 | max allocated: 22254.48291015625 | reserved: 24656.0 | max reserved: 77762.0
10.64.24.49: [Rank 3] (after 10 iterations) memory (MB) | allocated: 8883.994140625 | max allocated: 22252.70068359375 | reserved: 20824.0 | max reserved: 77484.0[Rank 7] (after 10 iterations) memory (MB) | allocated: 8883.994140625 | max allocated: 22252.96533203125 | reserved: 21518.0 | max reserved: 77812.0[Rank 5] (after 10 iterations) memory (MB) | allocated: 8883.994140625 | max allocated: 22250.07763671875 | reserved: 24642.0 | max reserved: 77498.0
10.64.24.49: 
10.64.24.49: 
10.64.24.50: [Rank 14] (after 10 iterations) memory (MB) | allocated: 7740.4716796875 | max allocated: 27540.9140625 | reserved: 39812.0 | max reserved: 39812.0
10.64.24.49: [Rank 2] (after 10 iterations) memory (MB) | allocated: 8883.994140625 | max allocated: 22257.19287109375 | reserved: 22738.0 | max reserved: 77728.0
10.64.24.50: 
10.64.24.50: 
10.64.24.50: [Rank 10] (after 10 iterations) memory (MB) | allocated: 7740.3779296875 | max allocated: 27541.5966796875 | reserved: 38504.0 | max reserved: 38504.0
10.64.24.50: [Rank 15] (after 10 iterations) memory (MB) | allocated: 7740.4716796875 | max allocated: 27540.6953125 | reserved: 37626.0 | max reserved: 37626.0
10.64.24.50: [Rank 8] (after 10 iterations) memory (MB) | allocated: 7740.8388671875 | max allocated: 27541.3017578125 | reserved: 39146.0 | max reserved: 39146.0
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 8883.994140625 | max allocated: 22253.63818359375 | reserved: 24312.0 | max reserved: 77730.0
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 8883.994140625 | max allocated: 22257.56396484375 | reserved: 22400.0 | max reserved: 77706.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (9557.70, 9618.79)
10.64.24.50:     forward-compute ................................: (2629.31, 4173.47)
10.64.24.50:     backward-compute ...............................: (2518.55, 4190.70)
10.64.24.50:     batch-generator ................................: (721.14, 749.31)
10.64.24.50:     forward-recv ...................................: (741.86, 742.33)
10.64.24.50:     forward-send ...................................: (2.36, 2.88)
10.64.24.50:     backward-recv ..................................: (103.32, 107.39)
10.64.24.50:     backward-send ..................................: (0.74, 0.83)
10.64.24.50:     forward-send-backward-recv .....................: (4049.58, 4052.99)
10.64.24.50:     backward-send-forward-recv .....................: (437.38, 441.66)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.39, 1.52)
10.64.24.50:     embedding-grads-all-reduce .....................: (2.44, 2.52)
10.64.24.50:     all-grads-sync .................................: (43.73, 80.03)
10.64.24.50:     params-all-gather ..............................: (2.68, 2.98)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (1.07, 1.17)
10.64.24.50:     optimizer-clip-main-grad .......................: (5.86, 6.09)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (5.23, 6.07)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.14, 2.37)
10.64.24.50:     optimizer ......................................: (18.97, 19.27)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 9212.1 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.085153E+01 | loss scale: 1.0 | grad norm: 53.503 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (9158.23, 9188.09)
10.64.24.50:     forward-compute ................................: (2613.49, 4165.91)
10.64.24.50:     backward-compute ...............................: (2099.33, 4844.56)
10.64.24.50:     batch-generator ................................: (64.30, 73.86)
10.64.24.50:     forward-recv ...................................: (31.68, 31.70)
10.64.24.50:     forward-send ...................................: (0.36, 0.37)
10.64.24.50:     backward-recv ..................................: (64.45, 68.03)
10.64.24.50:     backward-send ..................................: (0.64, 0.69)
10.64.24.50:     forward-send-backward-recv .....................: (4154.02, 4156.96)
10.64.24.50:     backward-send-forward-recv .....................: (1120.66, 1124.12)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.34, 1.40)
10.64.24.50:     embedding-grads-all-reduce .....................: (2.44, 2.51)
10.64.24.50:     all-grads-sync .................................: (1.16, 1.36)
10.64.24.50:     params-all-gather ..............................: (2.65, 2.95)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (1.05, 1.18)
10.64.24.50:     optimizer-clip-main-grad .......................: (3.15, 3.36)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (5.01, 5.79)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.14, 2.37)
10.64.24.50:     optimizer ......................................: (15.94, 16.25)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 8660.7 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.078852E+01 | loss scale: 1.0 | grad norm: 3.638 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (8605.91, 8637.01)
10.64.24.50:     forward-compute ................................: (2773.20, 3317.62)
10.64.24.50:     backward-compute ...............................: (2399.26, 4085.44)
10.64.24.50:     batch-generator ................................: (64.91, 74.73)
10.64.24.50:     forward-recv ...................................: (33.64, 33.68)
10.64.24.50:     forward-send ...................................: (0.38, 0.41)
10.64.24.50:     backward-recv ..................................: (60.90, 63.79)
10.64.24.50:     backward-send ..................................: (6.76, 9.01)
10.64.24.50:     forward-send-backward-recv .....................: (3103.40, 3106.61)
10.64.24.50:     backward-send-forward-recv .....................: (1142.14, 1146.28)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.33, 1.40)
10.64.24.50:     embedding-grads-all-reduce .....................: (2.42, 2.51)
10.64.24.50:     all-grads-sync .................................: (1.16, 1.35)
10.64.24.50:     params-all-gather ..............................: (2.64, 2.96)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (1.04, 1.11)
10.64.24.50:     optimizer-clip-main-grad .......................: (3.10, 3.31)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.02)
10.64.24.50:     optimizer-inner-step ...........................: (4.99, 5.75)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.14, 2.37)
10.64.24.50:     optimizer ......................................: (15.76, 16.09)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 17:48:13 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             326W / 700W |  49350MiB / 81559MiB |     89%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             366W / 700W |  48640MiB / 81559MiB |     96%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             257W / 700W |  48400MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   35C    P0             224W / 700W |  50540MiB / 81559MiB |     96%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             250W / 700W |  48672MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             298W / 700W |  48656MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             316W / 700W |  48994MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             225W / 700W |  49176MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 17:48:21 
10.64.24.49: rank 6: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 7, '2048': 198, '4096': 885, '8192': 696, '16384': 203, '32768': 55, '>32k': 4}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}rank 3: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 7, '2048': 198, '4096': 885, '8192': 696, '16384': 203, '32768': 55, '>32k': 4}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}rank 7: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 7, '2048': 198, '4096': 885, '8192': 696, '16384': 203, '32768': 55, '>32k': 4}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}rank 5: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 7, '2048': 198, '4096': 885, '8192': 696, '16384': 203, '32768': 55, '>32k': 4}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 7, '2048': 198, '4096': 885, '8192': 696, '16384': 203, '32768': 55, '>32k': 4}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: rank 2: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 7, '2048': 198, '4096': 885, '8192': 696, '16384': 203, '32768': 55, '>32k': 4}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: rank 1: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 7, '2048': 198, '4096': 885, '8192': 696, '16384': 203, '32768': 55, '>32k': 4}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: 
10.64.24.49: 
10.64.24.49: rank 0: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 7, '2048': 198, '4096': 885, '8192': 696, '16384': 203, '32768': 55, '>32k': 4}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 7, '2048': 198, '4096': 885, '8192': 696, '16384': 203, '32768': 55, '>32k': 4}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen16384_gbs512_mbs8_dp1_tp8_pp2_pack_sp_node1.log...
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen16384_gbs512_mbs8_dp1_tp8_pp2_pack_sp_node0.log...
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 7b model, gpu_num=16, seq_len = 16384, DP=2, MP=4, PP=2, GBS=512, MBS=4
10.64.24.50: use sequence-packing
10.64.24.50: use sequence parallel
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 7b model, gpu_num=16, seq_len = 16384, DP=2, MP=4, PP=2, GBS=512, MBS=4
10.64.24.49: use sequence-packing
10.64.24.49: use sequence parallel
10.64.24.50: [2024-03-11 17:48:37,055] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 17:48:37,055] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 17:48:37,055] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 17:48:37,055] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 17:48:38,034] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 17:48:38,034] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 17:48:38,034] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 17:48:38,034] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 4, pipeline-model-parallel size: 2 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 2
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 32
10.64.24.49:   encoder_seq_length .............................. 16384
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 16384
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 4096
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 16384
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 4
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 32
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 32
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 2
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 16384
10.64.24.49:   sequence_packing ................................ True
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 4
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 2
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ True
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 64
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 431 dummy tokens (new size: 50688)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: > initialized tensor model parallel with size 4
10.64.24.49: > initialized pipeline model parallel with size 2
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.060 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 7.763 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 11.347
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 17:49:06 
10.64.24.49: building GPT model ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 924827648
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 924827648
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 857726976
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 857726976
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 924827648
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 857726976
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 857726976
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 924827648
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (857726976 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (924827648 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 17:49:08 
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache end, time cost:  4.859 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Loading exists cache end, time cost:  4.863 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  4.960 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  6.553 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Cutting or padding data end, time cost:  18.524 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  18.647 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  18.703 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  18.771 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 17:49:34 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (1349.85, 1474.16)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 25871.34)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 17:49:34 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 17:50:40 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             258W / 700W |  50606MiB / 81559MiB |      5%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             290W / 700W |  50474MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             247W / 700W |  50528MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   35C    P0             245W / 700W |  50760MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             316W / 700W |  44660MiB / 81559MiB |     95%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             348W / 700W |  44790MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             325W / 700W |  44578MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             262W / 700W |  44950MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 10017.2 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.086936E+01 | loss scale: 1.0 | grad norm: 7.734 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: [Rank 11] (after 10 iterations) memory (MB) | allocated: 10105.1201171875 | max allocated: 33814.74658203125 | reserved: 38548.0 | max reserved: 38548.0
10.64.24.50: [Rank 9] (after 10 iterations) memory (MB) | allocated: 10104.3466796875 | max allocated: 33815.84814453125 | reserved: 38546.0 | max reserved: 38546.0
10.64.24.50: [Rank 8] (after 10 iterations) memory (MB) | allocated: 10104.3466796875 | max allocated: 33814.35205078125 | reserved: 38548.0 | max reserved: 38548.0
10.64.24.50: [Rank 10] (after 10 iterations) memory (MB) | allocated: 10104.3466796875 | max allocated: 33815.27392578125 | reserved: 38546.0 | max reserved: 38546.0
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 10859.908203125 | max allocated: 27276.427734375 | reserved: 46752.0 | max reserved: 46752.0
10.64.24.49: [Rank 3] (after 10 iterations) memory (MB) | allocated: 10859.908203125 | max allocated: 27276.595703125 | reserved: 47278.0 | max reserved: 47278.0
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 10859.908203125 | max allocated: 27275.806640625 | reserved: 46932.0 | max reserved: 46932.0
10.64.24.49: [Rank 2] (after 10 iterations) memory (MB) | allocated: 10859.908203125 | max allocated: 27275.2109375 | reserved: 46806.0 | max reserved: 46806.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (9554.87, 9629.27)
10.64.24.50:     forward-compute ................................: (2178.71, 4960.78)
10.64.24.50:     backward-compute ...............................: (2229.71, 3772.22)
10.64.24.50:     batch-generator ................................: (456.16, 509.56)
10.64.24.50:     forward-recv ...................................: (523.86, 536.27)
10.64.24.50:     forward-send ...................................: (4.50, 7.09)
10.64.24.50:     backward-recv ..................................: (94.73, 132.00)
10.64.24.50:     backward-send ..................................: (0.71, 0.86)
10.64.24.50:     forward-send-backward-recv .....................: (4780.23, 4911.09)
10.64.24.50:     backward-send-forward-recv .....................: (380.17, 426.71)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.38, 2.37)
10.64.24.50:     embedding-grads-all-reduce .....................: (4.61, 4.70)
10.64.24.50:     all-grads-sync .................................: (225.45, 233.17)
10.64.24.50:     params-all-gather ..............................: (5.93, 6.80)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.53, 0.80)
10.64.24.50:     optimizer-clip-main-grad .......................: (5.78, 5.91)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (5.02, 5.80)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.69, 1.91)
10.64.24.50:     optimizer ......................................: (20.93, 21.80)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 10424.5 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.085146E+01 | loss scale: 1.0 | grad norm: 27.678 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (10349.27, 10377.70)
10.64.24.50:     forward-compute ................................: (2475.39, 5667.45)
10.64.24.50:     backward-compute ...............................: (1967.28, 4326.55)
10.64.24.50:     batch-generator ................................: (55.23, 68.83)
10.64.24.50:     forward-recv ...................................: (21.87, 30.99)
10.64.24.50:     forward-send ...................................: (0.33, 0.42)
10.64.24.50:     backward-recv ..................................: (65.32, 83.25)
10.64.24.50:     backward-send ..................................: (0.64, 3.31)
10.64.24.50:     forward-send-backward-recv .....................: (5442.71, 5722.91)
10.64.24.50:     backward-send-forward-recv .....................: (1222.99, 1403.42)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.31, 1.62)
10.64.24.50:     embedding-grads-all-reduce .....................: (4.59, 5.04)
10.64.24.50:     all-grads-sync .................................: (8.18, 8.91)
10.64.24.50:     params-all-gather ..............................: (5.91, 6.36)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.52, 0.75)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.83, 2.96)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.87, 5.39)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.69, 1.90)
10.64.24.50:     optimizer ......................................: (17.40, 17.86)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 8933.3 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.078600E+01 | loss scale: 1.0 | grad norm: 3.225 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (8852.35, 8894.68)
10.64.24.50:     forward-compute ................................: (2460.10, 4696.32)
10.64.24.50:     backward-compute ...............................: (2122.40, 3720.46)
10.64.24.50:     batch-generator ................................: (55.62, 70.33)
10.64.24.50:     forward-recv ...................................: (27.49, 31.86)
10.64.24.50:     forward-send ...................................: (0.38, 0.48)
10.64.24.50:     backward-recv ..................................: (59.95, 79.96)
10.64.24.50:     backward-send ..................................: (0.62, 11.09)
10.64.24.50:     forward-send-backward-recv .....................: (3836.72, 4047.94)
10.64.24.50:     backward-send-forward-recv .....................: (390.51, 1097.76)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.31, 1.57)
10.64.24.50:     embedding-grads-all-reduce .....................: (4.59, 4.68)
10.64.24.50:     all-grads-sync .................................: (8.09, 8.94)
10.64.24.50:     params-all-gather ..............................: (5.93, 6.36)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.52, 0.73)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.80, 3.36)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.87, 5.36)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.69, 1.90)
10.64.24.50:     optimizer ......................................: (17.88, 18.30)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 17:54:35 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   35C    P0             196W / 700W |  63414MiB / 81559MiB |     48%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             273W / 700W |  63282MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             356W / 700W |  63336MiB / 81559MiB |     93%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   34C    P0             322W / 700W |  63566MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             241W / 700W |  55612MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             238W / 700W |  55742MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             318W / 700W |  55530MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             252W / 700W |  56168MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 17:54:43 
10.64.24.49: rank 1: {'packing_seq_len': {'128': 0, '256': 1, '512': 42, '1024': 374, '2048': 778, '4096': 597, '8192': 171, '16384': 63, '32768': 21, '>32k': 1}, 'real_seq_len': {'128': 1826, '256': 1876, '512': 1860, '1024': 1532, '2048': 678, '4096': 265, '8192': 98, '16384': 57, '32768': 0, '>32k': 0}}rank 3: {'packing_seq_len': {'128': 0, '256': 1, '512': 42, '1024': 374, '2048': 778, '4096': 597, '8192': 171, '16384': 63, '32768': 21, '>32k': 1}, 'real_seq_len': {'128': 1826, '256': 1876, '512': 1860, '1024': 1532, '2048': 678, '4096': 265, '8192': 98, '16384': 57, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 6: {'packing_seq_len': {'128': 0, '256': 1, '512': 45, '1024': 383, '2048': 777, '4096': 565, '8192': 188, '16384': 70, '32768': 18, '>32k': 1}, 'real_seq_len': {'128': 1839, '256': 1911, '512': 1853, '1024': 1480, '2048': 679, '4096': 260, '8192': 116, '16384': 54, '32768': 0, '>32k': 0}}
10.64.24.49: rank 7: {'packing_seq_len': {'128': 0, '256': 1, '512': 45, '1024': 383, '2048': 777, '4096': 565, '8192': 188, '16384': 70, '32768': 18, '>32k': 1}, 'real_seq_len': {'128': 1839, '256': 1911, '512': 1853, '1024': 1480, '2048': 679, '4096': 260, '8192': 116, '16384': 54, '32768': 0, '>32k': 0}}
10.64.24.49: rank 2: {'packing_seq_len': {'128': 0, '256': 1, '512': 42, '1024': 374, '2048': 778, '4096': 597, '8192': 171, '16384': 63, '32768': 21, '>32k': 1}, 'real_seq_len': {'128': 1826, '256': 1876, '512': 1860, '1024': 1532, '2048': 678, '4096': 265, '8192': 98, '16384': 57, '32768': 0, '>32k': 0}}
10.64.24.49: rank 5: {'packing_seq_len': {'128': 0, '256': 1, '512': 45, '1024': 383, '2048': 777, '4096': 565, '8192': 188, '16384': 70, '32768': 18, '>32k': 1}, 'real_seq_len': {'128': 1839, '256': 1911, '512': 1853, '1024': 1480, '2048': 679, '4096': 260, '8192': 116, '16384': 54, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 0, '256': 1, '512': 42, '1024': 374, '2048': 778, '4096': 597, '8192': 171, '16384': 63, '32768': 21, '>32k': 1}, 'real_seq_len': {'128': 1826, '256': 1876, '512': 1860, '1024': 1532, '2048': 678, '4096': 265, '8192': 98, '16384': 57, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 0, '256': 1, '512': 42, '1024': 374, '2048': 778, '4096': 597, '8192': 171, '16384': 63, '32768': 21, '>32k': 1}, 'real_seq_len': {'128': 1826, '256': 1876, '512': 1860, '1024': 1532, '2048': 678, '4096': 265, '8192': 98, '16384': 57, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 0, '256': 1, '512': 45, '1024': 383, '2048': 777, '4096': 565, '8192': 188, '16384': 70, '32768': 18, '>32k': 1}, 'real_seq_len': {'128': 1839, '256': 1911, '512': 1853, '1024': 1480, '2048': 679, '4096': 260, '8192': 116, '16384': 54, '32768': 0, '>32k': 0}}
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen16384_gbs512_mbs4_dp2_tp4_pp2_pack_sp_node0.log...
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen16384_gbs512_mbs4_dp2_tp4_pp2_pack_sp_node1.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 7b model, gpu_num=16, seq_len = 16384, DP=1, MP=4, PP=4, GBS=512, MBS=8
10.64.24.49: use sequence-packing
10.64.24.49: use sequence parallel
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 7b model, gpu_num=16, seq_len = 16384, DP=1, MP=4, PP=4, GBS=512, MBS=8
10.64.24.50: use sequence-packing
10.64.24.50: use sequence parallel
10.64.24.50: [2024-03-11 17:55:00,911] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 17:55:00,911] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 17:55:00,911] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 17:55:00,911] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 17:55:01,816] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 17:55:01,816] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 17:55:01,816] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 17:55:01,816] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 1, context-parallel size: 1 tensor-model-parallel size: 4, pipeline-model-parallel size: 4 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 1
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 32
10.64.24.49:   encoder_seq_length .............................. 16384
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 16384
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 4096
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 16384
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 8
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 32
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 32
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 4
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 16384
10.64.24.49:   sequence_packing ................................ True
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 4
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 4
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ True
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 64
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 431 dummy tokens (new size: 50688)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: > initialized tensor model parallel with size 4
10.64.24.49: > initialized pipeline model parallel with size 4
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.062 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.077 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 11.593
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 17:55:31 
10.64.24.49: building GPT model ...
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 402907136
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (3, 2): 402907136
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (2, 2): 402907136
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 402907136
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (402907136 elements):
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 402907136
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 402907136
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 402907136
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 402907136
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (402907136 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (3, 3): 454819840
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 521920512
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 454819840
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 521920512
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 521920512
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 521920512
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 454819840
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (2, 3): 454819840
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (454819840 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (521920512 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 17:55:32 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache end, time cost:  4.983 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.087 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.132 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.296 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Cutting or padding data end, time cost:  18.664 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  18.637 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  19.072 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  18.902 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 17:55:57 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (48.79, 1104.92)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 24651.68)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 17:55:57 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 17:57:09 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   35C    P0             180W / 700W |  53938MiB / 81559MiB |     95%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             232W / 700W |  54050MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             249W / 700W |  53864MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   34C    P0             285W / 700W |  53858MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             197W / 700W |  48838MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             236W / 700W |  49190MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             283W / 700W |  49006MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             210W / 700W |  48988MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 11195.2 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.087271E+01 | loss scale: 1.0 | grad norm: 7.915 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: [Rank 11] (after 10 iterations) memory (MB) | allocated: 7295.736328125 | max allocated: 20753.521484375 | reserved: 50292.0 | max reserved: 50292.0[Rank 9] (after 10 iterations) memory (MB) | allocated: 7296.455078125 | max allocated: 20754.23828125 | reserved: 50528.0 | max reserved: 50528.0[Rank 10] (after 10 iterations) memory (MB) | allocated: 7295.736328125 | max allocated: 20754.984375 | reserved: 50384.0 | max reserved: 50384.0
10.64.24.50: 
10.64.24.50: 
10.64.24.50: [Rank 8] (after 10 iterations) memory (MB) | allocated: 7296.474609375 | max allocated: 20754.59765625 | reserved: 50362.0 | max reserved: 50362.0
10.64.24.50: [Rank 13] (after 10 iterations) memory (MB) | allocated: 8188.8779296875 | max allocated: 35864.3349609375 | reserved: 43092.0 | max reserved: 43092.0
10.64.24.50: [Rank 14] (after 10 iterations) memory (MB) | allocated: 8189.0966796875 | max allocated: 35865.2568359375 | reserved: 43092.0 | max reserved: 43092.0
10.64.24.50: [Rank 15] (after 10 iterations) memory (MB) | allocated: 8188.8779296875 | max allocated: 35864.3349609375 | reserved: 43092.0 | max reserved: 43092.0
10.64.24.50: [Rank 12] (after 10 iterations) memory (MB) | allocated: 8188.8779296875 | max allocated: 35864.3349609375 | reserved: 43092.0 | max reserved: 43092.0
10.64.24.49: [Rank 6] (after 10 iterations) memory (MB) | allocated: 7295.751953125 | max allocated: 23803.0751953125 | reserved: 53604.0 | max reserved: 53604.0[Rank 7] (after 10 iterations) memory (MB) | allocated: 7295.751953125 | max allocated: 23804.2666015625 | reserved: 53826.0 | max reserved: 53826.0[Rank 1] (after 10 iterations) memory (MB) | allocated: 9321.486328125 | max allocated: 27494.4677734375 | reserved: 57990.0 | max reserved: 57990.0
10.64.24.49: 
10.64.24.49: 
10.64.24.49: [Rank 3] (after 10 iterations) memory (MB) | allocated: 9321.486328125 | max allocated: 27495.4794921875 | reserved: 58336.0 | max reserved: 58336.0[Rank 5] (after 10 iterations) memory (MB) | allocated: 7295.751953125 | max allocated: 23809.0771484375 | reserved: 53788.0 | max reserved: 53788.0
10.64.24.49: 
10.64.24.49: [Rank 4] (after 10 iterations) memory (MB) | allocated: 7295.751953125 | max allocated: 23801.2646484375 | reserved: 53484.0 | max reserved: 53484.0
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 9321.486328125 | max allocated: 27498.2177734375 | reserved: 57926.0 | max reserved: 57926.0
10.64.24.49: [Rank 2] (after 10 iterations) memory (MB) | allocated: 9321.486328125 | max allocated: 27499.8017578125 | reserved: 57804.0 | max reserved: 57804.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (10882.06, 11012.24)
10.64.24.50:     forward-compute ................................: (1497.60, 4461.03)
10.64.24.50:     backward-compute ...............................: (1749.79, 5284.24)
10.64.24.50:     batch-generator ................................: (298.05, 327.65)
10.64.24.50:     forward-recv ...................................: (324.33, 890.73)
10.64.24.50:     forward-send ...................................: (7.54, 553.13)
10.64.24.50:     backward-recv ..................................: (141.68, 395.00)
10.64.24.50:     backward-send ..................................: (0.77, 9.80)
10.64.24.50:     forward-send-backward-recv .....................: (6283.27, 6395.15)
10.64.24.50:     backward-send-forward-recv .....................: (230.94, 480.61)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.84, 0.94)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 4.72)
10.64.24.50:     all-grads-sync .................................: (56.90, 64.52)
10.64.24.50:     params-all-gather ..............................: (1.62, 2.06)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.54, 0.66)
10.64.24.50:     optimizer-clip-main-grad .......................: (5.54, 5.91)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.75, 6.30)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.63, 2.01)
10.64.24.50:     optimizer ......................................: (17.04, 17.48)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 9196.6 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.085471E+01 | loss scale: 1.0 | grad norm: 30.047 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (9106.42, 9173.40)
10.64.24.50:     forward-compute ................................: (1037.64, 3974.89)
10.64.24.50:     backward-compute ...............................: (1450.63, 4981.92)
10.64.24.50:     batch-generator ................................: (57.64, 70.86)
10.64.24.50:     forward-recv ...................................: (26.25, 58.67)
10.64.24.50:     forward-send ...................................: (0.45, 18.14)
10.64.24.50:     backward-recv ..................................: (104.09, 353.12)
10.64.24.50:     backward-send ..................................: (0.62, 16.66)
10.64.24.50:     forward-send-backward-recv .....................: (5902.08, 6222.26)
10.64.24.50:     backward-send-forward-recv .....................: (75.61, 256.25)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.78, 0.85)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 4.65)
10.64.24.50:     all-grads-sync .................................: (1.10, 1.42)
10.64.24.50:     params-all-gather ..............................: (1.63, 2.04)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.53, 0.60)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.61, 2.97)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.60, 5.94)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.63, 2.00)
10.64.24.50:     optimizer ......................................: (13.63, 14.05)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 9658.0 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.078844E+01 | loss scale: 1.0 | grad norm: 4.981 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (9546.66, 9634.62)
10.64.24.50:     forward-compute ................................: (1187.00, 4131.05)
10.64.24.50:     backward-compute ...............................: (1688.68, 5211.56)
10.64.24.50:     batch-generator ................................: (57.41, 72.02)
10.64.24.50:     forward-recv ...................................: (28.98, 69.85)
10.64.24.50:     forward-send ...................................: (0.52, 20.17)
10.64.24.50:     backward-recv ..................................: (102.01, 351.83)
10.64.24.50:     backward-send ..................................: (0.59, 30.14)
10.64.24.50:     forward-send-backward-recv .....................: (5918.76, 6206.03)
10.64.24.50:     backward-send-forward-recv .....................: (118.73, 339.80)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.78, 0.93)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 4.68)
10.64.24.50:     all-grads-sync .................................: (1.10, 1.42)
10.64.24.50:     params-all-gather ..............................: (1.63, 2.04)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.53, 0.60)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.61, 2.97)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.61, 5.91)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.63, 2.00)
10.64.24.50:     optimizer ......................................: (13.61, 14.02)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:01:07 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   35C    P0             205W / 700W |  61112MiB / 81559MiB |     51%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             289W / 700W |  61224MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             202W / 700W |  61038MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   34C    P0             181W / 700W |  61330MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   35C    P0             202W / 700W |  56586MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             215W / 700W |  56938MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             189W / 700W |  56754MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             219W / 700W |  56736MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 18:01:17 
10.64.24.49: rank 5: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 7, '2048': 198, '4096': 885, '8192': 696, '16384': 203, '32768': 55, '>32k': 4}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: rank 7: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 7, '2048': 198, '4096': 885, '8192': 696, '16384': 203, '32768': 55, '>32k': 4}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}rank 3: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 7, '2048': 198, '4096': 885, '8192': 696, '16384': 203, '32768': 55, '>32k': 4}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: rank 6: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 7, '2048': 198, '4096': 885, '8192': 696, '16384': 203, '32768': 55, '>32k': 4}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 1: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 7, '2048': 198, '4096': 885, '8192': 696, '16384': 203, '32768': 55, '>32k': 4}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: rank 2: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 7, '2048': 198, '4096': 885, '8192': 696, '16384': 203, '32768': 55, '>32k': 4}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 7, '2048': 198, '4096': 885, '8192': 696, '16384': 203, '32768': 55, '>32k': 4}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 7, '2048': 198, '4096': 885, '8192': 696, '16384': 203, '32768': 55, '>32k': 4}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 7, '2048': 198, '4096': 885, '8192': 696, '16384': 203, '32768': 55, '>32k': 4}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen16384_gbs512_mbs8_dp1_tp4_pp4_pack_sp_node0.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 7b model, gpu_num=16, seq_len = 16384, DP=2, MP=2, PP=2, GBS=512, MBS=4
10.64.24.49: DP must equal to 4!
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen16384_gbs512_mbs8_dp1_tp4_pp4_pack_sp_node1.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 7b model, gpu_num=16, seq_len = 8192, DP=2, MP=4, PP=2, GBS=512, MBS=8
10.64.24.49: use sequence-packing
10.64.24.49: use sequence parallel
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 7b model, gpu_num=16, seq_len = 16384, DP=2, MP=2, PP=2, GBS=512, MBS=4
10.64.24.50: DP must equal to 4!
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 7b model, gpu_num=16, seq_len = 8192, DP=2, MP=4, PP=2, GBS=512, MBS=8
10.64.24.50: use sequence-packing
10.64.24.50: use sequence parallel
10.64.24.49: [2024-03-11 18:01:35,602] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 18:01:35,602] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 18:01:35,602] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 18:01:35,602] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 18:01:34,736] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 18:01:34,736] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 18:01:34,736] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 18:01:34,736] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 4, pipeline-model-parallel size: 2 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 2
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 32
10.64.24.49:   encoder_seq_length .............................. 8192
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 16384
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 4096
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 8192
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 8
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 32
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 32
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 2
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 8192
10.64.24.49:   sequence_packing ................................ True
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 4
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 2
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ True
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 32
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 431 dummy tokens (new size: 50688)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: > initialized tensor model parallel with size 4
10.64.24.49: > initialized pipeline model parallel with size 2
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.059 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.040 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 10.411
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 18:02:03 
10.64.24.49: building GPT model ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 857726976
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 891273216
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 857726976
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 857726976
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 891273216
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 891273216
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 857726976
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 891273216
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (857726976 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (891273216 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 18:02:04 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache end, time cost:  4.926 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  4.953 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  4.983 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.027 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Cutting or padding data end, time cost:  10.459 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.466 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.608 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.545 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 18:02:20 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (1026.87, 1282.47)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 15931.54)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 18:02:20 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:03:07 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             292W / 700W |  52800MiB / 81559MiB |     37%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             311W / 700W |  52782MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             311W / 700W |  53346MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             308W / 700W |  52682MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             337W / 700W |  35766MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             341W / 700W |  35970MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             389W / 700W |  35900MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             423W / 700W |  35656MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 6888.3 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.090105E+01 | loss scale: 1.0 | grad norm: 14.270 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 10442.814453125 | max allocated: 30105.484375 | reserved: 49064.0 | max reserved: 49064.0
10.64.24.49: [Rank 2] (after 10 iterations) memory (MB) | allocated: 10442.814453125 | max allocated: 30103.51171875 | reserved: 49628.0 | max reserved: 49628.0
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 10442.814453125 | max allocated: 30100.90234375 | reserved: 49130.0 | max reserved: 49130.0
10.64.24.50: [Rank 10] (after 10 iterations) memory (MB) | allocated: 10070.7841796875 | max allocated: 31405.79052734375 | reserved: 34224.0 | max reserved: 34224.0
10.64.24.50: [Rank 11] (after 10 iterations) memory (MB) | allocated: 10070.7841796875 | max allocated: 31406.68115234375 | reserved: 34224.0 | max reserved: 34224.0
10.64.24.50: [Rank 8] (after 10 iterations) memory (MB) | allocated: 10070.7841796875 | max allocated: 31406.77099609375 | reserved: 34224.0 | max reserved: 34224.0
10.64.24.50: [Rank 9] (after 10 iterations) memory (MB) | allocated: 10071.2998046875 | max allocated: 31406.30615234375 | reserved: 34224.0 | max reserved: 34224.0
10.64.24.49: [Rank 3] (after 10 iterations) memory (MB) | allocated: 10442.814453125 | max allocated: 30103.5390625 | reserved: 49204.0 | max reserved: 49204.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (6429.65, 6515.28)
10.64.24.50:     forward-compute ................................: (1555.05, 3310.15)
10.64.24.50:     backward-compute ...............................: (1623.72, 2504.38)
10.64.24.50:     batch-generator ................................: (431.21, 453.65)
10.64.24.50:     forward-recv ...................................: (508.43, 517.15)
10.64.24.50:     forward-send ...................................: (2.60, 5.63)
10.64.24.50:     backward-recv ..................................: (132.03, 146.47)
10.64.24.50:     backward-send ..................................: (0.89, 10.03)
10.64.24.50:     forward-send-backward-recv .....................: (2810.25, 3009.43)
10.64.24.50:     backward-send-forward-recv .....................: (211.20, 228.06)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.42, 1.59)
10.64.24.50:     embedding-grads-all-reduce .....................: (4.61, 4.71)
10.64.24.50:     all-grads-sync .................................: (215.64, 232.66)
10.64.24.50:     params-all-gather ..............................: (5.92, 6.18)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.53, 0.69)
10.64.24.50:     optimizer-clip-main-grad .......................: (6.45, 6.52)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (5.04, 5.45)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.69, 1.83)
10.64.24.50:     optimizer ......................................: (21.09, 21.34)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 4714.7 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.086933E+01 | loss scale: 1.0 | grad norm: 51.941 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (4609.43, 4660.85)
10.64.24.50:     forward-compute ................................: (980.54, 2281.43)
10.64.24.50:     backward-compute ...............................: (1412.84, 2211.92)
10.64.24.50:     batch-generator ................................: (26.31, 36.72)
10.64.24.50:     forward-recv ...................................: (32.50, 35.06)
10.64.24.50:     forward-send ...................................: (0.45, 0.50)
10.64.24.50:     backward-recv ..................................: (64.49, 109.57)
10.64.24.50:     backward-send ..................................: (0.73, 10.40)
10.64.24.50:     forward-send-backward-recv .....................: (1996.38, 2031.25)
10.64.24.50:     backward-send-forward-recv .....................: (110.02, 191.33)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.33, 1.53)
10.64.24.50:     embedding-grads-all-reduce .....................: (4.54, 4.66)
10.64.24.50:     all-grads-sync .................................: (8.16, 8.66)
10.64.24.50:     params-all-gather ..............................: (5.92, 6.17)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.52, 0.68)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.61, 2.68)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.88, 5.26)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.69, 1.83)
10.64.24.50:     optimizer ......................................: (17.06, 17.31)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 6083.1 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.074354E+01 | loss scale: 1.0 | grad norm: 4.647 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (5954.92, 6040.81)
10.64.24.50:     forward-compute ................................: (1556.39, 2868.66)
10.64.24.50:     backward-compute ...............................: (1624.47, 2392.40)
10.64.24.50:     batch-generator ................................: (27.63, 37.49)
10.64.24.50:     forward-recv ...................................: (33.39, 560.02)
10.64.24.50:     forward-send ...................................: (0.45, 0.58)
10.64.24.50:     backward-recv ..................................: (62.05, 107.93)
10.64.24.50:     backward-send ..................................: (2.07, 12.16)
10.64.24.50:     forward-send-backward-recv .....................: (2524.45, 2600.84)
10.64.24.50:     backward-send-forward-recv .....................: (140.57, 692.99)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.33, 1.60)
10.64.24.50:     embedding-grads-all-reduce .....................: (4.58, 4.70)
10.64.24.50:     all-grads-sync .................................: (8.14, 8.63)
10.64.24.50:     params-all-gather ..............................: (5.89, 6.18)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.53, 0.70)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.64, 2.72)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.88, 5.14)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.69, 1.83)
10.64.24.50:     optimizer ......................................: (16.89, 17.18)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:05:22 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             340W / 700W |  62648MiB / 81559MiB |     21%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             404W / 700W |  62844MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             368W / 700W |  63194MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             349W / 700W |  62530MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             276W / 700W |  47892MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             273W / 700W |  47954MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             290W / 700W |  47456MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             323W / 700W |  47640MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: rank 1: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 335, '16384': 108, '32768': 15, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 258, '8192': 149, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: [after training is done] datetime: 2024-03-11 18:05:27 rank 6: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 5, '2048': 97, '4096': 422, '8192': 361, '16384': 129, '32768': 10, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1868, '512': 1862, '1024': 1501, '2048': 703, '4096': 267, '8192': 176, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 7: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 5, '2048': 97, '4096': 422, '8192': 361, '16384': 129, '32768': 10, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1868, '512': 1862, '1024': 1501, '2048': 703, '4096': 267, '8192': 176, '16384': 0, '32768': 0, '>32k': 0}}rank 5: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 5, '2048': 97, '4096': 422, '8192': 361, '16384': 129, '32768': 10, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1868, '512': 1862, '1024': 1501, '2048': 703, '4096': 267, '8192': 176, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 3: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 335, '16384': 108, '32768': 15, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 258, '8192': 149, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 2: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 335, '16384': 108, '32768': 15, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 258, '8192': 149, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 335, '16384': 108, '32768': 15, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 258, '8192': 149, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 5, '2048': 97, '4096': 422, '8192': 361, '16384': 129, '32768': 10, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1868, '512': 1862, '1024': 1501, '2048': 703, '4096': 267, '8192': 176, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 335, '16384': 108, '32768': 15, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 258, '8192': 149, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen8192_gbs512_mbs8_dp2_tp4_pp2_pack_sp_node1.log...
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen8192_gbs512_mbs8_dp2_tp4_pp2_pack_sp_node0.log...
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 7b model, gpu_num=16, seq_len = 8192, DP=4, MP=2, PP=2, GBS=512, MBS=4
10.64.24.50: use sequence-packing
10.64.24.50: use sequence parallel
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 7b model, gpu_num=16, seq_len = 8192, DP=4, MP=2, PP=2, GBS=512, MBS=4
10.64.24.49: use sequence-packing
10.64.24.49: use sequence parallel
10.64.24.50: [2024-03-11 18:05:37,487] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 18:05:37,487] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 18:05:37,487] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 18:05:37,487] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 18:05:38,495] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 18:05:38,495] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 18:05:38,495] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 18:05:38,495] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 4, context-parallel size: 1 tensor-model-parallel size: 2, pipeline-model-parallel size: 2 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 4
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 32
10.64.24.49:   encoder_seq_length .............................. 8192
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 16384
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 4096
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 8192
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 4
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 32
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 32
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 2
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 8192
10.64.24.49:   sequence_packing ................................ True
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 2
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 2
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ True
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 32
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
10.64.24.49: > initializing torch distributed ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: > initialized tensor model parallel with size 2
10.64.24.49: > initialized pipeline model parallel with size 2
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.061 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.290 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 11.151
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 18:06:07 
10.64.24.49: building GPT model ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1714528256
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1748074496
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1714528256
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1748074496
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1714528256 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1748074496 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 18:06:08 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache end, time cost:  4.975 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  4.978 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.038 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.046 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.086 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.090 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.287 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.298 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Cutting or padding data end, time cost:  10.493 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.706 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.796 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.754 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.720 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.814 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.699 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  11.712 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 18:06:26 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (1079.01, 1193.09)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 17435.56)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 18:06:26 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:07:18 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             161W / 700W |  47192MiB / 81559MiB |     79%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             184W / 700W |  47622MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             310W / 700W |  53182MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   35C    P0             284W / 700W |  53360MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             279W / 700W |  40516MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             282W / 700W |  40626MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             318W / 700W |  44866MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             342W / 700W |  44730MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 7818.8 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.089510E+01 | loss scale: 1.0 | grad norm: 7.052 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: [Rank 9] (after 10 iterations) memory (MB) | allocated: 14933.6513671875 | max allocated: 44158.52490234375 | reserved: 48618.0 | max reserved: 48618.0
10.64.24.50: [Rank 8] (after 10 iterations) memory (MB) | allocated: 14933.2607421875 | max allocated: 44158.86474609375 | reserved: 48618.0 | max reserved: 48618.0
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 15214.455078125 | max allocated: 39246.4853515625 | reserved: 43948.0 | max reserved: 43948.0
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 15214.455078125 | max allocated: 39250.4267578125 | reserved: 43518.0 | max reserved: 43518.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (7040.07, 7130.56)
10.64.24.50:     forward-compute ................................: (1221.26, 4295.52)
10.64.24.50:     backward-compute ...............................: (1532.53, 2450.02)
10.64.24.50:     batch-generator ................................: (207.96, 259.53)
10.64.24.50:     forward-recv ...................................: (284.90, 305.67)
10.64.24.50:     forward-send ...................................: (4.08, 6.06)
10.64.24.50:     backward-recv ..................................: (156.05, 190.81)
10.64.24.50:     backward-send ..................................: (0.92, 19.88)
10.64.24.50:     forward-send-backward-recv .....................: (3735.70, 4076.90)
10.64.24.50:     backward-send-forward-recv .....................: (184.77, 331.69)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.46, 1.72)
10.64.24.50:     embedding-grads-all-reduce .....................: (8.84, 8.95)
10.64.24.50:     all-grads-sync .................................: (424.79, 464.72)
10.64.24.50:     params-all-gather ..............................: (12.77, 13.09)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.26, 0.39)
10.64.24.50:     optimizer-clip-main-grad .......................: (6.57, 6.63)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.94, 5.14)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.46, 1.57)
10.64.24.50:     optimizer ......................................: (27.28, 27.56)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 5383.7 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.086971E+01 | loss scale: 1.0 | grad norm: 27.377 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (5224.42, 5290.36)
10.64.24.50:     forward-compute ................................: (862.15, 2976.31)
10.64.24.50:     backward-compute ...............................: (1357.20, 2148.50)
10.64.24.50:     batch-generator ................................: (24.47, 33.34)
10.64.24.50:     forward-recv ...................................: (24.84, 33.68)
10.64.24.50:     forward-send ...................................: (0.40, 0.51)
10.64.24.50:     backward-recv ..................................: (68.09, 137.51)
10.64.24.50:     backward-send ..................................: (0.74, 10.78)
10.64.24.50:     forward-send-backward-recv .....................: (2729.96, 2862.91)
10.64.24.50:     backward-send-forward-recv .....................: (147.28, 209.29)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.32, 1.43)
10.64.24.50:     embedding-grads-all-reduce .....................: (8.80, 9.10)
10.64.24.50:     all-grads-sync .................................: (19.05, 19.69)
10.64.24.50:     params-all-gather ..............................: (12.77, 13.06)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.24, 0.35)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.43, 2.45)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.80, 4.94)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.46, 1.56)
10.64.24.50:     optimizer ......................................: (22.58, 22.88)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 6682.2 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.073809E+01 | loss scale: 1.0 | grad norm: 3.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (6498.39, 6610.77)
10.64.24.50:     forward-compute ................................: (1500.95, 3502.85)
10.64.24.50:     backward-compute ...............................: (1539.04, 2312.14)
10.64.24.50:     batch-generator ................................: (25.38, 34.24)
10.64.24.50:     forward-recv ...................................: (26.31, 35.79)
10.64.24.50:     forward-send ...................................: (0.41, 0.53)
10.64.24.50:     backward-recv ..................................: (89.41, 162.66)
10.64.24.50:     backward-send ..................................: (0.81, 22.97)
10.64.24.50:     forward-send-backward-recv .....................: (2976.83, 3338.31)
10.64.24.50:     backward-send-forward-recv .....................: (716.79, 1038.41)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.34, 1.57)
10.64.24.50:     embedding-grads-all-reduce .....................: (8.84, 8.92)
10.64.24.50:     all-grads-sync .................................: (19.14, 19.80)
10.64.24.50:     params-all-gather ..............................: (12.79, 13.09)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.24, 0.37)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.43, 2.45)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.79, 4.93)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.46, 1.56)
10.64.24.50:     optimizer ......................................: (22.63, 22.92)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:09:50 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             335W / 700W |  50818MiB / 81559MiB |     91%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             348W / 700W |  51248MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             381W / 700W |  53194MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             394W / 700W |  53372MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             320W / 700W |  47348MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             316W / 700W |  47458MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             303W / 700W |  48226MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             299W / 700W |  48364MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 18:09:56 
10.64.24.49: rank 1: {'packing_seq_len': {'128': 0, '256': 1, '512': 26, '1024': 193, '2048': 387, '4096': 298, '8192': 79, '16384': 37, '32768': 3, '>32k': 0}, 'real_seq_len': {'128': 937, '256': 962, '512': 926, '1024': 756, '2048': 317, '4096': 124, '8192': 74, '16384': 0, '32768': 0, '>32k': 0}}rank 3: {'packing_seq_len': {'128': 0, '256': 0, '512': 19, '1024': 193, '2048': 379, '4096': 304, '8192': 86, '16384': 40, '32768': 3, '>32k': 0}, 'real_seq_len': {'128': 913, '256': 957, '512': 925, '1024': 755, '2048': 337, '4096': 134, '8192': 75, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 5: {'packing_seq_len': {'128': 0, '256': 0, '512': 16, '1024': 181, '2048': 391, '4096': 299, '8192': 92, '16384': 44, '32768': 1, '>32k': 0}, 'real_seq_len': {'128': 889, '256': 914, '512': 934, '1024': 776, '2048': 361, '4096': 141, '8192': 81, '16384': 0, '32768': 0, '>32k': 0}}rank 7: {'packing_seq_len': {'128': 0, '256': 1, '512': 26, '1024': 190, '2048': 398, '4096': 261, '8192': 102, '16384': 44, '32768': 2, '>32k': 0}, 'real_seq_len': {'128': 926, '256': 954, '512': 928, '1024': 725, '2048': 342, '4096': 126, '8192': 95, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 2: {'packing_seq_len': {'128': 0, '256': 0, '512': 19, '1024': 193, '2048': 379, '4096': 304, '8192': 86, '16384': 40, '32768': 3, '>32k': 0}, 'real_seq_len': {'128': 913, '256': 957, '512': 925, '1024': 755, '2048': 337, '4096': 134, '8192': 75, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 0, '256': 1, '512': 26, '1024': 193, '2048': 387, '4096': 298, '8192': 79, '16384': 37, '32768': 3, '>32k': 0}, 'real_seq_len': {'128': 937, '256': 962, '512': 926, '1024': 756, '2048': 317, '4096': 124, '8192': 74, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 0, '256': 0, '512': 16, '1024': 181, '2048': 391, '4096': 299, '8192': 92, '16384': 44, '32768': 1, '>32k': 0}, 'real_seq_len': {'128': 889, '256': 914, '512': 934, '1024': 776, '2048': 361, '4096': 141, '8192': 81, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 6: {'packing_seq_len': {'128': 0, '256': 1, '512': 26, '1024': 190, '2048': 398, '4096': 261, '8192': 102, '16384': 44, '32768': 2, '>32k': 0}, 'real_seq_len': {'128': 926, '256': 954, '512': 928, '1024': 725, '2048': 342, '4096': 126, '8192': 95, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 0, '256': 1, '512': 26, '1024': 193, '2048': 387, '4096': 298, '8192': 79, '16384': 37, '32768': 3, '>32k': 0}, 'real_seq_len': {'128': 937, '256': 962, '512': 926, '1024': 756, '2048': 317, '4096': 124, '8192': 74, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen8192_gbs512_mbs4_dp4_tp2_pp2_pack_sp_node1.log...
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 7b model, gpu_num=16, seq_len = 8192, DP=2, MP=2, PP=4, GBS=512, MBS=8
10.64.24.50: use sequence-packing
10.64.24.50: use sequence parallel
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen8192_gbs512_mbs4_dp4_tp2_pp2_pack_sp_node0.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 7b model, gpu_num=16, seq_len = 8192, DP=2, MP=2, PP=4, GBS=512, MBS=8
10.64.24.49: use sequence-packing
10.64.24.49: use sequence parallel
10.64.24.49: [2024-03-11 18:10:07,031] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 18:10:07,031] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 18:10:07,031] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 18:10:07,031] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 18:10:06,407] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 18:10:06,407] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 18:10:06,407] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 18:10:06,407] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 2, pipeline-model-parallel size: 4 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 2
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 32
10.64.24.49:   encoder_seq_length .............................. 8192
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 16384
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 4096
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 8192
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 8
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 32
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 32
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 4
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 8192
10.64.24.49:   sequence_packing ................................ True
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 2
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 4
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ True
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 32
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: > initialized tensor model parallel with size 2
10.64.24.49: > initialized pipeline model parallel with size 4
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.060 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.047 seconds
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 10.541
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 18:10:35 
10.64.24.49: building GPT model ...
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 805617664
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 805617664
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (805617664 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 805617664
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 805617664
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (805617664 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 942456832
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 908910592
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 942456832
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 908910592
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (908910592 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (942456832 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 18:10:35 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache end, time cost:  4.947 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  4.982 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.077 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.106 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.099 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.127 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.104 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.316 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Cutting or padding data end, time cost:  10.660 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.528 s
10.64.24.50: Cutting or padding data end, time cost:  10.717 sconsumed_train_samples = 0, dataloader_type = single
10.64.24.50: 
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.567 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.733 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.690 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.812 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.717 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 18:10:52 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (42.48, 750.90)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 16317.09)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 18:10:52 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:11:40 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             249W / 700W |  55294MiB / 81559MiB |     76%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             256W / 700W |  54838MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             374W / 700W |  43992MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   35C    P0             365W / 700W |  44148MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             200W / 700W |  49782MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             187W / 700W |  49468MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             249W / 700W |  35998MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             259W / 700W |  36016MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 7408.5 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.089585E+01 | loss scale: 1.0 | grad norm: 7.120 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: [Rank 8] (after 10 iterations) memory (MB) | allocated: 9486.220703125 | max allocated: 29302.08984375 | reserved: 35694.0 | max reserved: 35694.0
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 11029.314453125 | max allocated: 42397.7099609375 | reserved: 51412.0 | max reserved: 51412.0
10.64.24.49: [Rank 4] (after 10 iterations) memory (MB) | allocated: 9486.298828125 | max allocated: 37280.3017578125 | reserved: 46440.0 | max reserved: 46440.0
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 11029.314453125 | max allocated: 42393.6591796875 | reserved: 51868.0 | max reserved: 51868.0
10.64.24.49: [Rank 5] (after 10 iterations) memory (MB) | allocated: 9486.220703125 | max allocated: 37282.0634765625 | reserved: 46126.0 | max reserved: 46126.0
10.64.24.50: [Rank 12] (after 10 iterations) memory (MB) | allocated: 10669.1904296875 | max allocated: 39896.16357421875 | reserved: 45742.0 | max reserved: 45742.0
10.64.24.50: [Rank 13] (after 10 iterations) memory (MB) | allocated: 10669.1904296875 | max allocated: 39896.15966796875 | reserved: 45742.0 | max reserved: 45742.0
10.64.24.50: [Rank 9] (after 10 iterations) memory (MB) | allocated: 9486.205078125 | max allocated: 29303.05078125 | reserved: 35824.0 | max reserved: 35824.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (6816.53, 7016.41)
10.64.24.50:     forward-compute ................................: (946.75, 3233.52)
10.64.24.50:     backward-compute ...............................: (1296.27, 3051.02)
10.64.24.50:     batch-generator ................................: (135.82, 184.04)
10.64.24.50:     forward-recv ...................................: (220.22, 569.13)
10.64.24.50:     forward-send ...................................: (4.98, 302.97)
10.64.24.50:     backward-recv ..................................: (152.00, 487.29)
10.64.24.50:     backward-send ..................................: (1.03, 24.90)
10.64.24.50:     forward-send-backward-recv .....................: (3447.12, 3703.50)
10.64.24.50:     backward-send-forward-recv .....................: (85.88, 361.02)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.84, 1.12)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 9.01)
10.64.24.50:     all-grads-sync .................................: (201.46, 219.51)
10.64.24.50:     params-all-gather ..............................: (5.25, 6.12)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.28, 0.39)
10.64.24.50:     optimizer-clip-main-grad .......................: (5.53, 5.74)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.66, 5.50)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.42, 1.67)
10.64.24.50:     optimizer ......................................: (19.14, 20.02)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 5661.2 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.088877E+01 | loss scale: 1.0 | grad norm: 32.408 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (5451.14, 5574.46)
10.64.24.50:     forward-compute ................................: (672.01, 2638.23)
10.64.24.50:     backward-compute ...............................: (1105.08, 2763.48)
10.64.24.50:     batch-generator ................................: (24.69, 31.13)
10.64.24.50:     forward-recv ...................................: (32.59, 77.94)
10.64.24.50:     forward-send ...................................: (0.61, 21.31)
10.64.24.50:     backward-recv ..................................: (103.11, 401.21)
10.64.24.50:     backward-send ..................................: (0.81, 39.68)
10.64.24.50:     forward-send-backward-recv .....................: (3110.77, 3373.52)
10.64.24.50:     backward-send-forward-recv .....................: (44.58, 249.35)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.78, 1.03)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 8.93)
10.64.24.50:     all-grads-sync .................................: (7.63, 9.12)
10.64.24.50:     params-all-gather ..............................: (5.25, 6.12)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.26, 0.35)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.40, 2.61)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.51, 5.31)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.42, 1.67)
10.64.24.50:     optimizer ......................................: (15.69, 16.56)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 6036.1 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.075234E+01 | loss scale: 1.0 | grad norm: 2.240 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (5785.19, 5980.61)
10.64.24.50:     forward-compute ................................: (789.18, 2757.11)
10.64.24.50:     backward-compute ...............................: (1304.57, 2905.03)
10.64.24.50:     batch-generator ................................: (25.16, 31.10)
10.64.24.50:     forward-recv ...................................: (32.13, 83.83)
10.64.24.50:     forward-send ...................................: (0.61, 24.78)
10.64.24.50:     backward-recv ..................................: (98.57, 466.90)
10.64.24.50:     backward-send ..................................: (0.96, 41.85)
10.64.24.50:     forward-send-backward-recv .....................: (3141.38, 3344.92)
10.64.24.50:     backward-send-forward-recv .....................: (59.52, 299.66)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.79, 1.01)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 8.94)
10.64.24.50:     all-grads-sync .................................: (7.72, 9.14)
10.64.24.50:     params-all-gather ..............................: (5.25, 6.13)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.26, 0.35)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.40, 2.61)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.52, 5.30)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.42, 1.67)
10.64.24.50:     optimizer ......................................: (15.73, 16.61)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:14:09 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             293W / 700W |  59132MiB / 81559MiB |     25%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             280W / 700W |  58676MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             246W / 700W |  53898MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   35C    P0             236W / 700W |  54054MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             244W / 700W |  54042MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             256W / 700W |  53728MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             229W / 700W |  41078MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             312W / 700W |  41096MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 18:14:15 
10.64.24.49: rank 7: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 5, '2048': 97, '4096': 422, '8192': 361, '16384': 129, '32768': 10, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1868, '512': 1862, '1024': 1501, '2048': 703, '4096': 267, '8192': 176, '16384': 0, '32768': 0, '>32k': 0}}rank 3: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 5, '2048': 97, '4096': 422, '8192': 361, '16384': 129, '32768': 10, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1868, '512': 1862, '1024': 1501, '2048': 703, '4096': 267, '8192': 176, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 1: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 335, '16384': 108, '32768': 15, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 258, '8192': 149, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 5: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 335, '16384': 108, '32768': 15, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 258, '8192': 149, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 335, '16384': 108, '32768': 15, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 258, '8192': 149, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 2: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 5, '2048': 97, '4096': 422, '8192': 361, '16384': 129, '32768': 10, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1868, '512': 1862, '1024': 1501, '2048': 703, '4096': 267, '8192': 176, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 6: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 5, '2048': 97, '4096': 422, '8192': 361, '16384': 129, '32768': 10, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1868, '512': 1862, '1024': 1501, '2048': 703, '4096': 267, '8192': 176, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 335, '16384': 108, '32768': 15, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 258, '8192': 149, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 335, '16384': 108, '32768': 15, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 258, '8192': 149, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen8192_gbs512_mbs8_dp2_tp2_pp4_pack_sp_node1.log...
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen8192_gbs512_mbs8_dp2_tp2_pp4_pack_sp_node0.log...
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 7b model, gpu_num=16, seq_len = 8192, DP=2, MP=8, PP=1, GBS=512, MBS=8
10.64.24.50: use sequence-packing
10.64.24.50: use sequence parallel
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 7b model, gpu_num=16, seq_len = 8192, DP=2, MP=8, PP=1, GBS=512, MBS=8
10.64.24.49: use sequence-packing
10.64.24.49: use sequence parallel
10.64.24.49: [2024-03-11 18:14:29,949] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 18:14:29,949] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 18:14:29,949] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 18:14:29,949] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 18:14:29,141] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 18:14:29,141] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 18:14:29,141] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 18:14:29,141] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 8, pipeline-model-parallel size: 1 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 2
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 32
10.64.24.49:   encoder_seq_length .............................. 8192
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 16384
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 4096
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 8192
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 8
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 32
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 32
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 1
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 8192
10.64.24.49:   sequence_packing ................................ True
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 8
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 1
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ True
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 32
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 943 dummy tokens (new size: 51200)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: > initialized tensor model parallel with size 8
10.64.24.49: > initialized pipeline model parallel with size 1
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.061 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.134 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 10.738
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 18:14:57 
10.64.24.49: building GPT model ...
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (4, 0): 865984512
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 865984512
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 865984512
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 865984512
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (6, 0): 865984512
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 865984512
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (7, 0): 865984512
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (5, 0): 865984512
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (865984512 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 18:14:57 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache end, time cost:  4.749 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  4.868 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Cutting or padding data end, time cost:  10.470 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.546 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 18:15:13 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (472.45, 499.37)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 15733.66)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 18:15:13 
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:15:59 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             361W / 700W |  35758MiB / 81559MiB |     56%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             410W / 700W |  36096MiB / 81559MiB |     95%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             396W / 700W |  35978MiB / 81559MiB |     95%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             340W / 700W |  36312MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             335W / 700W |  36128MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             364W / 700W |  35970MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             337W / 700W |  36134MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             255W / 700W |  35380MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 6881.3 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.090235E+01 | loss scale: 1.0 | grad norm: 28.625 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.49: [Rank 6] (after 10 iterations) memory (MB) | allocated: 10153.7666015625 | max allocated: 27549.4560546875 | reserved: 33076.0 | max reserved: 33076.0[Rank 3] (after 10 iterations) memory (MB) | allocated: 10153.7666015625 | max allocated: 27549.4560546875 | reserved: 33254.0 | max reserved: 33254.0[Rank 4] (after 10 iterations) memory (MB) | allocated: 10153.7666015625 | max allocated: 27549.455078125 | reserved: 33070.0 | max reserved: 33070.0
10.64.24.49: [Rank 7] (after 10 iterations) memory (MB) | allocated: 10153.7666015625 | max allocated: 27549.4560546875 | reserved: 32562.0 | max reserved: 32562.0
10.64.24.49: 
10.64.24.49: 
10.64.24.49: [Rank 5] (after 10 iterations) memory (MB) | allocated: 10153.7666015625 | max allocated: 27549.4541015625 | reserved: 32912.0 | max reserved: 32912.0
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 10153.7666015625 | max allocated: 27549.455078125 | reserved: 32748.0 | max reserved: 32748.0
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 10153.7666015625 | max allocated: 27549.455078125 | reserved: 33038.0 | max reserved: 33038.0
10.64.24.49: [Rank 2] (after 10 iterations) memory (MB) | allocated: 10153.7666015625 | max allocated: 27549.455078125 | reserved: 32920.0 | max reserved: 32920.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (6579.78, 6610.99)
10.64.24.50:     forward-compute ................................: (3660.66, 3959.48)
10.64.24.50:     backward-compute ...............................: (2602.87, 2934.83)
10.64.24.50:     batch-generator ................................: (689.41, 714.98)
10.64.24.50:     layernorm-grads-all-reduce .....................: (2.49, 3.13)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.05)
10.64.24.50:     all-grads-sync .................................: (123.49, 130.00)
10.64.24.50:     params-all-gather ..............................: (22.11, 22.38)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (1.05, 1.35)
10.64.24.50:     optimizer-clip-main-grad .......................: (6.12, 6.15)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (5.25, 5.59)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.09, 2.29)
10.64.24.50:     optimizer ......................................: (38.08, 38.35)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 5158.1 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.086236E+01 | loss scale: 1.0 | grad norm: 92.384 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (5055.15, 5068.93)
10.64.24.50:     forward-compute ................................: (2514.26, 2704.95)
10.64.24.50:     backward-compute ...............................: (2344.09, 2528.12)
10.64.24.50:     batch-generator ................................: (36.84, 47.43)
10.64.24.50:     layernorm-grads-all-reduce .....................: (2.40, 2.83)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.03)
10.64.24.50:     all-grads-sync .................................: (38.92, 39.60)
10.64.24.50:     params-all-gather ..............................: (22.21, 22.37)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (1.02, 1.31)
10.64.24.50:     optimizer-clip-main-grad .......................: (3.21, 3.24)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (5.02, 5.14)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.09, 2.29)
10.64.24.50:     optimizer ......................................: (34.85, 35.01)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 6571.3 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.074106E+01 | loss scale: 1.0 | grad norm: 8.931 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (6448.90, 6484.99)
10.64.24.50:     forward-compute ................................: (3709.88, 3852.91)
10.64.24.50:     backward-compute ...............................: (2594.06, 2743.97)
10.64.24.50:     batch-generator ................................: (38.30, 50.59)
10.64.24.50:     layernorm-grads-all-reduce .....................: (2.41, 2.93)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 0.03)
10.64.24.50:     all-grads-sync .................................: (38.84, 39.43)
10.64.24.50:     params-all-gather ..............................: (22.21, 22.38)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (1.03, 1.31)
10.64.24.50:     optimizer-clip-main-grad .......................: (3.23, 3.25)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (5.03, 5.12)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.09, 2.25)
10.64.24.50:     optimizer ......................................: (34.67, 34.84)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:18:25 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             373W / 700W |  41750MiB / 81559MiB |     21%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             384W / 700W |  39948MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             442W / 700W |  41114MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             417W / 700W |  40592MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             368W / 700W |  41264MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             358W / 700W |  41422MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             398W / 700W |  41056MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             377W / 700W |  40302MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 18:18:31 
10.64.24.49: rank 3: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 335, '16384': 108, '32768': 15, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 258, '8192': 149, '16384': 0, '32768': 0, '>32k': 0}}rank 5: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 335, '16384': 108, '32768': 15, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 258, '8192': 149, '16384': 0, '32768': 0, '>32k': 0}}rank 4: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 335, '16384': 108, '32768': 15, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 258, '8192': 149, '16384': 0, '32768': 0, '>32k': 0}}rank 6: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 335, '16384': 108, '32768': 15, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 258, '8192': 149, '16384': 0, '32768': 0, '>32k': 0}}rank 7: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 335, '16384': 108, '32768': 15, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 258, '8192': 149, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: 
10.64.24.49: 
10.64.24.49: rank 1: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 335, '16384': 108, '32768': 15, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 258, '8192': 149, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 2: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 335, '16384': 108, '32768': 15, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 258, '8192': 149, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 5, '2048': 97, '4096': 422, '8192': 361, '16384': 129, '32768': 10, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1868, '512': 1862, '1024': 1501, '2048': 703, '4096': 267, '8192': 176, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 335, '16384': 108, '32768': 15, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 258, '8192': 149, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen8192_gbs512_mbs8_dp2_tp8_pp1_pack_sp_node1.log...
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen8192_gbs512_mbs8_dp2_tp8_pp1_pack_sp_node0.log...
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 7b model, gpu_num=16, seq_len = 8192, DP=4, MP=4, PP=1, GBS=512, MBS=4
10.64.24.50: use sequence-packing
10.64.24.50: use sequence parallel
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 7b model, gpu_num=16, seq_len = 8192, DP=4, MP=4, PP=1, GBS=512, MBS=4
10.64.24.49: use sequence-packing
10.64.24.49: use sequence parallel
10.64.24.50: [2024-03-11 18:18:41,775] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 18:18:41,775] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 18:18:41,775] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 18:18:41,775] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 18:18:42,715] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 18:18:42,715] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 18:18:42,715] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 18:18:42,715] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 4, context-parallel size: 1 tensor-model-parallel size: 4, pipeline-model-parallel size: 1 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 4
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 32
10.64.24.49:   encoder_seq_length .............................. 8192
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 16384
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 4096
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 8192
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 4
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 32
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 32
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 1
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 8192
10.64.24.49:   sequence_packing ................................ True
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 4
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 1
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ True
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 32
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 431 dummy tokens (new size: 50688)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: > initialized tensor model parallel with size 4
10.64.24.49: > initialized pipeline model parallel with size 1
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.061 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 7.901 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 10.807
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 18:19:11 
10.64.24.49: building GPT model ...
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1697095680
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 1697095680
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1697095680
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 1697095680
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1697095680 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 18:19:11 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache end, time cost:  4.831 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  4.843 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  4.871 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.065 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Cutting or padding data end, time cost:  10.545 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.565 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.637 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.490 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 18:19:27 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (462.31, 489.93)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 15896.54)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 18:19:27 
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:20:15 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             215W / 700W |  44618MiB / 81559MiB |      2%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             252W / 700W |  44668MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             290W / 700W |  44668MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             295W / 700W |  44428MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             342W / 700W |  46952MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             294W / 700W |  47000MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             299W / 700W |  46998MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             317W / 700W |  46784MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 7256.2 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.088688E+01 | loss scale: 1.0 | grad norm: 13.933 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.49: [Rank 3] (after 10 iterations) memory (MB) | allocated: 14777.0107421875 | max allocated: 40075.90625 | reserved: 41162.0 | max reserved: 41162.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (6762.93, 6801.34)
10.64.24.50:     forward-compute ................................: (4124.33, 4414.50)
10.64.24.50:     backward-compute ...............................: (2333.53, 2653.38)
10.64.24.50:     batch-generator ................................: (430.43, 471.12)
10.64.24.50:     layernorm-grads-all-reduce .....................: (2.50, 2.96)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.04)
10.64.24.50:     all-grads-sync .................................: (251.97, 257.72)
10.64.24.50:     params-all-gather ..............................: (32.02, 32.17)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.54, 0.74)
10.64.24.50:     optimizer-clip-main-grad .......................: (6.54, 6.56)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.98, 5.21)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.65, 1.75)
10.64.24.50:     optimizer ......................................: (47.00, 47.15)
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 14777.0107421875 | max allocated: 40076.037109375 | reserved: 41162.0 | max reserved: 41162.0
10.64.24.49: [Rank 2] (after 10 iterations) memory (MB) | allocated: 14777.0107421875 | max allocated: 40075.90625 | reserved: 41162.0 | max reserved: 41162.0
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 14777.0107421875 | max allocated: 40075.90625 | reserved: 41160.0 | max reserved: 41160.0
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 5389.8 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.086251E+01 | loss scale: 1.0 | grad norm: 59.925 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (5247.79, 5266.92)
10.64.24.50:     forward-compute ................................: (2942.94, 3112.80)
10.64.24.50:     backward-compute ...............................: (2123.90, 2294.41)
10.64.24.50:     batch-generator ................................: (34.33, 46.56)
10.64.24.50:     layernorm-grads-all-reduce .....................: (2.40, 2.88)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.04)
10.64.24.50:     all-grads-sync .................................: (56.36, 56.54)
10.64.24.50:     params-all-gather ..............................: (32.04, 32.20)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.53, 0.66)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.57, 2.59)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.81, 4.96)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.65, 1.75)
10.64.24.50:     optimizer ......................................: (42.63, 42.78)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 6959.6 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.072976E+01 | loss scale: 1.0 | grad norm: 3.666 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (6802.15, 6841.57)
10.64.24.50:     forward-compute ................................: (4299.78, 4471.85)
10.64.24.50:     backward-compute ...............................: (2317.17, 2493.13)
10.64.24.50:     batch-generator ................................: (35.29, 48.10)
10.64.24.50:     layernorm-grads-all-reduce .....................: (2.39, 2.75)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.03)
10.64.24.50:     all-grads-sync .................................: (56.34, 57.19)
10.64.24.50:     params-all-gather ..............................: (32.04, 32.20)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.53, 0.66)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.59, 2.61)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.81, 4.88)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.65, 1.75)
10.64.24.50:     optimizer ......................................: (42.50, 42.66)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:22:49 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             397W / 700W |  46204MiB / 81559MiB |     61%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             398W / 700W |  46252MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             333W / 700W |  46252MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             359W / 700W |  46012MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             404W / 700W |  46952MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             421W / 700W |  47000MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             393W / 700W |  46998MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             394W / 700W |  46784MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 18:22:55 
10.64.24.49: rank 1: {'packing_seq_len': {'128': 0, '256': 1, '512': 26, '1024': 193, '2048': 387, '4096': 298, '8192': 79, '16384': 37, '32768': 3, '>32k': 0}, 'real_seq_len': {'128': 937, '256': 962, '512': 926, '1024': 756, '2048': 317, '4096': 124, '8192': 74, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 3: {'packing_seq_len': {'128': 0, '256': 1, '512': 26, '1024': 193, '2048': 387, '4096': 298, '8192': 79, '16384': 37, '32768': 3, '>32k': 0}, 'real_seq_len': {'128': 937, '256': 962, '512': 926, '1024': 756, '2048': 317, '4096': 124, '8192': 74, '16384': 0, '32768': 0, '>32k': 0}}rank 2: {'packing_seq_len': {'128': 0, '256': 1, '512': 26, '1024': 193, '2048': 387, '4096': 298, '8192': 79, '16384': 37, '32768': 3, '>32k': 0}, 'real_seq_len': {'128': 937, '256': 962, '512': 926, '1024': 756, '2048': 317, '4096': 124, '8192': 74, '16384': 0, '32768': 0, '>32k': 0}}rank 7: {'packing_seq_len': {'128': 0, '256': 0, '512': 19, '1024': 193, '2048': 379, '4096': 304, '8192': 86, '16384': 40, '32768': 3, '>32k': 0}, 'real_seq_len': {'128': 913, '256': 957, '512': 925, '1024': 755, '2048': 337, '4096': 134, '8192': 75, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 5: {'packing_seq_len': {'128': 0, '256': 0, '512': 19, '1024': 193, '2048': 379, '4096': 304, '8192': 86, '16384': 40, '32768': 3, '>32k': 0}, 'real_seq_len': {'128': 913, '256': 957, '512': 925, '1024': 755, '2048': 337, '4096': 134, '8192': 75, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: 
10.64.24.49: rank 6: {'packing_seq_len': {'128': 0, '256': 0, '512': 19, '1024': 193, '2048': 379, '4096': 304, '8192': 86, '16384': 40, '32768': 3, '>32k': 0}, 'real_seq_len': {'128': 913, '256': 957, '512': 925, '1024': 755, '2048': 337, '4096': 134, '8192': 75, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 0, '256': 0, '512': 16, '1024': 181, '2048': 391, '4096': 299, '8192': 92, '16384': 44, '32768': 1, '>32k': 0}, 'real_seq_len': {'128': 889, '256': 914, '512': 934, '1024': 776, '2048': 361, '4096': 141, '8192': 81, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 0, '256': 0, '512': 19, '1024': 193, '2048': 379, '4096': 304, '8192': 86, '16384': 40, '32768': 3, '>32k': 0}, 'real_seq_len': {'128': 913, '256': 957, '512': 925, '1024': 755, '2048': 337, '4096': 134, '8192': 75, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 0, '256': 1, '512': 26, '1024': 193, '2048': 387, '4096': 298, '8192': 79, '16384': 37, '32768': 3, '>32k': 0}, 'real_seq_len': {'128': 937, '256': 962, '512': 926, '1024': 756, '2048': 317, '4096': 124, '8192': 74, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen8192_gbs512_mbs4_dp4_tp4_pp1_pack_sp_node0.log...
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen8192_gbs512_mbs4_dp4_tp4_pp1_pack_sp_node1.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 7b model, gpu_num=16, seq_len = 4096, DP=2, MP=4, PP=2, GBS=512, MBS=16
10.64.24.49: use sequence-packing
10.64.24.49: use sequence parallel
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 7b model, gpu_num=16, seq_len = 4096, DP=2, MP=4, PP=2, GBS=512, MBS=16
10.64.24.50: use sequence-packing
10.64.24.50: use sequence parallel
10.64.24.50: [2024-03-11 18:23:05,558] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 18:23:05,558] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 18:23:05,558] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 18:23:05,558] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 18:23:06,542] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 18:23:06,542] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 18:23:06,542] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 18:23:06,542] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 4, pipeline-model-parallel size: 2 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 2
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 32
10.64.24.49:   encoder_seq_length .............................. 4096
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 16384
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 4096
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 4096
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 16
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 32
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 32
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 2
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 4096
10.64.24.49:   sequence_packing ................................ True
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 4
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 2
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ True
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 16
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 431 dummy tokens (new size: 50688)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: > initialized tensor model parallel with size 4
10.64.24.49: > initialized pipeline model parallel with size 2
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.059 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.746 seconds
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 11.395
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 18:23:35 
10.64.24.49: building GPT model ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 857726976
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 857726976
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 857726976
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 857726976
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 874496000
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 874496000
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 874496000
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 874496000
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (857726976 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (874496000 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 18:23:36 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache end, time cost:  4.983 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Loading exists cache end, time cost:  4.992 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.033 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.216 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Cutting or padding data end, time cost:  4.981 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  4.969 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  5.086 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  5.127 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 18:23:47 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (997.59, 1144.28)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 10666.93)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 18:23:47 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:24:24 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             453W / 700W |  39634MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             445W / 700W |  39698MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             416W / 700W |  39714MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             386W / 700W |  39460MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             361W / 700W |  49718MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             379W / 700W |  47572MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             379W / 700W |  49368MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             322W / 700W |  48178MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 5067.9 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.095869E+01 | loss scale: 1.0 | grad norm: 26.093 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: [Rank 9] (after 10 iterations) memory (MB) | allocated: 10062.0361328125 | max allocated: 28798.181640625 | reserved: 32274.0 | max reserved: 32274.0
10.64.24.50: [Rank 11] (after 10 iterations) memory (MB) | allocated: 10062.0361328125 | max allocated: 28799.107421875 | reserved: 32288.0 | max reserved: 32288.0
10.64.24.50: [Rank 10] (after 10 iterations) memory (MB) | allocated: 10062.5751953125 | max allocated: 28798.056640625 | reserved: 32274.0 | max reserved: 32274.0
10.64.24.49: [Rank 2] (after 10 iterations) memory (MB) | allocated: 10219.720703125 | max allocated: 28032.9248046875 | reserved: 35992.0 | max reserved: 35992.0[Rank 3] (after 10 iterations) memory (MB) | allocated: 10219.720703125 | max allocated: 28032.7568359375 | reserved: 35978.0 | max reserved: 35978.0
10.64.24.49: 
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 10219.720703125 | max allocated: 28033.5771484375 | reserved: 35960.0 | max reserved: 35960.0
10.64.24.50: [Rank 8] (after 10 iterations) memory (MB) | allocated: 10061.9091796875 | max allocated: 28798.056640625 | reserved: 32276.0 | max reserved: 32276.0
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 10219.720703125 | max allocated: 28032.7099609375 | reserved: 35976.0 | max reserved: 35976.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (4568.70, 4679.83)
10.64.24.50:     forward-compute ................................: (1294.24, 2247.55)
10.64.24.50:     backward-compute ...............................: (1292.10, 1731.32)
10.64.24.50:     batch-generator ................................: (410.36, 441.64)
10.64.24.50:     forward-recv ...................................: (524.76, 536.79)
10.64.24.50:     forward-send ...................................: (2.87, 5.46)
10.64.24.50:     backward-recv ..................................: (110.10, 135.33)
10.64.24.50:     backward-send ..................................: (0.95, 1.10)
10.64.24.50:     forward-send-backward-recv .....................: (1707.92, 1794.98)
10.64.24.50:     backward-send-forward-recv .....................: (103.27, 114.93)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.39, 1.62)
10.64.24.50:     embedding-grads-all-reduce .....................: (4.58, 4.73)
10.64.24.50:     all-grads-sync .................................: (217.84, 241.93)
10.64.24.50:     params-all-gather ..............................: (5.93, 6.13)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.52, 0.68)
10.64.24.50:     optimizer-clip-main-grad .......................: (6.60, 6.65)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (5.04, 5.33)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.69, 1.81)
10.64.24.50:     optimizer ......................................: (21.11, 21.29)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 3157.0 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.090938E+01 | loss scale: 1.0 | grad norm: 104.653 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (3016.94, 3104.64)
10.64.24.50:     forward-compute ................................: (723.82, 1400.35)
10.64.24.50:     backward-compute ...............................: (1126.87, 1550.00)
10.64.24.50:     batch-generator ................................: (13.31, 18.89)
10.64.24.50:     forward-recv ...................................: (41.24, 51.34)
10.64.24.50:     forward-send ...................................: (0.57, 0.70)
10.64.24.50:     backward-recv ..................................: (81.80, 128.45)
10.64.24.50:     backward-send ..................................: (0.89, 0.97)
10.64.24.50:     forward-send-backward-recv .....................: (994.38, 997.69)
10.64.24.50:     backward-send-forward-recv .....................: (72.58, 80.68)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.32, 1.39)
10.64.24.50:     embedding-grads-all-reduce .....................: (4.60, 4.70)
10.64.24.50:     all-grads-sync .................................: (8.14, 8.64)
10.64.24.50:     params-all-gather ..............................: (5.93, 6.12)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.51, 0.61)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.59, 2.63)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.88, 5.00)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.69, 1.80)
10.64.24.50:     optimizer ......................................: (16.50, 16.69)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 3455.0 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.067585E+01 | loss scale: 1.0 | grad norm: 12.396 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (3285.91, 3396.77)
10.64.24.50:     forward-compute ................................: (827.11, 1489.46)
10.64.24.50:     backward-compute ...............................: (1280.52, 1691.81)
10.64.24.50:     batch-generator ................................: (13.05, 19.19)
10.64.24.50:     forward-recv ...................................: (52.07, 52.45)
10.64.24.50:     forward-send ...................................: (0.66, 0.70)
10.64.24.50:     backward-recv ..................................: (105.32, 117.04)
10.64.24.50:     backward-send ..................................: (0.95, 1.04)
10.64.24.50:     forward-send-backward-recv .....................: (994.45, 1016.84)
10.64.24.50:     backward-send-forward-recv .....................: (89.91, 115.27)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.31, 1.42)
10.64.24.50:     embedding-grads-all-reduce .....................: (4.55, 4.67)
10.64.24.50:     all-grads-sync .................................: (8.10, 8.63)
10.64.24.50:     params-all-gather ..............................: (5.91, 6.12)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.52, 0.62)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.58, 2.62)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.87, 4.99)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.69, 1.80)
10.64.24.50:     optimizer ......................................: (16.44, 16.66)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:25:47 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             398W / 700W |  55874MiB / 81559MiB |     65%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   46C    P0             343W / 700W |  56370MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             334W / 700W |  56270MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             326W / 700W |  56164MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             379W / 700W |  49720MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   46C    P0             390W / 700W |  47574MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             441W / 700W |  49370MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             388W / 700W |  48180MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 18:25:51 rank 2: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 0, '2048': 0, '4096': 13, '8192': 240, '16384': 238, '32768': 21, '>32k': 0}, 'real_seq_len': {'128': 1854, '256': 1885, '512': 1867, '1024': 1467, '2048': 693, '4096': 426, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 7: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 0, '2048': 0, '4096': 18, '8192': 213, '16384': 262, '32768': 19, '>32k': 0}, 'real_seq_len': {'128': 1811, '256': 1902, '512': 1846, '1024': 1545, '2048': 664, '4096': 424, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 3: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 0, '2048': 0, '4096': 13, '8192': 240, '16384': 238, '32768': 21, '>32k': 0}, 'real_seq_len': {'128': 1854, '256': 1885, '512': 1867, '1024': 1467, '2048': 693, '4096': 426, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 6: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 0, '2048': 0, '4096': 18, '8192': 213, '16384': 262, '32768': 19, '>32k': 0}, 'real_seq_len': {'128': 1811, '256': 1902, '512': 1846, '1024': 1545, '2048': 664, '4096': 424, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}rank 5: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 0, '2048': 0, '4096': 18, '8192': 213, '16384': 262, '32768': 19, '>32k': 0}, 'real_seq_len': {'128': 1811, '256': 1902, '512': 1846, '1024': 1545, '2048': 664, '4096': 424, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 1: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 0, '2048': 0, '4096': 13, '8192': 240, '16384': 238, '32768': 21, '>32k': 0}, 'real_seq_len': {'128': 1854, '256': 1885, '512': 1867, '1024': 1467, '2048': 693, '4096': 426, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 0, '2048': 0, '4096': 13, '8192': 240, '16384': 238, '32768': 21, '>32k': 0}, 'real_seq_len': {'128': 1854, '256': 1885, '512': 1867, '1024': 1467, '2048': 693, '4096': 426, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 0, '2048': 0, '4096': 18, '8192': 213, '16384': 262, '32768': 19, '>32k': 0}, 'real_seq_len': {'128': 1811, '256': 1902, '512': 1846, '1024': 1545, '2048': 664, '4096': 424, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 0, '2048': 0, '4096': 13, '8192': 240, '16384': 238, '32768': 21, '>32k': 0}, 'real_seq_len': {'128': 1854, '256': 1885, '512': 1867, '1024': 1467, '2048': 693, '4096': 426, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen4096_gbs512_mbs16_dp2_tp4_pp2_pack_sp_node0.log...
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen4096_gbs512_mbs16_dp2_tp4_pp2_pack_sp_node1.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 7b model, gpu_num=16, seq_len = 4096, DP=4, MP=2, PP=2, GBS=512, MBS=8
10.64.24.49: use sequence-packing
10.64.24.49: use sequence parallel
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 7b model, gpu_num=16, seq_len = 4096, DP=4, MP=2, PP=2, GBS=512, MBS=8
10.64.24.50: use sequence-packing
10.64.24.50: use sequence parallel
10.64.24.50: [2024-03-11 18:25:59,205] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 18:25:59,205] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 18:25:59,205] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 18:25:59,205] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 18:26:00,222] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 18:26:00,222] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 18:26:00,222] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 18:26:00,222] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 4, context-parallel size: 1 tensor-model-parallel size: 2, pipeline-model-parallel size: 2 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 4
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 32
10.64.24.49:   encoder_seq_length .............................. 4096
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 16384
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 4096
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 4096
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 8
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 32
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 32
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 2
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 4096
10.64.24.49:   sequence_packing ................................ True
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 2
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 2
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ True
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 16
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: > initialized tensor model parallel with size 2
10.64.24.49: > initialized pipeline model parallel with size 2
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.062 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.381 seconds
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 11.307
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 18:26:28 
10.64.24.49: building GPT model ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1714528256
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1731297280
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1714528256
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1731297280
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1714528256 elements):
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1731297280 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 18:26:29 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache end, time cost:  5.108 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.134 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.156 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.152 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.177 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.214 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.273 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Loading exists cache end, time cost:  6.386 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Cutting or padding data end, time cost:  4.973 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  5.142 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  5.082 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  5.140 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  5.127 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  5.267 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  5.185 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  5.280 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 18:26:41 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (862.65, 1004.41)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 11918.85)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 18:26:41 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:27:18 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             325W / 700W |  44572MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             355W / 700W |  45098MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             373W / 700W |  46538MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             357W / 700W |  46506MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             304W / 700W |  56530MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             294W / 700W |  56478MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             344W / 700W |  43960MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             286W / 700W |  44158MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 5367.4 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.094813E+01 | loss scale: 1.0 | grad norm: 13.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: [Rank 9] (after 10 iterations) memory (MB) | allocated: 14893.2451171875 | max allocated: 35859.28955078125 | reserved: 41786.0 | max reserved: 41786.0
10.64.24.50: [Rank 8] (after 10 iterations) memory (MB) | allocated: 14893.2451171875 | max allocated: 35858.74267578125 | reserved: 41786.0 | max reserved: 41786.0
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 15013.205078125 | max allocated: 36931.78515625 | reserved: 40902.0 | max reserved: 40902.0
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 15013.205078125 | max allocated: 36924.7578125 | reserved: 41428.0 | max reserved: 41428.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (4601.02, 4731.92)
10.64.24.50:     forward-compute ................................: (916.15, 2724.14)
10.64.24.50:     backward-compute ...............................: (1140.84, 1674.09)
10.64.24.50:     batch-generator ................................: (208.61, 233.31)
10.64.24.50:     forward-recv ...................................: (272.79, 300.41)
10.64.24.50:     forward-send ...................................: (4.08, 5.92)
10.64.24.50:     backward-recv ..................................: (131.43, 194.74)
10.64.24.50:     backward-send ..................................: (0.93, 4.51)
10.64.24.50:     forward-send-backward-recv .....................: (2170.13, 2342.78)
10.64.24.50:     backward-send-forward-recv .....................: (82.01, 122.20)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.40, 1.58)
10.64.24.50:     embedding-grads-all-reduce .....................: (8.87, 9.01)
10.64.24.50:     all-grads-sync .................................: (423.17, 434.55)
10.64.24.50:     params-all-gather ..............................: (12.75, 12.96)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.28, 0.39)
10.64.24.50:     optimizer-clip-main-grad .......................: (5.47, 5.49)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.95, 5.15)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.47, 1.59)
10.64.24.50:     optimizer ......................................: (26.06, 26.26)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 3460.2 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.090988E+01 | loss scale: 1.0 | grad norm: 53.490 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (3257.41, 3363.43)
10.64.24.50:     forward-compute ................................: (603.88, 1783.20)
10.64.24.50:     backward-compute ...............................: (1031.73, 1470.11)
10.64.24.50:     batch-generator ................................: (12.10, 17.59)
10.64.24.50:     forward-recv ...................................: (33.30, 48.06)
10.64.24.50:     forward-send ...................................: (0.53, 0.72)
10.64.24.50:     backward-recv ..................................: (104.32, 163.38)
10.64.24.50:     backward-send ..................................: (0.91, 3.07)
10.64.24.50:     forward-send-backward-recv .....................: (1350.92, 1476.11)
10.64.24.50:     backward-send-forward-recv .....................: (66.44, 114.05)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.33, 1.58)
10.64.24.50:     embedding-grads-all-reduce .....................: (8.83, 9.07)
10.64.24.50:     all-grads-sync .................................: (19.14, 19.74)
10.64.24.50:     params-all-gather ..............................: (12.79, 12.99)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.26, 0.34)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.43, 2.45)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.80, 4.93)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.47, 1.59)
10.64.24.50:     optimizer ......................................: (22.80, 23.00)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 3817.9 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.066439E+01 | loss scale: 1.0 | grad norm: 8.618 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (3549.31, 3685.90)
10.64.24.50:     forward-compute ................................: (707.99, 1839.66)
10.64.24.50:     backward-compute ...............................: (1204.66, 1614.31)
10.64.24.50:     batch-generator ................................: (12.39, 17.50)
10.64.24.50:     forward-recv ...................................: (43.96, 45.12)
10.64.24.50:     forward-send ...................................: (0.66, 0.68)
10.64.24.50:     backward-recv ..................................: (114.85, 169.15)
10.64.24.50:     backward-send ..................................: (1.01, 5.02)
10.64.24.50:     forward-send-backward-recv .....................: (1491.83, 1508.21)
10.64.24.50:     backward-send-forward-recv .....................: (105.24, 132.25)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.33, 1.56)
10.64.24.50:     embedding-grads-all-reduce .....................: (8.86, 9.03)
10.64.24.50:     all-grads-sync .................................: (19.15, 19.74)
10.64.24.50:     params-all-gather ..............................: (12.77, 12.99)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.26, 0.34)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.42, 2.44)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.80, 4.88)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.47, 1.56)
10.64.24.50:     optimizer ......................................: (22.49, 22.71)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:28:51 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             334W / 700W |  53352MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             390W / 700W |  53346MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             391W / 700W |  55768MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             421W / 700W |  55736MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             353W / 700W |  60250MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             348W / 700W |  60198MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             363W / 700W |  47322MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             346W / 700W |  48312MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: rank 5: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 0, '2048': 49, '4096': 231, '8192': 201, '16384': 30, '32768': 1, '>32k': 0}, 'real_seq_len': {'128': 912, '256': 947, '512': 928, '1024': 774, '2048': 331, '4096': 204, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}[after training is done] datetime: 2024-03-11 18:28:55 
10.64.24.49: 
10.64.24.49: rank 1: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 52, '4096': 232, '8192': 194, '16384': 32, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 938, '256': 972, '512': 923, '1024': 737, '2048': 323, '4096': 203, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}rank 3: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 3, '2048': 53, '4096': 202, '8192': 204, '16384': 50, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 916, '256': 913, '512': 944, '1024': 730, '2048': 370, '4096': 223, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 7: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 44, '4096': 220, '8192': 201, '16384': 45, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 899, '256': 955, '512': 918, '1024': 771, '2048': 333, '4096': 220, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 2: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 3, '2048': 53, '4096': 202, '8192': 204, '16384': 50, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 916, '256': 913, '512': 944, '1024': 730, '2048': 370, '4096': 223, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 0, '2048': 49, '4096': 231, '8192': 201, '16384': 30, '32768': 1, '>32k': 0}, 'real_seq_len': {'128': 912, '256': 947, '512': 928, '1024': 774, '2048': 331, '4096': 204, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 52, '4096': 232, '8192': 194, '16384': 32, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 938, '256': 972, '512': 923, '1024': 737, '2048': 323, '4096': 203, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 52, '4096': 232, '8192': 194, '16384': 32, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 938, '256': 972, '512': 923, '1024': 737, '2048': 323, '4096': 203, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 6: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 44, '4096': 220, '8192': 201, '16384': 45, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 899, '256': 955, '512': 918, '1024': 771, '2048': 333, '4096': 220, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen4096_gbs512_mbs8_dp4_tp2_pp2_pack_sp_node0.log...
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen4096_gbs512_mbs8_dp4_tp2_pp2_pack_sp_node1.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 7b model, gpu_num=16, seq_len = 4096, DP=4, MP=4, PP=1, GBS=512, MBS=8
10.64.24.49: use sequence-packing
10.64.24.49: use sequence parallel
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 7b model, gpu_num=16, seq_len = 4096, DP=4, MP=4, PP=1, GBS=512, MBS=8
10.64.24.50: use sequence-packing
10.64.24.50: use sequence parallel
10.64.24.50: [2024-03-11 18:29:02,767] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 18:29:02,767] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 18:29:02,767] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 18:29:02,767] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 18:29:03,768] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 18:29:03,768] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 18:29:03,768] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 18:29:03,768] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: using world size: 16, data-parallel size: 4, context-parallel size: 1 tensor-model-parallel size: 4, pipeline-model-parallel size: 1 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 4
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 32
10.64.24.49:   encoder_seq_length .............................. 4096
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 16384
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 4096
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 4096
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 8
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 32
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 32
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 1
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 4096
10.64.24.49:   sequence_packing ................................ True
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 4
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 1
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ True
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 16
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 431 dummy tokens (new size: 50688)
10.64.24.49: > initializing torch distributed ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: > initialized tensor model parallel with size 4
10.64.24.49: > initialized pipeline model parallel with size 1
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.059 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.206 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 11.615
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 18:29:32 
10.64.24.49: building GPT model ...
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 1680318464
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1680318464
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 1680318464
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1680318464
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1680318464 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 18:29:32 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache end, time cost:  4.815 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Loading exists cache end, time cost:  4.833 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Loading exists cache end, time cost:  4.904 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Loading exists cache end, time cost:  4.997 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Cutting or padding data end, time cost:  5.174 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  5.155 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  5.012 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  5.208 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 18:29:43 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (295.51, 316.98)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 10355.57)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 18:29:43 
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:30:16 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             374W / 700W |  35812MiB / 81559MiB |     39%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             373W / 700W |  35860MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             401W / 700W |  35860MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             390W / 700W |  35620MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             434W / 700W |  34760MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             420W / 700W |  34808MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             358W / 700W |  34808MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             486W / 700W |  34568MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 4963.9 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.093370E+01 | loss scale: 1.0 | grad norm: 25.593 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.49: [Rank 2] (after 10 iterations) memory (MB) | allocated: 14576.6904296875 | max allocated: 31587.97509765625 | reserved: 32354.0 | max reserved: 32354.0
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 14577.0107421875 | max allocated: 31588.29931640625 | reserved: 32354.0 | max reserved: 32354.0[Rank 0] (after 10 iterations) memory (MB) | allocated: 14577.1279296875 | max allocated: 31588.41259765625 | reserved: 32354.0 | max reserved: 32354.0
10.64.24.49: 
10.64.24.49: [Rank 3] (after 10 iterations) memory (MB) | allocated: 14576.6904296875 | max allocated: 31588.32666015625 | reserved: 32354.0 | max reserved: 32354.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (4496.62, 4534.09)
10.64.24.50:     forward-compute ................................: (2676.46, 2892.59)
10.64.24.50:     backward-compute ...............................: (1596.42, 1839.38)
10.64.24.50:     batch-generator ................................: (421.20, 448.53)
10.64.24.50:     layernorm-grads-all-reduce .....................: (2.48, 2.81)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.03)
10.64.24.50:     all-grads-sync .................................: (246.78, 251.71)
10.64.24.50:     params-all-gather ..............................: (31.79, 31.87)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.54, 0.69)
10.64.24.50:     optimizer-clip-main-grad .......................: (5.48, 5.51)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.95, 5.07)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.64, 1.74)
10.64.24.50:     optimizer ......................................: (45.41, 45.49)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 3516.5 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.089208E+01 | loss scale: 1.0 | grad norm: 109.571 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (3361.12, 3388.03)
10.64.24.50:     forward-compute ................................: (1755.10, 1895.55)
10.64.24.50:     backward-compute ...............................: (1467.79, 1606.69)
10.64.24.50:     batch-generator ................................: (17.28, 20.76)
10.64.24.50:     layernorm-grads-all-reduce .....................: (2.37, 2.83)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 0.03)
10.64.24.50:     all-grads-sync .................................: (55.83, 56.07)
10.64.24.50:     params-all-gather ..............................: (31.78, 31.88)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.52, 0.63)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.55, 2.58)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.79, 4.84)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.64, 1.77)
10.64.24.50:     optimizer ......................................: (42.16, 42.26)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 3867.6 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.064353E+01 | loss scale: 1.0 | grad norm: 7.131 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (3671.98, 3717.66)
10.64.24.50:     forward-compute ................................: (1937.77, 2027.78)
10.64.24.50:     backward-compute ...............................: (1646.98, 1772.43)
10.64.24.50:     batch-generator ................................: (17.26, 21.14)
10.64.24.50:     layernorm-grads-all-reduce .....................: (2.39, 2.74)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 0.03)
10.64.24.50:     all-grads-sync .................................: (55.86, 56.15)
10.64.24.50:     params-all-gather ..............................: (31.78, 31.87)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.52, 0.64)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.56, 2.67)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.78, 4.84)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.64, 1.74)
10.64.24.50:     optimizer ......................................: (42.21, 42.31)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:31:50 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             445W / 700W |  40564MiB / 81559MiB |     91%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   46C    P0             467W / 700W |  40612MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             422W / 700W |  40612MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             411W / 700W |  40372MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             418W / 700W |  40312MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   46C    P0             414W / 700W |  40360MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             351W / 700W |  40360MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             351W / 700W |  40120MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 18:31:54 
10.64.24.49: rank 3: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 52, '4096': 232, '8192': 194, '16384': 32, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 938, '256': 972, '512': 923, '1024': 737, '2048': 323, '4096': 203, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 5: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 3, '2048': 53, '4096': 202, '8192': 204, '16384': 50, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 916, '256': 913, '512': 944, '1024': 730, '2048': 370, '4096': 223, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}rank 1: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 52, '4096': 232, '8192': 194, '16384': 32, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 938, '256': 972, '512': 923, '1024': 737, '2048': 323, '4096': 203, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}rank 7: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 3, '2048': 53, '4096': 202, '8192': 204, '16384': 50, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 916, '256': 913, '512': 944, '1024': 730, '2048': 370, '4096': 223, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 2: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 52, '4096': 232, '8192': 194, '16384': 32, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 938, '256': 972, '512': 923, '1024': 737, '2048': 323, '4096': 203, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 6: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 3, '2048': 53, '4096': 202, '8192': 204, '16384': 50, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 916, '256': 913, '512': 944, '1024': 730, '2048': 370, '4096': 223, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.50: rank 8: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 0, '2048': 49, '4096': 231, '8192': 201, '16384': 30, '32768': 1, '>32k': 0}, 'real_seq_len': {'128': 912, '256': 947, '512': 928, '1024': 774, '2048': 331, '4096': 204, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 3, '2048': 53, '4096': 202, '8192': 204, '16384': 50, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 916, '256': 913, '512': 944, '1024': 730, '2048': 370, '4096': 223, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 52, '4096': 232, '8192': 194, '16384': 32, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 938, '256': 972, '512': 923, '1024': 737, '2048': 323, '4096': 203, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen4096_gbs512_mbs8_dp4_tp4_pp1_pack_sp_node0.log...
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen4096_gbs512_mbs8_dp4_tp4_pp1_pack_sp_node1.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 7b model, gpu_num=16, seq_len = 4096, DP=2, MP=2, PP=4, GBS=512, MBS=16
10.64.24.49: use sequence-packing
10.64.24.49: use sequence parallel
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 7b model, gpu_num=16, seq_len = 4096, DP=2, MP=2, PP=4, GBS=512, MBS=16
10.64.24.50: use sequence-packing
10.64.24.50: use sequence parallel
10.64.24.50: [2024-03-11 18:32:01,351] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 18:32:01,351] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 18:32:01,351] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 18:32:01,351] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 18:32:02,324] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 18:32:02,324] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 18:32:02,324] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 18:32:02,324] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 2, pipeline-model-parallel size: 4 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 2
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 32
10.64.24.49:   encoder_seq_length .............................. 4096
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 16384
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 4096
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 4096
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 16
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 32
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 32
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 4
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 4096
10.64.24.49:   sequence_packing ................................ True
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 2
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 4
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ True
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 16
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: > initialized tensor model parallel with size 2
10.64.24.49: > initialized pipeline model parallel with size 4
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.061 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 7.951 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 11.561
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 18:32:30 
10.64.24.49: building GPT model ...
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 805617664
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 805617664
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 805617664
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 805617664
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (805617664 elements):
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (805617664 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 908910592
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 925679616
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 908910592
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 925679616
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (908910592 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (925679616 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 18:32:31 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache end, time cost:  4.930 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.086 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.116 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.140 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.188 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.207 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.218 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.280 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Cutting or padding data end, time cost:  5.122 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  5.037 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  5.125 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  5.067 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  5.002 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  5.083 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  5.205 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  5.191 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 18:32:41 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (41.25, 578.51)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 10582.59)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 18:32:42 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:33:18 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             350W / 700W |  49728MiB / 81559MiB |     13%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             295W / 700W |  50120MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             223W / 700W |  59208MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             251W / 700W |  59006MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             313W / 700W |  42806MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             255W / 700W |  42810MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             250W / 700W |  47436MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             257W / 700W |  47700MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 5267.9 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.094827E+01 | loss scale: 1.0 | grad norm: 13.020 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: [Rank 8] (after 10 iterations) memory (MB) | allocated: 9499.220703125 | max allocated: 27428.935546875 | reserved: 31682.0 | max reserved: 31682.0
10.64.24.50: [Rank 12] (after 10 iterations) memory (MB) | allocated: 10681.2841796875 | max allocated: 37309.9267578125 | reserved: 40502.0 | max reserved: 40502.0
10.64.24.49: [Rank 5] (after 10 iterations) memory (MB) | allocated: 9499.396484375 | max allocated: 32703.4208984375 | reserved: 39476.0 | max reserved: 39476.0
10.64.24.49: [Rank 4] (after 10 iterations) memory (MB) | allocated: 9499.248046875 | max allocated: 32702.5576171875 | reserved: 39472.0 | max reserved: 39472.0
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 10806.220703125 | max allocated: 39894.10302734375 | reserved: 46700.0 | max reserved: 46700.0[Rank 0] (after 10 iterations) memory (MB) | allocated: 10806.220703125 | max allocated: 39894.75146484375 | reserved: 46308.0 | max reserved: 46308.0
10.64.24.49: 
10.64.24.50: [Rank 13] (after 10 iterations) memory (MB) | allocated: 10681.2841796875 | max allocated: 37309.9970703125 | reserved: 40502.0 | max reserved: 40502.0
10.64.24.50: [Rank 9] (after 10 iterations) memory (MB) | allocated: 9499.220703125 | max allocated: 27428.845703125 | reserved: 32046.0 | max reserved: 32046.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (4574.21, 4835.23)
10.64.24.50:     forward-compute ................................: (808.61, 1973.93)
10.64.24.50:     backward-compute ...............................: (1074.53, 1965.20)
10.64.24.50:     batch-generator ................................: (125.10, 159.77)
10.64.24.50:     forward-recv ...................................: (217.49, 658.75)
10.64.24.50:     forward-send ...................................: (5.51, 357.47)
10.64.24.50:     backward-recv ..................................: (145.35, 434.44)
10.64.24.50:     backward-send ..................................: (1.15, 16.60)
10.64.24.50:     forward-send-backward-recv .....................: (1883.98, 1974.53)
10.64.24.50:     backward-send-forward-recv .....................: (46.42, 173.36)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.84, 1.24)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 9.03)
10.64.24.50:     all-grads-sync .................................: (220.96, 236.45)
10.64.24.50:     params-all-gather ..............................: (5.22, 6.04)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.30, 0.38)
10.64.24.50:     optimizer-clip-main-grad .......................: (6.24, 6.45)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.64, 5.53)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.43, 1.65)
10.64.24.50:     optimizer ......................................: (20.15, 20.94)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 3719.6 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.094142E+01 | loss scale: 1.0 | grad norm: 60.903 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (3445.23, 3651.21)
10.64.24.50:     forward-compute ................................: (551.49, 1559.02)
10.64.24.50:     backward-compute ...............................: (926.90, 1780.09)
10.64.24.50:     batch-generator ................................: (11.76, 15.68)
10.64.24.50:     forward-recv ...................................: (48.36, 120.56)
10.64.24.50:     forward-send ...................................: (0.81, 27.99)
10.64.24.50:     backward-recv ..................................: (128.88, 397.70)
10.64.24.50:     backward-send ..................................: (1.03, 18.73)
10.64.24.50:     forward-send-backward-recv .....................: (1554.32, 1682.71)
10.64.24.50:     backward-send-forward-recv .....................: (25.96, 118.87)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.78, 0.98)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 9.01)
10.64.24.50:     all-grads-sync .................................: (7.58, 9.30)
10.64.24.50:     params-all-gather ..............................: (5.18, 6.04)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.28, 0.35)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.39, 2.60)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.52, 5.24)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.42, 1.64)
10.64.24.50:     optimizer ......................................: (15.52, 16.37)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 3993.8 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.068829E+01 | loss scale: 1.0 | grad norm: 5.719 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (3663.73, 3920.69)
10.64.24.50:     forward-compute ................................: (629.50, 1620.76)
10.64.24.50:     backward-compute ...............................: (1064.79, 1912.64)
10.64.24.50:     batch-generator ................................: (11.83, 16.16)
10.64.24.50:     forward-recv ...................................: (54.04, 122.16)
10.64.24.50:     forward-send ...................................: (0.96, 25.45)
10.64.24.50:     backward-recv ..................................: (150.72, 467.84)
10.64.24.50:     backward-send ..................................: (1.15, 23.87)
10.64.24.50:     forward-send-backward-recv .....................: (1518.51, 1683.74)
10.64.24.50:     backward-send-forward-recv .....................: (29.00, 171.10)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.78, 0.96)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 8.97)
10.64.24.50:     all-grads-sync .................................: (7.66, 9.06)
10.64.24.50:     params-all-gather ..............................: (5.24, 6.03)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.28, 0.35)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.38, 2.58)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.52, 5.25)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.42, 1.64)
10.64.24.50:     optimizer ......................................: (15.57, 16.36)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:34:55 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             395W / 700W |  60608MiB / 81559MiB |     36%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             373W / 700W |  60864MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             415W / 700W |  59212MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             422W / 700W |  59010MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             267W / 700W |  53352MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             276W / 700W |  54468MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             257W / 700W |  47436MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             297W / 700W |  47700MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 18:35:00 rank 5: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 0, '2048': 0, '4096': 13, '8192': 240, '16384': 238, '32768': 21, '>32k': 0}, 'real_seq_len': {'128': 1854, '256': 1885, '512': 1867, '1024': 1467, '2048': 693, '4096': 426, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}rank 1: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 0, '2048': 0, '4096': 13, '8192': 240, '16384': 238, '32768': 21, '>32k': 0}, 'real_seq_len': {'128': 1854, '256': 1885, '512': 1867, '1024': 1467, '2048': 693, '4096': 426, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: 
10.64.24.49: rank 7: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 0, '2048': 0, '4096': 18, '8192': 213, '16384': 262, '32768': 19, '>32k': 0}, 'real_seq_len': {'128': 1811, '256': 1902, '512': 1846, '1024': 1545, '2048': 664, '4096': 424, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 3: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 0, '2048': 0, '4096': 18, '8192': 213, '16384': 262, '32768': 19, '>32k': 0}, 'real_seq_len': {'128': 1811, '256': 1902, '512': 1846, '1024': 1545, '2048': 664, '4096': 424, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 0, '2048': 0, '4096': 13, '8192': 240, '16384': 238, '32768': 21, '>32k': 0}, 'real_seq_len': {'128': 1854, '256': 1885, '512': 1867, '1024': 1467, '2048': 693, '4096': 426, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 6: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 0, '2048': 0, '4096': 18, '8192': 213, '16384': 262, '32768': 19, '>32k': 0}, 'real_seq_len': {'128': 1811, '256': 1902, '512': 1846, '1024': 1545, '2048': 664, '4096': 424, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 0, '2048': 0, '4096': 13, '8192': 240, '16384': 238, '32768': 21, '>32k': 0}, 'real_seq_len': {'128': 1854, '256': 1885, '512': 1867, '1024': 1467, '2048': 693, '4096': 426, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 2: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 0, '2048': 0, '4096': 18, '8192': 213, '16384': 262, '32768': 19, '>32k': 0}, 'real_seq_len': {'128': 1811, '256': 1902, '512': 1846, '1024': 1545, '2048': 664, '4096': 424, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 0, '2048': 0, '4096': 13, '8192': 240, '16384': 238, '32768': 21, '>32k': 0}, 'real_seq_len': {'128': 1854, '256': 1885, '512': 1867, '1024': 1467, '2048': 693, '4096': 426, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen4096_gbs512_mbs16_dp2_tp2_pp4_pack_sp_node1.log...
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 13b model, gpu_num=16, seq_len = 16384, DP=1, MP=4, PP=4, GBS=512, MBS=4
10.64.24.50: use sequence-packing
10.64.24.50: use sequence parallel
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen4096_gbs512_mbs16_dp2_tp2_pp4_pack_sp_node0.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 13b model, gpu_num=16, seq_len = 16384, DP=1, MP=4, PP=4, GBS=512, MBS=4
10.64.24.49: use sequence-packing
10.64.24.49: use sequence parallel
10.64.24.50: [2024-03-11 18:35:09,898] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 18:35:09,898] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 18:35:09,898] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 18:35:09,898] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 18:35:11,632] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 18:35:11,632] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 18:35:11,632] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 18:35:11,632] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 1, context-parallel size: 1 tensor-model-parallel size: 4, pipeline-model-parallel size: 4 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 1
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 40
10.64.24.49:   encoder_seq_length .............................. 16384
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 20480
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 5120
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 16384
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 4
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 40
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 40
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 4
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 16384
10.64.24.49:   sequence_packing ................................ True
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 4
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 4
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ True
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 128
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 431 dummy tokens (new size: 50688)
10.64.24.49: > initializing torch distributed ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: > initialized tensor model parallel with size 4
10.64.24.49: > initialized pipeline model parallel with size 4
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.061 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.724 seconds
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 11.818
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 18:35:39 
10.64.24.49: building GPT model ...
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 786828800
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 786828800
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 786828800
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 786828800
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (3, 2): 786828800
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (2, 2): 786828800
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 786828800
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 786828800
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (786828800 elements):
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (786828800 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (3, 3): 851719680
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 935595520
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 935595520
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (2, 3): 851719680
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 851719680
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 935595520
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 851719680
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 935595520
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (851719680 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (935595520 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 18:35:40 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache end, time cost:  4.885 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Loading exists cache end, time cost:  4.909 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  4.970 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  4.983 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Cutting or padding data end, time cost:  18.615 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  18.701 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  18.646 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  18.761 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 18:36:04 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (52.88, 1307.75)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 24156.76)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 18:36:05 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:37:48 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             282W / 700W |  49344MiB / 81559MiB |     19%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             304W / 700W |  49632MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             292W / 700W |  49846MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             246W / 700W |  49856MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             286W / 700W |  44428MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             302W / 700W |  44754MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             219W / 700W |  44612MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             258W / 700W |  44538MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 16161.5 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.086542E+01 | loss scale: 1.0 | grad norm: 9.419 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: [Rank 9] (after 10 iterations) memory (MB) | allocated: 13933.9169921875 | max allocated: 32521.77978515625 | reserved: 49214.0 | max reserved: 49214.0[Rank 11] (after 10 iterations) memory (MB) | allocated: 13935.8076171875 | max allocated: 32521.24755859375 | reserved: 49590.0 | max reserved: 49590.0[Rank 8] (after 10 iterations) memory (MB) | allocated: 13933.9169921875 | max allocated: 32520.69921875 | reserved: 49850.0 | max reserved: 49850.0[Rank 10] (after 10 iterations) memory (MB) | allocated: 13936.5029296875 | max allocated: 32518.96435546875 | reserved: 49434.0 | max reserved: 49434.0
10.64.24.50: 
10.64.24.50: 
10.64.24.50: 
10.64.24.50: [Rank 13] (after 10 iterations) memory (MB) | allocated: 15039.1484375 | max allocated: 38588.98876953125 | reserved: 41722.0 | max reserved: 41722.0
10.64.24.50: [Rank 14] (after 10 iterations) memory (MB) | allocated: 15039.1484375 | max allocated: 38589.88720703125 | reserved: 41722.0 | max reserved: 41722.0
10.64.24.50: [Rank 12] (after 10 iterations) memory (MB) | allocated: 15039.85546875 | max allocated: 38589.77392578125 | reserved: 41722.0 | max reserved: 41722.0[Rank 15] (after 10 iterations) memory (MB) | allocated: 15039.1484375 | max allocated: 38589.13720703125 | reserved: 41722.0 | max reserved: 41722.0
10.64.24.50: 
10.64.24.49: [Rank 6] (after 10 iterations) memory (MB) | allocated: 13944.9267578125 | max allocated: 34229.48828125 | reserved: 52120.0 | max reserved: 52120.0[Rank 5] (after 10 iterations) memory (MB) | allocated: 13943.7841796875 | max allocated: 34224.974609375 | reserved: 52260.0 | max reserved: 52260.0[Rank 3] (after 10 iterations) memory (MB) | allocated: 16490.5732421875 | max allocated: 37848.830078125 | reserved: 57892.0 | max reserved: 57892.0[Rank 2] (after 10 iterations) memory (MB) | allocated: 16490.5732421875 | max allocated: 37844.61181640625 | reserved: 57642.0 | max reserved: 57642.0
10.64.24.49: 
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 16490.5732421875 | max allocated: 37847.119140625 | reserved: 57188.0 | max reserved: 57188.0
10.64.24.49: 
10.64.24.49: [Rank 7] (after 10 iterations) memory (MB) | allocated: 13941.9267578125 | max allocated: 34229.0009765625 | reserved: 52284.0 | max reserved: 52284.0
10.64.24.49: [Rank 4] (after 10 iterations) memory (MB) | allocated: 13946.61669921875 | max allocated: 34232.1572265625 | reserved: 51982.0 | max reserved: 51982.0
10.64.24.49: 
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 16490.5732421875 | max allocated: 37844.556640625 | reserved: 57428.0 | max reserved: 57428.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (15811.02, 15961.34)
10.64.24.50:     forward-compute ................................: (2717.52, 6021.27)
10.64.24.50:     backward-compute ...............................: (3543.08, 7489.08)
10.64.24.50:     batch-generator ................................: (345.47, 379.34)
10.64.24.50:     forward-recv ...................................: (330.14, 895.85)
10.64.24.50:     forward-send ...................................: (6.41, 556.83)
10.64.24.50:     backward-recv ..................................: (67.09, 318.36)
10.64.24.50:     backward-send ..................................: (8.29, 38.60)
10.64.24.50:     forward-send-backward-recv .....................: (7062.15, 8144.38)
10.64.24.50:     backward-send-forward-recv .....................: (1316.56, 1770.15)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.98, 1.14)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 5.84)
10.64.24.50:     all-grads-sync .................................: (60.73, 69.08)
10.64.24.50:     params-all-gather ..............................: (2.64, 3.09)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.69, 0.77)
10.64.24.50:     optimizer-clip-main-grad .......................: (7.46, 7.93)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.04, 10.96)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.86, 3.34)
10.64.24.50:     optimizer ......................................: (26.17, 26.62)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 13244.2 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.082763E+01 | loss scale: 1.0 | grad norm: 5.940 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (13138.15, 13208.72)
10.64.24.50:     forward-compute ................................: (2157.18, 5414.83)
10.64.24.50:     backward-compute ...............................: (3012.63, 6944.35)
10.64.24.50:     batch-generator ................................: (102.24, 120.10)
10.64.24.50:     forward-recv ...................................: (31.04, 54.04)
10.64.24.50:     forward-send ...................................: (0.37, 14.83)
10.64.24.50:     backward-recv ..................................: (62.95, 197.44)
10.64.24.50:     backward-send ..................................: (0.57, 14.06)
10.64.24.50:     forward-send-backward-recv .....................: (6898.27, 7272.24)
10.64.24.50:     backward-send-forward-recv .....................: (697.84, 923.76)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.93, 1.02)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 5.74)
10.64.24.50:     all-grads-sync .................................: (2.12, 2.51)
10.64.24.50:     params-all-gather ..............................: (2.63, 3.08)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.66, 0.74)
10.64.24.50:     optimizer-clip-main-grad .......................: (4.60, 5.06)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (8.78, 10.44)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.86, 3.33)
10.64.24.50:     optimizer ......................................: (22.72, 23.18)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 14163.9 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.076530E+01 | loss scale: 1.0 | grad norm: 2.473 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (14063.22, 14128.42)
10.64.24.50:     forward-compute ................................: (2374.29, 5625.87)
10.64.24.50:     backward-compute ...............................: (3412.23, 7344.57)
10.64.24.50:     batch-generator ................................: (103.79, 121.93)
10.64.24.50:     forward-recv ...................................: (33.25, 62.22)
10.64.24.50:     forward-send ...................................: (0.42, 22.76)
10.64.24.50:     backward-recv ..................................: (61.13, 172.51)
10.64.24.50:     backward-send ..................................: (4.80, 19.94)
10.64.24.50:     forward-send-backward-recv .....................: (6661.66, 7529.94)
10.64.24.50:     backward-send-forward-recv .....................: (993.20, 1462.44)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.93, 1.01)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 5.77)
10.64.24.50:     all-grads-sync .................................: (2.12, 2.51)
10.64.24.50:     params-all-gather ..............................: (2.64, 3.16)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.67, 0.74)
10.64.24.50:     optimizer-clip-main-grad .......................: (4.59, 5.06)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (8.79, 10.45)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.86, 3.33)
10.64.24.50:     optimizer ......................................: (22.71, 23.24)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:43:34 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             254W / 700W |  60408MiB / 81559MiB |     78%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             218W / 700W |  60696MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             257W / 700W |  60910MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   35C    P0             203W / 700W |  60920MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             205W / 700W |  55102MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             206W / 700W |  55754MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             222W / 700W |  55612MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             254W / 700W |  55212MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 18:43:48 
10.64.24.49: rank 6: {'packing_seq_len': {'128': 0, '256': 2, '512': 87, '1024': 757, '2048': 1555, '4096': 1162, '8192': 359, '16384': 133, '32768': 39, '>32k': 2}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}rank 2: {'packing_seq_len': {'128': 0, '256': 2, '512': 87, '1024': 757, '2048': 1555, '4096': 1162, '8192': 359, '16384': 133, '32768': 39, '>32k': 2}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}rank 3: {'packing_seq_len': {'128': 0, '256': 2, '512': 87, '1024': 757, '2048': 1555, '4096': 1162, '8192': 359, '16384': 133, '32768': 39, '>32k': 2}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}rank 5: {'packing_seq_len': {'128': 0, '256': 2, '512': 87, '1024': 757, '2048': 1555, '4096': 1162, '8192': 359, '16384': 133, '32768': 39, '>32k': 2}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}rank 7: {'packing_seq_len': {'128': 0, '256': 2, '512': 87, '1024': 757, '2048': 1555, '4096': 1162, '8192': 359, '16384': 133, '32768': 39, '>32k': 2}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: 
10.64.24.49: 
10.64.24.49: 
10.64.24.49: rank 1: {'packing_seq_len': {'128': 0, '256': 2, '512': 87, '1024': 757, '2048': 1555, '4096': 1162, '8192': 359, '16384': 133, '32768': 39, '>32k': 2}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 0, '256': 2, '512': 87, '1024': 757, '2048': 1555, '4096': 1162, '8192': 359, '16384': 133, '32768': 39, '>32k': 2}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 0, '256': 2, '512': 87, '1024': 757, '2048': 1555, '4096': 1162, '8192': 359, '16384': 133, '32768': 39, '>32k': 2}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 0, '256': 2, '512': 87, '1024': 757, '2048': 1555, '4096': 1162, '8192': 359, '16384': 133, '32768': 39, '>32k': 2}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen16384_gbs512_mbs4_dp1_tp4_pp4_pack_sp_node1.log...
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen16384_gbs512_mbs4_dp1_tp4_pp4_pack_sp_node0.log...
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 13b model, gpu_num=16, seq_len = 16384, DP=2, MP=2, PP=4, GBS=512, MBS=2
10.64.24.50: use sequence-packing
10.64.24.50: use sequence parallel
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 13b model, gpu_num=16, seq_len = 16384, DP=2, MP=2, PP=4, GBS=512, MBS=2
10.64.24.49: use sequence-packing
10.64.24.49: use sequence parallel
10.64.24.50: [2024-03-11 18:44:03,870] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 18:44:03,870] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 18:44:03,870] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 18:44:03,870] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 18:44:04,808] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 18:44:04,808] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 18:44:04,808] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 18:44:04,808] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 2, pipeline-model-parallel size: 4 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 2
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 40
10.64.24.49:   encoder_seq_length .............................. 16384
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 20480
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 5120
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 16384
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 2
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 40
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 40
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 4
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 16384
10.64.24.49:   sequence_packing ................................ True
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 2
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 4
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ True
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 128
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: > initialized tensor model parallel with size 2
10.64.24.49: > initialized pipeline model parallel with size 4
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.060 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 7.929 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 10.903
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 18:44:33 
10.64.24.49: building GPT model ...
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1573350400
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 1573350400
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1573350400
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1573350400
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1573350400 elements):
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1573350400 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 1702466560
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1786342400
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1702466560
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1786342400
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1702466560 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1786342400 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 18:44:34 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache end, time cost:  5.040 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.039 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.040 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.083 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.088 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.129 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.237 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.310 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Cutting or padding data end, time cost:  18.620 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  18.662 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  18.701 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  18.873 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  18.839 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  18.888 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  18.783 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  18.858 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 18:44:59 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (60.67, 1431.79)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 24576.27)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 18:44:59 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:47:01 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             271W / 700W |  63212MiB / 81559MiB |     69%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             233W / 700W |  63152MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             302W / 700W |  56764MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   34C    P0             292W / 700W |  56978MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             225W / 700W |  58230MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             207W / 700W |  58524MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             251W / 700W |  52418MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             262W / 700W |  52360MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 19223.7 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.086404E+01 | loss scale: 1.0 | grad norm: 4.851 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: [Rank 9] (after 10 iterations) memory (MB) | allocated: 18303.0234375 | max allocated: 48203.69140625 | reserved: 52368.0 | max reserved: 52368.0
10.64.24.50: [Rank 8] (after 10 iterations) memory (MB) | allocated: 18302.89453125 | max allocated: 48201.9140625 | reserved: 52252.0 | max reserved: 52252.0
10.64.24.50: [Rank 13] (after 10 iterations) memory (MB) | allocated: 19780.556640625 | max allocated: 44388.46435546875 | reserved: 48402.0 | max reserved: 48402.0
10.64.24.50: [Rank 12] (after 10 iterations) memory (MB) | allocated: 19780.4345703125 | max allocated: 44387.78076171875 | reserved: 48082.0 | max reserved: 48082.0
10.64.24.49: [Rank 4] (after 10 iterations) memory (MB) | allocated: 18303.701171875 | max allocated: 51744.12744140625 | reserved: 54924.0 | max reserved: 54924.0[Rank 5] (after 10 iterations) memory (MB) | allocated: 18303.37109375 | max allocated: 51741.11376953125 | reserved: 55216.0 | max reserved: 55216.0[Rank 0] (after 10 iterations) memory (MB) | allocated: 20736.638671875 | max allocated: 57025.46484375 | reserved: 59786.0 | max reserved: 59786.0
10.64.24.49: 
10.64.24.49: 
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 20736.638671875 | max allocated: 57024.189453125 | reserved: 59726.0 | max reserved: 59726.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (18566.78, 18733.96)
10.64.24.50:     forward-compute ................................: (2450.45, 8979.69)
10.64.24.50:     backward-compute ...............................: (3778.98, 7313.76)
10.64.24.50:     batch-generator ................................: (206.08, 265.84)
10.64.24.50:     forward-recv ...................................: (207.95, 527.94)
10.64.24.50:     forward-send ...................................: (3.98, 293.53)
10.64.24.50:     backward-recv ..................................: (90.38, 410.49)
10.64.24.50:     backward-send ..................................: (9.30, 44.32)
10.64.24.50:     forward-send-backward-recv .....................: (9709.52, 11387.11)
10.64.24.50:     backward-send-forward-recv .....................: (1261.91, 2004.06)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.03, 1.24)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 11.68)
10.64.24.50:     all-grads-sync .................................: (229.41, 245.25)
10.64.24.50:     params-all-gather ..............................: (9.72, 11.23)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.36, 0.46)
10.64.24.50:     optimizer-clip-main-grad .......................: (8.32, 8.65)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (8.97, 10.25)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.59, 2.95)
10.64.24.50:     optimizer ......................................: (32.74, 34.22)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 15293.1 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.082072E+01 | loss scale: 1.0 | grad norm: 1.971 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (15125.24, 15221.22)
10.64.24.50:     forward-compute ................................: (2059.72, 7313.30)
10.64.24.50:     backward-compute ...............................: (3302.54, 6828.38)
10.64.24.50:     batch-generator ................................: (95.99, 118.40)
10.64.24.50:     forward-recv ...................................: (20.29, 67.47)
10.64.24.50:     forward-send ...................................: (0.31, 21.25)
10.64.24.50:     backward-recv ..................................: (74.75, 264.25)
10.64.24.50:     backward-send ..................................: (0.59, 27.60)
10.64.24.50:     forward-send-backward-recv .....................: (8310.59, 9200.04)
10.64.24.50:     backward-send-forward-recv .....................: (764.82, 1212.42)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.94, 1.03)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 11.22)
10.64.24.50:     all-grads-sync .................................: (14.71, 16.94)
10.64.24.50:     params-all-gather ..............................: (9.53, 11.21)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.34, 0.52)
10.64.24.50:     optimizer-clip-main-grad .......................: (4.26, 4.60)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.02)
10.64.24.50:     optimizer-inner-step ...........................: (8.70, 9.92)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.59, 2.95)
10.64.24.50:     optimizer ......................................: (28.01, 29.70)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 16951.4 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.075896E+01 | loss scale: 1.0 | grad norm: 1.150 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (16792.02, 16872.52)
10.64.24.50:     forward-compute ................................: (2185.35, 8345.58)
10.64.24.50:     backward-compute ...............................: (3589.20, 7261.97)
10.64.24.50:     batch-generator ................................: (93.33, 119.84)
10.64.24.50:     forward-recv ...................................: (19.68, 77.91)
10.64.24.50:     forward-send ...................................: (0.33, 34.67)
10.64.24.50:     backward-recv ..................................: (57.28, 267.76)
10.64.24.50:     backward-send ..................................: (0.61, 28.00)
10.64.24.50:     forward-send-backward-recv .....................: (8719.72, 10388.60)
10.64.24.50:     backward-send-forward-recv .....................: (980.20, 1783.99)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.95, 1.03)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 11.20)
10.64.24.50:     all-grads-sync .................................: (14.65, 17.05)
10.64.24.50:     params-all-gather ..............................: (9.72, 11.20)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.34, 0.44)
10.64.24.50:     optimizer-clip-main-grad .......................: (4.76, 5.09)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (8.70, 9.91)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.59, 2.95)
10.64.24.50:     optimizer ......................................: (28.64, 30.12)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:53:50 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   35C    P0             290W / 700W |  68412MiB / 81559MiB |     32%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             274W / 700W |  68750MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             318W / 700W |  76212MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   34C    P0             252W / 700W |  76212MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             296W / 700W |  64486MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             269W / 700W |  64778MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             241W / 700W |  69248MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             231W / 700W |  68956MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 18:54:05 
10.64.24.49: rank 5: {'packing_seq_len': {'128': 45, '256': 365, '512': 983, '1024': 1247, '2048': 889, '4096': 386, '8192': 117, '16384': 47, '32768': 17, '>32k': 0}, 'real_seq_len': {'128': 1846, '256': 1920, '512': 1806, '1024': 1532, '2048': 669, '4096': 263, '8192': 106, '16384': 50, '32768': 0, '>32k': 0}}
10.64.24.49: rank 1: {'packing_seq_len': {'128': 45, '256': 365, '512': 983, '1024': 1247, '2048': 889, '4096': 386, '8192': 117, '16384': 47, '32768': 17, '>32k': 0}, 'real_seq_len': {'128': 1846, '256': 1920, '512': 1806, '1024': 1532, '2048': 669, '4096': 263, '8192': 106, '16384': 50, '32768': 0, '>32k': 0}}rank 7: {'packing_seq_len': {'128': 32, '256': 352, '512': 961, '1024': 1221, '2048': 977, '4096': 357, '8192': 128, '16384': 51, '32768': 17, '>32k': 0}, 'real_seq_len': {'128': 1819, '256': 1867, '512': 1907, '1024': 1480, '2048': 688, '4096': 262, '8192': 108, '16384': 61, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 3: {'packing_seq_len': {'128': 32, '256': 352, '512': 961, '1024': 1221, '2048': 977, '4096': 357, '8192': 128, '16384': 51, '32768': 17, '>32k': 0}, 'real_seq_len': {'128': 1819, '256': 1867, '512': 1907, '1024': 1480, '2048': 688, '4096': 262, '8192': 108, '16384': 61, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 45, '256': 365, '512': 983, '1024': 1247, '2048': 889, '4096': 386, '8192': 117, '16384': 47, '32768': 17, '>32k': 0}, 'real_seq_len': {'128': 1846, '256': 1920, '512': 1806, '1024': 1532, '2048': 669, '4096': 263, '8192': 106, '16384': 50, '32768': 0, '>32k': 0}}
10.64.24.49: rank 6: {'packing_seq_len': {'128': 32, '256': 352, '512': 961, '1024': 1221, '2048': 977, '4096': 357, '8192': 128, '16384': 51, '32768': 17, '>32k': 0}, 'real_seq_len': {'128': 1819, '256': 1867, '512': 1907, '1024': 1480, '2048': 688, '4096': 262, '8192': 108, '16384': 61, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 45, '256': 365, '512': 983, '1024': 1247, '2048': 889, '4096': 386, '8192': 117, '16384': 47, '32768': 17, '>32k': 0}, 'real_seq_len': {'128': 1846, '256': 1920, '512': 1806, '1024': 1532, '2048': 669, '4096': 263, '8192': 106, '16384': 50, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 45, '256': 365, '512': 983, '1024': 1247, '2048': 889, '4096': 386, '8192': 117, '16384': 47, '32768': 17, '>32k': 0}, 'real_seq_len': {'128': 1846, '256': 1920, '512': 1806, '1024': 1532, '2048': 669, '4096': 263, '8192': 106, '16384': 50, '32768': 0, '>32k': 0}}
10.64.24.49: rank 2: {'packing_seq_len': {'128': 32, '256': 352, '512': 961, '1024': 1221, '2048': 977, '4096': 357, '8192': 128, '16384': 51, '32768': 17, '>32k': 0}, 'real_seq_len': {'128': 1819, '256': 1867, '512': 1907, '1024': 1480, '2048': 688, '4096': 262, '8192': 108, '16384': 61, '32768': 0, '>32k': 0}}
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen16384_gbs512_mbs2_dp2_tp2_pp4_pack_sp_node0.log...
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen16384_gbs512_mbs2_dp2_tp2_pp4_pack_sp_node1.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 13b model, gpu_num=16, seq_len = 8192, DP=2, MP=2, PP=4, GBS=512, MBS=4
10.64.24.49: use sequence-packing
10.64.24.49: use sequence parallel
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 13b model, gpu_num=16, seq_len = 8192, DP=2, MP=2, PP=4, GBS=512, MBS=4
10.64.24.50: use sequence-packing
10.64.24.50: use sequence parallel
10.64.24.49: [2024-03-11 18:54:23,778] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 18:54:23,778] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 18:54:23,778] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 18:54:23,778] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 18:54:22,906] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 18:54:22,906] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 18:54:22,906] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 18:54:22,906] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 2, pipeline-model-parallel size: 4 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 2
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 40
10.64.24.49:   encoder_seq_length .............................. 8192
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 20480
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 5120
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 8192
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 4
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 40
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 40
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 4
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 8192
10.64.24.49:   sequence_packing ................................ True
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 2
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 4
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ True
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 64
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: > initialized tensor model parallel with size 2
10.64.24.49: > initialized pipeline model parallel with size 4
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.060 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.568 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 10.812
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 18:54:52 
10.64.24.49: building GPT model ...
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1573350400
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 1573350400
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1573350400 elements):
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1573350400
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1573350400
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1573350400 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 1702466560
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1744399360
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1702466560
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1744399360
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1702466560 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1744399360 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 18:54:53 
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache end, time cost:  4.979 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  4.993 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.095 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.113 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.206 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.228 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.287 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.341 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Cutting or padding data end, time cost:  10.412 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.537 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.642 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.761 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.709 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.744 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.832 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  11.320 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 18:55:10 
10.64.24.49: done with setup ...
10.64.24.49: training ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (48.74, 1029.68)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 17021.48)
10.64.24.49: [before the start of training step] datetime: 2024-03-11 18:55:10 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 18:56:27 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             198W / 700W |  60526MiB / 81559MiB |     17%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             199W / 700W |  60212MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             316W / 700W |  57898MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             314W / 700W |  58190MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             163W / 700W |  53584MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             170W / 700W |  53358MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             304W / 700W |  51878MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             247W / 700W |  51886MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 12153.7 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.089456E+01 | loss scale: 1.0 | grad norm: 8.874 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: [Rank 9] (after 10 iterations) memory (MB) | allocated: 18302.662109375 | max allocated: 36647.08203125 | reserved: 47674.0 | max reserved: 47674.0
10.64.24.50: [Rank 13] (after 10 iterations) memory (MB) | allocated: 19780.2783203125 | max allocated: 44462.34375 | reserved: 48128.0 | max reserved: 48128.0
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 20229.080078125 | max allocated: 50834.19873046875 | reserved: 57100.0 | max reserved: 57100.0
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 20229.080078125 | max allocated: 50827.48095703125 | reserved: 56786.0 | max reserved: 56786.0
10.64.24.49: [Rank 4] (after 10 iterations) memory (MB) | allocated: 18302.759765625 | max allocated: 38380.6884765625 | reserved: 50242.0 | max reserved: 50242.0
10.64.24.49: [Rank 5] (after 10 iterations) memory (MB) | allocated: 18302.662109375 | max allocated: 38376.0703125 | reserved: 50016.0 | max reserved: 50016.0
10.64.24.50: [Rank 8] (after 10 iterations) memory (MB) | allocated: 18302.662109375 | max allocated: 36644.921875 | reserved: 47448.0 | max reserved: 47448.0
10.64.24.50: [Rank 12] (after 10 iterations) memory (MB) | allocated: 19780.2783203125 | max allocated: 44462.3115234375 | reserved: 48128.0 | max reserved: 48128.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (11440.87, 11651.12)
10.64.24.50:     forward-compute ................................: (1692.30, 5442.12)
10.64.24.50:     backward-compute ...............................: (2742.91, 4685.96)
10.64.24.50:     batch-generator ................................: (165.01, 213.62)
10.64.24.50:     forward-recv ...................................: (209.80, 579.45)
10.64.24.50:     forward-send ...................................: (4.28, 348.83)
10.64.24.50:     backward-recv ..................................: (101.24, 457.34)
10.64.24.50:     backward-send ..................................: (0.83, 43.25)
10.64.24.50:     forward-send-backward-recv .....................: (4763.42, 6108.94)
10.64.24.50:     backward-send-forward-recv .....................: (864.27, 1427.92)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.99, 1.24)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 11.13)
10.64.24.50:     all-grads-sync .................................: (222.79, 248.25)
10.64.24.50:     params-all-gather ..............................: (9.53, 11.05)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.36, 0.45)
10.64.24.50:     optimizer-clip-main-grad .......................: (7.71, 8.01)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (8.95, 10.10)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.59, 2.88)
10.64.24.50:     optimizer ......................................: (31.85, 33.37)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 9408.9 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.081682E+01 | loss scale: 1.0 | grad norm: 3.388 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (9168.94, 9308.27)
10.64.24.50:     forward-compute ................................: (1390.62, 4319.61)
10.64.24.50:     backward-compute ...............................: (2422.61, 4194.80)
10.64.24.50:     batch-generator ................................: (46.81, 60.40)
10.64.24.50:     forward-recv ...................................: (28.91, 84.31)
10.64.24.50:     forward-send ...................................: (0.43, 32.56)
10.64.24.50:     backward-recv ..................................: (55.07, 323.51)
10.64.24.50:     backward-send ..................................: (1.67, 47.10)
10.64.24.50:     forward-send-backward-recv .....................: (4215.50, 4896.72)
10.64.24.50:     backward-send-forward-recv .....................: (578.68, 961.39)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.94, 1.15)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 11.14)
10.64.24.50:     all-grads-sync .................................: (14.85, 16.79)
10.64.24.50:     params-all-gather ..............................: (9.57, 10.99)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.34, 0.44)
10.64.24.50:     optimizer-clip-main-grad .......................: (4.25, 4.81)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (8.71, 9.74)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.58, 2.87)
10.64.24.50:     optimizer ......................................: (28.18, 29.60)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 10444.5 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.069445E+01 | loss scale: 1.0 | grad norm: 1.820 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (10195.50, 10377.15)
10.64.24.50:     forward-compute ................................: (1554.37, 4892.71)
10.64.24.50:     backward-compute ...............................: (2711.51, 4520.19)
10.64.24.50:     batch-generator ................................: (47.71, 60.56)
10.64.24.50:     forward-recv ...................................: (34.47, 83.39)
10.64.24.50:     forward-send ...................................: (0.54, 24.52)
10.64.24.50:     backward-recv ..................................: (48.72, 376.71)
10.64.24.50:     backward-send ..................................: (0.63, 61.11)
10.64.24.50:     forward-send-backward-recv .....................: (4483.36, 5495.04)
10.64.24.50:     backward-send-forward-recv .....................: (750.55, 1267.57)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.94, 1.21)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 11.13)
10.64.24.50:     all-grads-sync .................................: (14.57, 16.68)
10.64.24.50:     params-all-gather ..............................: (9.56, 11.00)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.33, 0.42)
10.64.24.50:     optimizer-clip-main-grad .......................: (4.22, 4.49)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (8.70, 9.69)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.59, 2.88)
10.64.24.50:     optimizer ......................................: (27.52, 28.96)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 19:00:40 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             289W / 700W |  69044MiB / 81559MiB |     80%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             292W / 700W |  68362MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             272W / 700W |  59322MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             420W / 700W |  59890MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             322W / 700W |  63944MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             292W / 700W |  64086MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             204W / 700W |  54940MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             392W / 700W |  54612MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 19:00:50 
10.64.24.49: rank 5: {'packing_seq_len': {'128': 0, '256': 1, '512': 42, '1024': 374, '2048': 778, '4096': 597, '8192': 171, '16384': 81, '32768': 4, '>32k': 0}, 'real_seq_len': {'128': 1826, '256': 1876, '512': 1860, '1024': 1532, '2048': 678, '4096': 265, '8192': 155, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 1: {'packing_seq_len': {'128': 0, '256': 1, '512': 42, '1024': 374, '2048': 778, '4096': 597, '8192': 171, '16384': 81, '32768': 4, '>32k': 0}, 'real_seq_len': {'128': 1826, '256': 1876, '512': 1860, '1024': 1532, '2048': 678, '4096': 265, '8192': 155, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 7: {'packing_seq_len': {'128': 0, '256': 1, '512': 45, '1024': 383, '2048': 777, '4096': 565, '8192': 188, '16384': 84, '32768': 5, '>32k': 0}, 'real_seq_len': {'128': 1839, '256': 1911, '512': 1853, '1024': 1480, '2048': 679, '4096': 260, '8192': 170, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 3: {'packing_seq_len': {'128': 0, '256': 1, '512': 45, '1024': 383, '2048': 777, '4096': 565, '8192': 188, '16384': 84, '32768': 5, '>32k': 0}, 'real_seq_len': {'128': 1839, '256': 1911, '512': 1853, '1024': 1480, '2048': 679, '4096': 260, '8192': 170, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 2: {'packing_seq_len': {'128': 0, '256': 1, '512': 45, '1024': 383, '2048': 777, '4096': 565, '8192': 188, '16384': 84, '32768': 5, '>32k': 0}, 'real_seq_len': {'128': 1839, '256': 1911, '512': 1853, '1024': 1480, '2048': 679, '4096': 260, '8192': 170, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 0, '256': 1, '512': 42, '1024': 374, '2048': 778, '4096': 597, '8192': 171, '16384': 81, '32768': 4, '>32k': 0}, 'real_seq_len': {'128': 1826, '256': 1876, '512': 1860, '1024': 1532, '2048': 678, '4096': 265, '8192': 155, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 6: {'packing_seq_len': {'128': 0, '256': 1, '512': 45, '1024': 383, '2048': 777, '4096': 565, '8192': 188, '16384': 84, '32768': 5, '>32k': 0}, 'real_seq_len': {'128': 1839, '256': 1911, '512': 1853, '1024': 1480, '2048': 679, '4096': 260, '8192': 170, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 0, '256': 1, '512': 42, '1024': 374, '2048': 778, '4096': 597, '8192': 171, '16384': 81, '32768': 4, '>32k': 0}, 'real_seq_len': {'128': 1826, '256': 1876, '512': 1860, '1024': 1532, '2048': 678, '4096': 265, '8192': 155, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 0, '256': 1, '512': 42, '1024': 374, '2048': 778, '4096': 597, '8192': 171, '16384': 81, '32768': 4, '>32k': 0}, 'real_seq_len': {'128': 1826, '256': 1876, '512': 1860, '1024': 1532, '2048': 678, '4096': 265, '8192': 155, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen8192_gbs512_mbs4_dp2_tp2_pp4_pack_sp_node1.log...
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen8192_gbs512_mbs4_dp2_tp2_pp4_pack_sp_node0.log...
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 13b model, gpu_num=16, seq_len = 8192, DP=2, MP=4, PP=2, GBS=512, MBS=2
10.64.24.50: use sequence-packing
10.64.24.50: use sequence parallel
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 13b model, gpu_num=16, seq_len = 8192, DP=2, MP=4, PP=2, GBS=512, MBS=2
10.64.24.49: use sequence-packing
10.64.24.49: use sequence parallel
10.64.24.50: [2024-03-11 19:01:05,698] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 19:01:05,698] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 19:01:05,698] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 19:01:05,698] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 19:01:06,691] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 19:01:06,691] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 19:01:06,691] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 19:01:06,691] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 4, pipeline-model-parallel size: 2 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 2
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 40
10.64.24.49:   encoder_seq_length .............................. 8192
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 20480
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 5120
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 8192
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 2
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 40
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 40
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 2
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 8192
10.64.24.49:   sequence_packing ................................ True
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 4
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 2
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ True
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 128
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 431 dummy tokens (new size: 50688)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: > initialized tensor model parallel with size 4
10.64.24.49: > initialized pipeline model parallel with size 2
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.061 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 7.770 seconds
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 11.756
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 19:01:35 
10.64.24.49: building GPT model ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1680481280
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1638548480
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 1680481280
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 1638548480
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 1680481280
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 1638548480
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1680481280
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1638548480
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1638548480 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1680481280 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 19:01:36 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache end, time cost:  5.050 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.216 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.293 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.424 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Cutting or padding data end, time cost:  10.842 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.699 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.943 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  11.141 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 19:01:53 
10.64.24.49: done with setup ...
10.64.24.49: training ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (1069.24, 1211.18)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 16903.40)
10.64.24.49: [before the start of training step] datetime: 2024-03-11 19:01:53 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 19:03:41 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             260W / 700W |  45172MiB / 81559MiB |     95%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             246W / 700W |  45186MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             223W / 700W |  45282MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   35C    P0             242W / 700W |  44990MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             416W / 700W |  55150MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             381W / 700W |  55922MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             336W / 700W |  56026MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             367W / 700W |  55534MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 17041.7 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.088648E+01 | loss scale: 1.0 | grad norm: 17.493 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: [Rank 9] (after 10 iterations) memory (MB) | allocated: 18969.9638671875 | max allocated: 33121.8603515625 | reserved: 35042.0 | max reserved: 35042.0[Rank 11] (after 10 iterations) memory (MB) | allocated: 18969.9638671875 | max allocated: 33121.8603515625 | reserved: 34910.0 | max reserved: 34910.0
10.64.24.50: 
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 19461.4375 | max allocated: 36294.0166015625 | reserved: 41468.0 | max reserved: 41468.0[Rank 0] (after 10 iterations) memory (MB) | allocated: 19460.4375 | max allocated: 36294.4716796875 | reserved: 41500.0 | max reserved: 41500.0
10.64.24.49: 
10.64.24.50: [Rank 10] (after 10 iterations) memory (MB) | allocated: 18969.9638671875 | max allocated: 33120.8193359375 | reserved: 34630.0 | max reserved: 34630.0
10.64.24.49: [Rank 2] (after 10 iterations) memory (MB) | allocated: 19453.4375 | max allocated: 36286.921875 | reserved: 41564.0 | max reserved: 41564.0
10.64.24.49: [Rank 3] (after 10 iterations) memory (MB) | allocated: 19455.4375 | max allocated: 36287.5322265625 | reserved: 41514.0 | max reserved: 41514.0
10.64.24.50: [Rank 8] (after 10 iterations) memory (MB) | allocated: 18969.9638671875 | max allocated: 33121.4111328125 | reserved: 34368.0 | max reserved: 34368.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (16574.88, 16646.93)
10.64.24.50:     forward-compute ................................: (4680.02, 8961.63)
10.64.24.50:     backward-compute ...............................: (4766.97, 5500.74)
10.64.24.50:     batch-generator ................................: (516.47, 544.15)
10.64.24.50:     forward-recv ...................................: (473.29, 505.31)
10.64.24.50:     forward-send ...................................: (2.76, 8.76)
10.64.24.50:     backward-recv ..................................: (70.40, 80.28)
10.64.24.50:     backward-send ..................................: (9.13, 10.94)
10.64.24.50:     forward-send-backward-recv .....................: (6806.37, 6877.44)
10.64.24.50:     backward-send-forward-recv .....................: (1579.34, 1730.10)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.72, 2.14)
10.64.24.50:     embedding-grads-all-reduce .....................: (5.71, 5.88)
10.64.24.50:     all-grads-sync .................................: (237.40, 247.41)
10.64.24.50:     params-all-gather ..............................: (10.27, 10.91)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.71, 0.96)
10.64.24.50:     optimizer-clip-main-grad .......................: (7.91, 8.01)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.47, 9.98)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.93, 3.09)
10.64.24.50:     optimizer ......................................: (33.68, 34.67)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 12070.8 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.084381E+01 | loss scale: 1.0 | grad norm: 10.721 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (11972.26, 12008.17)
10.64.24.50:     forward-compute ................................: (3801.35, 6209.25)
10.64.24.50:     backward-compute ...............................: (4288.32, 5036.74)
10.64.24.50:     batch-generator ................................: (104.88, 145.02)
10.64.24.50:     forward-recv ...................................: (23.72, 25.41)
10.64.24.50:     forward-send ...................................: (0.29, 0.30)
10.64.24.50:     backward-recv ..................................: (40.03, 51.86)
10.64.24.50:     backward-send ..................................: (0.48, 1.97)
10.64.24.50:     forward-send-backward-recv .....................: (3387.96, 3710.33)
10.64.24.50:     backward-send-forward-recv .....................: (729.85, 1227.18)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.63, 2.02)
10.64.24.50:     embedding-grads-all-reduce .....................: (5.63, 5.71)
10.64.24.50:     all-grads-sync .................................: (15.41, 16.08)
10.64.24.50:     params-all-gather ..............................: (10.40, 10.90)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.67, 0.86)
10.64.24.50:     optimizer-clip-main-grad .......................: (4.72, 4.80)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.18, 9.51)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.93, 3.08)
10.64.24.50:     optimizer ......................................: (29.20, 29.71)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 15530.1 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.070873E+01 | loss scale: 1.0 | grad norm: 5.475 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (15436.72, 15468.61)
10.64.24.50:     forward-compute ................................: (4484.42, 8246.52)
10.64.24.50:     backward-compute ...............................: (4594.99, 5385.48)
10.64.24.50:     batch-generator ................................: (107.30, 146.25)
10.64.24.50:     forward-recv ...................................: (27.14, 32.22)
10.64.24.50:     forward-send ...................................: (0.30, 0.35)
10.64.24.50:     backward-recv ..................................: (34.90, 51.51)
10.64.24.50:     backward-send ..................................: (0.45, 0.47)
10.64.24.50:     forward-send-backward-recv .....................: (6053.10, 6122.06)
10.64.24.50:     backward-send-forward-recv .....................: (1861.55, 1937.24)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.60, 2.23)
10.64.24.50:     embedding-grads-all-reduce .....................: (5.65, 5.76)
10.64.24.50:     all-grads-sync .................................: (15.52, 15.92)
10.64.24.50:     params-all-gather ..............................: (10.28, 10.87)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.66, 0.86)
10.64.24.50:     optimizer-clip-main-grad .......................: (4.73, 4.82)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.17, 9.53)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.93, 3.16)
10.64.24.50:     optimizer ......................................: (29.23, 29.82)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 19:09:32 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             305W / 700W |  51300MiB / 81559MiB |     93%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             309W / 700W |  51154MiB / 81559MiB |     95%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             300W / 700W |  51222MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   35C    P0             261W / 700W |  50640MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             306W / 700W |  65410MiB / 81559MiB |     95%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             309W / 700W |  66454MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             283W / 700W |  66284MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             320W / 700W |  66066MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 19:09:44 
10.64.24.49: rank 6: {'packing_seq_len': {'128': 32, '256': 352, '512': 961, '1024': 1221, '2048': 977, '4096': 357, '8192': 128, '16384': 68, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1819, '256': 1867, '512': 1907, '1024': 1480, '2048': 688, '4096': 262, '8192': 169, '16384': 0, '32768': 0, '>32k': 0}}rank 7: {'packing_seq_len': {'128': 32, '256': 352, '512': 961, '1024': 1221, '2048': 977, '4096': 357, '8192': 128, '16384': 68, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1819, '256': 1867, '512': 1907, '1024': 1480, '2048': 688, '4096': 262, '8192': 169, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 5: {'packing_seq_len': {'128': 32, '256': 352, '512': 961, '1024': 1221, '2048': 977, '4096': 357, '8192': 128, '16384': 68, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1819, '256': 1867, '512': 1907, '1024': 1480, '2048': 688, '4096': 262, '8192': 169, '16384': 0, '32768': 0, '>32k': 0}}rank 1: {'packing_seq_len': {'128': 45, '256': 365, '512': 983, '1024': 1247, '2048': 889, '4096': 386, '8192': 117, '16384': 64, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1846, '256': 1920, '512': 1806, '1024': 1532, '2048': 669, '4096': 263, '8192': 156, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 2: {'packing_seq_len': {'128': 45, '256': 365, '512': 983, '1024': 1247, '2048': 889, '4096': 386, '8192': 117, '16384': 64, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1846, '256': 1920, '512': 1806, '1024': 1532, '2048': 669, '4096': 263, '8192': 156, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 3: {'packing_seq_len': {'128': 45, '256': 365, '512': 983, '1024': 1247, '2048': 889, '4096': 386, '8192': 117, '16384': 64, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1846, '256': 1920, '512': 1806, '1024': 1532, '2048': 669, '4096': 263, '8192': 156, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 45, '256': 365, '512': 983, '1024': 1247, '2048': 889, '4096': 386, '8192': 117, '16384': 64, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1846, '256': 1920, '512': 1806, '1024': 1532, '2048': 669, '4096': 263, '8192': 156, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 32, '256': 352, '512': 961, '1024': 1221, '2048': 977, '4096': 357, '8192': 128, '16384': 68, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1819, '256': 1867, '512': 1907, '1024': 1480, '2048': 688, '4096': 262, '8192': 169, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 45, '256': 365, '512': 983, '1024': 1247, '2048': 889, '4096': 386, '8192': 117, '16384': 64, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1846, '256': 1920, '512': 1806, '1024': 1532, '2048': 669, '4096': 263, '8192': 156, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen8192_gbs512_mbs2_dp2_tp4_pp2_pack_sp_node1.log...
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen8192_gbs512_mbs2_dp2_tp4_pp2_pack_sp_node0.log...
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 13b model, gpu_num=16, seq_len = 8192, DP=2, MP=8, PP=1, GBS=512, MBS=2
10.64.24.50: use sequence-packing
10.64.24.50: use sequence parallel
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 13b model, gpu_num=16, seq_len = 8192, DP=2, MP=8, PP=1, GBS=512, MBS=2
10.64.24.49: use sequence-packing
10.64.24.49: use sequence parallel
10.64.24.50: [2024-03-11 19:09:54,637] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 19:09:54,637] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 19:09:54,637] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 19:09:54,637] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 19:09:55,567] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 19:09:55,567] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 19:09:55,567] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 19:09:55,567] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 8, pipeline-model-parallel size: 1 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 2
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 40
10.64.24.49:   encoder_seq_length .............................. 8192
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 20480
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 5120
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 8192
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 2
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 40
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 40
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 1
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 8192
10.64.24.49:   sequence_packing ................................ True
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 8
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 1
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ True
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 128
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 943 dummy tokens (new size: 51200)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: > initialized tensor model parallel with size 8
10.64.24.49: > initialized pipeline model parallel with size 1
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.061 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.273 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 12.079
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 19:10:24 
10.64.24.49: building GPT model ...
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 1648993280
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (7, 0): 1648993280
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (4, 0): 1648993280
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (6, 0): 1648993280
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 1648993280
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1648993280
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (5, 0): 1648993280
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1648993280
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1648993280 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 19:10:25 
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache end, time cost:  4.791 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  4.795 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Cutting or padding data end, time cost:  10.501 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.674 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 19:10:40 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (598.02, 621.83)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 15787.39)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 19:10:40 
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 19:12:43 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             291W / 700W |  37868MiB / 81559MiB |      0%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             263W / 700W |  38336MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             231W / 700W |  38004MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             247W / 700W |  38446MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             240W / 700W |  38196MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             268W / 700W |  37714MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             231W / 700W |  38404MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             259W / 700W |  37882MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 19924.1 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.090498E+01 | loss scale: 1.0 | grad norm: 34.945 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 19164.32177734375 | max allocated: 32326.36083984375 | reserved: 37776.0 | max reserved: 37776.0
10.64.24.49: [Rank 3] (after 10 iterations) memory (MB) | allocated: 19164.32177734375 | max allocated: 32328.67724609375 | reserved: 38160.0 | max reserved: 38160.0
10.64.24.49: [Rank 2] (after 10 iterations) memory (MB) | allocated: 19164.32177734375 | max allocated: 32324.72802734375 | reserved: 37736.0 | max reserved: 37736.0
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 19164.32177734375 | max allocated: 32325.43212890625 | reserved: 37918.0 | max reserved: 37918.0
10.64.24.49: [Rank 4] (after 10 iterations) memory (MB) | allocated: 19164.32177734375 | max allocated: 32327.06494140625 | reserved: 37802.0 | max reserved: 37802.0
10.64.24.49: [Rank 7] (after 10 iterations) memory (MB) | allocated: 19164.32177734375 | max allocated: 32327.76904296875 | reserved: 38132.0 | max reserved: 38132.0[Rank 6] (after 10 iterations) memory (MB) | allocated: 19164.32177734375 | max allocated: 32326.45263671875 | reserved: 37870.0 | max reserved: 37870.0
10.64.24.49: 
10.64.24.49: [Rank 5] (after 10 iterations) memory (MB) | allocated: 19164.32177734375 | max allocated: 32326.36083984375 | reserved: 37580.0 | max reserved: 37580.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (19552.71, 19570.38)
10.64.24.50:     forward-compute ................................: (11195.33, 11834.57)
10.64.24.50:     backward-compute ...............................: (7658.09, 8278.64)
10.64.24.50:     batch-generator ................................: (780.71, 848.54)
10.64.24.50:     layernorm-grads-all-reduce .....................: (3.12, 3.81)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.04)
10.64.24.50:     all-grads-sync .................................: (141.77, 159.21)
10.64.24.50:     params-all-gather ..............................: (41.09, 41.48)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (1.38, 1.72)
10.64.24.50:     optimizer-clip-main-grad .......................: (9.58, 9.62)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.67, 28.20)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (3.49, 3.64)
10.64.24.50:     optimizer ......................................: (85.28, 85.67)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 17996.6 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.083942E+01 | loss scale: 1.0 | grad norm: 24.662 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (17842.23, 17843.77)
10.64.24.50:     forward-compute ................................: (9917.43, 10565.14)
10.64.24.50:     backward-compute ...............................: (7198.94, 7845.73)
10.64.24.50:     batch-generator ................................: (128.29, 188.47)
10.64.24.50:     layernorm-grads-all-reduce .....................: (3.00, 3.84)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.03)
10.64.24.50:     all-grads-sync .................................: (73.51, 74.79)
10.64.24.50:     params-all-gather ..............................: (41.01, 41.40)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (1.33, 1.72)
10.64.24.50:     optimizer-clip-main-grad .......................: (5.31, 5.35)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.37, 9.53)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (3.48, 3.63)
10.64.24.50:     optimizer ......................................: (61.77, 62.16)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 19606.5 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.071742E+01 | loss scale: 1.0 | grad norm: 8.842 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (19452.83, 19458.29)
10.64.24.50:     forward-compute ................................: (11297.08, 11828.65)
10.64.24.50:     backward-compute ...............................: (7546.70, 8080.06)
10.64.24.50:     batch-generator ................................: (127.84, 195.15)
10.64.24.50:     layernorm-grads-all-reduce .....................: (2.99, 3.49)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.03)
10.64.24.50:     all-grads-sync .................................: (73.36, 74.70)
10.64.24.50:     params-all-gather ..............................: (41.13, 41.47)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (1.33, 1.66)
10.64.24.50:     optimizer-clip-main-grad .......................: (5.30, 5.32)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.37, 9.48)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (3.48, 3.63)
10.64.24.50:     optimizer ......................................: (61.77, 62.10)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 19:20:32 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             280W / 700W |  45730MiB / 81559MiB |     55%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             270W / 700W |  45610MiB / 81559MiB |     96%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             271W / 700W |  45570MiB / 81559MiB |     95%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   35C    P0             254W / 700W |  46020MiB / 81559MiB |     96%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             297W / 700W |  46142MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             281W / 700W |  45600MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             255W / 700W |  46024MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             278W / 700W |  45592MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: rank 2: {'packing_seq_len': {'128': 45, '256': 365, '512': 983, '1024': 1247, '2048': 889, '4096': 386, '8192': 117, '16384': 64, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1846, '256': 1920, '512': 1806, '1024': 1532, '2048': 669, '4096': 263, '8192': 156, '16384': 0, '32768': 0, '>32k': 0}}[after training is done] datetime: 2024-03-11 19:20:49 
10.64.24.49: 
10.64.24.49: rank 7: {'packing_seq_len': {'128': 45, '256': 365, '512': 983, '1024': 1247, '2048': 889, '4096': 386, '8192': 117, '16384': 64, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1846, '256': 1920, '512': 1806, '1024': 1532, '2048': 669, '4096': 263, '8192': 156, '16384': 0, '32768': 0, '>32k': 0}}rank 6: {'packing_seq_len': {'128': 45, '256': 365, '512': 983, '1024': 1247, '2048': 889, '4096': 386, '8192': 117, '16384': 64, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1846, '256': 1920, '512': 1806, '1024': 1532, '2048': 669, '4096': 263, '8192': 156, '16384': 0, '32768': 0, '>32k': 0}}rank 4: {'packing_seq_len': {'128': 45, '256': 365, '512': 983, '1024': 1247, '2048': 889, '4096': 386, '8192': 117, '16384': 64, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1846, '256': 1920, '512': 1806, '1024': 1532, '2048': 669, '4096': 263, '8192': 156, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: 
10.64.24.49: rank 5: {'packing_seq_len': {'128': 45, '256': 365, '512': 983, '1024': 1247, '2048': 889, '4096': 386, '8192': 117, '16384': 64, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1846, '256': 1920, '512': 1806, '1024': 1532, '2048': 669, '4096': 263, '8192': 156, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 3: {'packing_seq_len': {'128': 45, '256': 365, '512': 983, '1024': 1247, '2048': 889, '4096': 386, '8192': 117, '16384': 64, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1846, '256': 1920, '512': 1806, '1024': 1532, '2048': 669, '4096': 263, '8192': 156, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 1: {'packing_seq_len': {'128': 45, '256': 365, '512': 983, '1024': 1247, '2048': 889, '4096': 386, '8192': 117, '16384': 64, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1846, '256': 1920, '512': 1806, '1024': 1532, '2048': 669, '4096': 263, '8192': 156, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 32, '256': 352, '512': 961, '1024': 1221, '2048': 977, '4096': 357, '8192': 128, '16384': 68, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1819, '256': 1867, '512': 1907, '1024': 1480, '2048': 688, '4096': 262, '8192': 169, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 45, '256': 365, '512': 983, '1024': 1247, '2048': 889, '4096': 386, '8192': 117, '16384': 64, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1846, '256': 1920, '512': 1806, '1024': 1532, '2048': 669, '4096': 263, '8192': 156, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen8192_gbs512_mbs2_dp2_tp8_pp1_pack_sp_node0.log...
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen8192_gbs512_mbs2_dp2_tp8_pp1_pack_sp_node1.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 13b model, gpu_num=16, seq_len = 4096, DP=2, MP=4, PP=2, GBS=512, MBS=8
10.64.24.49: use sequence-packing
10.64.24.49: use sequence parallel
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 13b model, gpu_num=16, seq_len = 4096, DP=2, MP=4, PP=2, GBS=512, MBS=8
10.64.24.50: use sequence-packing
10.64.24.50: use sequence parallel
10.64.24.49: [2024-03-11 19:20:59,744] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 19:20:59,744] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 19:20:59,744] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 19:20:59,744] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 19:20:58,903] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 19:20:58,903] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 19:20:58,903] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 19:20:58,903] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 4, pipeline-model-parallel size: 2 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 2
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 40
10.64.24.49:   encoder_seq_length .............................. 4096
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 20480
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 5120
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 4096
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 8
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 40
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 40
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 2
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 4096
10.64.24.49:   sequence_packing ................................ True
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 4
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 2
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ True
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 32
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 431 dummy tokens (new size: 50688)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: > initialized tensor model parallel with size 4
10.64.24.49: > initialized pipeline model parallel with size 2
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.060 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 7.735 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 10.424
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 19:21:27 
10.64.24.49: building GPT model ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 1638548480
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 1659509760
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 1638548480
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 1659509760
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1659509760
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1638548480
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1638548480 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1659509760 elements):
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1638548480
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1659509760
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 19:21:28 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache end, time cost:  4.856 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Loading exists cache end, time cost:  4.882 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.106 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.108 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Cutting or padding data end, time cost:  4.933 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  4.914 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  4.981 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  5.194 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 19:21:38 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (893.55, 954.54)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 10499.97)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 19:21:38 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 19:22:33 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             328W / 700W |  47718MiB / 81559MiB |     48%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             344W / 700W |  47738MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             308W / 700W |  47930MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             346W / 700W |  47334MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             369W / 700W |  47018MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             342W / 700W |  46512MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             417W / 700W |  47314MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             317W / 700W |  46594MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 8162.1 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.092928E+01 | loss scale: 1.0 | grad norm: 32.089 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: [Rank 9] (after 10 iterations) memory (MB) | allocated: 18995.3740234375 | max allocated: 35819.85693359375 | reserved: 38004.0 | max reserved: 38004.0
10.64.24.50: [Rank 11] (after 10 iterations) memory (MB) | allocated: 18995.3740234375 | max allocated: 35819.85693359375 | reserved: 38004.0 | max reserved: 38004.0[Rank 8] (after 10 iterations) memory (MB) | allocated: 18995.3740234375 | max allocated: 35819.6611328125 | reserved: 38004.0 | max reserved: 38004.0
10.64.24.50: 
10.64.24.49: [Rank 3] (after 10 iterations) memory (MB) | allocated: 19241.162109375 | max allocated: 38802.53759765625 | reserved: 43852.0 | max reserved: 43852.0
10.64.24.49: [Rank 2] (after 10 iterations) memory (MB) | allocated: 19241.162109375 | max allocated: 38801.3115234375 | reserved: 44208.0 | max reserved: 44208.0[Rank 0] (after 10 iterations) memory (MB) | allocated: 19241.162109375 | max allocated: 38798.6611328125 | reserved: 44044.0 | max reserved: 44044.0
10.64.24.49: 
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 19241.162109375 | max allocated: 38801.92822265625 | reserved: 44016.0 | max reserved: 44016.0
10.64.24.50: [Rank 10] (after 10 iterations) memory (MB) | allocated: 18995.3740234375 | max allocated: 35819.8544921875 | reserved: 38004.0 | max reserved: 38004.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (7641.04, 7751.48)
10.64.24.50:     forward-compute ................................: (2001.45, 3773.58)
10.64.24.50:     backward-compute ...............................: (2496.88, 3128.92)
10.64.24.50:     batch-generator ................................: (431.94, 451.38)
10.64.24.50:     forward-recv ...................................: (497.18, 529.80)
10.64.24.50:     forward-send ...................................: (2.49, 6.47)
10.64.24.50:     backward-recv ..................................: (121.80, 170.08)
10.64.24.50:     backward-send ..................................: (0.89, 8.56)
10.64.24.50:     forward-send-backward-recv .....................: (2601.54, 2915.84)
10.64.24.50:     backward-send-forward-recv .....................: (383.35, 479.07)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.66, 2.16)
10.64.24.50:     embedding-grads-all-reduce .....................: (5.71, 5.78)
10.64.24.50:     all-grads-sync .................................: (221.67, 251.91)
10.64.24.50:     params-all-gather ..............................: (10.33, 11.45)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.67, 0.85)
10.64.24.50:     optimizer-clip-main-grad .......................: (7.45, 7.50)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.46, 9.86)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.93, 3.04)
10.64.24.50:     optimizer ......................................: (32.27, 33.41)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 5878.1 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.085267E+01 | loss scale: 1.0 | grad norm: 20.691 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (5722.48, 5794.91)
10.64.24.50:     forward-compute ................................: (1405.06, 2692.13)
10.64.24.50:     backward-compute ...............................: (2213.36, 2739.07)
10.64.24.50:     batch-generator ................................: (25.75, 36.54)
10.64.24.50:     forward-recv ...................................: (44.61, 52.19)
10.64.24.50:     forward-send ...................................: (0.48, 0.54)
10.64.24.50:     backward-recv ..................................: (66.61, 104.74)
10.64.24.50:     backward-send ..................................: (0.79, 11.35)
10.64.24.50:     forward-send-backward-recv .....................: (1831.68, 1951.58)
10.64.24.50:     backward-send-forward-recv .....................: (280.75, 423.00)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.60, 1.88)
10.64.24.50:     embedding-grads-all-reduce .....................: (5.65, 5.81)
10.64.24.50:     all-grads-sync .................................: (15.45, 16.03)
10.64.24.50:     params-all-gather ..............................: (10.40, 10.81)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.65, 0.78)
10.64.24.50:     optimizer-clip-main-grad .......................: (4.62, 4.67)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.17, 9.34)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.93, 3.04)
10.64.24.50:     optimizer ......................................: (28.77, 29.17)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 7046.4 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.060186E+01 | loss scale: 1.0 | grad norm: 9.741 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (6851.64, 6970.03)
10.64.24.50:     forward-compute ................................: (1829.72, 3198.62)
10.64.24.50:     backward-compute ...............................: (2523.33, 3006.90)
10.64.24.50:     batch-generator ................................: (26.11, 36.45)
10.64.24.50:     forward-recv ...................................: (49.24, 49.82)
10.64.24.50:     forward-send ...................................: (0.50, 0.52)
10.64.24.50:     backward-recv ..................................: (60.35, 121.17)
10.64.24.50:     backward-send ..................................: (1.25, 12.04)
10.64.24.50:     forward-send-backward-recv .....................: (2269.75, 2353.94)
10.64.24.50:     backward-send-forward-recv .....................: (607.38, 794.67)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.59, 1.88)
10.64.24.50:     embedding-grads-all-reduce .....................: (5.63, 5.72)
10.64.24.50:     all-grads-sync .................................: (15.45, 15.99)
10.64.24.50:     params-all-gather ..............................: (10.34, 10.83)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.64, 0.77)
10.64.24.50:     optimizer-clip-main-grad .......................: (4.62, 4.67)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.16, 9.33)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.93, 3.04)
10.64.24.50:     optimizer ......................................: (28.73, 29.22)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 19:25:18 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             458W / 700W |  56878MiB / 81559MiB |     93%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             469W / 700W |  57170MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             510W / 700W |  57090MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             456W / 700W |  56494MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             348W / 700W |  51702MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             355W / 700W |  51876MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             406W / 700W |  51862MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             337W / 700W |  51142MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 19:25:25 
10.64.24.49: rank 2: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 395, '16384': 62, '32768': 1, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 407, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}rank 3: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 395, '16384': 62, '32768': 1, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 407, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 5: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 5, '2048': 97, '4096': 422, '8192': 405, '16384': 95, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1868, '512': 1862, '1024': 1501, '2048': 703, '4096': 443, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 6: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 5, '2048': 97, '4096': 422, '8192': 405, '16384': 95, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1868, '512': 1862, '1024': 1501, '2048': 703, '4096': 443, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 1: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 395, '16384': 62, '32768': 1, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 407, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 7: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 5, '2048': 97, '4096': 422, '8192': 405, '16384': 95, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1868, '512': 1862, '1024': 1501, '2048': 703, '4096': 443, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 395, '16384': 62, '32768': 1, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 407, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 5, '2048': 97, '4096': 422, '8192': 405, '16384': 95, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1868, '512': 1862, '1024': 1501, '2048': 703, '4096': 443, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 395, '16384': 62, '32768': 1, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 407, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen4096_gbs512_mbs8_dp2_tp4_pp2_pack_sp_node1.log...
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen4096_gbs512_mbs8_dp2_tp4_pp2_pack_sp_node0.log...
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 13b model, gpu_num=16, seq_len = 4096, DP=2, MP=2, PP=4, GBS=512, MBS=8
10.64.24.50: use sequence-packing
10.64.24.50: use sequence parallel
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 13b model, gpu_num=16, seq_len = 4096, DP=2, MP=2, PP=4, GBS=512, MBS=8
10.64.24.49: use sequence-packing
10.64.24.49: use sequence parallel
10.64.24.49: [2024-03-11 19:25:32,430] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 19:25:32,430] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 19:25:32,430] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 19:25:32,430] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 19:25:31,554] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 19:25:31,554] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 19:25:31,554] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 19:25:31,554] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 2, pipeline-model-parallel size: 4 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 2
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 40
10.64.24.49:   encoder_seq_length .............................. 4096
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 20480
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 5120
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 4096
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 8
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 40
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 40
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 4
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 4096
10.64.24.49:   sequence_packing ................................ True
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 2
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 4
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ True
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 32
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: > initialized tensor model parallel with size 2
10.64.24.49: > initialized pipeline model parallel with size 4
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.059 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 7.826 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 11.516
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 19:26:00 
10.64.24.49: building GPT model ...
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1573350400
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1573350400
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 1573350400
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1573350400 elements):
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1573350400
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1573350400 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 1702466560
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1723427840
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1702466560
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1723427840
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1702466560 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1723427840 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 19:26:00 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache end, time cost:  5.020 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.137 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.159 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.178 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.188 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.191 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.570 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Loading exists cache end, time cost:  6.688 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Cutting or padding data end, time cost:  5.080 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  5.011 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  5.029 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  5.036 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  5.040 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  5.250 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  5.407 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  5.583 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 19:26:13 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (43.72, 624.40)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 12512.53)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 19:26:13 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 19:27:06 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             306W / 700W |  67332MiB / 81559MiB |     64%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             236W / 700W |  66840MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             471W / 700W |  59968MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             351W / 700W |  59272MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             196W / 700W |  58284MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             281W / 700W |  57914MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             312W / 700W |  53870MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             280W / 700W |  53758MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 8111.0 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.095048E+01 | loss scale: 1.0 | grad norm: 16.374 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: [Rank 9] (after 10 iterations) memory (MB) | allocated: 18264.498046875 | max allocated: 37952.5751953125 | reserved: 42066.0 | max reserved: 42066.0
10.64.24.50: [Rank 8] (after 10 iterations) memory (MB) | allocated: 18264.498046875 | max allocated: 37953.7236328125 | reserved: 41678.0 | max reserved: 41678.0
10.64.24.50: [Rank 13] (after 10 iterations) memory (MB) | allocated: 19742.1142578125 | max allocated: 40529.54833984375 | reserved: 44966.0 | max reserved: 44966.0
10.64.24.50: [Rank 12] (after 10 iterations) memory (MB) | allocated: 19742.1142578125 | max allocated: 40528.74560546875 | reserved: 44966.0 | max reserved: 44966.0
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 19952.087890625 | max allocated: 55242.86083984375 | reserved: 63906.0 | max reserved: 63906.0[Rank 4] (after 10 iterations) memory (MB) | allocated: 18265.5517578125 | max allocated: 47923.6552734375 | reserved: 54944.0 | max reserved: 54944.0
10.64.24.49: 
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 19953.068359375 | max allocated: 55246.56689453125 | reserved: 63414.0 | max reserved: 63414.0
10.64.24.49: [Rank 5] (after 10 iterations) memory (MB) | allocated: 18265.478515625 | max allocated: 47924.810546875 | reserved: 54574.0 | max reserved: 54574.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (7387.29, 7680.31)
10.64.24.50:     forward-compute ................................: (1347.56, 3377.16)
10.64.24.50:     backward-compute ...............................: (2117.06, 3210.64)
10.64.24.50:     batch-generator ................................: (143.27, 168.97)
10.64.24.50:     forward-recv ...................................: (214.86, 598.20)
10.64.24.50:     forward-send ...................................: (4.52, 330.45)
10.64.24.50:     backward-recv ..................................: (124.86, 469.01)
10.64.24.50:     backward-send ..................................: (1.04, 39.70)
10.64.24.50:     forward-send-backward-recv .....................: (2508.14, 3138.84)
10.64.24.50:     backward-send-forward-recv .....................: (378.61, 629.97)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.97, 1.25)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 11.13)
10.64.24.50:     all-grads-sync .................................: (228.79, 244.83)
10.64.24.50:     params-all-gather ..............................: (9.71, 10.83)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.35, 0.44)
10.64.24.50:     optimizer-clip-main-grad .......................: (7.28, 7.53)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (8.93, 10.06)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.59, 2.85)
10.64.24.50:     optimizer ......................................: (31.26, 32.36)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 6160.8 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.080152E+01 | loss scale: 1.0 | grad norm: 8.078 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (5850.58, 6053.61)
10.64.24.50:     forward-compute ................................: (1036.97, 2756.24)
10.64.24.50:     backward-compute ...............................: (1864.40, 2844.97)
10.64.24.50:     batch-generator ................................: (22.60, 31.68)
10.64.24.50:     forward-recv ...................................: (50.40, 122.53)
10.64.24.50:     forward-send ...................................: (0.67, 31.62)
10.64.24.50:     backward-recv ..................................: (73.26, 361.51)
10.64.24.50:     backward-send ..................................: (0.91, 56.83)
10.64.24.50:     forward-send-backward-recv .....................: (2100.24, 2572.13)
10.64.24.50:     backward-send-forward-recv .....................: (215.97, 565.73)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.92, 1.02)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 11.15)
10.64.24.50:     all-grads-sync .................................: (14.87, 16.56)
10.64.24.50:     params-all-gather ..............................: (9.67, 10.86)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.34, 0.42)
10.64.24.50:     optimizer-clip-main-grad .......................: (4.22, 4.48)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (8.70, 9.61)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.59, 2.85)
10.64.24.50:     optimizer ......................................: (27.53, 28.72)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 6781.1 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.058227E+01 | loss scale: 1.0 | grad norm: 3.536 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (6381.19, 6672.68)
10.64.24.50:     forward-compute ................................: (1206.34, 2883.92)
10.64.24.50:     backward-compute ...............................: (2156.00, 3061.54)
10.64.24.50:     batch-generator ................................: (22.54, 31.07)
10.64.24.50:     forward-recv ...................................: (47.01, 116.45)
10.64.24.50:     forward-send ...................................: (0.72, 33.13)
10.64.24.50:     backward-recv ..................................: (73.49, 441.45)
10.64.24.50:     backward-send ..................................: (1.05, 56.04)
10.64.24.50:     forward-send-backward-recv .....................: (2185.14, 2661.65)
10.64.24.50:     backward-send-forward-recv .....................: (335.08, 649.68)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.92, 1.07)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 11.19)
10.64.24.50:     all-grads-sync .................................: (14.78, 16.59)
10.64.24.50:     params-all-gather ..............................: (9.53, 10.88)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.34, 0.41)
10.64.24.50:     optimizer-clip-main-grad .......................: (4.21, 4.47)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (8.68, 9.59)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.59, 2.85)
10.64.24.50:     optimizer ......................................: (27.34, 28.69)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 19:29:50 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             440W / 700W |  71966MiB / 81559MiB |     84%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             420W / 700W |  70508MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             416W / 700W |  64466MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             427W / 700W |  63246MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             415W / 700W |  61858MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             423W / 700W |  61198MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             282W / 700W |  57860MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             352W / 700W |  57710MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 19:29:57 
10.64.24.49: rank 5: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 395, '16384': 62, '32768': 1, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 407, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 1: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 395, '16384': 62, '32768': 1, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 407, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 7: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 5, '2048': 97, '4096': 422, '8192': 405, '16384': 95, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1868, '512': 1862, '1024': 1501, '2048': 703, '4096': 443, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 3: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 5, '2048': 97, '4096': 422, '8192': 405, '16384': 95, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1868, '512': 1862, '1024': 1501, '2048': 703, '4096': 443, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 395, '16384': 62, '32768': 1, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 407, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 2: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 5, '2048': 97, '4096': 422, '8192': 405, '16384': 95, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1868, '512': 1862, '1024': 1501, '2048': 703, '4096': 443, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 6: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 5, '2048': 97, '4096': 422, '8192': 405, '16384': 95, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1868, '512': 1862, '1024': 1501, '2048': 703, '4096': 443, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 395, '16384': 62, '32768': 1, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 407, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 0, '256': 0, '512': 0, '1024': 2, '2048': 101, '4096': 463, '8192': 395, '16384': 62, '32768': 1, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1919, '512': 1851, '1024': 1511, '2048': 654, '4096': 407, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen4096_gbs512_mbs8_dp2_tp2_pp4_pack_sp_node0.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 13b model, gpu_num=16, seq_len = 4096, DP=2, MP=8, PP=1, GBS=512, MBS=4
10.64.24.49: use sequence-packing
10.64.24.49: use sequence parallel
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen4096_gbs512_mbs8_dp2_tp2_pp4_pack_sp_node1.log...
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 13b model, gpu_num=16, seq_len = 4096, DP=2, MP=8, PP=1, GBS=512, MBS=4
10.64.24.50: use sequence-packing
10.64.24.50: use sequence parallel
10.64.24.50: [2024-03-11 19:30:09,216] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 19:30:09,216] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 19:30:09,216] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 19:30:09,216] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 19:30:10,186] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 19:30:10,186] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 19:30:10,186] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 19:30:10,186] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 8, pipeline-model-parallel size: 1 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 2
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 40
10.64.24.49:   encoder_seq_length .............................. 4096
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 20480
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 5120
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 4096
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 4
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 40
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 40
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 1
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 4096
10.64.24.49:   sequence_packing ................................ True
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 8
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 1
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ True
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 64
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 943 dummy tokens (new size: 51200)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: > initialized tensor model parallel with size 8
10.64.24.49: > initialized pipeline model parallel with size 1
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.060 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.306 seconds
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 10.798
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 19:30:38 
10.64.24.49: building GPT model ...
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (7, 0): 1628021760
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (4, 0): 1628021760
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (5, 0): 1628021760
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 1628021760
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1628021760
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 1628021760
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (6, 0): 1628021760
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1628021760
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1628021760 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 19:30:39 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache end, time cost:  4.784 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Loading exists cache end, time cost:  4.990 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Cutting or padding data end, time cost:  4.933 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  5.077 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 19:30:49 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (376.41, 421.23)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 10268.39)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 19:30:49 
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 19:32:05 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             293W / 700W |  37726MiB / 81559MiB |     27%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             299W / 700W |  37772MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             284W / 700W |  37818MiB / 81559MiB |     96%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   36C    P0             300W / 700W |  38154MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             280W / 700W |  37938MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             260W / 700W |  37988MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             273W / 700W |  37998MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             260W / 700W |  37564MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 11591.6 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.095533E+01 | loss scale: 1.0 | grad norm: 64.226 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 18904.0986328125 | max allocated: 30151.52685546875 | reserved: 34716.0 | max reserved: 34716.0
10.64.24.49: [Rank 3] (after 10 iterations) memory (MB) | allocated: 18904.0986328125 | max allocated: 30151.7265625 | reserved: 35098.0 | max reserved: 35098.0[Rank 2] (after 10 iterations) memory (MB) | allocated: 18904.0986328125 | max allocated: 30152.21240234375 | reserved: 34762.0 | max reserved: 34762.0
10.64.24.49: 
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 18904.0986328125 | max allocated: 30151.30224609375 | reserved: 34716.0 | max reserved: 34716.0
10.64.24.49: [Rank 4] (after 10 iterations) memory (MB) | allocated: 18904.0986328125 | max allocated: 30152.208984375 | reserved: 34880.0 | max reserved: 34880.0
10.64.24.49: [Rank 6] (after 10 iterations) memory (MB) | allocated: 18904.0986328125 | max allocated: 30151.30224609375 | reserved: 34940.0 | max reserved: 34940.0
10.64.24.49: [Rank 7] (after 10 iterations) memory (MB) | allocated: 18904.0986328125 | max allocated: 30151.30908203125 | reserved: 34748.0 | max reserved: 34748.0
10.64.24.49: [Rank 5] (after 10 iterations) memory (MB) | allocated: 18904.0986328125 | max allocated: 30151.3056640625 | reserved: 34932.0 | max reserved: 34932.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (11239.72, 11248.50)
10.64.24.50:     forward-compute ................................: (6310.76, 6656.78)
10.64.24.50:     backward-compute ...............................: (4542.57, 4898.98)
10.64.24.50:     batch-generator ................................: (715.10, 755.60)
10.64.24.50:     layernorm-grads-all-reduce .....................: (3.28, 3.90)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.05)
10.64.24.50:     all-grads-sync .................................: (145.00, 158.84)
10.64.24.50:     params-all-gather ..............................: (40.53, 40.85)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (1.40, 1.69)
10.64.24.50:     optimizer-clip-main-grad .......................: (9.71, 10.33)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.56, 10.08)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (3.46, 3.67)
10.64.24.50:     optimizer ......................................: (67.47, 67.77)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 9700.3 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.083654E+01 | loss scale: 1.0 | grad norm: 51.907 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (9544.68, 9546.21)
10.64.24.50:     forward-compute ................................: (5112.98, 5259.19)
10.64.24.50:     backward-compute ...............................: (4243.52, 4396.35)
10.64.24.50:     batch-generator ................................: (68.77, 108.87)
10.64.24.50:     layernorm-grads-all-reduce .....................: (2.96, 3.49)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.03)
10.64.24.50:     all-grads-sync .................................: (72.99, 73.48)
10.64.24.50:     params-all-gather ..............................: (40.64, 40.90)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (1.35, 1.53)
10.64.24.50:     optimizer-clip-main-grad .......................: (5.25, 5.28)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.24, 9.36)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (3.46, 3.57)
10.64.24.50:     optimizer ......................................: (61.03, 61.29)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 10290.1 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.060979E+01 | loss scale: 1.0 | grad norm: 18.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (10137.25, 10140.83)
10.64.24.50:     forward-compute ................................: (5380.71, 5571.94)
10.64.24.50:     backward-compute ...............................: (4527.25, 4720.88)
10.64.24.50:     batch-generator ................................: (71.39, 107.16)
10.64.24.50:     layernorm-grads-all-reduce .....................: (2.98, 3.46)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.03)
10.64.24.50:     all-grads-sync .................................: (72.74, 73.95)
10.64.24.50:     params-all-gather ..............................: (40.61, 40.98)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (1.35, 1.54)
10.64.24.50:     optimizer-clip-main-grad .......................: (5.19, 5.23)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.23, 9.32)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (3.46, 3.58)
10.64.24.50:     optimizer ......................................: (60.86, 61.24)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 19:36:14 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   38C    P0             338W / 700W |  39970MiB / 81559MiB |     39%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             327W / 700W |  40146MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             367W / 700W |  39930MiB / 81559MiB |     96%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   37C    P0             291W / 700W |  40010MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             320W / 700W |  40184MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             318W / 700W |  40246MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             275W / 700W |  40232MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             275W / 700W |  39810MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 19:36:27 
10.64.24.49: rank 6: {'packing_seq_len': {'128': 0, '256': 1, '512': 42, '1024': 374, '2048': 778, '4096': 597, '8192': 235, '16384': 21, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1826, '256': 1876, '512': 1860, '1024': 1532, '2048': 678, '4096': 420, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}rank 3: {'packing_seq_len': {'128': 0, '256': 1, '512': 42, '1024': 374, '2048': 778, '4096': 597, '8192': 235, '16384': 21, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1826, '256': 1876, '512': 1860, '1024': 1532, '2048': 678, '4096': 420, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}rank 4: {'packing_seq_len': {'128': 0, '256': 1, '512': 42, '1024': 374, '2048': 778, '4096': 597, '8192': 235, '16384': 21, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1826, '256': 1876, '512': 1860, '1024': 1532, '2048': 678, '4096': 420, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: 
10.64.24.49: rank 7: {'packing_seq_len': {'128': 0, '256': 1, '512': 42, '1024': 374, '2048': 778, '4096': 597, '8192': 235, '16384': 21, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1826, '256': 1876, '512': 1860, '1024': 1532, '2048': 678, '4096': 420, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}rank 2: {'packing_seq_len': {'128': 0, '256': 1, '512': 42, '1024': 374, '2048': 778, '4096': 597, '8192': 235, '16384': 21, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1826, '256': 1876, '512': 1860, '1024': 1532, '2048': 678, '4096': 420, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 1: {'packing_seq_len': {'128': 0, '256': 1, '512': 42, '1024': 374, '2048': 778, '4096': 597, '8192': 235, '16384': 21, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1826, '256': 1876, '512': 1860, '1024': 1532, '2048': 678, '4096': 420, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 5: {'packing_seq_len': {'128': 0, '256': 1, '512': 42, '1024': 374, '2048': 778, '4096': 597, '8192': 235, '16384': 21, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1826, '256': 1876, '512': 1860, '1024': 1532, '2048': 678, '4096': 420, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: terminate called after throwing an instance of 'c10::Error'
10.64.24.50:   what():  CUDA driver error: unknown error
10.64.24.50: Exception raised from _hasPrimaryContext at /opt/pytorch/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp:67 (most recent call first):
10.64.24.50: frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f1d3d7b5449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
10.64.24.50: frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f1d3d7701d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
10.64.24.50: frame #2: <unknown function> + 0xd55cef (0x7f1ca2614cef in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #3: c10::cuda::MaybeSetDevice(int) + 0xc (0x7f1d3d84dbac in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10_cuda.so)
10.64.24.50: frame #4: <unknown function> + 0xd4e4a7 (0x7f1ca260d4a7 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #5: <unknown function> + 0xe44cb8 (0x7f1ca2703cb8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #6: <unknown function> + 0xd520ea (0x7f1ca26110ea in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #7: c10d::ProcessGroupNCCL::WorkNCCL::~WorkNCCL() + 0x127 (0x7f1ca26d6f57 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #8: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x22a (0x7f1ca26d908a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #9: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x80 (0x7f1ca26d92f0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #10: <unknown function> + 0xdc253 (0x7f1ca14b0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
10.64.24.50: frame #11: <unknown function> + 0x94ac3 (0x7f1d3dfecac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
10.64.24.50: frame #12: <unknown function> + 0x126a40 (0x7f1d3e07ea40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
10.64.24.50: 
10.64.24.49: rank 0: {'packing_seq_len': {'128': 0, '256': 1, '512': 42, '1024': 374, '2048': 778, '4096': 597, '8192': 235, '16384': 21, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1826, '256': 1876, '512': 1860, '1024': 1532, '2048': 678, '4096': 420, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 0, '256': 1, '512': 45, '1024': 383, '2048': 777, '4096': 565, '8192': 255, '16384': 22, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1839, '256': 1911, '512': 1853, '1024': 1480, '2048': 679, '4096': 430, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: [2024-03-11 19:41:36,618] torch.distributed.elastic.agent.server.api: [ERROR] Error waiting on exit barrier. Elapsed: 300.1038267612457 seconds
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen4096_gbs512_mbs4_dp2_tp8_pp1_pack_sp_node0.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 7b model, gpu_num=16, seq_len = 16384, DP=2, MP=2, PP=4, GBS=512, MBS=1
10.64.24.49: use sequence parallel
10.64.24.49: [2024-03-11 19:41:39,270] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 19:41:39,270] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 19:41:39,270] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 19:41:39,270] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 19:52:56,600] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 1 (pid: 2267589) of binary: /usr/bin/python
10.64.24.50: Traceback (most recent call last):
10.64.24.50:   File "/usr/local/bin/torchrun", line 33, in <module>
10.64.24.50:     sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
10.64.24.50:     return f(*args, **kwargs)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
10.64.24.50:     run(args)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
10.64.24.50:     elastic_launch(
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
10.64.24.50:     return launch_agent(self._config, self._entrypoint, list(args))
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
10.64.24.50:     raise ChildFailedError(
10.64.24.50: torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
10.64.24.50: ========================================================
10.64.24.50: pretrain_gpt.py FAILED
10.64.24.50: --------------------------------------------------------
10.64.24.50: Failures:
10.64.24.50:   <NO_OTHER_FAILURES>
10.64.24.50: --------------------------------------------------------
10.64.24.50: Root Cause (first observed failure):
10.64.24.50: [0]:
10.64.24.50:   time      : 2024-03-11_19:52:56
10.64.24.50:   host      : SYM206-GPU-A0204-P2-Node50
10.64.24.50:   rank      : 9 (local_rank: 1)
10.64.24.50:   exitcode  : -6 (pid: 2267589)
10.64.24.50:   error_file: <N/A>
10.64.24.50:   traceback : Signal 6 (SIGABRT) received by PID 2267589
10.64.24.50: ========================================================
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen4096_gbs512_mbs4_dp2_tp8_pp1_pack_sp_node1.log...
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 7b model, gpu_num=16, seq_len = 16384, DP=2, MP=2, PP=4, GBS=512, MBS=1
10.64.24.50: use sequence parallel
10.64.24.50: [2024-03-11 19:52:59,036] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 19:52:59,036] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 19:52:59,036] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 19:52:59,036] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 2, pipeline-model-parallel size: 4 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 2
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 32
10.64.24.49:   encoder_seq_length .............................. 16384
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 16384
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 4096
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 16384
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 1
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 32
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 32
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 4
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 16384
10.64.24.49:   sequence_packing ................................ False
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 2
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 4
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ False
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 256
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: > initialized tensor model parallel with size 2
10.64.24.49: > initialized pipeline model parallel with size 4
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.060 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 9.387 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 11.587
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 19:53:27 
10.64.24.49: building GPT model ...
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 805617664
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 805617664
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 805617664
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (805617664 elements):
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 805617664
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (805617664 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 908910592
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 976011264
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (908910592 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 908910592
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 976011264
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (976011264 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 19:53:29 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache end, time cost:  5.115 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.117 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.151 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.156 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.172 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.185 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.263 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  6.134 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Cutting or padding data end, time cost:  18.732 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  18.796 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  18.830 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  18.691 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  18.893 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  18.938 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  19.045 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  19.716 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 19:53:55 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (37.40, 1267.80)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 26363.89)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 19:53:55 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 20:02:00 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             386W / 700W |  53898MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             502W / 700W |  53616MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   52C    P0             502W / 700W |  53836MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             415W / 700W |  54092MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             387W / 700W |  42868MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             371W / 700W |  42864MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   48C    P0             320W / 700W |  42800MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             270W / 700W |  42804MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 80070.5 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 8.905957E+00 | loss scale: 1.0 | grad norm: 820.112 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: [Rank 9] (after 10 iterations) memory (MB) | allocated: 9540.376953125 | max allocated: 27382.3935546875 | reserved: 30752.0 | max reserved: 30752.0
10.64.24.50: [Rank 8] (after 10 iterations) memory (MB) | allocated: 9540.376953125 | max allocated: 27382.3935546875 | reserved: 30752.0 | max reserved: 30752.0
10.64.24.50: [Rank 13] (after 10 iterations) memory (MB) | allocated: 10723.4404296875 | max allocated: 23505.54248046875 | reserved: 27064.0 | max reserved: 27064.0
10.64.24.50: [Rank 12] (after 10 iterations) memory (MB) | allocated: 10723.4404296875 | max allocated: 23505.54248046875 | reserved: 27064.0 | max reserved: 27064.0
10.64.24.49: [Rank 5] (after 10 iterations) memory (MB) | allocated: 9540.376953125 | max allocated: 36095.40185546875 | reserved: 39524.0 | max reserved: 39524.0
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 11363.376953125 | max allocated: 46824.47265625 | reserved: 50472.0 | max reserved: 50472.0[Rank 1] (after 10 iterations) memory (MB) | allocated: 11363.376953125 | max allocated: 46824.47265625 | reserved: 50190.0 | max reserved: 50190.0
10.64.24.49: 
10.64.24.49: [Rank 4] (after 10 iterations) memory (MB) | allocated: 9540.376953125 | max allocated: 36095.40185546875 | reserved: 39528.0 | max reserved: 39528.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (79167.38, 79718.33)
10.64.24.50:     forward-compute ................................: (24750.12, 28245.67)
10.64.24.50:     backward-compute ...............................: (45024.33, 50160.65)
10.64.24.50:     batch-generator ................................: (283.22, 382.77)
10.64.24.50:     forward-recv ...................................: (282.32, 781.61)
10.64.24.50:     forward-send ...................................: (5.44, 298.99)
10.64.24.50:     backward-recv ..................................: (124.35, 332.92)
10.64.24.50:     backward-send ..................................: (1.38, 4.06)
10.64.24.50:     forward-send-backward-recv .....................: (3872.62, 8392.70)
10.64.24.50:     backward-send-forward-recv .....................: (576.67, 694.09)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.89, 1.03)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 9.07)
10.64.24.50:     all-grads-sync .................................: (215.71, 228.61)
10.64.24.50:     params-all-gather ..............................: (5.27, 6.34)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.31, 0.42)
10.64.24.50:     optimizer-clip-main-grad .......................: (6.23, 6.50)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.68, 5.75)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.42, 1.74)
10.64.24.50:     optimizer ......................................: (20.46, 21.53)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 78594.8 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 4.190917E-01 | loss scale: 1.0 | grad norm: 4.812 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (78003.64, 78551.97)
10.64.24.50:     forward-compute ................................: (24559.03, 28004.70)
10.64.24.50:     backward-compute ...............................: (44778.72, 50010.63)
10.64.24.50:     batch-generator ................................: (159.90, 243.45)
10.64.24.50:     forward-recv ...................................: (99.43, 288.72)
10.64.24.50:     forward-send ...................................: (1.30, 5.35)
10.64.24.50:     backward-recv ..................................: (124.30, 332.82)
10.64.24.50:     backward-send ..................................: (1.38, 4.06)
10.64.24.50:     forward-send-backward-recv .....................: (3362.78, 8043.99)
10.64.24.50:     backward-send-forward-recv .....................: (538.14, 649.53)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.85, 0.95)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 9.04)
10.64.24.50:     all-grads-sync .................................: (7.71, 9.43)
10.64.24.50:     params-all-gather ..............................: (5.23, 6.35)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.30, 1.00)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.47, 2.74)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.56, 5.99)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.42, 1.92)
10.64.24.50:     optimizer ......................................: (17.71, 18.83)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 78270.7 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 4.024467E-01 | loss scale: 1.0 | grad norm: 1.020 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (77683.78, 78230.88)
10.64.24.50:     forward-compute ................................: (24540.05, 27928.98)
10.64.24.50:     backward-compute ...............................: (44571.41, 49712.15)
10.64.24.50:     batch-generator ................................: (152.52, 240.46)
10.64.24.50:     forward-recv ...................................: (98.98, 288.70)
10.64.24.50:     forward-send ...................................: (1.30, 4.60)
10.64.24.50:     backward-recv ..................................: (123.29, 326.91)
10.64.24.50:     backward-send ..................................: (1.37, 4.08)
10.64.24.50:     forward-send-backward-recv .....................: (3362.35, 7941.56)
10.64.24.50:     backward-send-forward-recv .....................: (536.83, 643.73)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.85, 0.96)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 9.10)
10.64.24.50:     all-grads-sync .................................: (7.53, 9.43)
10.64.24.50:     params-all-gather ..............................: (5.24, 6.34)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.30, 0.41)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.47, 2.73)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.56, 5.57)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.42, 1.74)
10.64.24.50:     optimizer ......................................: (16.15, 17.25)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 20:34:43 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             372W / 700W |  53898MiB / 81559MiB |     15%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             414W / 700W |  53898MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             445W / 700W |  53836MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             349W / 700W |  54092MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             352W / 700W |  42868MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             358W / 700W |  42864MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   47C    P0             279W / 700W |  42800MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             286W / 700W |  42804MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 20:36:02 
10.64.24.49: rank 5: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}}rank 1: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 3: {'packing_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 111, '16384': 49, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 111, '16384': 49, '32768': 0, '>32k': 0}}
10.64.24.49: rank 7: {'packing_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 111, '16384': 49, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 111, '16384': 49, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}}
10.64.24.49: rank 2: {'packing_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 111, '16384': 49, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 111, '16384': 49, '32768': 0, '>32k': 0}}
10.64.24.49: rank 6: {'packing_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 111, '16384': 49, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 111, '16384': 49, '32768': 0, '>32k': 0}}
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen16384_gbs512_mbs1_dp2_tp2_pp4_pad_sp_node1.log...
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 7b model, gpu_num=16, seq_len = 16384, DP=4, MP=2, PP=2, GBS=512, MBS=1
10.64.24.50: use sequence parallel
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen16384_gbs512_mbs1_dp2_tp2_pp4_pad_sp_node0.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 7b model, gpu_num=16, seq_len = 16384, DP=4, MP=2, PP=2, GBS=512, MBS=1
10.64.24.49: use sequence parallel
10.64.24.50: [2024-03-11 20:36:24,131] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 20:36:24,131] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 20:36:24,131] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 20:36:24,131] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 20:36:25,094] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 20:36:25,094] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 20:36:25,094] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 20:36:25,094] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 4, context-parallel size: 1 tensor-model-parallel size: 2, pipeline-model-parallel size: 2 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 4
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 32
10.64.24.49:   encoder_seq_length .............................. 16384
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 16384
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 4096
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 16384
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 1
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 32
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 32
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 2
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 16384
10.64.24.49:   sequence_packing ................................ False
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 2
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 2
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ False
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 128
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: > initialized tensor model parallel with size 2
10.64.24.49: > initialized pipeline model parallel with size 2
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.061 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 7.920 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 10.913
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 20:36:53 
10.64.24.49: building GPT model ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1714528256
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1781628928
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1714528256 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1714528256
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1781628928
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1781628928 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 20:36:55 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...
10.64.24.49: 
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: 
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache end, time cost:  5.070 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.072 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.098 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.114 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.104 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.153 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.199 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.264 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Cutting or padding data end, time cost:  18.571 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  18.728 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  18.651 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  18.725 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  18.830 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  18.828 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  18.910 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  18.796 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 20:37:19 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (1418.13, 1565.77)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 24477.53)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 20:37:19 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 20:45:20 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             510W / 700W |  60810MiB / 81559MiB |     81%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             402W / 700W |  60788MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             416W / 700W |  60816MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             454W / 700W |  60790MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             455W / 700W |  61008MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             451W / 700W |  60832MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   48C    P0             374W / 700W |  60768MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             330W / 700W |  60538MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 80527.6 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 9.544933E+00 | loss scale: 1.0 | grad norm: 838.537 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 15484.845703125 | max allocated: 50881.4091796875 | reserved: 57136.0 | max reserved: 57136.0
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 15484.845703125 | max allocated: 50881.4091796875 | reserved: 57114.0 | max reserved: 57114.0
10.64.24.50: [Rank 9] (after 10 iterations) memory (MB) | allocated: 15036.9169921875 | max allocated: 36531.96435546875 | reserved: 43940.0 | max reserved: 43940.0
10.64.24.50: [Rank 8] (after 10 iterations) memory (MB) | allocated: 15036.9169921875 | max allocated: 36531.96435546875 | reserved: 42672.0 | max reserved: 42672.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (79571.83, 79957.34)
10.64.24.50:     forward-compute ................................: (25554.18, 30682.21)
10.64.24.50:     backward-compute ...............................: (47130.60, 48859.52)
10.64.24.50:     batch-generator ................................: (284.59, 379.47)
10.64.24.50:     forward-recv ...................................: (435.57, 465.17)
10.64.24.50:     forward-send ...................................: (3.55, 9.74)
10.64.24.50:     backward-recv ..................................: (1115.34, 1121.97)
10.64.24.50:     backward-send ..................................: (1.63, 1.69)
10.64.24.50:     forward-send-backward-recv .....................: (4934.21, 6060.88)
10.64.24.50:     backward-send-forward-recv .....................: (332.52, 1278.25)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.47, 1.80)
10.64.24.50:     embedding-grads-all-reduce .....................: (8.90, 9.04)
10.64.24.50:     all-grads-sync .................................: (438.71, 441.11)
10.64.24.50:     params-all-gather ..............................: (12.76, 13.39)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.27, 0.43)
10.64.24.50:     optimizer-clip-main-grad .......................: (5.51, 5.59)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.96, 5.51)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.46, 1.60)
10.64.24.50:     optimizer ......................................: (26.59, 27.23)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 77793.6 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 4.032166E-01 | loss scale: 1.0 | grad norm: 2.526 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (77348.24, 77735.29)
10.64.24.50:     forward-compute ................................: (25196.84, 29901.53)
10.64.24.50:     backward-compute ...............................: (46872.88, 48652.98)
10.64.24.50:     batch-generator ................................: (90.64, 184.34)
10.64.24.50:     forward-recv ...................................: (190.29, 192.51)
10.64.24.50:     forward-send ...................................: (1.56, 1.59)
10.64.24.50:     backward-recv ..................................: (211.96, 222.82)
10.64.24.50:     backward-send ..................................: (1.62, 1.68)
10.64.24.50:     forward-send-backward-recv .....................: (4040.26, 5293.85)
10.64.24.50:     backward-send-forward-recv .....................: (329.95, 1227.01)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.43, 1.66)
10.64.24.50:     embedding-grads-all-reduce .....................: (8.87, 9.10)
10.64.24.50:     all-grads-sync .................................: (19.03, 20.08)
10.64.24.50:     params-all-gather ..............................: (12.77, 13.30)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.28, 0.47)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.55, 2.75)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.84, 5.11)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.46, 1.60)
10.64.24.50:     optimizer ......................................: (23.27, 23.80)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 80491.4 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 4.089606E-01 | loss scale: 1.0 | grad norm: 0.726 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (80047.56, 80433.33)
10.64.24.50:     forward-compute ................................: (25201.37, 32664.36)
10.64.24.50:     backward-compute ...............................: (46807.54, 48579.17)
10.64.24.50:     batch-generator ................................: (89.66, 162.20)
10.64.24.50:     forward-recv ...................................: (190.66, 192.17)
10.64.24.50:     forward-send ...................................: (1.56, 1.59)
10.64.24.50:     backward-recv ..................................: (214.46, 220.05)
10.64.24.50:     backward-send ..................................: (1.61, 1.68)
10.64.24.50:     forward-send-backward-recv .....................: (6936.07, 8033.48)
10.64.24.50:     backward-send-forward-recv .....................: (328.48, 1379.18)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.41, 1.65)
10.64.24.50:     embedding-grads-all-reduce .....................: (8.88, 9.03)
10.64.24.50:     all-grads-sync .................................: (19.10, 20.07)
10.64.24.50:     params-all-gather ..............................: (12.77, 13.71)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.28, 0.45)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.43, 2.50)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.83, 5.10)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.47, 1.60)
10.64.24.50:     optimizer ......................................: (23.05, 24.00)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 21:18:23 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             484W / 700W |  61066MiB / 81559MiB |     84%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             434W / 700W |  60788MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             449W / 700W |  60816MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             462W / 700W |  60790MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             446W / 700W |  61008MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             444W / 700W |  60832MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   48C    P0             375W / 700W |  60768MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             365W / 700W |  60538MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: rank 7: {'packing_seq_len': {'128': 912, '256': 911, '512': 963, '1024': 748, '2048': 345, '4096': 136, '8192': 54, '16384': 27, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 912, '256': 911, '512': 963, '1024': 748, '2048': 345, '4096': 136, '8192': 54, '16384': 27, '32768': 0, '>32k': 0}}[after training is done] datetime: 2024-03-11 21:20:06 
10.64.24.49: 
10.64.24.49: rank 3: {'packing_seq_len': {'128': 938, '256': 968, '512': 883, '1024': 778, '2048': 329, '4096': 121, '8192': 57, '16384': 22, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 938, '256': 968, '512': 883, '1024': 778, '2048': 329, '4096': 121, '8192': 57, '16384': 22, '32768': 0, '>32k': 0}}
10.64.24.49: rank 1: {'packing_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 49, '16384': 28, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 49, '16384': 28, '32768': 0, '>32k': 0}}
10.64.24.49: rank 5: {'packing_seq_len': {'128': 907, '256': 956, '512': 944, '1024': 732, '2048': 343, '4096': 126, '8192': 54, '16384': 34, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 907, '256': 956, '512': 944, '1024': 732, '2048': 343, '4096': 126, '8192': 54, '16384': 34, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 49, '16384': 28, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 49, '16384': 28, '32768': 0, '>32k': 0}}
10.64.24.49: rank 6: {'packing_seq_len': {'128': 912, '256': 911, '512': 963, '1024': 748, '2048': 345, '4096': 136, '8192': 54, '16384': 27, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 912, '256': 911, '512': 963, '1024': 748, '2048': 345, '4096': 136, '8192': 54, '16384': 27, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 49, '16384': 28, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 49, '16384': 28, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 907, '256': 956, '512': 944, '1024': 732, '2048': 343, '4096': 126, '8192': 54, '16384': 34, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 907, '256': 956, '512': 944, '1024': 732, '2048': 343, '4096': 126, '8192': 54, '16384': 34, '32768': 0, '>32k': 0}}
10.64.24.49: rank 2: {'packing_seq_len': {'128': 938, '256': 968, '512': 883, '1024': 778, '2048': 329, '4096': 121, '8192': 57, '16384': 22, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 938, '256': 968, '512': 883, '1024': 778, '2048': 329, '4096': 121, '8192': 57, '16384': 22, '32768': 0, '>32k': 0}}
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen16384_gbs512_mbs1_dp4_tp2_pp2_pad_sp_node0.log...
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen16384_gbs512_mbs1_dp4_tp2_pp2_pad_sp_node1.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 7b model, gpu_num=16, seq_len = 16384, DP=1, MP=2, PP=8, GBS=512, MBS=1
10.64.24.49: use sequence parallel
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 7b model, gpu_num=16, seq_len = 16384, DP=1, MP=2, PP=8, GBS=512, MBS=1
10.64.24.50: use sequence parallel
10.64.24.49: [2024-03-11 21:20:21,051] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 21:20:21,051] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 21:20:21,051] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 21:20:21,051] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 21:20:20,256] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 21:20:20,256] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 21:20:20,256] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 21:20:20,256] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 1, context-parallel size: 1 tensor-model-parallel size: 2, pipeline-model-parallel size: 8 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 1
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 32
10.64.24.49:   encoder_seq_length .............................. 16384
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 16384
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 4096
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 16384
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 1
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 32
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 32
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 8
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 16384
10.64.24.49:   sequence_packing ................................ False
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 2
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 8
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ False
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 512
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: > initialized tensor model parallel with size 2
10.64.24.49: > initialized pipeline model parallel with size 8
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.061 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.806 seconds
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 10.590
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 21:20:48 
10.64.24.49: building GPT model ...
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 402808832
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 402808832
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 402808832
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 6): 402808832
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 402808832
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 402808832
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 402808832
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (402808832 elements):
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (402808832 elements):
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (402808832 elements):
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (402808832 elements):
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 5): 402808832
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 402808832
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 402808832
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 402808832
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 402808832
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (402808832 elements):
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (402808832 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 573202432
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 506101760
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 7): 506101760
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 573202432
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (506101760 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (573202432 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 21:20:49 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache end, time cost:  4.943 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Loading exists cache end, time cost:  4.982 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.139 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.140 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.216 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.246 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.359 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.373 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Cutting or padding data end, time cost:  18.624 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  18.758 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  18.683 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  18.701 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  19.011 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  18.980 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  19.037 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  19.189 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 21:21:14 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (34.21, 968.20)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 24862.80)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 21:21:14 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 21:29:57 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             390W / 700W |  51058MiB / 81559MiB |      6%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   48C    P0             424W / 700W |  50750MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   48C    P0             383W / 700W |  42960MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             344W / 700W |  43212MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             333W / 700W |  38856MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             360W / 700W |  38736MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   46C    P0             283W / 700W |  34256MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             293W / 700W |  34512MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 86618.7 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 8.246214E+00 | loss scale: 1.0 | grad norm: 798.571 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: [Rank 9] (after 10 iterations) memory (MB) | allocated: 7234.673828125 | max allocated: 25076.69140625 | reserved: 27040.0 | max reserved: 27040.0[Rank 13] (after 10 iterations) memory (MB) | allocated: 7234.673828125 | max allocated: 16363.6826171875 | reserved: 18074.0 | max reserved: 18074.0
10.64.24.50: 
10.64.24.50: [Rank 10] (after 10 iterations) memory (MB) | allocated: 7234.673828125 | max allocated: 20720.18701171875 | reserved: 22682.0 | max reserved: 22682.0
10.64.24.50: [Rank 8] (after 10 iterations) memory (MB) | allocated: 7234.673828125 | max allocated: 25076.69140625 | reserved: 27168.0 | max reserved: 27168.0
10.64.24.50: [Rank 12] (after 10 iterations) memory (MB) | allocated: 7234.673828125 | max allocated: 16363.6826171875 | reserved: 18326.0 | max reserved: 18326.0
10.64.24.50: [Rank 15] (after 10 iterations) memory (MB) | allocated: 9008.8154296875 | max allocated: 17434.53857421875 | reserved: 19996.0 | max reserved: 19996.0
10.64.24.49: [Rank 3] (after 10 iterations) memory (MB) | allocated: 7234.673828125 | max allocated: 38146.20458984375 | reserved: 40360.0 | max reserved: 40360.0[Rank 7] (after 10 iterations) memory (MB) | allocated: 7234.673828125 | max allocated: 29433.19580078125 | reserved: 31652.0 | max reserved: 31652.0
10.64.24.49: 
10.64.24.49: [Rank 4] (after 10 iterations) memory (MB) | allocated: 7234.673828125 | max allocated: 33789.7001953125 | reserved: 36004.0 | max reserved: 36004.0[Rank 2] (after 10 iterations) memory (MB) | allocated: 7234.673828125 | max allocated: 38146.20458984375 | reserved: 40108.0 | max reserved: 40108.0
10.64.24.49: 
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 10032.673828125 | max allocated: 45622.833984375 | reserved: 48112.0 | max reserved: 48112.0
10.64.24.49: [Rank 6] (after 10 iterations) memory (MB) | allocated: 7234.673828125 | max allocated: 29433.19580078125 | reserved: 31396.0 | max reserved: 31396.0
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 10032.673828125 | max allocated: 45622.833984375 | reserved: 47804.0 | max reserved: 47804.0
10.64.24.50: [Rank 14] (after 10 iterations) memory (MB) | allocated: 9008.8154296875 | max allocated: 17434.53857421875 | reserved: 19996.0 | max reserved: 19996.0
10.64.24.50: [Rank 11] (after 10 iterations) memory (MB) | allocated: 7234.673828125 | max allocated: 20720.18701171875 | reserved: 22682.0 | max reserved: 22682.0
10.64.24.49: [Rank 5] (after 10 iterations) memory (MB) | allocated: 7234.673828125 | max allocated: 33789.7001953125 | reserved: 35884.0 | max reserved: 35884.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (85793.49, 86423.61)
10.64.24.50:     forward-compute ................................: (24786.36, 30923.53)
10.64.24.50:     backward-compute ...............................: (43823.05, 54049.95)
10.64.24.50:     batch-generator ................................: (371.03, 490.56)
10.64.24.50:     forward-recv ...................................: (180.16, 1008.70)
10.64.24.50:     forward-send ...................................: (4.57, 538.82)
10.64.24.50:     backward-recv ..................................: (77.38, 461.27)
10.64.24.50:     backward-send ..................................: (1.36, 9.23)
10.64.24.50:     forward-send-backward-recv .....................: (5869.30, 15087.79)
10.64.24.50:     backward-send-forward-recv .....................: (1099.92, 1309.00)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.59, 0.69)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 8.98)
10.64.24.50:     all-grads-sync .................................: (58.77, 64.65)
10.64.24.50:     params-all-gather ..............................: (1.38, 1.87)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.32, 0.38)
10.64.24.50:     optimizer-clip-main-grad .......................: (7.07, 7.59)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.69, 6.69)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.42, 1.96)
10.64.24.50:     optimizer ......................................: (19.13, 19.60)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 85728.5 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 4.215681E-01 | loss scale: 1.0 | grad norm: 5.153 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (85066.77, 85697.03)
10.64.24.50:     forward-compute ................................: (24719.87, 30850.16)
10.64.24.50:     backward-compute ...............................: (43801.00, 53841.04)
10.64.24.50:     batch-generator ................................: (299.11, 413.41)
10.64.24.50:     forward-recv ...................................: (58.37, 341.77)
10.64.24.50:     forward-send ...................................: (1.27, 9.76)
10.64.24.50:     backward-recv ..................................: (77.41, 463.22)
10.64.24.50:     backward-send ..................................: (1.35, 9.16)
10.64.24.50:     forward-send-backward-recv .....................: (6050.43, 15087.13)
10.64.24.50:     backward-send-forward-recv .....................: (1031.08, 1239.17)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.57, 0.63)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 9.01)
10.64.24.50:     all-grads-sync .................................: (1.11, 1.58)
10.64.24.50:     params-all-gather ..............................: (1.38, 1.88)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.32, 0.37)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.57, 3.10)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.55, 6.49)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.42, 1.96)
10.64.24.50:     optimizer ......................................: (13.73, 14.31)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 85414.0 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 4.036655E-01 | loss scale: 1.0 | grad norm: 0.898 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (84754.82, 85382.57)
10.64.24.50:     forward-compute ................................: (24666.28, 30741.35)
10.64.24.50:     backward-compute ...............................: (43681.53, 53688.67)
10.64.24.50:     batch-generator ................................: (296.61, 400.96)
10.64.24.50:     forward-recv ...................................: (59.26, 341.82)
10.64.24.50:     forward-send ...................................: (1.27, 9.04)
10.64.24.50:     backward-recv ..................................: (76.84, 462.52)
10.64.24.50:     backward-send ..................................: (1.35, 9.13)
10.64.24.50:     forward-send-backward-recv .....................: (5934.22, 14950.55)
10.64.24.50:     backward-send-forward-recv .....................: (1022.56, 1233.08)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.58, 0.63)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 9.00)
10.64.24.50:     all-grads-sync .................................: (1.11, 1.57)
10.64.24.50:     params-all-gather ..............................: (1.38, 1.88)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.32, 0.37)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.44, 2.91)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.55, 6.49)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.42, 1.96)
10.64.24.50:     optimizer ......................................: (13.56, 14.05)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 22:05:37 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             397W / 700W |  51058MiB / 81559MiB |     39%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             426W / 700W |  51032MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             327W / 700W |  43216MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             347W / 700W |  43212MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             344W / 700W |  38856MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             356W / 700W |  38736MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   46C    P0             294W / 700W |  34512MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             266W / 700W |  34512MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 22:07:03 
10.64.24.49: rank 1: {'packing_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}rank 3: {'packing_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}rank 7: {'packing_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: 
10.64.24.49: rank 5: {'packing_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: rank 2: {'packing_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: rank 6: {'packing_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen16384_gbs512_mbs1_dp1_tp2_pp8_pad_sp_node0.log...
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen16384_gbs512_mbs1_dp1_tp2_pp8_pad_sp_node1.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 7b model, gpu_num=16, seq_len = 16384, DP=4, MP=4, PP=1, GBS=512, MBS=1
10.64.24.49: use sequence parallel
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 7b model, gpu_num=16, seq_len = 16384, DP=4, MP=4, PP=1, GBS=512, MBS=1
10.64.24.50: use sequence parallel
10.64.24.49: [2024-03-11 22:07:21,483] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 22:07:21,483] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 22:07:21,483] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 22:07:21,483] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 22:07:20,598] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 22:07:20,598] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 22:07:20,598] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 22:07:20,598] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 4, context-parallel size: 1 tensor-model-parallel size: 4, pipeline-model-parallel size: 1 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 4
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 32
10.64.24.49:   encoder_seq_length .............................. 16384
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 16384
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 4096
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 16384
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 1
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 32
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 32
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 1
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 16384
10.64.24.49:   sequence_packing ................................ False
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 4
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 1
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ False
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 128
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 431 dummy tokens (new size: 50688)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: > initialized tensor model parallel with size 4
10.64.24.49: > initialized pipeline model parallel with size 1
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.060 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 7.933 seconds
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 10.900
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 22:07:48 
10.64.24.49: building GPT model ...
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 1730650112
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 1730650112
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1730650112
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1730650112
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1730650112 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 22:07:49 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache end, time cost:  4.875 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  4.934 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.082 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Loading exists cache end, time cost:  6.016 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Cutting or padding data end, time cost:  18.679 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  18.692 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  18.605 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  18.936 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 22:08:15 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (822.64, 957.04)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 25443.37)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 22:08:15 
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 22:17:05 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             532W / 700W |  48030MiB / 81559MiB |     50%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             546W / 700W |  48090MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   52C    P0             519W / 700W |  48018MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             523W / 700W |  47670MiB / 81559MiB |     96%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   46C    P0             517W / 700W |  48604MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   52C    P0             500W / 700W |  47952MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             518W / 700W |  47920MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             489W / 700W |  47628MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 87981.6 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 1.029338E+01 | loss scale: 1.0 | grad norm: 818.434 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 15047.9404296875 | max allocated: 34535.11279296875 | reserved: 44584.0 | max reserved: 44584.0
10.64.24.49: [Rank 2] (after 10 iterations) memory (MB) | allocated: 15047.9404296875 | max allocated: 34535.11279296875 | reserved: 44512.0 | max reserved: 44512.0
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 15047.9404296875 | max allocated: 34535.11279296875 | reserved: 44572.0 | max reserved: 44572.0
10.64.24.49: [Rank 3] (after 10 iterations) memory (MB) | allocated: 15047.9404296875 | max allocated: 34535.11279296875 | reserved: 44404.0 | max reserved: 44404.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (87575.73, 87582.80)
10.64.24.50:     forward-compute ................................: (34478.37, 35409.31)
10.64.24.50:     backward-compute ...............................: (52097.49, 53037.51)
10.64.24.50:     batch-generator ................................: (650.42, 795.68)
10.64.24.50:     layernorm-grads-all-reduce .....................: (2.56, 2.83)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.04)
10.64.24.50:     all-grads-sync .................................: (253.51, 258.07)
10.64.24.50:     params-all-gather ..............................: (32.64, 32.99)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.57, 0.76)
10.64.24.50:     optimizer-clip-main-grad .......................: (5.52, 5.56)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (5.07, 5.38)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.64, 1.80)
10.64.24.50:     optimizer ......................................: (46.73, 47.08)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 88023.8 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 3.990933E-01 | loss scale: 1.0 | grad norm: 4.723 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (87907.46, 87915.00)
10.64.24.50:     forward-compute ................................: (34993.36, 36008.14)
10.64.24.50:     backward-compute ...............................: (51830.60, 52854.86)
10.64.24.50:     batch-generator ................................: (140.32, 230.47)
10.64.24.50:     layernorm-grads-all-reduce .....................: (2.50, 2.65)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.03)
10.64.24.50:     all-grads-sync .................................: (57.46, 57.54)
10.64.24.50:     params-all-gather ..............................: (32.66, 33.00)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.55, 0.74)
10.64.24.50:     optimizer-clip-main-grad .......................: (3.18, 3.21)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.92, 5.04)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.64, 1.80)
10.64.24.50:     optimizer ......................................: (43.94, 44.29)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 87896.4 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 4.021412E-01 | loss scale: 1.0 | grad norm: 1.323 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (87780.04, 87787.99)
10.64.24.50:     forward-compute ................................: (34986.61, 36065.78)
10.64.24.50:     backward-compute ...............................: (51644.29, 52734.42)
10.64.24.50:     batch-generator ................................: (138.54, 243.31)
10.64.24.50:     layernorm-grads-all-reduce .....................: (2.45, 2.63)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.03)
10.64.24.50:     all-grads-sync .................................: (57.46, 57.51)
10.64.24.50:     params-all-gather ..............................: (32.66, 33.00)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.54, 0.74)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.50, 2.52)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.92, 5.46)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.64, 1.79)
10.64.24.50:     optimizer ......................................: (43.76, 44.09)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 22:53:37 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             532W / 700W |  48822MiB / 81559MiB |     45%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             543W / 700W |  48090MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             521W / 700W |  48018MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             521W / 700W |  47670MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             483W / 700W |  48604MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             478W / 700W |  47952MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   48C    P0             483W / 700W |  47920MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             469W / 700W |  47628MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 22:55:09 
10.64.24.49: rank 1: {'packing_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 49, '16384': 28, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 49, '16384': 28, '32768': 0, '>32k': 0}}rank 6: {'packing_seq_len': {'128': 938, '256': 968, '512': 883, '1024': 778, '2048': 329, '4096': 121, '8192': 57, '16384': 22, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 938, '256': 968, '512': 883, '1024': 778, '2048': 329, '4096': 121, '8192': 57, '16384': 22, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 5: {'packing_seq_len': {'128': 938, '256': 968, '512': 883, '1024': 778, '2048': 329, '4096': 121, '8192': 57, '16384': 22, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 938, '256': 968, '512': 883, '1024': 778, '2048': 329, '4096': 121, '8192': 57, '16384': 22, '32768': 0, '>32k': 0}}
10.64.24.49: rank 7: {'packing_seq_len': {'128': 938, '256': 968, '512': 883, '1024': 778, '2048': 329, '4096': 121, '8192': 57, '16384': 22, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 938, '256': 968, '512': 883, '1024': 778, '2048': 329, '4096': 121, '8192': 57, '16384': 22, '32768': 0, '>32k': 0}}
10.64.24.49: rank 2: {'packing_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 49, '16384': 28, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 49, '16384': 28, '32768': 0, '>32k': 0}}
10.64.24.49: rank 3: {'packing_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 49, '16384': 28, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 49, '16384': 28, '32768': 0, '>32k': 0}}
10.64.24.49: terminate called after throwing an instance of 'c10::Error'
10.64.24.49:   what():  CUDA driver error: unknown error
10.64.24.49: Exception raised from _hasPrimaryContext at /opt/pytorch/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp:67 (most recent call first):
10.64.24.49: frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f1b6f1b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
10.64.24.49: frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f1b6f16b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
10.64.24.49: frame #2: <unknown function> + 0xd55cef (0x7f1ae9a14cef in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.49: frame #3: c10::cuda::MaybeSetDevice(int) + 0xc (0x7f1b79fd6bac in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10_cuda.so)
10.64.24.49: frame #4: <unknown function> + 0xd4e4a7 (0x7f1ae9a0d4a7 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.49: frame #5: <unknown function> + 0xe44cb8 (0x7f1ae9b03cb8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.49: frame #6: <unknown function> + 0xd520ea (0x7f1ae9a110ea in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.49: frame #7: c10d::ProcessGroupNCCL::WorkNCCL::~WorkNCCL() + 0x127 (0x7f1ae9ad6f57 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.49: frame #8: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x22a (0x7f1ae9ad908a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.49: frame #9: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x80 (0x7f1ae9ad92f0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.49: frame #10: <unknown function> + 0xdc253 (0x7f1ae88b0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
10.64.24.49: frame #11: <unknown function> + 0x94ac3 (0x7f1b85279ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
10.64.24.49: frame #12: <unknown function> + 0x126a40 (0x7f1b8530ba40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
10.64.24.49: 
10.64.24.49: [2024-03-11 22:55:14,408] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1355779 closing signal SIGTERM
10.64.24.49: [2024-03-11 22:55:14,408] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1355783 closing signal SIGTERM
10.64.24.49: [2024-03-11 22:55:15,473] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 6 (pid: 1355785) of binary: /usr/bin/python
10.64.24.49: Traceback (most recent call last):
10.64.24.49:   File "/usr/local/bin/torchrun", line 33, in <module>
10.64.24.49:     sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
10.64.24.49:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
10.64.24.49:     return f(*args, **kwargs)
10.64.24.49:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
10.64.24.49:     run(args)
10.64.24.49:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
10.64.24.49:     elastic_launch(
10.64.24.49:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
10.64.24.49:     return launch_agent(self._config, self._entrypoint, list(args))
10.64.24.49:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
10.64.24.49:     raise ChildFailedError(
10.64.24.49: torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
10.64.24.49: ========================================================
10.64.24.49: pretrain_gpt.py FAILED
10.64.24.49: --------------------------------------------------------
10.64.24.49: Failures:
10.64.24.49:   <NO_OTHER_FAILURES>
10.64.24.49: --------------------------------------------------------
10.64.24.49: Root Cause (first observed failure):
10.64.24.49: [0]:
10.64.24.49:   time      : 2024-03-11_22:55:14
10.64.24.49:   host      : SYM206-GPU-A0203-P2-Node49
10.64.24.49:   rank      : 6 (local_rank: 6)
10.64.24.49:   exitcode  : -6 (pid: 1355785)
10.64.24.49:   error_file: <N/A>
10.64.24.49:   traceback : Signal 6 (SIGABRT) received by PID 1355785
10.64.24.49: ========================================================
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen16384_gbs512_mbs1_dp4_tp4_pp1_pad_sp_node0.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 7b model, gpu_num=16, seq_len = 8192, DP=4, MP=2, PP=2, GBS=512, MBS=2
10.64.24.49: use sequence parallel
10.64.24.50: rank 8: {'packing_seq_len': {'128': 907, '256': 956, '512': 944, '1024': 732, '2048': 343, '4096': 126, '8192': 54, '16384': 34, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 907, '256': 956, '512': 944, '1024': 732, '2048': 343, '4096': 126, '8192': 54, '16384': 34, '32768': 0, '>32k': 0}}
10.64.24.49: [2024-03-11 22:55:18,627] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 22:55:18,627] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 22:55:18,627] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 22:55:18,627] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 22:55:23,391] torch.distributed.elastic.agent.server.api: [ERROR] Error waiting on exit barrier. Elapsed: 0.0008690357208251953 seconds
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen16384_gbs512_mbs1_dp4_tp4_pp1_pad_sp_node1.log...
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 7b model, gpu_num=16, seq_len = 8192, DP=4, MP=2, PP=2, GBS=512, MBS=2
10.64.24.50: use sequence parallel
10.64.24.50: [2024-03-11 22:55:25,789] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 22:55:25,789] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 22:55:25,789] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 22:55:25,789] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 4, context-parallel size: 1 tensor-model-parallel size: 2, pipeline-model-parallel size: 2 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 4
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 32
10.64.24.49:   encoder_seq_length .............................. 8192
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 16384
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 4096
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 8192
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 2
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 32
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 32
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 2
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 8192
10.64.24.49:   sequence_packing ................................ False
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 2
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 2
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ False
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 64
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: > initialized tensor model parallel with size 2
10.64.24.49: > initialized pipeline model parallel with size 2
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.060 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.255 seconds
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 10.728
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 22:55:53 
10.64.24.49: building GPT model ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1714528256
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1748074496
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1714528256 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1748074496
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1714528256
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1748074496 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 22:55:55 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...
10.64.24.49: 
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...
10.64.24.50: 
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache end, time cost:  5.067 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.080 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.080 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.121 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.182 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.228 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.374 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  6.714 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Cutting or padding data end, time cost:  10.539 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.617 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.791 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.816 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.797 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  11.073 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  11.743 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.943 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 22:56:13 
10.64.24.49: done with setup ...
10.64.24.49: training ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (1068.80, 1212.09)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 18034.06)
10.64.24.49: [before the start of training step] datetime: 2024-03-11 22:56:13 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 22:59:40 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             474W / 700W |  62424MiB / 81559MiB |     52%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             397W / 700W |  62300MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             472W / 700W |  62596MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             389W / 700W |  62472MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             379W / 700W |  62360MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             380W / 700W |  62492MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   48C    P0             381W / 700W |  62132MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             425W / 700W |  62192MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 33756.8 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 9.703425E+00 | loss scale: 1.0 | grad norm: 790.379 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 15196.845703125 | max allocated: 52641.4091796875 | reserved: 58626.0 | max reserved: 58626.0
10.64.24.50: [Rank 8] (after 10 iterations) memory (MB) | allocated: 15036.9169921875 | max allocated: 37556.1826171875 | reserved: 43656.0 | max reserved: 43656.0
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 15196.845703125 | max allocated: 52641.4091796875 | reserved: 58750.0 | max reserved: 58750.0
10.64.24.50: [Rank 9] (after 10 iterations) memory (MB) | allocated: 15036.9169921875 | max allocated: 37556.1826171875 | reserved: 43656.0 | max reserved: 43656.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (32858.72, 33182.09)
10.64.24.50:     forward-compute ................................: (10498.04, 12902.52)
10.64.24.50:     backward-compute ...............................: (19331.12, 20408.07)
10.64.24.50:     batch-generator ................................: (254.35, 307.64)
10.64.24.50:     forward-recv ...................................: (411.85, 450.72)
10.64.24.50:     forward-send ...................................: (3.84, 11.94)
10.64.24.50:     backward-recv ..................................: (189.15, 193.51)
10.64.24.50:     backward-send ..................................: (1.60, 1.74)
10.64.24.50:     forward-send-backward-recv .....................: (2715.66, 3107.38)
10.64.24.50:     backward-send-forward-recv .....................: (165.77, 178.39)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.46, 1.90)
10.64.24.50:     embedding-grads-all-reduce .....................: (8.90, 9.05)
10.64.24.50:     all-grads-sync .................................: (440.12, 441.89)
10.64.24.50:     params-all-gather ..............................: (12.79, 13.07)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.28, 0.45)
10.64.24.50:     optimizer-clip-main-grad .......................: (5.64, 5.68)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.98, 5.29)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.47, 1.58)
10.64.24.50:     optimizer ......................................: (26.66, 26.95)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 34390.0 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 7.874741E-01 | loss scale: 1.0 | grad norm: 2.169 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (34010.94, 34332.28)
10.64.24.50:     forward-compute ................................: (10741.90, 13711.57)
10.64.24.50:     backward-compute ...............................: (19262.45, 20270.85)
10.64.24.50:     batch-generator ................................: (54.34, 86.25)
10.64.24.50:     forward-recv ...................................: (159.89, 161.40)
10.64.24.50:     forward-send ...................................: (1.56, 1.61)
10.64.24.50:     backward-recv ..................................: (738.07, 742.69)
10.64.24.50:     backward-send ..................................: (1.63, 1.69)
10.64.24.50:     forward-send-backward-recv .....................: (2632.82, 3538.59)
10.64.24.50:     backward-send-forward-recv .....................: (167.54, 1193.37)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.42, 1.72)
10.64.24.50:     embedding-grads-all-reduce .....................: (8.90, 9.04)
10.64.24.50:     all-grads-sync .................................: (19.13, 20.06)
10.64.24.50:     params-all-gather ..............................: (12.76, 13.07)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.28, 0.44)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.51, 2.63)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.83, 5.12)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.47, 1.58)
10.64.24.50:     optimizer ......................................: (23.07, 23.38)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 33042.0 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 7.658665E-01 | loss scale: 1.0 | grad norm: 1.897 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (32662.33, 32985.26)
10.64.24.50:     forward-compute ................................: (10676.36, 12626.97)
10.64.24.50:     backward-compute ...............................: (19217.04, 20241.45)
10.64.24.50:     batch-generator ................................: (53.62, 84.96)
10.64.24.50:     forward-recv ...................................: (159.63, 161.51)
10.64.24.50:     forward-send ...................................: (1.55, 1.60)
10.64.24.50:     backward-recv ..................................: (187.28, 193.04)
10.64.24.50:     backward-send ..................................: (1.63, 1.70)
10.64.24.50:     forward-send-backward-recv .....................: (2428.00, 2854.21)
10.64.24.50:     backward-send-forward-recv .....................: (166.38, 634.91)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.40, 1.64)
10.64.24.50:     embedding-grads-all-reduce .....................: (8.90, 9.02)
10.64.24.50:     all-grads-sync .................................: (19.07, 19.72)
10.64.24.50:     params-all-gather ..............................: (12.77, 13.10)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.27, 0.44)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.46, 2.50)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.84, 4.99)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.47, 1.57)
10.64.24.50:     optimizer ......................................: (22.79, 23.11)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 23:13:37 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             469W / 700W |  62424MiB / 81559MiB |     11%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             482W / 700W |  62300MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             438W / 700W |  62596MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             426W / 700W |  62472MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             434W / 700W |  62616MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             370W / 700W |  62492MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   48C    P0             383W / 700W |  62196MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             397W / 700W |  62192MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 23:14:10 
10.64.24.49: rank 7: {'packing_seq_len': {'128': 23, '256': 173, '512': 478, '1024': 599, '2048': 493, '4096': 174, '8192': 72, '16384': 36, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 894, '256': 936, '512': 963, '1024': 741, '2048': 337, '4096': 136, '8192': 89, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 5: {'packing_seq_len': {'128': 20, '256': 191, '512': 502, '1024': 631, '2048': 426, '4096': 190, '8192': 56, '16384': 32, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 945, '256': 975, '512': 890, '1024': 739, '2048': 342, '4096': 124, '8192': 81, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 1: {'packing_seq_len': {'128': 25, '256': 174, '512': 481, '1024': 616, '2048': 463, '4096': 196, '8192': 61, '16384': 32, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 901, '256': 945, '512': 916, '1024': 793, '2048': 327, '4096': 139, '8192': 75, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 3: {'packing_seq_len': {'128': 9, '256': 179, '512': 483, '1024': 622, '2048': 484, '4096': 183, '8192': 56, '16384': 32, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 925, '256': 931, '512': 944, '1024': 739, '2048': 351, '4096': 126, '8192': 80, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 25, '256': 174, '512': 481, '1024': 616, '2048': 463, '4096': 196, '8192': 61, '16384': 32, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 901, '256': 945, '512': 916, '1024': 793, '2048': 327, '4096': 139, '8192': 75, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 20, '256': 191, '512': 502, '1024': 631, '2048': 426, '4096': 190, '8192': 56, '16384': 32, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 945, '256': 975, '512': 890, '1024': 739, '2048': 342, '4096': 124, '8192': 81, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 25, '256': 174, '512': 481, '1024': 616, '2048': 463, '4096': 196, '8192': 61, '16384': 32, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 901, '256': 945, '512': 916, '1024': 793, '2048': 327, '4096': 139, '8192': 75, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 6: {'packing_seq_len': {'128': 23, '256': 173, '512': 478, '1024': 599, '2048': 493, '4096': 174, '8192': 72, '16384': 36, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 894, '256': 936, '512': 963, '1024': 741, '2048': 337, '4096': 136, '8192': 89, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 2: {'packing_seq_len': {'128': 9, '256': 179, '512': 483, '1024': 622, '2048': 484, '4096': 183, '8192': 56, '16384': 32, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 925, '256': 931, '512': 944, '1024': 739, '2048': 351, '4096': 126, '8192': 80, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen8192_gbs512_mbs2_dp4_tp2_pp2_pad_sp_node0.log...
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen8192_gbs512_mbs2_dp4_tp2_pp2_pad_sp_node1.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 7b model, gpu_num=16, seq_len = 8192, DP=8, MP=2, PP=1, GBS=512, MBS=1
10.64.24.49: use sequence parallel
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 7b model, gpu_num=16, seq_len = 8192, DP=8, MP=2, PP=1, GBS=512, MBS=1
10.64.24.50: use sequence parallel
10.64.24.50: [2024-03-11 23:14:24,113] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 23:14:24,113] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 23:14:24,113] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 23:14:24,113] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 23:14:25,074] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 23:14:25,074] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 23:14:25,074] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 23:14:25,074] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 8, context-parallel size: 1 tensor-model-parallel size: 2, pipeline-model-parallel size: 1 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 8
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 32
10.64.24.49:   encoder_seq_length .............................. 8192
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 16384
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 4096
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 8192
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 1
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 32
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 32
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 1
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 8192
10.64.24.49:   sequence_packing ................................ False
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 2
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 1
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ False
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 64
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: > initialized tensor model parallel with size 2
10.64.24.49: > initialized pipeline model parallel with size 1
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.060 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 7.824 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 10.727
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 23:14:53 
10.64.24.49: building GPT model ...
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 3359318016
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 3359318016
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (3359318016 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 23:14:53 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...
10.64.24.50: 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: 
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache end, time cost:  5.011 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.018 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.021 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.061 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.059 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.063 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.131 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  6.650 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Cutting or padding data end, time cost:  10.520 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.674 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.717 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.637 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.763 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.820 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.807 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  11.079 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 23:15:12 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (462.19, 502.90)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 18172.58)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 23:15:12 
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 23:18:50 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             527W / 700W |  53446MiB / 81559MiB |      0%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             455W / 700W |  53234MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             535W / 700W |  53508MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             447W / 700W |  53360MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             427W / 700W |  53340MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             414W / 700W |  53430MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   48C    P0             404W / 700W |  53454MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             425W / 700W |  53380MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 35699.2 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 9.510474E+00 | loss scale: 1.0 | grad norm: 771.491 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 24157.5107421875 | max allocated: 43634.12841796875 | reserved: 49460.0 | max reserved: 49460.0
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 24157.5107421875 | max allocated: 43634.12841796875 | reserved: 49248.0 | max reserved: 49248.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (34726.92, 34742.21)
10.64.24.50:     forward-compute ................................: (13288.52, 14331.73)
10.64.24.50:     backward-compute ...............................: (20360.41, 21403.16)
10.64.24.50:     batch-generator ................................: (272.75, 310.56)
10.64.24.50:     layernorm-grads-all-reduce .....................: (2.59, 3.57)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.05)
10.64.24.50:     all-grads-sync .................................: (752.80, 783.52)
10.64.24.50:     params-all-gather ..............................: (40.68, 40.81)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.27, 0.44)
10.64.24.50:     optimizer-clip-main-grad .......................: (6.44, 6.47)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.88, 5.29)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.42, 1.52)
10.64.24.50:     optimizer ......................................: (55.13, 55.23)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 33648.7 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 7.969889E-01 | loss scale: 1.0 | grad norm: 4.337 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (33504.84, 33521.10)
10.64.24.50:     forward-compute ................................: (12142.86, 13141.67)
10.64.24.50:     backward-compute ...............................: (20328.86, 21344.20)
10.64.24.50:     batch-generator ................................: (64.86, 90.78)
10.64.24.50:     layernorm-grads-all-reduce .....................: (2.45, 2.92)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.03)
10.64.24.50:     all-grads-sync .................................: (68.77, 69.69)
10.64.24.50:     params-all-gather ..............................: (40.67, 40.80)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.26, 0.40)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.44, 2.46)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.75, 4.87)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.42, 1.51)
10.64.24.50:     optimizer ......................................: (50.40, 50.52)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 33600.8 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 7.609707E-01 | loss scale: 1.0 | grad norm: 2.044 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (33458.28, 33474.03)
10.64.24.50:     forward-compute ................................: (12149.44, 13129.20)
10.64.24.50:     backward-compute ...............................: (20282.92, 21290.71)
10.64.24.50:     batch-generator ................................: (63.55, 92.14)
10.64.24.50:     layernorm-grads-all-reduce .....................: (2.45, 3.06)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.03)
10.64.24.50:     all-grads-sync .................................: (68.78, 69.06)
10.64.24.50:     params-all-gather ..............................: (40.67, 40.80)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.25, 0.42)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.42, 2.44)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.75, 4.84)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.42, 1.51)
10.64.24.50:     optimizer ......................................: (50.36, 50.48)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 23:32:54 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             552W / 700W |  53446MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             573W / 700W |  53234MiB / 81559MiB |     96%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             520W / 700W |  53508MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             526W / 700W |  53360MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   46C    P0             499W / 700W |  53340MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   52C    P0             482W / 700W |  53430MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             512W / 700W |  53454MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             447W / 700W |  53380MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 23:33:27 
10.64.24.49: rank 3: {'packing_seq_len': {'128': 461, '256': 473, '512': 445, '1024': 405, '2048': 162, '4096': 66, '8192': 36, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 461, '256': 473, '512': 445, '1024': 405, '2048': 162, '4096': 66, '8192': 36, '16384': 0, '32768': 0, '>32k': 0}}rank 5: {'packing_seq_len': {'128': 476, '256': 464, '512': 456, '1024': 371, '2048': 177, '4096': 64, '8192': 40, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 476, '256': 464, '512': 456, '1024': 371, '2048': 177, '4096': 64, '8192': 40, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 1: {'packing_seq_len': {'128': 440, '256': 472, '512': 471, '1024': 388, '2048': 165, '4096': 73, '8192': 39, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 440, '256': 472, '512': 471, '1024': 388, '2048': 165, '4096': 73, '8192': 39, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 7: {'packing_seq_len': {'128': 449, '256': 467, '512': 488, '1024': 368, '2048': 174, '4096': 62, '8192': 40, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 449, '256': 467, '512': 488, '1024': 368, '2048': 174, '4096': 62, '8192': 40, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 476, '256': 464, '512': 456, '1024': 371, '2048': 177, '4096': 64, '8192': 40, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 476, '256': 464, '512': 456, '1024': 371, '2048': 177, '4096': 64, '8192': 40, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 2: {'packing_seq_len': {'128': 461, '256': 473, '512': 445, '1024': 405, '2048': 162, '4096': 66, '8192': 36, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 461, '256': 473, '512': 445, '1024': 405, '2048': 162, '4096': 66, '8192': 36, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 440, '256': 472, '512': 471, '1024': 388, '2048': 165, '4096': 73, '8192': 39, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 440, '256': 472, '512': 471, '1024': 388, '2048': 165, '4096': 73, '8192': 39, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 6: {'packing_seq_len': {'128': 449, '256': 467, '512': 488, '1024': 368, '2048': 174, '4096': 62, '8192': 40, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 449, '256': 467, '512': 488, '1024': 368, '2048': 174, '4096': 62, '8192': 40, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 468, '256': 480, '512': 452, '1024': 366, '2048': 175, '4096': 69, '8192': 38, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 468, '256': 480, '512': 452, '1024': 366, '2048': 175, '4096': 69, '8192': 38, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen8192_gbs512_mbs1_dp8_tp2_pp1_pad_sp_node1.log...
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen8192_gbs512_mbs1_dp8_tp2_pp1_pad_sp_node0.log...
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 7b model, gpu_num=16, seq_len = 4096, DP=4, MP=2, PP=2, GBS=512, MBS=4
10.64.24.50: use sequence parallel
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 7b model, gpu_num=16, seq_len = 4096, DP=4, MP=2, PP=2, GBS=512, MBS=4
10.64.24.49: use sequence parallel
10.64.24.50: [2024-03-11 23:33:38,639] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 23:33:38,639] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 23:33:38,639] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 23:33:38,639] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 23:33:39,581] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 23:33:39,581] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 23:33:39,581] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 23:33:39,581] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 4, context-parallel size: 1 tensor-model-parallel size: 2, pipeline-model-parallel size: 2 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 4
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 32
10.64.24.49:   encoder_seq_length .............................. 4096
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 16384
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 4096
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 4096
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 4
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 32
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 32
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 2
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 4096
10.64.24.49:   sequence_packing ................................ False
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 2
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 2
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ False
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 32
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: > initialized tensor model parallel with size 2
10.64.24.49: > initialized pipeline model parallel with size 2
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.060 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.050 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 11.673
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 23:34:08 
10.64.24.49: building GPT model ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1731297280
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1714528256
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1714528256
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1731297280
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1714528256 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1731297280 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 23:34:09 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...
10.64.24.50: 
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache end, time cost:  5.032 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.033 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.098 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.101 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.158 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.207 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.330 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Loading exists cache end, time cost:  6.287 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Cutting or padding data end, time cost:  4.944 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  4.968 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  5.089 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  5.098 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  5.206 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  5.161 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  5.100 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  5.990 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 23:34:22 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (928.00, 973.55)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 12540.84)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 23:34:22 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 23:36:03 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             408W / 700W |  62202MiB / 81559MiB |     20%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             445W / 700W |  61950MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             482W / 700W |  62252MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             503W / 700W |  62252MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             514W / 700W |  62260MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             494W / 700W |  62256MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   47C    P0             388W / 700W |  62026MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             403W / 700W |  62022MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 15953.4 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 9.804596E+00 | loss scale: 1.0 | grad norm: 720.026 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: [Rank 9] (after 10 iterations) memory (MB) | allocated: 15036.9169921875 | max allocated: 37556.1669921875 | reserved: 44612.0 | max reserved: 44612.0
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 15052.845703125 | max allocated: 52497.4091796875 | reserved: 58276.0 | max reserved: 58276.0
10.64.24.50: [Rank 8] (after 10 iterations) memory (MB) | allocated: 15036.9169921875 | max allocated: 37556.1669921875 | reserved: 44844.0 | max reserved: 44844.0
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 15052.845703125 | max allocated: 52497.4091796875 | reserved: 58528.0 | max reserved: 58528.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (15082.36, 15357.61)
10.64.24.50:     forward-compute ................................: (4835.57, 6102.71)
10.64.24.50:     backward-compute ...............................: (8424.49, 8801.28)
10.64.24.50:     batch-generator ................................: (200.77, 268.16)
10.64.24.50:     forward-recv ...................................: (386.43, 424.35)
10.64.24.50:     forward-send ...................................: (3.59, 9.80)
10.64.24.50:     backward-recv ..................................: (168.55, 173.01)
10.64.24.50:     backward-send ..................................: (1.63, 1.73)
10.64.24.50:     forward-send-backward-recv .....................: (1753.02, 1895.81)
10.64.24.50:     backward-send-forward-recv .....................: (83.96, 90.42)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.42, 1.87)
10.64.24.50:     embedding-grads-all-reduce .....................: (8.88, 9.02)
10.64.24.50:     all-grads-sync .................................: (423.31, 465.02)
10.64.24.50:     params-all-gather ..............................: (12.78, 12.98)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.29, 0.45)
10.64.24.50:     optimizer-clip-main-grad .......................: (5.51, 5.54)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.98, 5.18)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.47, 1.57)
10.64.24.50:     optimizer ......................................: (26.23, 26.42)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 14439.5 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.499193E+00 | loss scale: 1.0 | grad norm: 11.929 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (14107.86, 14382.18)
10.64.24.50:     forward-compute ................................: (4555.89, 5382.62)
10.64.24.50:     backward-compute ...............................: (8394.67, 8774.14)
10.64.24.50:     batch-generator ................................: (26.45, 39.58)
10.64.24.50:     forward-recv ...................................: (142.58, 144.54)
10.64.24.50:     forward-send ...................................: (1.56, 1.61)
10.64.24.50:     backward-recv ..................................: (168.75, 172.43)
10.64.24.50:     backward-send ..................................: (1.64, 1.79)
10.64.24.50:     forward-send-backward-recv .....................: (1045.98, 1237.27)
10.64.24.50:     backward-send-forward-recv .....................: (81.49, 87.85)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.32, 1.63)
10.64.24.50:     embedding-grads-all-reduce .....................: (8.86, 9.06)
10.64.24.50:     all-grads-sync .................................: (19.14, 19.74)
10.64.24.50:     params-all-gather ..............................: (12.74, 12.97)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.28, 0.40)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.45, 2.53)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.82, 5.00)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.47, 1.56)
10.64.24.50:     optimizer ......................................: (23.37, 23.60)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 15187.3 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.422352E+00 | loss scale: 1.0 | grad norm: 2.868 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (14856.46, 15131.19)
10.64.24.50:     forward-compute ................................: (4813.27, 5895.23)
10.64.24.50:     backward-compute ...............................: (8371.34, 8754.93)
10.64.24.50:     batch-generator ................................: (27.33, 39.48)
10.64.24.50:     forward-recv ...................................: (142.57, 144.41)
10.64.24.50:     forward-send ...................................: (1.56, 1.60)
10.64.24.50:     backward-recv ..................................: (168.03, 175.20)
10.64.24.50:     backward-send ..................................: (1.62, 1.76)
10.64.24.50:     forward-send-backward-recv .....................: (1560.65, 1750.62)
10.64.24.50:     backward-send-forward-recv .....................: (313.37, 368.32)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.36, 1.64)
10.64.24.50:     embedding-grads-all-reduce .....................: (8.86, 8.93)
10.64.24.50:     all-grads-sync .................................: (19.08, 19.74)
10.64.24.50:     params-all-gather ..............................: (12.75, 12.99)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.28, 0.38)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.44, 2.46)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.82, 4.89)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.47, 1.56)
10.64.24.50:     optimizer ......................................: (22.58, 22.81)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 23:42:12 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             403W / 700W |  62202MiB / 81559MiB |     22%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             470W / 700W |  62206MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             513W / 700W |  62252MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             498W / 700W |  62252MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             517W / 700W |  62260MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             484W / 700W |  62256MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   48C    P0             388W / 700W |  62026MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             392W / 700W |  62022MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 23:42:28 
10.64.24.49: rank 1: {'packing_seq_len': {'128': 0, '256': 1, '512': 26, '1024': 193, '2048': 387, '4096': 298, '8192': 112, '16384': 7, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 937, '256': 962, '512': 926, '1024': 756, '2048': 317, '4096': 198, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 5: {'packing_seq_len': {'128': 0, '256': 0, '512': 16, '1024': 181, '2048': 391, '4096': 299, '8192': 123, '16384': 14, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 889, '256': 914, '512': 934, '1024': 776, '2048': 361, '4096': 222, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 7: {'packing_seq_len': {'128': 0, '256': 1, '512': 26, '1024': 190, '2048': 398, '4096': 261, '8192': 136, '16384': 12, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 926, '256': 954, '512': 928, '1024': 725, '2048': 342, '4096': 221, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 3: {'packing_seq_len': {'128': 0, '256': 0, '512': 19, '1024': 193, '2048': 379, '4096': 304, '8192': 119, '16384': 10, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 913, '256': 957, '512': 925, '1024': 755, '2048': 337, '4096': 209, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 0, '256': 0, '512': 16, '1024': 181, '2048': 391, '4096': 299, '8192': 123, '16384': 14, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 889, '256': 914, '512': 934, '1024': 776, '2048': 361, '4096': 222, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 6: {'packing_seq_len': {'128': 0, '256': 1, '512': 26, '1024': 190, '2048': 398, '4096': 261, '8192': 136, '16384': 12, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 926, '256': 954, '512': 928, '1024': 725, '2048': 342, '4096': 221, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 0, '256': 1, '512': 26, '1024': 193, '2048': 387, '4096': 298, '8192': 112, '16384': 7, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 937, '256': 962, '512': 926, '1024': 756, '2048': 317, '4096': 198, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 2: {'packing_seq_len': {'128': 0, '256': 0, '512': 19, '1024': 193, '2048': 379, '4096': 304, '8192': 119, '16384': 10, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 913, '256': 957, '512': 925, '1024': 755, '2048': 337, '4096': 209, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 0, '256': 1, '512': 26, '1024': 193, '2048': 387, '4096': 298, '8192': 112, '16384': 7, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 937, '256': 962, '512': 926, '1024': 756, '2048': 317, '4096': 198, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen4096_gbs512_mbs4_dp4_tp2_pp2_pad_sp_node0.log...
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen4096_gbs512_mbs4_dp4_tp2_pp2_pad_sp_node1.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 7b model, gpu_num=16, seq_len = 4096, DP=8, MP=2, PP=1, GBS=512, MBS=2
10.64.24.49: use sequence parallel
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 7b model, gpu_num=16, seq_len = 4096, DP=8, MP=2, PP=1, GBS=512, MBS=2
10.64.24.50: use sequence parallel
10.64.24.50: [2024-03-11 23:42:37,528] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 23:42:37,528] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 23:42:37,528] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 23:42:37,528] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 23:42:38,494] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 23:42:38,494] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 23:42:38,494] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 23:42:38,494] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 8, context-parallel size: 1 tensor-model-parallel size: 2, pipeline-model-parallel size: 1 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 8
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 32
10.64.24.49:   encoder_seq_length .............................. 4096
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 16384
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 4096
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 4096
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 2
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 32
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 32
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 1
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 4096
10.64.24.49:   sequence_packing ................................ False
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 2
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 1
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ False
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 32
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: > initialized tensor model parallel with size 2
10.64.24.49: > initialized pipeline model parallel with size 1
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.060 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.221 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 11.594
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 23:43:07 
10.64.24.49: building GPT model ...
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 3342540800
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 3342540800
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (3342540800 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 23:43:07 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...
10.64.24.50: 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: 
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache end, time cost:  5.020 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.037 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.053 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.170 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.179 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.299 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.319 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.399 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 4097 begin ...
10.64.24.50: Cutting or padding data end, time cost:  5.015 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  5.229 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  5.103 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  5.240 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  5.162 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  5.089 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  5.225 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  6.204 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 23:43:19 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (291.05, 322.15)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 11983.81)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 23:43:19 
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 23:45:04 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             549W / 700W |  54606MiB / 81559MiB |     19%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             525W / 700W |  54116MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             568W / 700W |  54590MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             519W / 700W |  53962MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             507W / 700W |  53946MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             492W / 700W |  53798MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   48C    P0             504W / 700W |  54058MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             502W / 700W |  53984MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 16743.9 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 9.546417E+00 | loss scale: 1.0 | grad norm: 700.405 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 24036.7177734375 | max allocated: 44537.4443359375 | reserved: 50620.0 | max reserved: 50620.0
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 24036.7177734375 | max allocated: 44537.4443359375 | reserved: 50130.0 | max reserved: 50130.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (15834.31, 15844.32)
10.64.24.50:     forward-compute ................................: (6377.29, 6742.56)
10.64.24.50:     backward-compute ...............................: (9075.11, 9451.13)
10.64.24.50:     batch-generator ................................: (244.98, 282.17)
10.64.24.50:     layernorm-grads-all-reduce .....................: (2.59, 3.25)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.05)
10.64.24.50:     all-grads-sync .................................: (691.13, 743.85)
10.64.24.50:     params-all-gather ..............................: (40.41, 40.50)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.26, 0.45)
10.64.24.50:     optimizer-clip-main-grad .......................: (5.61, 5.66)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.80, 5.07)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.42, 1.51)
10.64.24.50:     optimizer ......................................: (53.78, 53.87)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 15126.4 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.521153E+00 | loss scale: 1.0 | grad norm: 13.610 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (14989.69, 14999.86)
10.64.24.50:     forward-compute ................................: (5584.55, 5915.08)
10.64.24.50:     backward-compute ...............................: (9048.51, 9399.01)
10.64.24.50:     batch-generator ................................: (35.24, 46.49)
10.64.24.50:     layernorm-grads-all-reduce .....................: (2.44, 2.75)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.03)
10.64.24.50:     all-grads-sync .................................: (68.38, 69.52)
10.64.24.50:     params-all-gather ..............................: (40.43, 40.54)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.26, 0.40)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.39, 2.43)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.67, 4.78)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.42, 1.51)
10.64.24.50:     optimizer ......................................: (50.14, 50.24)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 14891.7 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 1.416151E+00 | loss scale: 1.0 | grad norm: 3.034 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (14755.93, 14767.29)
10.64.24.50:     forward-compute ................................: (5365.92, 5706.27)
10.64.24.50:     backward-compute ...............................: (9034.55, 9384.19)
10.64.24.50:     batch-generator ................................: (35.83, 46.42)
10.64.24.50:     layernorm-grads-all-reduce .....................: (2.42, 2.63)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.03)
10.64.24.50:     all-grads-sync .................................: (68.36, 68.44)
10.64.24.50:     params-all-gather ..............................: (40.42, 40.50)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.25, 0.38)
10.64.24.50:     optimizer-clip-main-grad .......................: (2.37, 2.39)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (4.67, 4.73)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (1.42, 1.51)
10.64.24.50:     optimizer ......................................: (49.97, 50.06)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Mon Mar 11 23:51:22 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             544W / 700W |  54606MiB / 81559MiB |     25%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             493W / 700W |  54116MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             519W / 700W |  54590MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             500W / 700W |  53962MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             516W / 700W |  54734MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             484W / 700W |  53798MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   48C    P0             503W / 700W |  54058MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             502W / 700W |  53984MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-11 23:51:37 
10.64.24.49: rank 5: {'packing_seq_len': {'128': 7, '256': 95, '512': 253, '1024': 310, '2048': 220, '4096': 96, '8192': 43, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 462, '256': 501, '512': 440, '1024': 380, '2048': 162, '4096': 103, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 3: {'packing_seq_len': {'128': 5, '256': 90, '512': 256, '1024': 303, '2048': 243, '4096': 87, '8192': 40, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 467, '256': 487, '512': 475, '1024': 353, '2048': 163, '4096': 103, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}rank 7: {'packing_seq_len': {'128': 14, '256': 84, '512': 228, '1024': 312, '2048': 253, '4096': 85, '8192': 48, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 451, '256': 456, '512': 485, '1024': 375, '2048': 175, '4096': 106, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 1: {'packing_seq_len': {'128': 12, '256': 93, '512': 233, '1024': 318, '2048': 240, '4096': 86, '8192': 42, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 470, '256': 475, '512': 451, '1024': 403, '2048': 154, '4096': 95, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 13, '256': 81, '512': 248, '1024': 298, '2048': 223, '4096': 110, '8192': 51, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 431, '256': 470, '512': 465, '1024': 390, '2048': 173, '4096': 119, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 12, '256': 93, '512': 233, '1024': 318, '2048': 240, '4096': 86, '8192': 42, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 470, '256': 475, '512': 451, '1024': 403, '2048': 154, '4096': 95, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 2: {'packing_seq_len': {'128': 5, '256': 90, '512': 256, '1024': 303, '2048': 243, '4096': 87, '8192': 40, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 467, '256': 487, '512': 475, '1024': 353, '2048': 163, '4096': 103, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 6: {'packing_seq_len': {'128': 14, '256': 84, '512': 228, '1024': 312, '2048': 253, '4096': 85, '8192': 48, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 451, '256': 456, '512': 485, '1024': 375, '2048': 175, '4096': 106, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 7, '256': 95, '512': 253, '1024': 310, '2048': 220, '4096': 96, '8192': 43, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 462, '256': 501, '512': 440, '1024': 380, '2048': 162, '4096': 103, '8192': 0, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen4096_gbs512_mbs2_dp8_tp2_pp1_pad_sp_node1.log...
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 13b model, gpu_num=16, seq_len = 16384, DP=1, MP=4, PP=4, GBS=512, MBS=1
10.64.24.50: use sequence parallel
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_7b_seqlen4096_gbs512_mbs2_dp8_tp2_pp1_pad_sp_node0.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 13b model, gpu_num=16, seq_len = 16384, DP=1, MP=4, PP=4, GBS=512, MBS=1
10.64.24.49: use sequence parallel
10.64.24.50: [2024-03-11 23:51:46,440] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-11 23:51:46,440] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-11 23:51:46,440] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-11 23:51:46,440] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 23:51:47,547] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-11 23:51:47,547] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-11 23:51:47,547] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-11 23:51:47,547] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 1, context-parallel size: 1 tensor-model-parallel size: 4, pipeline-model-parallel size: 4 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 1
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 40
10.64.24.49:   encoder_seq_length .............................. 16384
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 20480
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 5120
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 16384
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 1
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 40
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 40
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 4
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 16384
10.64.24.49:   sequence_packing ................................ False
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 4
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 4
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ False
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 512
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 431 dummy tokens (new size: 50688)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: > initialized tensor model parallel with size 4
10.64.24.49: > initialized pipeline model parallel with size 4
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.061 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.206 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 11.378
10.64.24.49: [after megatron is initialized] datetime: 2024-03-11 23:52:15 
10.64.24.49: building GPT model ...
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (2, 2): 786828800
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (3, 2): 786828800
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 786828800
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 786828800
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 786828800
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 786828800
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 786828800
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 786828800
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (786828800 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (786828800 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 935595520
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (2, 3): 851719680
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (3, 3): 851719680
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 935595520
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 851719680
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 935595520
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 851719680
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 935595520
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (851719680 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (935595520 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-11 23:52:17 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache end, time cost:  5.034 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.059 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Loading exists cache end, time cost:  6.035 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Loading exists cache end, time cost:  8.368 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Cutting or padding data end, time cost:  18.561 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  18.715 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  18.911 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  18.898 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-11 23:52:45 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (55.06, 1458.81)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 27732.09)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-11 23:52:45 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Tue Mar 12 00:07:25 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             474W / 700W |  52502MiB / 81559MiB |     79%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             437W / 700W |  52516MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             445W / 700W |  52404MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             426W / 700W |  52204MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             377W / 700W |  42082MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             423W / 700W |  42236MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   47C    P0             357W / 700W |  42158MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             314W / 700W |  42184MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 145875.8 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 8.399383E+00 | loss scale: 1.0 | grad norm: 453.210 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: [Rank 11] (after 10 iterations) memory (MB) | allocated: 13826.2998046875 | max allocated: 27790.0703125 | reserved: 32134.0 | max reserved: 32134.0
10.64.24.50: [Rank 10] (after 10 iterations) memory (MB) | allocated: 13826.2998046875 | max allocated: 27790.0703125 | reserved: 32226.0 | max reserved: 32226.0[Rank 9] (after 10 iterations) memory (MB) | allocated: 13826.2998046875 | max allocated: 27790.0703125 | reserved: 32200.0 | max reserved: 32200.0
10.64.24.50: 
10.64.24.50: [Rank 8] (after 10 iterations) memory (MB) | allocated: 13826.2998046875 | max allocated: 27790.0703125 | reserved: 32050.0 | max reserved: 32050.0
10.64.24.50: [Rank 13] (after 10 iterations) memory (MB) | allocated: 14941.4375 | max allocated: 23809.51025390625 | reserved: 28922.0 | max reserved: 28922.0[Rank 14] (after 10 iterations) memory (MB) | allocated: 14941.4375 | max allocated: 23809.51025390625 | reserved: 28922.0 | max reserved: 28922.0
10.64.24.50: 
10.64.24.49: [Rank 5] (after 10 iterations) memory (MB) | allocated: 13826.2998046875 | max allocated: 34596.95556640625 | reserved: 39248.0 | max reserved: 39248.0[Rank 7] (after 10 iterations) memory (MB) | allocated: 13826.2998046875 | max allocated: 34596.95556640625 | reserved: 39276.0 | max reserved: 39276.0
10.64.24.49: 
10.64.24.49: [Rank 3] (after 10 iterations) memory (MB) | allocated: 16301.2998046875 | max allocated: 43999.9033203125 | reserved: 49210.0 | max reserved: 49210.0[Rank 1] (after 10 iterations) memory (MB) | allocated: 16301.2998046875 | max allocated: 43999.9033203125 | reserved: 49282.0 | max reserved: 49282.0
10.64.24.49: 
10.64.24.49: [Rank 2] (after 10 iterations) memory (MB) | allocated: 16301.2998046875 | max allocated: 43999.9033203125 | reserved: 49170.0 | max reserved: 49170.0[Rank 6] (after 10 iterations) memory (MB) | allocated: 13826.2998046875 | max allocated: 34596.95556640625 | reserved: 39010.0 | max reserved: 39010.0
10.64.24.49: 
10.64.24.49: [Rank 4] (after 10 iterations) memory (MB) | allocated: 13826.2998046875 | max allocated: 34596.95556640625 | reserved: 38982.0 | max reserved: 38982.0
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 16301.2998046875 | max allocated: 43999.9033203125 | reserved: 49316.0 | max reserved: 49316.0
10.64.24.50: [Rank 15] (after 10 iterations) memory (MB) | allocated: 14941.4375 | max allocated: 23809.51025390625 | reserved: 28882.0 | max reserved: 28882.0
10.64.24.50: [Rank 12] (after 10 iterations) memory (MB) | allocated: 14941.4375 | max allocated: 23809.51025390625 | reserved: 28918.0 | max reserved: 28918.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (145163.15, 145676.88)
10.64.24.50:     forward-compute ................................: (47703.27, 51471.30)
10.64.24.50:     backward-compute ...............................: (84225.09, 94471.47)
10.64.24.50:     batch-generator ................................: (588.66, 945.62)
10.64.24.50:     forward-recv ...................................: (383.82, 1105.82)
10.64.24.50:     forward-send ...................................: (6.81, 530.62)
10.64.24.50:     backward-recv ..................................: (108.32, 244.75)
10.64.24.50:     backward-send ..................................: (0.92, 2.67)
10.64.24.50:     forward-send-backward-recv .....................: (1325.29, 1581.19)
10.64.24.50:     backward-send-forward-recv .....................: (3358.07, 10949.48)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.05, 1.16)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 6.23)
10.64.24.50:     all-grads-sync .................................: (58.23, 68.45)
10.64.24.50:     params-all-gather ..............................: (2.64, 3.07)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.76, 0.81)
10.64.24.50:     optimizer-clip-main-grad .......................: (7.51, 7.98)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.10, 10.84)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.86, 3.34)
10.64.24.50:     optimizer ......................................: (26.16, 26.59)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 143929.1 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 5.647198E-01 | loss scale: 1.0 | grad norm: 5.056 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (143379.15, 143889.50)
10.64.24.50:     forward-compute ................................: (47385.27, 51113.23)
10.64.24.50:     backward-compute ...............................: (83715.59, 93926.96)
10.64.24.50:     batch-generator ................................: (330.15, 633.54)
10.64.24.50:     forward-recv ...................................: (97.74, 281.13)
10.64.24.50:     forward-send ...................................: (0.87, 2.45)
10.64.24.50:     backward-recv ..................................: (108.36, 245.04)
10.64.24.50:     backward-send ..................................: (0.92, 2.67)
10.64.24.50:     forward-send-backward-recv .....................: (998.28, 1195.05)
10.64.24.50:     backward-send-forward-recv .....................: (3046.86, 10945.21)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.02, 1.10)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 5.84)
10.64.24.50:     all-grads-sync .................................: (2.13, 2.53)
10.64.24.50:     params-all-gather ..............................: (2.64, 3.06)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.76, 0.81)
10.64.24.50:     optimizer-clip-main-grad .......................: (4.61, 5.08)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (8.84, 10.54)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.86, 3.34)
10.64.24.50:     optimizer ......................................: (22.86, 23.29)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 143900.6 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 4.051283E-01 | loss scale: 1.0 | grad norm: 2.459 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (143350.61, 143860.84)
10.64.24.50:     forward-compute ................................: (47376.11, 51064.27)
10.64.24.50:     backward-compute ...............................: (83689.11, 93924.96)
10.64.24.50:     batch-generator ................................: (335.45, 679.31)
10.64.24.50:     forward-recv ...................................: (97.44, 281.14)
10.64.24.50:     forward-send ...................................: (0.87, 2.45)
10.64.24.50:     backward-recv ..................................: (108.31, 243.85)
10.64.24.50:     backward-send ..................................: (0.92, 2.67)
10.64.24.50:     forward-send-backward-recv .....................: (998.71, 1200.46)
10.64.24.50:     backward-send-forward-recv .....................: (3238.04, 10950.15)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.01, 1.08)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 5.86)
10.64.24.50:     all-grads-sync .................................: (2.13, 2.53)
10.64.24.50:     params-all-gather ..............................: (2.64, 3.08)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.75, 0.80)
10.64.24.50:     optimizer-clip-main-grad .......................: (4.60, 5.06)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (8.84, 10.54)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.86, 3.34)
10.64.24.50:     optimizer ......................................: (23.18, 23.63)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Tue Mar 12 01:07:26 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             445W / 700W |  52502MiB / 81559MiB |     65%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             450W / 700W |  52516MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             429W / 700W |  52404MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             418W / 700W |  52204MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             405W / 700W |  42082MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             389W / 700W |  42396MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   47C    P0             371W / 700W |  42318MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             392W / 700W |  42184MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-12 01:09:50 
10.64.24.49: rank 5: {'packing_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}rank 3: {'packing_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}rank 6: {'packing_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}rank 7: {'packing_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 1: {'packing_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 2: {'packing_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 4: {'packing_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 3665, '256': 3787, '512': 3713, '1024': 3012, '2048': 1357, '4096': 525, '8192': 214, '16384': 111, '32768': 0, '>32k': 0}}
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen16384_gbs512_mbs1_dp1_tp4_pp4_pad_sp_node0.log...
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen16384_gbs512_mbs1_dp1_tp4_pp4_pad_sp_node1.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.49: use gpt 13b model, gpu_num=16, seq_len = 16384, DP=2, MP=8, PP=1, GBS=512, MBS=1
10.64.24.49: use sequence parallel
10.64.24.50: use gpt 13b model, gpu_num=16, seq_len = 16384, DP=2, MP=8, PP=1, GBS=512, MBS=1
10.64.24.50: use sequence parallel
10.64.24.50: [2024-03-12 01:10:09,621] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-12 01:10:09,621] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-12 01:10:09,621] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-12 01:10:09,621] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-12 01:10:11,276] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-12 01:10:11,276] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-12 01:10:11,276] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-12 01:10:11,276] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 8, pipeline-model-parallel size: 1 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 2
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 40
10.64.24.49:   encoder_seq_length .............................. 16384
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 20480
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 5120
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 16384
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 1
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 40
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 40
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 1
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 16384
10.64.24.49:   sequence_packing ................................ False
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 8
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 1
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ False
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 256
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 943 dummy tokens (new size: 51200)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: > initialized tensor model parallel with size 8
10.64.24.49: > initialized pipeline model parallel with size 1
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.064 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.076 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 10.647
10.64.24.49: [after megatron is initialized] datetime: 2024-03-12 01:10:39 
10.64.24.49: building GPT model ...
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 1690936320
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (4, 0): 1690936320
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (5, 0): 1690936320
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 1690936320
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1690936320
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1690936320
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (7, 0): 1690936320
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (6, 0): 1690936320
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1690936320 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-12 01:10:40 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache end, time cost:  4.948 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.49: Loading exists cache end, time cost:  4.966 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 16385 begin ...
10.64.24.50: Cutting or padding data end, time cost:  18.462 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  18.635 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-12 01:11:04 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (1029.68, 1066.24)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 24014.17)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-12 01:11:04 
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Tue Mar 12 01:29:04 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             508W / 700W |  51346MiB / 81559MiB |     48%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   48C    P0             520W / 700W |  50574MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             465W / 700W |  50424MiB / 81559MiB |     95%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             469W / 700W |  50454MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             468W / 700W |  50444MiB / 81559MiB |     96%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             479W / 700W |  50400MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   46C    P0             474W / 700W |  50430MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             453W / 700W |  50214MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 177604.6 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 7.775580E+00 | loss scale: 1.0 | grad norm: 346.306 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 19670.609375 | max allocated: 34335.58642578125 | reserved: 47516.0 | max reserved: 47516.0
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 19670.609375 | max allocated: 34335.58642578125 | reserved: 48336.0 | max reserved: 48336.0
10.64.24.49: [Rank 2] (after 10 iterations) memory (MB) | allocated: 19670.609375 | max allocated: 34336.08642578125 | reserved: 47766.0 | max reserved: 47766.0
10.64.24.49: [Rank 3] (after 10 iterations) memory (MB) | allocated: 19670.609375 | max allocated: 34335.58642578125 | reserved: 47796.0 | max reserved: 47796.0
10.64.24.49: [Rank 4] (after 10 iterations) memory (MB) | allocated: 19670.609375 | max allocated: 34335.58642578125 | reserved: 47786.0 | max reserved: 47786.0
10.64.24.49: [Rank 5] (after 10 iterations) memory (MB) | allocated: 19671.109375 | max allocated: 34336.08642578125 | reserved: 47742.0 | max reserved: 47742.0
10.64.24.49: [Rank 6] (after 10 iterations) memory (MB) | allocated: 19670.609375 | max allocated: 34335.58642578125 | reserved: 47772.0 | max reserved: 47772.0
10.64.24.49: [Rank 7] (after 10 iterations) memory (MB) | allocated: 19670.609375 | max allocated: 34335.58642578125 | reserved: 47796.0 | max reserved: 47796.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (176349.78, 176353.50)
10.64.24.50:     forward-compute ................................: (69681.51, 70473.49)
10.64.24.50:     backward-compute ...............................: (105702.00, 106512.26)
10.64.24.50:     batch-generator ................................: (969.28, 1201.30)
10.64.24.50:     layernorm-grads-all-reduce .....................: (3.13, 3.42)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.04)
10.64.24.50:     all-grads-sync .................................: (156.60, 165.95)
10.64.24.50:     params-all-gather ..............................: (42.12, 42.52)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (1.40, 1.66)
10.64.24.50:     optimizer-clip-main-grad .......................: (8.30, 8.33)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.93, 927.82)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (3.52, 3.72)
10.64.24.50:     optimizer ......................................: (984.74, 985.10)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 177543.5 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 5.718032E-01 | loss scale: 1.0 | grad norm: 6.833 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (177390.09, 177392.36)
10.64.24.50:     forward-compute ................................: (71507.33, 72106.59)
10.64.24.50:     backward-compute ...............................: (105108.84, 105726.53)
10.64.24.50:     batch-generator ................................: (304.80, 502.10)
10.64.24.50:     layernorm-grads-all-reduce .....................: (3.07, 3.25)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.03)
10.64.24.50:     all-grads-sync .................................: (75.63, 76.25)
10.64.24.50:     params-all-gather ..............................: (41.99, 42.41)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (1.37, 1.65)
10.64.24.50:     optimizer-clip-main-grad .......................: (5.35, 5.47)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.62, 9.71)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (3.52, 3.71)
10.64.24.50:     optimizer ......................................: (63.11, 63.54)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 175745.7 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 3.994883E-01 | loss scale: 1.0 | grad norm: 1.159 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (175591.47, 175593.60)
10.64.24.50:     forward-compute ................................: (69711.04, 70349.83)
10.64.24.50:     backward-compute ...............................: (105067.64, 105724.15)
10.64.24.50:     batch-generator ................................: (307.00, 526.83)
10.64.24.50:     layernorm-grads-all-reduce .....................: (3.04, 3.22)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.03)
10.64.24.50:     all-grads-sync .................................: (75.49, 76.25)
10.64.24.50:     params-all-gather ..............................: (41.98, 43.41)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (1.38, 1.65)
10.64.24.50:     optimizer-clip-main-grad .......................: (5.36, 5.39)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.63, 9.73)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (3.51, 3.71)
10.64.24.50:     optimizer ......................................: (63.00, 64.57)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Tue Mar 12 02:42:22 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             429W / 700W |  51346MiB / 81559MiB |     21%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   48C    P0             428W / 700W |  50974MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             463W / 700W |  50824MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             444W / 700W |  50854MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             448W / 700W |  50844MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             448W / 700W |  50800MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   47C    P0             395W / 700W |  50830MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             412W / 700W |  50614MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-12 02:45:29 rank 2: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 1: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}}
10.64.24.49: rank 6: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}}
10.64.24.49: rank 3: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}}rank 7: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}}
10.64.24.49: rank 5: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 0: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 103, '16384': 62, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 111, '16384': 49, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 111, '16384': 49, '32768': 0, '>32k': 0}}
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen16384_gbs512_mbs1_dp2_tp8_pp1_pad_sp_node1.log...
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen16384_gbs512_mbs1_dp2_tp8_pp1_pad_sp_node0.log...
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 13b model, gpu_num=16, seq_len = 8192, DP=4, MP=2, PP=2, GBS=512, MBS=1
10.64.24.50: use sequence parallel
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 13b model, gpu_num=16, seq_len = 8192, DP=4, MP=2, PP=2, GBS=512, MBS=1
10.64.24.49: use sequence parallel
10.64.24.49: [2024-03-12 02:45:44,701] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-12 02:45:44,701] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-12 02:45:44,701] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-12 02:45:44,701] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-12 02:45:43,894] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-12 02:45:43,894] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-12 02:45:43,894] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-12 02:45:43,894] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 4, context-parallel size: 1 tensor-model-parallel size: 2, pipeline-model-parallel size: 2 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 4
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 40
10.64.24.49:   encoder_seq_length .............................. 8192
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 20480
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 5120
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 8192
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 1
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 40
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 40
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 2
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 8192
10.64.24.49:   sequence_packing ................................ False
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 2
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 2
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ False
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 128
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: > initialized tensor model parallel with size 2
10.64.24.49: > initialized pipeline model parallel with size 2
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.062 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.181 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 11.248
10.64.24.49: [after megatron is initialized] datetime: 2024-03-12 02:46:12 
10.64.24.49: building GPT model ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 3275816960
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 3317749760
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 3275816960
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 3317749760
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (3275816960 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (3317749760 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-12 02:46:13 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...
10.64.24.49: 
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: 
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache end, time cost:  5.133 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.138 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.147 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.183 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.336 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.891 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  6.696 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  7.814 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Cutting or padding data end, time cost:  10.540 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.614 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.712 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.755 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.747 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.832 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.953 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  11.042 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-12 02:46:33 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (1126.36, 1414.91)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 19523.45)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-12 02:46:33 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Tue Mar 12 02:52:43 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             476W / 700W |  64876MiB / 81559MiB |     43%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             508W / 700W |  65250MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             475W / 700W |  64696MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             431W / 700W |  64662MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             413W / 700W |  64812MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             444W / 700W |  64870MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   48C    P0             403W / 700W |  64560MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             461W / 700W |  64408MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 61323.9 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 7.650615E+00 | loss scale: 1.0 | grad norm: 273.382 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: [Rank 9] (after 10 iterations) memory (MB) | allocated: 28380.5654296875 | max allocated: 44044.88232421875 | reserved: 49434.0 | max reserved: 49434.0
10.64.24.50: [Rank 8] (after 10 iterations) memory (MB) | allocated: 28380.5654296875 | max allocated: 44044.88232421875 | reserved: 49394.0 | max reserved: 49394.0
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 28661.37109375 | max allocated: 56289.1767578125 | reserved: 61576.0 | max reserved: 61576.0
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 28661.37109375 | max allocated: 56289.1767578125 | reserved: 61202.0 | max reserved: 61202.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (60442.59, 60729.47)
10.64.24.50:     forward-compute ................................: (18983.27, 23613.91)
10.64.24.50:     backward-compute ...............................: (35143.63, 36791.71)
10.64.24.50:     batch-generator ................................: (303.55, 419.78)
10.64.24.50:     forward-recv ...................................: (394.20, 413.00)
10.64.24.50:     forward-send ...................................: (3.20, 9.34)
10.64.24.50:     backward-recv ..................................: (161.49, 168.60)
10.64.24.50:     backward-send ..................................: (1.07, 1.10)
10.64.24.50:     forward-send-backward-recv .....................: (5454.01, 6338.13)
10.64.24.50:     backward-send-forward-recv .....................: (731.29, 903.64)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.76, 2.08)
10.64.24.50:     embedding-grads-all-reduce .....................: (11.05, 11.24)
10.64.24.50:     all-grads-sync .................................: (440.69, 442.82)
10.64.24.50:     params-all-gather ..............................: (23.63, 24.07)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.38, 0.51)
10.64.24.50:     optimizer-clip-main-grad .......................: (8.35, 8.52)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.37, 10.01)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.67, 2.78)
10.64.24.50:     optimizer ......................................: (46.45, 46.86)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 58194.0 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.126181E+00 | loss scale: 1.0 | grad norm: 7.649 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (57811.96, 58097.80)
10.64.24.50:     forward-compute ................................: (18739.20, 21483.00)
10.64.24.50:     backward-compute ...............................: (34985.76, 36577.04)
10.64.24.50:     batch-generator ................................: (100.71, 212.54)
10.64.24.50:     forward-recv ...................................: (140.75, 142.58)
10.64.24.50:     forward-send ...................................: (1.01, 1.14)
10.64.24.50:     backward-recv ..................................: (162.66, 167.49)
10.64.24.50:     backward-send ..................................: (1.07, 1.10)
10.64.24.50:     forward-send-backward-recv .....................: (3263.01, 4118.62)
10.64.24.50:     backward-send-forward-recv .....................: (703.00, 793.88)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.68, 2.05)
10.64.24.50:     embedding-grads-all-reduce .....................: (10.97, 11.21)
10.64.24.50:     all-grads-sync .................................: (36.17, 37.08)
10.64.24.50:     params-all-gather ..............................: (23.67, 24.34)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.37, 0.49)
10.64.24.50:     optimizer-clip-main-grad .......................: (4.32, 4.36)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.12, 9.35)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.67, 2.78)
10.64.24.50:     optimizer ......................................: (41.25, 41.92)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 59656.2 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 7.756909E-01 | loss scale: 1.0 | grad norm: 2.448 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (59275.06, 59560.60)
10.64.24.50:     forward-compute ................................: (19282.76, 22528.39)
10.64.24.50:     backward-compute ...............................: (34966.79, 36554.96)
10.64.24.50:     batch-generator ................................: (100.94, 221.73)
10.64.24.50:     forward-recv ...................................: (140.81, 142.47)
10.64.24.50:     forward-send ...................................: (1.01, 1.04)
10.64.24.50:     backward-recv ..................................: (161.58, 168.50)
10.64.24.50:     backward-send ..................................: (1.07, 1.11)
10.64.24.50:     forward-send-backward-recv .....................: (4274.87, 5063.74)
10.64.24.50:     backward-send-forward-recv .....................: (1167.58, 1339.44)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.70, 1.99)
10.64.24.50:     embedding-grads-all-reduce .....................: (11.04, 11.18)
10.64.24.50:     all-grads-sync .................................: (36.27, 37.04)
10.64.24.50:     params-all-gather ..............................: (23.59, 24.03)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.37, 0.49)
10.64.24.50:     optimizer-clip-main-grad .......................: (4.31, 4.35)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.13, 9.36)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.67, 2.78)
10.64.24.50:     optimizer ......................................: (41.10, 41.54)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Tue Mar 12 03:17:22 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             504W / 700W |  64876MiB / 81559MiB |     20%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             463W / 700W |  65250MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             513W / 700W |  64990MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             487W / 700W |  64822MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             471W / 700W |  64812MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             472W / 700W |  64870MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   48C    P0             437W / 700W |  64560MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             446W / 700W |  64408MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: rank 1: {'packing_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 77, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 77, '16384': 0, '32768': 0, '>32k': 0}}rank 7: {'packing_seq_len': {'128': 912, '256': 911, '512': 963, '1024': 748, '2048': 345, '4096': 136, '8192': 81, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 912, '256': 911, '512': 963, '1024': 748, '2048': 345, '4096': 136, '8192': 81, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 5: {'packing_seq_len': {'128': 907, '256': 956, '512': 944, '1024': 732, '2048': 343, '4096': 126, '8192': 88, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 907, '256': 956, '512': 944, '1024': 732, '2048': 343, '4096': 126, '8192': 88, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: [after training is done] datetime: 2024-03-12 03:18:20 
10.64.24.49: rank 3: {'packing_seq_len': {'128': 938, '256': 968, '512': 883, '1024': 778, '2048': 329, '4096': 121, '8192': 79, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 938, '256': 968, '512': 883, '1024': 778, '2048': 329, '4096': 121, '8192': 79, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 6: {'packing_seq_len': {'128': 912, '256': 911, '512': 963, '1024': 748, '2048': 345, '4096': 136, '8192': 81, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 912, '256': 911, '512': 963, '1024': 748, '2048': 345, '4096': 136, '8192': 81, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 907, '256': 956, '512': 944, '1024': 732, '2048': 343, '4096': 126, '8192': 88, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 907, '256': 956, '512': 944, '1024': 732, '2048': 343, '4096': 126, '8192': 88, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 2: {'packing_seq_len': {'128': 938, '256': 968, '512': 883, '1024': 778, '2048': 329, '4096': 121, '8192': 79, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 938, '256': 968, '512': 883, '1024': 778, '2048': 329, '4096': 121, '8192': 79, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 77, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 77, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 77, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 77, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen8192_gbs512_mbs1_dp4_tp2_pp2_pad_sp_node1.log...
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen8192_gbs512_mbs1_dp4_tp2_pp2_pad_sp_node0.log...
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 13b model, gpu_num=16, seq_len = 8192, DP=2, MP=2, PP=4, GBS=512, MBS=1
10.64.24.50: use sequence parallel
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 13b model, gpu_num=16, seq_len = 8192, DP=2, MP=2, PP=4, GBS=512, MBS=1
10.64.24.49: use sequence parallel
10.64.24.49: [2024-03-12 03:18:34,235] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-12 03:18:34,235] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-12 03:18:34,235] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-12 03:18:34,235] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-12 03:18:33,401] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-12 03:18:33,401] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-12 03:18:33,401] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-12 03:18:33,401] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 2, pipeline-model-parallel size: 4 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 2
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 40
10.64.24.49:   encoder_seq_length .............................. 8192
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 20480
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 5120
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 8192
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 1
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 40
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 40
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 4
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 8192
10.64.24.49:   sequence_packing ................................ False
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 2
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 4
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ False
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 256
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: > initialized tensor model parallel with size 2
10.64.24.49: > initialized pipeline model parallel with size 4
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.060 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.029 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 10.602
10.64.24.49: [after megatron is initialized] datetime: 2024-03-12 03:19:01 
10.64.24.49: building GPT model ...
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 1573350400
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1573350400
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1573350400
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1573350400 elements):
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1573350400
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1573350400 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 1702466560
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1744399360
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1702466560
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1744399360
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1702466560 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1744399360 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-12 03:19:02 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...> building GPT2BPETokenizer tokenizer ...
10.64.24.50: 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: Loading exists cache end, time cost:  5.012 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.089 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.084 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.096 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.116 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.119 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.188 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.208 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Cutting or padding data end, time cost:  10.595 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.597 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.692 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.740 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.770 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.761 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.859 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.786 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-12 03:19:18 
10.64.24.49: done with setup ...
10.64.24.49: training ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (56.18, 851.52)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 16359.67)
10.64.24.49: [before the start of training step] datetime: 2024-03-12 03:19:18 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Tue Mar 12 03:25:22 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             460W / 700W |  54008MiB / 81559MiB |     49%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             460W / 700W |  53910MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             438W / 700W |  54040MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             422W / 700W |  54018MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             368W / 700W |  44928MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             347W / 700W |  44868MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   47C    P0             302W / 700W |  44828MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             361W / 700W |  44828MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 60027.3 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 9.670396E+00 | loss scale: 1.0 | grad norm: 710.601 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: [Rank 9] (after 10 iterations) memory (MB) | allocated: 18249.712890625 | max allocated: 32183.4833984375 | reserved: 34842.0 | max reserved: 34842.0
10.64.24.50: [Rank 8] (after 10 iterations) memory (MB) | allocated: 18249.712890625 | max allocated: 32183.4833984375 | reserved: 34828.0 | max reserved: 34828.0
10.64.24.50: [Rank 13] (after 10 iterations) memory (MB) | allocated: 19727.3291015625 | max allocated: 28584.82373046875 | reserved: 31668.0 | max reserved: 31668.0
10.64.24.50: [Rank 12] (after 10 iterations) memory (MB) | allocated: 19727.3291015625 | max allocated: 28584.82373046875 | reserved: 31608.0 | max reserved: 31608.0
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 20127.068359375 | max allocated: 47795.140625 | reserved: 50484.0 | max reserved: 50484.0
10.64.24.49: [Rank 5] (after 10 iterations) memory (MB) | allocated: 18249.712890625 | max allocated: 38990.36865234375 | reserved: 41528.0 | max reserved: 41528.0
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 20127.068359375 | max allocated: 47795.140625 | reserved: 50582.0 | max reserved: 50582.0
10.64.24.49: [Rank 4] (after 10 iterations) memory (MB) | allocated: 18249.712890625 | max allocated: 38990.36865234375 | reserved: 41588.0 | max reserved: 41588.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (59225.02, 59636.57)
10.64.24.50:     forward-compute ................................: (18577.81, 21101.72)
10.64.24.50:     backward-compute ...............................: (34045.07, 37084.91)
10.64.24.50:     batch-generator ................................: (281.72, 389.85)
10.64.24.50:     forward-recv ...................................: (265.33, 713.43)
10.64.24.50:     forward-send ...................................: (4.15, 281.81)
10.64.24.50:     backward-recv ..................................: (89.87, 253.71)
10.64.24.50:     backward-send ..................................: (0.96, 2.78)
10.64.24.50:     forward-send-backward-recv .....................: (3420.55, 5754.69)
10.64.24.50:     backward-send-forward-recv .....................: (403.94, 492.70)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.04, 1.17)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 11.25)
10.64.24.50:     all-grads-sync .................................: (216.60, 240.35)
10.64.24.50:     params-all-gather ..............................: (9.62, 11.03)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.38, 0.47)
10.64.24.50:     optimizer-clip-main-grad .......................: (7.43, 7.70)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (8.96, 10.05)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.59, 2.90)
10.64.24.50:     optimizer ......................................: (31.32, 32.71)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 58593.5 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.062949E+00 | loss scale: 1.0 | grad norm: 8.721 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (58122.67, 58531.39)
10.64.24.50:     forward-compute ................................: (18325.01, 20639.87)
10.64.24.50:     backward-compute ...............................: (33807.94, 36842.09)
10.64.24.50:     batch-generator ................................: (151.02, 241.38)
10.64.24.50:     forward-recv ...................................: (72.84, 214.10)
10.64.24.50:     forward-send ...................................: (0.89, 3.21)
10.64.24.50:     backward-recv ..................................: (89.82, 253.42)
10.64.24.50:     backward-send ..................................: (0.97, 2.75)
10.64.24.50:     forward-send-backward-recv .....................: (2964.99, 5453.53)
10.64.24.50:     backward-send-forward-recv .....................: (374.33, 443.41)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.00, 1.17)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 11.22)
10.64.24.50:     all-grads-sync .................................: (14.86, 16.83)
10.64.24.50:     params-all-gather ..............................: (9.66, 11.05)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.37, 0.46)
10.64.24.50:     optimizer-clip-main-grad .......................: (4.24, 4.52)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (8.72, 9.78)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.59, 2.90)
10.64.24.50:     optimizer ......................................: (27.78, 29.18)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 58685.5 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 7.529642E-01 | loss scale: 1.0 | grad norm: 2.846 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (58213.81, 58623.06)
10.64.24.50:     forward-compute ................................: (18315.83, 20801.58)
10.64.24.50:     backward-compute ...............................: (33820.09, 36966.28)
10.64.24.50:     batch-generator ................................: (150.35, 241.83)
10.64.24.50:     forward-recv ...................................: (72.73, 213.53)
10.64.24.50:     forward-send ...................................: (0.88, 3.41)
10.64.24.50:     backward-recv ..................................: (90.47, 254.42)
10.64.24.50:     backward-send ..................................: (0.97, 2.76)
10.64.24.50:     forward-send-backward-recv .....................: (3092.30, 5535.60)
10.64.24.50:     backward-send-forward-recv .....................: (374.42, 442.86)
10.64.24.50:     layernorm-grads-all-reduce .....................: (0.99, 1.15)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.02, 11.20)
10.64.24.50:     all-grads-sync .................................: (14.80, 16.77)
10.64.24.50:     params-all-gather ..............................: (9.67, 11.00)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.37, 0.46)
10.64.24.50:     optimizer-clip-main-grad .......................: (4.24, 4.52)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (8.72, 9.77)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.59, 2.89)
10.64.24.50:     optimizer ......................................: (27.79, 29.12)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Tue Mar 12 03:49:50 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             402W / 700W |  54008MiB / 81559MiB |     67%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             483W / 700W |  53910MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             412W / 700W |  54040MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             439W / 700W |  54018MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             406W / 700W |  44928MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             425W / 700W |  44968MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   47C    P0             421W / 700W |  44988MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             334W / 700W |  44988MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-12 03:50:49 
10.64.24.49: rank 5: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 7: {'packing_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 160, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 160, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 1: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 3: {'packing_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 160, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 160, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 2: {'packing_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 160, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 160, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 6: {'packing_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 160, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 160, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen8192_gbs512_mbs1_dp2_tp2_pp4_pad_sp_node0.log...
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen8192_gbs512_mbs1_dp2_tp2_pp4_pad_sp_node1.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 13b model, gpu_num=16, seq_len = 8192, DP=4, MP=4, PP=1, GBS=512, MBS=1
10.64.24.49: use sequence parallel
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 13b model, gpu_num=16, seq_len = 8192, DP=4, MP=4, PP=1, GBS=512, MBS=1
10.64.24.50: use sequence parallel
10.64.24.49: [2024-03-12 03:51:03,686] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-12 03:51:03,686] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-12 03:51:03,686] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-12 03:51:03,686] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-12 03:51:02,766] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-12 03:51:02,766] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-12 03:51:02,766] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-12 03:51:02,766] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 4, context-parallel size: 1 tensor-model-parallel size: 4, pipeline-model-parallel size: 1 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 4
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 40
10.64.24.49:   encoder_seq_length .............................. 8192
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 20480
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 5120
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 8192
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 1
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 40
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 40
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 1
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 8192
10.64.24.49:   sequence_packing ................................ False
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 4
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 1
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ False
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 128
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 431 dummy tokens (new size: 50688)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: > initialized tensor model parallel with size 4
10.64.24.49: > initialized pipeline model parallel with size 1
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.060 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 7.911 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 10.947
10.64.24.49: [after megatron is initialized] datetime: 2024-03-12 03:51:31 
10.64.24.49: building GPT model ...
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 3254149120
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 3254149120
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 3254149120
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 3254149120
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (3254149120 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-12 03:51:32 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache end, time cost:  4.932 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  4.946 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  4.949 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  6.353 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Cutting or padding data end, time cost:  10.454 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.487 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.555 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  11.044 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-12 03:51:49 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (582.37, 653.71)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 17771.76)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-12 03:51:50 
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Tue Mar 12 03:58:31 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             503W / 700W |  53450MiB / 81559MiB |     18%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             506W / 700W |  53056MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             500W / 700W |  53036MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             435W / 700W |  53072MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             470W / 700W |  53364MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             440W / 700W |  53074MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   47C    P0             451W / 700W |  53126MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             460W / 700W |  52860MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 66551.7 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 7.903540E+00 | loss scale: 1.0 | grad norm: 296.718 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 28105.2451171875 | max allocated: 42759.63623046875 | reserved: 49550.0 | max reserved: 49550.0[Rank 2] (after 10 iterations) memory (MB) | allocated: 28105.2451171875 | max allocated: 42759.63623046875 | reserved: 49530.0 | max reserved: 49530.0
10.64.24.49: 
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 28105.2451171875 | max allocated: 42759.63623046875 | reserved: 49992.0 | max reserved: 49992.0
10.64.24.49: [Rank 3] (after 10 iterations) memory (MB) | allocated: 28105.2451171875 | max allocated: 42759.63623046875 | reserved: 49806.0 | max reserved: 49806.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (66052.63, 66057.70)
10.64.24.50:     forward-compute ................................: (25003.23, 25629.53)
10.64.24.50:     backward-compute ...............................: (40340.42, 40977.80)
10.64.24.50:     batch-generator ................................: (548.65, 645.96)
10.64.24.50:     layernorm-grads-all-reduce .....................: (3.13, 3.56)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.04)
10.64.24.50:     all-grads-sync .................................: (301.15, 309.80)
10.64.24.50:     params-all-gather ..............................: (60.19, 60.32)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.75, 0.93)
10.64.24.50:     optimizer-clip-main-grad .......................: (8.53, 8.57)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.38, 9.60)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.89, 3.00)
10.64.24.50:     optimizer ......................................: (83.06, 83.18)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 65129.4 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.159371E+00 | loss scale: 1.0 | grad norm: 7.672 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (64929.24, 64932.92)
10.64.24.50:     forward-compute ................................: (24068.71, 24676.92)
10.64.24.50:     backward-compute ...............................: (40173.97, 40788.23)
10.64.24.50:     batch-generator ................................: (152.61, 228.27)
10.64.24.50:     layernorm-grads-all-reduce .....................: (3.04, 3.28)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.03)
10.64.24.50:     all-grads-sync .................................: (107.89, 109.04)
10.64.24.50:     params-all-gather ..............................: (60.20, 60.94)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.74, 0.88)
10.64.24.50:     optimizer-clip-main-grad .......................: (4.79, 4.82)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.11, 9.21)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.89, 3.00)
10.64.24.50:     optimizer ......................................: (78.63, 79.37)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 65596.5 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 7.702547E-01 | loss scale: 1.0 | grad norm: 2.580 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (65396.50, 65401.86)
10.64.24.50:     forward-compute ................................: (24547.49, 25148.90)
10.64.24.50:     backward-compute ...............................: (40167.73, 40778.27)
10.64.24.50:     batch-generator ................................: (155.62, 242.59)
10.64.24.50:     layernorm-grads-all-reduce .....................: (3.05, 3.28)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.03)
10.64.24.50:     all-grads-sync .................................: (107.89, 108.06)
10.64.24.50:     params-all-gather ..............................: (60.14, 60.36)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.74, 0.87)
10.64.24.50:     optimizer-clip-main-grad .......................: (4.72, 4.76)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.11, 9.21)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.89, 3.00)
10.64.24.50:     optimizer ......................................: (78.51, 78.72)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Tue Mar 12 04:25:51 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             501W / 700W |  53450MiB / 81559MiB |     13%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             487W / 700W |  53452MiB / 81559MiB |     96%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   50C    P0             488W / 700W |  53432MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             504W / 700W |  53072MiB / 81559MiB |     96%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             482W / 700W |  53364MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             511W / 700W |  53074MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   48C    P0             464W / 700W |  53126MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             482W / 700W |  52860MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-12 04:26:55 
10.64.24.49: rank 2: {'packing_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 77, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 77, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 7: {'packing_seq_len': {'128': 938, '256': 968, '512': 883, '1024': 778, '2048': 329, '4096': 121, '8192': 79, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 938, '256': 968, '512': 883, '1024': 778, '2048': 329, '4096': 121, '8192': 79, '16384': 0, '32768': 0, '>32k': 0}}rank 6: {'packing_seq_len': {'128': 938, '256': 968, '512': 883, '1024': 778, '2048': 329, '4096': 121, '8192': 79, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 938, '256': 968, '512': 883, '1024': 778, '2048': 329, '4096': 121, '8192': 79, '16384': 0, '32768': 0, '>32k': 0}}rank 5: {'packing_seq_len': {'128': 938, '256': 968, '512': 883, '1024': 778, '2048': 329, '4096': 121, '8192': 79, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 938, '256': 968, '512': 883, '1024': 778, '2048': 329, '4096': 121, '8192': 79, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: 
10.64.24.49: rank 1: {'packing_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 77, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 77, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 3: {'packing_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 77, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 77, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 907, '256': 956, '512': 944, '1024': 732, '2048': 343, '4096': 126, '8192': 88, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 907, '256': 956, '512': 944, '1024': 732, '2048': 343, '4096': 126, '8192': 88, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 938, '256': 968, '512': 883, '1024': 778, '2048': 329, '4096': 121, '8192': 79, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 938, '256': 968, '512': 883, '1024': 778, '2048': 329, '4096': 121, '8192': 79, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 77, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 908, '256': 952, '512': 923, '1024': 754, '2048': 340, '4096': 142, '8192': 77, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen8192_gbs512_mbs1_dp4_tp4_pp1_pad_sp_node1.log...
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen8192_gbs512_mbs1_dp4_tp4_pp1_pad_sp_node0.log...
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 13b model, gpu_num=16, seq_len = 8192, DP=2, MP=4, PP=2, GBS=512, MBS=1
10.64.24.50: use sequence parallel
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 13b model, gpu_num=16, seq_len = 8192, DP=2, MP=4, PP=2, GBS=512, MBS=1
10.64.24.49: use sequence parallel
10.64.24.49: [2024-03-12 04:27:08,303] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-12 04:27:08,303] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-12 04:27:08,303] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-12 04:27:08,303] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-12 04:27:07,393] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-12 04:27:07,393] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-12 04:27:07,393] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-12 04:27:07,393] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 4, pipeline-model-parallel size: 2 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 2
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 40
10.64.24.49:   encoder_seq_length .............................. 8192
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 20480
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 5120
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 8192
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 1
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 40
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 40
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 2
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 8192
10.64.24.49:   sequence_packing ................................ False
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 4
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 2
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ False
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 256
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 431 dummy tokens (new size: 50688)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: > initialized tensor model parallel with size 4
10.64.24.49: > initialized pipeline model parallel with size 2
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.062 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.966 seconds
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 11.022
10.64.24.49: [after megatron is initialized] datetime: 2024-03-12 04:27:36 
10.64.24.49: building GPT model ...
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1638548480
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 1638548480
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1680481280
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 1680481280
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1680481280
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1638548480
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 1638548480
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 1680481280
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1638548480 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1680481280 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-12 04:27:37 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache end, time cost:  4.985 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.042 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Loading exists cache end, time cost:  5.050 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  5.065 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.49: Cutting or padding data end, time cost:  10.396 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.559 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.50: Cutting or padding data end, time cost:  10.669 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.794 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-12 04:27:53 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (1156.97, 1337.83)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 16125.93)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-12 04:27:53 
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.50: [W ProcessGroupNCCL.cpp:1707] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Tue Mar 12 04:34:46 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             541W / 700W |  40882MiB / 81559MiB |     57%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             541W / 700W |  40888MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             505W / 700W |  40652MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             505W / 700W |  40704MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             502W / 700W |  40824MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             512W / 700W |  40610MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   48C    P0             506W / 700W |  40660MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             525W / 700W |  40218MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 67366.8 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 8.011921E+00 | loss scale: 1.0 | grad norm: 277.621 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: [Rank 11] (after 10 iterations) memory (MB) | allocated: 18955.4521484375 | max allocated: 26792.94091796875 | reserved: 30838.0 | max reserved: 30838.0
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 19397.703125 | max allocated: 33251.7587890625 | reserved: 37166.0 | max reserved: 37166.0
10.64.24.49: [Rank 3] (after 10 iterations) memory (MB) | allocated: 19397.703125 | max allocated: 33251.7587890625 | reserved: 37222.0 | max reserved: 37222.0
10.64.24.49: [Rank 0] (after 10 iterations) memory (MB) | allocated: 19396.703125 | max allocated: 33250.7587890625 | reserved: 37208.0 | max reserved: 37208.0
10.64.24.49: [Rank 2] (after 10 iterations) memory (MB) | allocated: 19396.703125 | max allocated: 33250.7587890625 | reserved: 36930.0 | max reserved: 36930.0
10.64.24.50: [Rank 10] (after 10 iterations) memory (MB) | allocated: 18955.4521484375 | max allocated: 26792.94091796875 | reserved: 30788.0 | max reserved: 30788.0
10.64.24.50: [Rank 9] (after 10 iterations) memory (MB) | allocated: 18955.4521484375 | max allocated: 26792.94091796875 | reserved: 30442.0 | max reserved: 30442.0
10.64.24.50: [Rank 8] (after 10 iterations) memory (MB) | allocated: 18955.4521484375 | max allocated: 26792.94091796875 | reserved: 30756.0 | max reserved: 30756.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (66824.32, 66987.16)
10.64.24.50:     forward-compute ................................: (23233.84, 24653.03)
10.64.24.50:     backward-compute ...............................: (39630.97, 41303.46)
10.64.24.50:     batch-generator ................................: (613.74, 793.47)
10.64.24.50:     forward-recv ...................................: (527.71, 544.97)
10.64.24.50:     forward-send ...................................: (3.18, 10.71)
10.64.24.50:     backward-recv ..................................: (83.62, 87.21)
10.64.24.50:     backward-send ..................................: (0.57, 0.64)
10.64.24.50:     forward-send-backward-recv .....................: (2167.31, 2365.68)
10.64.24.50:     backward-send-forward-recv .....................: (1387.20, 1892.71)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.75, 1.91)
10.64.24.50:     embedding-grads-all-reduce .....................: (5.75, 5.83)
10.64.24.50:     all-grads-sync .................................: (211.48, 245.97)
10.64.24.50:     params-all-gather ..............................: (10.35, 10.91)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.75, 0.98)
10.64.24.50:     optimizer-clip-main-grad .......................: (7.73, 7.82)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.50, 9.94)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.94, 3.10)
10.64.24.50:     optimizer ......................................: (32.84, 33.39)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 67002.8 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.147714E+00 | loss scale: 1.0 | grad norm: 7.419 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (66783.44, 66944.71)
10.64.24.50:     forward-compute ................................: (23241.54, 24743.90)
10.64.24.50:     backward-compute ...............................: (39444.61, 41111.97)
10.64.24.50:     batch-generator ................................: (197.26, 377.22)
10.64.24.50:     forward-recv ...................................: (84.73, 89.60)
10.64.24.50:     forward-send ...................................: (0.55, 0.57)
10.64.24.50:     backward-recv ..................................: (83.38, 86.60)
10.64.24.50:     backward-send ..................................: (0.56, 0.58)
10.64.24.50:     forward-send-backward-recv .....................: (2323.64, 2502.56)
10.64.24.50:     backward-send-forward-recv .....................: (1868.21, 2470.82)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.68, 1.78)
10.64.24.50:     embedding-grads-all-reduce .....................: (5.73, 5.80)
10.64.24.50:     all-grads-sync .................................: (15.38, 16.11)
10.64.24.50:     params-all-gather ..............................: (10.33, 10.96)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.73, 0.84)
10.64.24.50:     optimizer-clip-main-grad .......................: (4.71, 4.80)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.24, 9.51)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.94, 3.10)
10.64.24.50:     optimizer ......................................: (29.09, 29.72)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 66975.6 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 7.736883E-01 | loss scale: 1.0 | grad norm: 2.407 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (66756.76, 66917.34)
10.64.24.50:     forward-compute ................................: (23229.12, 24755.17)
10.64.24.50:     backward-compute ...............................: (39418.06, 41104.18)
10.64.24.50:     batch-generator ................................: (201.25, 390.97)
10.64.24.50:     forward-recv ...................................: (84.74, 84.77)
10.64.24.50:     forward-send ...................................: (0.55, 0.56)
10.64.24.50:     backward-recv ..................................: (84.49, 86.14)
10.64.24.50:     backward-send ..................................: (0.56, 0.58)
10.64.24.50:     forward-send-backward-recv .....................: (2314.12, 2497.41)
10.64.24.50:     backward-send-forward-recv .....................: (1849.61, 2455.49)
10.64.24.50:     layernorm-grads-all-reduce .....................: (1.69, 1.80)
10.64.24.50:     embedding-grads-all-reduce .....................: (5.70, 5.85)
10.64.24.50:     all-grads-sync .................................: (15.38, 16.09)
10.64.24.50:     params-all-gather ..............................: (10.42, 11.03)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (0.75, 0.86)
10.64.24.50:     optimizer-clip-main-grad .......................: (4.72, 4.80)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.02)
10.64.24.50:     optimizer-inner-step ...........................: (9.24, 9.52)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (2.94, 3.10)
10.64.24.50:     optimizer ......................................: (29.23, 29.83)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Tue Mar 12 05:02:31 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             552W / 700W |  40882MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             534W / 700W |  40888MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             522W / 700W |  40786MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             510W / 700W |  40704MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             505W / 700W |  40824MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   51C    P0             547W / 700W |  40610MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   48C    P0             506W / 700W |  40660MiB / 81559MiB |     97%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   44C    P0             511W / 700W |  40352MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: [after training is done] datetime: 2024-03-12 05:03:35 
10.64.24.49: rank 1: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}}rank 2: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 5: {'packing_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 160, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 160, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 6: {'packing_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 160, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 160, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 3: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}}rank 7: {'packing_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 160, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 160, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.50: rank 8: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 0: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 4: {'packing_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 160, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 160, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen8192_gbs512_mbs1_dp2_tp4_pp2_pad_sp_node0.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 13b model, gpu_num=16, seq_len = 8192, DP=2, MP=8, PP=1, GBS=512, MBS=1
10.64.24.49: use sequence parallel
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen8192_gbs512_mbs1_dp2_tp4_pp2_pad_sp_node1.log...
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 13b model, gpu_num=16, seq_len = 8192, DP=2, MP=8, PP=1, GBS=512, MBS=1
10.64.24.50: use sequence parallel
10.64.24.49: [2024-03-12 05:03:47,929] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-12 05:03:47,929] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-12 05:03:47,929] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-12 05:03:47,929] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-12 05:03:47,031] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-12 05:03:47,031] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-12 05:03:47,031] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-12 05:03:47,031] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 8, pipeline-model-parallel size: 1 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 2
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 40
10.64.24.49:   encoder_seq_length .............................. 8192
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 20480
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 5120
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 8192
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 1
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 40
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 40
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 1
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 8192
10.64.24.49:   sequence_packing ................................ False
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 8
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 1
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ False
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 256
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 943 dummy tokens (new size: 51200)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: > initialized tensor model parallel with size 8
10.64.24.49: > initialized pipeline model parallel with size 1
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.061 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.026 seconds
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 11.267
10.64.24.49: [after megatron is initialized] datetime: 2024-03-12 05:04:15 
10.64.24.49: building GPT model ...
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (7, 0): 1648993280
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 1648993280
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 1648993280
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (6, 0): 1648993280
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1648993280
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (4, 0): 1648993280
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (5, 0): 1648993280
10.64.24.49:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1648993280
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.49: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1648993280 elements):
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.49:   storage = bucket.data.storage()._untyped()
10.64.24.49: > learning rate decay style: cosine
10.64.24.49: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-12 05:04:15 
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: > building GPT2BPETokenizer tokenizer ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.50: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
10.64.24.49: Loading exists cache from /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_cache.pkl begin ...
10.64.24.49: Loading exists cache end, time cost:  4.751 s
10.64.24.49: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Loading exists cache end, time cost:  4.867 s
10.64.24.50: Cutting or padding data to max_seq_len + 1 = 8193 begin ...
10.64.24.50: Cutting or padding data end, time cost:  10.367 s
10.64.24.50: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: Cutting or padding data end, time cost:  10.575 s
10.64.24.49: consumed_train_samples = 0, dataloader_type = single
10.64.24.49: [after dataloaders are built] datetime: 2024-03-12 05:04:31 
10.64.24.49: done with setup ...
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     model-and-optimizer-setup ......................: (595.51, 618.08)
10.64.24.50:     train/valid/test-data-iterators-setup ..........: (0.02, 15607.33)
10.64.24.49: training ...
10.64.24.49: [before the start of training step] datetime: 2024-03-12 05:04:31 
10.64.24.50: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Tue Mar 12 05:13:06 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             441W / 700W |  34960MiB / 81559MiB |     63%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   47C    P0             477W / 700W |  34730MiB / 81559MiB |     95%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   47C    P0             459W / 700W |  34748MiB / 81559MiB |     96%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   39C    P0             472W / 700W |  34682MiB / 81559MiB |     96%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             411W / 700W |  34752MiB / 81559MiB |     96%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   48C    P0             432W / 700W |  34740MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   45C    P0             421W / 700W |  34866MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             415W / 700W |  34658MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.50:  iteration       10/      32 | consumed samples:         5120 | elapsed time per iteration (ms): 84303.1 | learning rate: 4.687E-07 | global batch size:   512 | lm loss: 7.974046E+00 | loss scale: 1.0 | grad norm: 348.219 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.49: [Rank 1] (after 10 iterations) memory (MB) | allocated: 19111.58740234375 | max allocated: 26444.658203125 | reserved: 31872.0 | max reserved: 31872.0
10.64.24.49: [Rank 2] (after 10 iterations) memory (MB) | allocated: 19111.58740234375 | max allocated: 26444.658203125 | reserved: 31890.0 | max reserved: 31890.0[Rank 0] (after 10 iterations) memory (MB) | allocated: 19111.58740234375 | max allocated: 26444.658203125 | reserved: 31950.0 | max reserved: 31950.0
10.64.24.49: 
10.64.24.49: [Rank 3] (after 10 iterations) memory (MB) | allocated: 19111.58740234375 | max allocated: 26444.658203125 | reserved: 31824.0 | max reserved: 31824.0
10.64.24.49: [Rank 4] (after 10 iterations) memory (MB) | allocated: 19111.58740234375 | max allocated: 26444.158203125 | reserved: 31694.0 | max reserved: 31694.0
10.64.24.49: [Rank 5] (after 10 iterations) memory (MB) | allocated: 19111.58740234375 | max allocated: 26444.658203125 | reserved: 31882.0 | max reserved: 31882.0[Rank 6] (after 10 iterations) memory (MB) | allocated: 19111.58740234375 | max allocated: 26444.658203125 | reserved: 31808.0 | max reserved: 31808.0
10.64.24.49: 
10.64.24.49: [Rank 7] (after 10 iterations) memory (MB) | allocated: 19111.58740234375 | max allocated: 26444.658203125 | reserved: 31840.0 | max reserved: 31840.0
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (83482.11, 83483.98)
10.64.24.50:     forward-compute ................................: (33619.41, 34075.87)
10.64.24.50:     backward-compute ...............................: (49233.70, 49703.52)
10.64.24.50:     batch-generator ................................: (955.14, 1120.72)
10.64.24.50:     layernorm-grads-all-reduce .....................: (3.08, 3.38)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.03)
10.64.24.50:     all-grads-sync .................................: (138.63, 146.02)
10.64.24.50:     params-all-gather ..............................: (41.20, 41.46)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (1.39, 1.66)
10.64.24.50:     optimizer-clip-main-grad .......................: (8.02, 8.05)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.74, 513.12)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (3.50, 3.63)
10.64.24.50:     optimizer ......................................: (568.41, 568.64)
10.64.24.50:  iteration       20/      32 | consumed samples:        10240 | elapsed time per iteration (ms): 84017.3 | learning rate: 9.375E-07 | global batch size:   512 | lm loss: 1.148809E+00 | loss scale: 1.0 | grad norm: 8.462 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (83867.99, 83869.88)
10.64.24.50:     forward-compute ................................: (34164.41, 34558.72)
10.64.24.50:     backward-compute ...............................: (49136.24, 49543.42)
10.64.24.50:     batch-generator ................................: (295.28, 439.54)
10.64.24.50:     layernorm-grads-all-reduce .....................: (2.99, 3.31)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.03)
10.64.24.50:     all-grads-sync .................................: (73.84, 74.27)
10.64.24.50:     params-all-gather ..............................: (40.94, 41.43)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (1.39, 1.64)
10.64.24.50:     optimizer-clip-main-grad .......................: (5.26, 5.29)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.42, 9.47)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (3.50, 3.62)
10.64.24.50:     optimizer ......................................: (61.51, 61.99)
10.64.24.50:  iteration       30/      32 | consumed samples:        15360 | elapsed time per iteration (ms): 83063.7 | learning rate: 1.406E-06 | global batch size:   512 | lm loss: 7.785290E-01 | loss scale: 1.0 | grad norm: 2.403 | number of skipped iterations:   0 | number of nan iterations:   0 |
10.64.24.50: (min, max) time across ranks (ms):
10.64.24.50:     forward-backward ...............................: (82914.52, 82915.62)
10.64.24.50:     forward-compute ................................: (33227.93, 33598.22)
10.64.24.50:     backward-compute ...............................: (49142.62, 49525.96)
10.64.24.50:     batch-generator ................................: (298.28, 456.65)
10.64.24.50:     layernorm-grads-all-reduce .....................: (3.00, 3.27)
10.64.24.50:     embedding-grads-all-reduce .....................: (0.03, 0.03)
10.64.24.50:     all-grads-sync .................................: (73.76, 74.69)
10.64.24.50:     params-all-gather ..............................: (41.14, 41.55)
10.64.24.50:     optimizer-copy-to-main-grad ....................: (1.37, 1.62)
10.64.24.50:     optimizer-clip-main-grad .......................: (5.30, 5.33)
10.64.24.50:     optimizer-count-zeros ..........................: (0.01, 0.01)
10.64.24.50:     optimizer-inner-step ...........................: (9.41, 9.49)
10.64.24.50:     optimizer-copy-main-to-model-params ............: (3.49, 3.62)
10.64.24.50:     optimizer ......................................: (61.82, 62.23)
10.64.24.49: device 0: print cuda memory usage: 
10.64.24.49: Tue Mar 12 05:47:44 2024       
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
10.64.24.49: |-----------------------------------------+----------------------+----------------------+
10.64.24.49: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
10.64.24.49: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
10.64.24.49: |                                         |                      |               MIG M. |
10.64.24.49: |=========================================+======================+======================|
10.64.24.49: |   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |
10.64.24.49: | N/A   41C    P0             508W / 700W |  34960MiB / 81559MiB |     30%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |
10.64.24.49: | N/A   48C    P0             473W / 700W |  35130MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             443W / 700W |  35148MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
10.64.24.49: | N/A   40C    P0             454W / 700W |  35082MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |
10.64.24.49: | N/A   43C    P0             439W / 700W |  35152MiB / 81559MiB |     98%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   5  NVIDIA H100 80GB HBM3          On  | 00000000:8C:00.0 Off |                    0 |
10.64.24.49: | N/A   49C    P0             437W / 700W |  35140MiB / 81559MiB |     99%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |
10.64.24.49: | N/A   46C    P0             411W / 700W |  35266MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49: |   7  NVIDIA H100 80GB HBM3          On  | 00000000:E5:00.0 Off |                    0 |
10.64.24.49: | N/A   42C    P0             485W / 700W |  34858MiB / 81559MiB |    100%      Default |
10.64.24.49: |                                         |                      |             Disabled |
10.64.24.49: +-----------------------------------------+----------------------+----------------------+
10.64.24.49:                                                                                          
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: | Processes:                                                                            |
10.64.24.49: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
10.64.24.49: |        ID   ID                                                             Usage      |
10.64.24.49: |=======================================================================================|
10.64.24.49: +---------------------------------------------------------------------------------------+
10.64.24.49: rank 5: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}}rank 7: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}}[after training is done] datetime: 2024-03-12 05:49:14 
10.64.24.49: 
10.64.24.49: 
10.64.24.49: rank 4: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 3: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}}rank 6: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: 
10.64.24.49: rank 1: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.49: rank 2: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: terminate called after throwing an instance of 'c10::Error'
10.64.24.50:   what():  CUDA driver error: unknown error
10.64.24.50: Exception raised from _hasPrimaryContext at /opt/pytorch/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp:67 (most recent call first):
10.64.24.50: frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f9b62bb0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
10.64.24.50: frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f9b62b6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
10.64.24.50: frame #2: <unknown function> + 0xd55cef (0x7f9ad2614cef in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #3: c10::cuda::MaybeSetDevice(int) + 0xc (0x7f9b6c9d6bac in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10_cuda.so)
10.64.24.50: frame #4: <unknown function> + 0xd4e4a7 (0x7f9ad260d4a7 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #5: <unknown function> + 0xe44cb8 (0x7f9ad2703cb8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #6: <unknown function> + 0xd520ea (0x7f9ad26110ea in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #7: c10d::ProcessGroupNCCL::WorkNCCL::~WorkNCCL() + 0x127 (0x7f9ad26d6f57 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #8: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x22a (0x7f9ad26d908a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #9: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x80 (0x7f9ad26d92f0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #10: <unknown function> + 0xdc253 (0x7f9ad14b0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
10.64.24.50: frame #11: <unknown function> + 0x94ac3 (0x7f9b6df13ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
10.64.24.50: frame #12: <unknown function> + 0x126a40 (0x7f9b6dfa5a40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
10.64.24.50: 
10.64.24.49: rank 0: {'packing_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1815, '256': 1908, '512': 1867, '1024': 1486, '2048': 683, '4096': 268, '8192': 165, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: rank 8: {'packing_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 160, '16384': 0, '32768': 0, '>32k': 0}, 'real_seq_len': {'128': 1850, '256': 1879, '512': 1846, '1024': 1526, '2048': 674, '4096': 257, '8192': 160, '16384': 0, '32768': 0, '>32k': 0}}
10.64.24.50: [2024-03-12 05:49:19,730] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 1 (pid: 2438039) of binary: /usr/bin/python
10.64.24.50: Traceback (most recent call last):
10.64.24.50:   File "/usr/local/bin/torchrun", line 33, in <module>
10.64.24.50:     sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
10.64.24.50:     return f(*args, **kwargs)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
10.64.24.50:     run(args)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
10.64.24.50:     elastic_launch(
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
10.64.24.50:     return launch_agent(self._config, self._entrypoint, list(args))
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
10.64.24.50:     raise ChildFailedError(
10.64.24.50: torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
10.64.24.50: ========================================================
10.64.24.50: pretrain_gpt.py FAILED
10.64.24.50: --------------------------------------------------------
10.64.24.50: Failures:
10.64.24.50:   <NO_OTHER_FAILURES>
10.64.24.50: --------------------------------------------------------
10.64.24.50: Root Cause (first observed failure):
10.64.24.50: [0]:
10.64.24.50:   time      : 2024-03-12_05:49:19
10.64.24.50:   host      : SYM206-GPU-A0204-P2-Node50
10.64.24.50:   rank      : 9 (local_rank: 1)
10.64.24.50:   exitcode  : -6 (pid: 2438039)
10.64.24.50:   error_file: <N/A>
10.64.24.50:   traceback : Signal 6 (SIGABRT) received by PID 2438039
10.64.24.50: ========================================================
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen8192_gbs512_mbs1_dp2_tp8_pp1_pad_sp_node1.log...
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 13b model, gpu_num=16, seq_len = 4096, DP=4, MP=2, PP=2, GBS=512, MBS=2
10.64.24.50: use sequence parallel
10.64.24.50: [2024-03-12 05:49:22,108] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-12 05:49:22,108] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-12 05:49:22,108] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-12 05:49:22,108] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: Traceback (most recent call last):
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/pretrain_gpt.py", line 310, in <module>
10.64.24.50:     pretrain(train_dataset_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 97, in pretrain
10.64.24.50:     initialize_megatron(extra_args_provider=extra_args_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 83, in initialize_megatron
10.64.24.50:     finish_mpu_init()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 60, in finish_mpu_init
10.64.24.50:     _initialize_distributed()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 245, in _initialize_distributed
10.64.24.50:     mpu.initialize_model_parallel(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/parallel_state.py", line 274, in initialize_model_parallel
10.64.24.50:     group_gloo = torch.distributed.new_group(ranks, backend="gloo")
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
10.64.24.50:     func_return = func(*args, **kwargs)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
10.64.24.50:     return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
10.64.24.50:     pg, pg_store = _new_process_group_helper(
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
10.64.24.50:     backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
10.64.24.50: RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 45
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: Traceback (most recent call last):
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/pretrain_gpt.py", line 310, in <module>
10.64.24.50:     pretrain(train_dataset_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 97, in pretrain
10.64.24.50:     initialize_megatron(extra_args_provider=extra_args_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 83, in initialize_megatron
10.64.24.50:     finish_mpu_init()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 60, in finish_mpu_init
10.64.24.50:     _initialize_distributed()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 245, in _initialize_distributed
10.64.24.50:     mpu.initialize_model_parallel(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/parallel_state.py", line 274, in initialize_model_parallel
10.64.24.50:     group_gloo = torch.distributed.new_group(ranks, backend="gloo")
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
10.64.24.50:     func_return = func(*args, **kwargs)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
10.64.24.50:     return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
10.64.24.50:     pg, pg_store = _new_process_group_helper(
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
10.64.24.50:     backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
10.64.24.50: RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 45
10.64.24.50: [E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
10.64.24.50: [E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
10.64.24.50: [E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
10.64.24.50: Traceback (most recent call last):
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/pretrain_gpt.py", line 310, in <module>
10.64.24.50:     pretrain(train_dataset_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 97, in pretrain
10.64.24.50:     initialize_megatron(extra_args_provider=extra_args_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 83, in initialize_megatron
10.64.24.50:     finish_mpu_init()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 60, in finish_mpu_init
10.64.24.50:     _initialize_distributed()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 245, in _initialize_distributed
10.64.24.50:     mpu.initialize_model_parallel(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/parallel_state.py", line 274, in initialize_model_parallel
10.64.24.50:     group_gloo = torch.distributed.new_group(ranks, backend="gloo")
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
10.64.24.50:     func_return = func(*args, **kwargs)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
10.64.24.50:     return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
10.64.24.50:     pg, pg_store = _new_process_group_helper(
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
10.64.24.50:     backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
10.64.24.50: RuntimeError: [enforce fail at /opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/address.cc:45] sizeof(impl_) == bytes.size(). 136 vs 45
10.64.24.50: Traceback (most recent call last):
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/pretrain_gpt.py", line 310, in <module>
10.64.24.50:     pretrain(train_dataset_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 97, in pretrain
10.64.24.50:     initialize_megatron(extra_args_provider=extra_args_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 83, in initialize_megatron
10.64.24.50:     finish_mpu_init()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 60, in finish_mpu_init
10.64.24.50:     _initialize_distributed()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 245, in _initialize_distributed
10.64.24.50:     mpu.initialize_model_parallel(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/parallel_state.py", line 274, in initialize_model_parallel
10.64.24.50:     group_gloo = torch.distributed.new_group(ranks, backend="gloo")
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
10.64.24.50:     func_return = func(*args, **kwargs)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
10.64.24.50:     return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
10.64.24.50:     pg, pg_store = _new_process_group_helper(
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
10.64.24.50:     backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
10.64.24.50: RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
10.64.24.50: Traceback (most recent call last):
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/pretrain_gpt.py", line 310, in <module>
10.64.24.50: Traceback (most recent call last):
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/pretrain_gpt.py", line 310, in <module>
10.64.24.50:     pretrain(train_dataset_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 97, in pretrain
10.64.24.50:     pretrain(train_dataset_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 97, in pretrain
10.64.24.50:     initialize_megatron(extra_args_provider=extra_args_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 83, in initialize_megatron
10.64.24.50:     initialize_megatron(extra_args_provider=extra_args_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 83, in initialize_megatron
10.64.24.50:     finish_mpu_init()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 60, in finish_mpu_init
10.64.24.50:     finish_mpu_init()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 60, in finish_mpu_init
10.64.24.50:     _initialize_distributed()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 245, in _initialize_distributed
10.64.24.50:     _initialize_distributed()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 245, in _initialize_distributed
10.64.24.50:     mpu.initialize_model_parallel(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/parallel_state.py", line 274, in initialize_model_parallel
10.64.24.50:     mpu.initialize_model_parallel(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/parallel_state.py", line 274, in initialize_model_parallel
10.64.24.50:     group_gloo = torch.distributed.new_group(ranks, backend="gloo")
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
10.64.24.50:     func_return = func(*args, **kwargs)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
10.64.24.50:     group_gloo = torch.distributed.new_group(ranks, backend="gloo")
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
10.64.24.50:     func_return = func(*args, **kwargs)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
10.64.24.50:     return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
10.64.24.50:     return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
10.64.24.50:     pg, pg_store = _new_process_group_helper(
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
10.64.24.50:     pg, pg_store = _new_process_group_helper(
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
10.64.24.50:     backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
10.64.24.50: RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
10.64.24.50:     backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
10.64.24.50: RuntimeError: Gloo connectFullMesh failed with [/opt/pytorch/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
10.64.24.50: [2024-03-12 05:49:42,130] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2450826 closing signal SIGTERM
10.64.24.50: [2024-03-12 05:49:42,131] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2450828 closing signal SIGTERM
10.64.24.50: [2024-03-12 05:49:42,162] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 2450825) of binary: /usr/bin/python
10.64.24.50: Traceback (most recent call last):
10.64.24.50:   File "/usr/local/bin/torchrun", line 33, in <module>
10.64.24.50:     sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
10.64.24.50:     return f(*args, **kwargs)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
10.64.24.50:     run(args)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
10.64.24.50:     elastic_launch(
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
10.64.24.50:     return launch_agent(self._config, self._entrypoint, list(args))
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
10.64.24.50:     raise ChildFailedError(
10.64.24.50: torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
10.64.24.50: ============================================================
10.64.24.50: pretrain_gpt.py FAILED
10.64.24.50: ------------------------------------------------------------
10.64.24.50: Failures:
10.64.24.50: [1]:
10.64.24.50:   time      : 2024-03-12_05:49:42
10.64.24.50:   host      : SYM206-GPU-A0204-P2-Node50
10.64.24.50:   rank      : 10 (local_rank: 2)
10.64.24.50:   exitcode  : 1 (pid: 2450827)
10.64.24.50:   error_file: <N/A>
10.64.24.50:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
10.64.24.50: [2]:
10.64.24.50:   time      : 2024-03-12_05:49:42
10.64.24.50:   host      : SYM206-GPU-A0204-P2-Node50
10.64.24.50:   rank      : 12 (local_rank: 4)
10.64.24.50:   exitcode  : 1 (pid: 2450829)
10.64.24.50:   error_file: <N/A>
10.64.24.50:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
10.64.24.50: [3]:
10.64.24.50:   time      : 2024-03-12_05:49:42
10.64.24.50:   host      : SYM206-GPU-A0204-P2-Node50
10.64.24.50:   rank      : 13 (local_rank: 5)
10.64.24.50:   exitcode  : 1 (pid: 2450830)
10.64.24.50:   error_file: <N/A>
10.64.24.50:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
10.64.24.50: [4]:
10.64.24.50:   time      : 2024-03-12_05:49:42
10.64.24.50:   host      : SYM206-GPU-A0204-P2-Node50
10.64.24.50:   rank      : 14 (local_rank: 6)
10.64.24.50:   exitcode  : 1 (pid: 2450831)
10.64.24.50:   error_file: <N/A>
10.64.24.50:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
10.64.24.50: [5]:
10.64.24.50:   time      : 2024-03-12_05:49:42
10.64.24.50:   host      : SYM206-GPU-A0204-P2-Node50
10.64.24.50:   rank      : 15 (local_rank: 7)
10.64.24.50:   exitcode  : 1 (pid: 2450832)
10.64.24.50:   error_file: <N/A>
10.64.24.50:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
10.64.24.50: ------------------------------------------------------------
10.64.24.50: Root Cause (first observed failure):
10.64.24.50: [0]:
10.64.24.50:   time      : 2024-03-12_05:49:42
10.64.24.50:   host      : SYM206-GPU-A0204-P2-Node50
10.64.24.50:   rank      : 8 (local_rank: 0)
10.64.24.50:   exitcode  : 1 (pid: 2450825)
10.64.24.50:   error_file: <N/A>
10.64.24.50:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
10.64.24.50: ============================================================
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen4096_gbs512_mbs2_dp4_tp2_pp2_pad_sp_node1.log...
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 13b model, gpu_num=16, seq_len = 4096, DP=8, MP=2, PP=1, GBS=512, MBS=1
10.64.24.50: use sequence parallel
10.64.24.50: [2024-03-12 05:49:44,914] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-12 05:49:44,914] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-12 05:49:44,914] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-12 05:49:44,914] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: [2024-03-12 05:54:20,806] torch.distributed.elastic.agent.server.api: [ERROR] Error waiting on exit barrier. Elapsed: 300.1038148403168 seconds
10.64.24.50: [E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with Connection reset by peer
10.64.24.50: [E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with Connection reset by peer
10.64.24.50: [E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with Connection reset by peer
10.64.24.50: [E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with Connection reset by peer
10.64.24.50: [E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with Connection reset by peer
10.64.24.50: [E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with Connection reset by peer
10.64.24.50: [E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with Connection reset by peer
10.64.24.50: [E ProcessGroupGloo.cpp:138] Gloo connectFullMesh failed with Connection reset by peer
10.64.24.50: Traceback (most recent call last):
10.64.24.50: Traceback (most recent call last):
10.64.24.50: Traceback (most recent call last):
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/pretrain_gpt.py", line 310, in <module>
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/pretrain_gpt.py", line 310, in <module>
10.64.24.50: Traceback (most recent call last):
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/pretrain_gpt.py", line 310, in <module>
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/pretrain_gpt.py", line 310, in <module>
10.64.24.50:     pretrain(train_dataset_provider,    
10.64.24.50:     pretrain(train_dataset_provider,pretrain(train_dataset_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 97, in pretrain
10.64.24.50: 
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 97, in pretrain
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 97, in pretrain
10.64.24.50:     pretrain(train_dataset_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 97, in pretrain
10.64.24.50:     initialize_megatron(extra_args_provider=extra_args_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 83, in initialize_megatron
10.64.24.50:     initialize_megatron(extra_args_provider=extra_args_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 83, in initialize_megatron
10.64.24.50:     initialize_megatron(extra_args_provider=extra_args_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 83, in initialize_megatron
10.64.24.50:     initialize_megatron(extra_args_provider=extra_args_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 83, in initialize_megatron
10.64.24.50:     finish_mpu_init()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 60, in finish_mpu_init
10.64.24.50:     finish_mpu_init()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 60, in finish_mpu_init
10.64.24.50:     finish_mpu_init()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 60, in finish_mpu_init
10.64.24.50:     finish_mpu_init()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 60, in finish_mpu_init
10.64.24.50:     _initialize_distributed()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 245, in _initialize_distributed
10.64.24.50:     _initialize_distributed()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 245, in _initialize_distributed
10.64.24.50:     _initialize_distributed()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 245, in _initialize_distributed
10.64.24.50:     _initialize_distributed()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 245, in _initialize_distributed
10.64.24.50:     mpu.initialize_model_parallel(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/parallel_state.py", line 274, in initialize_model_parallel
10.64.24.50:     mpu.initialize_model_parallel(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/parallel_state.py", line 274, in initialize_model_parallel
10.64.24.50:     mpu.initialize_model_parallel(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/parallel_state.py", line 274, in initialize_model_parallel
10.64.24.50:     mpu.initialize_model_parallel(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/parallel_state.py", line 274, in initialize_model_parallel
10.64.24.50: Traceback (most recent call last):
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/pretrain_gpt.py", line 310, in <module>
10.64.24.50:     group_gloo = torch.distributed.new_group(ranks, backend="gloo")
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
10.64.24.50:     func_return = func(*args, **kwargs)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
10.64.24.50:         group_gloo = torch.distributed.new_group(ranks, backend="gloo")group_gloo = torch.distributed.new_group(ranks, backend="gloo")
10.64.24.50: 
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
10.64.24.50:     func_return = func(*args, **kwargs)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
10.64.24.50:     func_return = func(*args, **kwargs)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
10.64.24.50:     group_gloo = torch.distributed.new_group(ranks, backend="gloo")
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
10.64.24.50:         func_return = func(*args, **kwargs)return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
10.64.24.50: 
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
10.64.24.50:         return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)pretrain(train_dataset_provider,
10.64.24.50: 
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 97, in pretrain
10.64.24.50:     return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
10.64.24.50:     pg, pg_store = _new_process_group_helper(
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
10.64.24.50:     return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
10.64.24.50:     backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
10.64.24.50: RuntimeError: Gloo connectFullMesh failed with Connection reset by peer
10.64.24.50:     pg, pg_store = _new_process_group_helper(
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
10.64.24.50: Traceback (most recent call last):
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/pretrain_gpt.py", line 310, in <module>
10.64.24.50:     pg, pg_store = _new_process_group_helper(
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
10.64.24.50:     backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
10.64.24.50: RuntimeError: Gloo connectFullMesh failed with Connection reset by peer
10.64.24.50: Traceback (most recent call last):
10.64.24.50: Traceback (most recent call last):
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/pretrain_gpt.py", line 310, in <module>
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/pretrain_gpt.py", line 310, in <module>
10.64.24.50:     backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
10.64.24.50: RuntimeError: Gloo connectFullMesh failed with Connection reset by peer
10.64.24.50:     pg, pg_store = _new_process_group_helper(
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
10.64.24.50:     initialize_megatron(extra_args_provider=extra_args_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 83, in initialize_megatron
10.64.24.50:     backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
10.64.24.50: RuntimeError: Gloo connectFullMesh failed with Connection reset by peer
10.64.24.50:     pretrain(train_dataset_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 97, in pretrain
10.64.24.50:     finish_mpu_init()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 60, in finish_mpu_init
10.64.24.50:     pretrain(train_dataset_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 97, in pretrain
10.64.24.50:     initialize_megatron(extra_args_provider=extra_args_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 83, in initialize_megatron
10.64.24.50:     pretrain(train_dataset_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 97, in pretrain
10.64.24.50:     _initialize_distributed()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 245, in _initialize_distributed
10.64.24.50:     initialize_megatron(extra_args_provider=extra_args_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 83, in initialize_megatron
10.64.24.50:     initialize_megatron(extra_args_provider=extra_args_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 83, in initialize_megatron
10.64.24.50:     mpu.initialize_model_parallel(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/parallel_state.py", line 274, in initialize_model_parallel
10.64.24.50:     finish_mpu_init()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 60, in finish_mpu_init
10.64.24.50:         finish_mpu_init()finish_mpu_init()
10.64.24.50: 
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 60, in finish_mpu_init
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 60, in finish_mpu_init
10.64.24.50:     _initialize_distributed()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 245, in _initialize_distributed
10.64.24.50:     _initialize_distributed()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 245, in _initialize_distributed
10.64.24.50:     mpu.initialize_model_parallel(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/parallel_state.py", line 274, in initialize_model_parallel
10.64.24.50:     group_gloo = torch.distributed.new_group(ranks, backend="gloo")
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
10.64.24.50:     func_return = func(*args, **kwargs)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
10.64.24.50:     return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
10.64.24.50:     pg, pg_store = _new_process_group_helper(
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
10.64.24.50:     backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
10.64.24.50: RuntimeError: Gloo connectFullMesh failed with Connection reset by peer
10.64.24.50:     group_gloo = torch.distributed.new_group(ranks, backend="gloo")
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
10.64.24.50:     func_return = func(*args, **kwargs)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
10.64.24.50:     return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
10.64.24.50:     pg, pg_store = _new_process_group_helper(
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
10.64.24.50:     backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
10.64.24.50: RuntimeError: Gloo connectFullMesh failed with Connection reset by peer
10.64.24.50:     _initialize_distributed()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py", line 245, in _initialize_distributed
10.64.24.50:     mpu.initialize_model_parallel(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/parallel_state.py", line 274, in initialize_model_parallel
10.64.24.50:     mpu.initialize_model_parallel(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/parallel_state.py", line 274, in initialize_model_parallel
10.64.24.50:     group_gloo = torch.distributed.new_group(ranks, backend="gloo")
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
10.64.24.50:     func_return = func(*args, **kwargs)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
10.64.24.50:     group_gloo = torch.distributed.new_group(ranks, backend="gloo")
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 74, in wrapper
10.64.24.50:     func_return = func(*args, **kwargs)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3870, in new_group
10.64.24.50:     return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
10.64.24.50:     pg, pg_store = _new_process_group_helper(
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
10.64.24.50:     backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
10.64.24.50:     return _new_group_with_tag(ranks, timeout, backend, pg_options, None, use_local_synchronization=use_local_synchronization)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3933, in _new_group_with_tag
10.64.24.50:     pg, pg_store = _new_process_group_helper(
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 1269, in _new_process_group_helper
10.64.24.50: RuntimeError    backend_class = ProcessGroupGloo(backend_prefix_store, group_rank, group_size, timeout=timeout)
10.64.24.50: : Gloo connectFullMesh failed with Connection reset by peer
10.64.24.50: RuntimeError: Gloo connectFullMesh failed with Connection reset by peer
10.64.24.49: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen8192_gbs512_mbs1_dp2_tp8_pp1_pad_sp_node0.log...
10.64.24.49: local_ip = 10.64.24.49, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 0
10.64.24.49: use gpt 13b model, gpu_num=16, seq_len = 4096, DP=4, MP=2, PP=2, GBS=512, MBS=2
10.64.24.49: use sequence parallel
10.64.24.49: [2024-03-12 05:54:23,209] torch.distributed.run: [WARNING] 
10.64.24.49: [2024-03-12 05:54:23,209] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: [2024-03-12 05:54:23,209] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.49: [2024-03-12 05:54:23,209] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-12 05:54:25,201] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 2451682) of binary: /usr/bin/python
10.64.24.50: Traceback (most recent call last):
10.64.24.50:   File "/usr/local/bin/torchrun", line 33, in <module>
10.64.24.50:     sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
10.64.24.50:     return f(*args, **kwargs)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
10.64.24.50:     run(args)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
10.64.24.50:     elastic_launch(
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
10.64.24.50:     return launch_agent(self._config, self._entrypoint, list(args))
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
10.64.24.50:     raise ChildFailedError(
10.64.24.50: torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
10.64.24.50: ============================================================
10.64.24.50: pretrain_gpt.py FAILED
10.64.24.50: ------------------------------------------------------------
10.64.24.50: Failures:
10.64.24.50: [1]:
10.64.24.50:   time      : 2024-03-12_05:54:25
10.64.24.50:   host      : SYM206-GPU-A0204-P2-Node50
10.64.24.50:   rank      : 9 (local_rank: 1)
10.64.24.50:   exitcode  : 1 (pid: 2451683)
10.64.24.50:   error_file: <N/A>
10.64.24.50:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
10.64.24.50: [2]:
10.64.24.50:   time      : 2024-03-12_05:54:25
10.64.24.50:   host      : SYM206-GPU-A0204-P2-Node50
10.64.24.50:   rank      : 10 (local_rank: 2)
10.64.24.50:   exitcode  : 1 (pid: 2451684)
10.64.24.50:   error_file: <N/A>
10.64.24.50:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
10.64.24.50: [3]:
10.64.24.50:   time      : 2024-03-12_05:54:25
10.64.24.50:   host      : SYM206-GPU-A0204-P2-Node50
10.64.24.50:   rank      : 11 (local_rank: 3)
10.64.24.50:   exitcode  : 1 (pid: 2451685)
10.64.24.50:   error_file: <N/A>
10.64.24.50:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
10.64.24.50: [4]:
10.64.24.50:   time      : 2024-03-12_05:54:25
10.64.24.50:   host      : SYM206-GPU-A0204-P2-Node50
10.64.24.50:   rank      : 12 (local_rank: 4)
10.64.24.50:   exitcode  : 1 (pid: 2451686)
10.64.24.50:   error_file: <N/A>
10.64.24.50:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
10.64.24.50: [5]:
10.64.24.50:   time      : 2024-03-12_05:54:25
10.64.24.50:   host      : SYM206-GPU-A0204-P2-Node50
10.64.24.50:   rank      : 13 (local_rank: 5)
10.64.24.50:   exitcode  : 1 (pid: 2451687)
10.64.24.50:   error_file: <N/A>
10.64.24.50:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
10.64.24.50: [6]:
10.64.24.50:   time      : 2024-03-12_05:54:25
10.64.24.50:   host      : SYM206-GPU-A0204-P2-Node50
10.64.24.50:   rank      : 14 (local_rank: 6)
10.64.24.50:   exitcode  : 1 (pid: 2451688)
10.64.24.50:   error_file: <N/A>
10.64.24.50:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
10.64.24.50: [7]:
10.64.24.50:   time      : 2024-03-12_05:54:25
10.64.24.50:   host      : SYM206-GPU-A0204-P2-Node50
10.64.24.50:   rank      : 15 (local_rank: 7)
10.64.24.50:   exitcode  : 1 (pid: 2451689)
10.64.24.50:   error_file: <N/A>
10.64.24.50:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
10.64.24.50: ------------------------------------------------------------
10.64.24.50: Root Cause (first observed failure):
10.64.24.50: [0]:
10.64.24.50:   time      : 2024-03-12_05:54:25
10.64.24.50:   host      : SYM206-GPU-A0204-P2-Node50
10.64.24.50:   rank      : 8 (local_rank: 0)
10.64.24.50:   exitcode  : 1 (pid: 2451682)
10.64.24.50:   error_file: <N/A>
10.64.24.50:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
10.64.24.50: ============================================================
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen4096_gbs512_mbs1_dp8_tp2_pp1_pad_sp_node1.log...
10.64.24.50: local_ip = 10.64.24.50, master_ip = 10.64.24.49, nodes_num = 2, gpus_num = 16, node_rank = 1
10.64.24.50: use gpt 13b model, gpu_num=16, seq_len = 4096, DP=2, MP=2, PP=4, GBS=512, MBS=2
10.64.24.50: use sequence parallel
10.64.24.50: [2024-03-12 05:54:27,588] torch.distributed.run: [WARNING] 
10.64.24.50: [2024-03-12 05:54:27,588] torch.distributed.run: [WARNING] *****************************************
10.64.24.50: [2024-03-12 05:54:27,588] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
10.64.24.50: [2024-03-12 05:54:27,588] torch.distributed.run: [WARNING] *****************************************
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.50: Zarr-based strategies will not be registered because of missing packages
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: using world size: 16, data-parallel size: 4, context-parallel size: 1 tensor-model-parallel size: 2, pipeline-model-parallel size: 2 
10.64.24.49: WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
10.64.24.49: accumulate and all-reduce gradients in fp32 for bfloat16 data type.
10.64.24.49: using torch.bfloat16 for parameters ...
10.64.24.49: ------------------------ arguments ------------------------
10.64.24.49:   accumulate_allreduce_grads_in_fp32 .............. True
10.64.24.49:   adam_beta1 ...................................... 0.9
10.64.24.49:   adam_beta2 ...................................... 0.999
10.64.24.49:   adam_eps ........................................ 1e-08
10.64.24.49:   add_bias_linear ................................. True
10.64.24.49:   add_position_embedding .......................... True
10.64.24.49:   adlr_autoresume ................................. False
10.64.24.49:   adlr_autoresume_interval ........................ 1000
10.64.24.49:   apply_layernorm_1p .............................. False
10.64.24.49:   apply_query_key_layer_scaling ................... False
10.64.24.49:   apply_residual_connection_post_layernorm ........ False
10.64.24.49:   async_tensor_model_parallel_allreduce ........... False
10.64.24.49:   attention_dropout ............................... 0.1
10.64.24.49:   attention_softmax_in_fp32 ....................... False
10.64.24.49:   barrier_with_L1_time ............................ True
10.64.24.49:   bert_binary_head ................................ True
10.64.24.49:   bert_embedder_type .............................. megatron
10.64.24.49:   bert_load ....................................... None
10.64.24.49:   bf16 ............................................ True
10.64.24.49:   bias_dropout_fusion ............................. False
10.64.24.49:   bias_gelu_fusion ................................ False
10.64.24.49:   biencoder_projection_dim ........................ 0
10.64.24.49:   biencoder_shared_query_context_model ............ False
10.64.24.49:   block_data_path ................................. None
10.64.24.49:   check_for_nan_in_loss_and_grad .................. True
10.64.24.49:   classes_fraction ................................ 1.0
10.64.24.49:   clip_grad ....................................... 1.0
10.64.24.49:   clone_scatter_output_in_embedding ............... True
10.64.24.49:   consumed_train_samples .......................... 0
10.64.24.49:   consumed_valid_samples .......................... 0
10.64.24.49:   context_parallel_size ........................... 1
10.64.24.49:   data_cache_path ................................. None
10.64.24.49:   data_parallel_random_init ....................... False
10.64.24.49:   data_parallel_size .............................. 4
10.64.24.49:   data_path ....................................... ['/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0_content_document']
10.64.24.49:   data_per_class_fraction ......................... 1.0
10.64.24.49:   data_sharding ................................... True
10.64.24.49:   dataloader_type ................................. single
10.64.24.49:   decoder_num_layers .............................. None
10.64.24.49:   decoder_seq_length .............................. None
10.64.24.49:   delay_grad_reduce ............................... True
10.64.24.49:   delay_param_gather .............................. False
10.64.24.49:   dino_bottleneck_size ............................ 256
10.64.24.49:   dino_freeze_last_layer .......................... 1
10.64.24.49:   dino_head_hidden_size ........................... 2048
10.64.24.49:   dino_local_crops_number ......................... 10
10.64.24.49:   dino_local_img_size ............................. 96
10.64.24.49:   dino_norm_last_layer ............................ False
10.64.24.49:   dino_teacher_temp ............................... 0.07
10.64.24.49:   dino_warmup_teacher_temp ........................ 0.04
10.64.24.49:   dino_warmup_teacher_temp_epochs ................. 30
10.64.24.49:   distribute_saved_activations .................... False
10.64.24.49:   distributed_backend ............................. nccl
10.64.24.49:   distributed_timeout_minutes ..................... 10
10.64.24.49:   embedding_path .................................. None
10.64.24.49:   empty_unused_memory_level ....................... 0
10.64.24.49:   encoder_num_layers .............................. 40
10.64.24.49:   encoder_seq_length .............................. 4096
10.64.24.49:   end_weight_decay ................................ 0.01
10.64.24.49:   eod_mask_loss ................................... False
10.64.24.49:   eval_interval ................................... 1000
10.64.24.49:   eval_iters ...................................... 10
10.64.24.49:   evidence_data_path .............................. None
10.64.24.49:   exit_duration_in_mins ........................... None
10.64.24.49:   exit_interval ................................... None
10.64.24.49:   exit_on_missing_checkpoint ...................... False
10.64.24.49:   exit_signal_handler ............................. False
10.64.24.49:   expert_model_parallel_size ...................... 1
10.64.24.49:   ffn_hidden_size ................................. 20480
10.64.24.49:   finetune ........................................ False
10.64.24.49:   fp16 ............................................ False
10.64.24.49:   fp16_lm_cross_entropy ........................... False
10.64.24.49:   fp32_residual_connection ........................ False
10.64.24.49:   fp8 ............................................. None
10.64.24.49:   fp8_amax_compute_algo ........................... most_recent
10.64.24.49:   fp8_amax_history_len ............................ 1
10.64.24.49:   fp8_interval .................................... 1
10.64.24.49:   fp8_margin ...................................... 0
10.64.24.49:   fp8_wgrad ....................................... True
10.64.24.49:   global_batch_size ............................... 512
10.64.24.49:   gradient_accumulation_fusion .................... False
10.64.24.49:   group_query_attention ........................... False
10.64.24.49:   head_lr_mult .................................... 1.0
10.64.24.49:   hidden_dropout .................................. 0.1
10.64.24.49:   hidden_size ..................................... 5120
10.64.24.49:   hysteresis ...................................... 2
10.64.24.49:   ict_head_size ................................... None
10.64.24.49:   ict_load ........................................ None
10.64.24.49:   img_h ........................................... 224
10.64.24.49:   img_w ........................................... 224
10.64.24.49:   indexer_batch_size .............................. 128
10.64.24.49:   indexer_log_interval ............................ 1000
10.64.24.49:   inference_batch_times_seqlen_threshold .......... 512
10.64.24.49:   init_method_std ................................. 0.02
10.64.24.49:   init_method_xavier_uniform ...................... False
10.64.24.49:   initial_loss_scale .............................. 4294967296
10.64.24.49:   iter_per_epoch .................................. 1250
10.64.24.49:   json_file ....................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/web/refinedweb0.json
10.64.24.49:   json_key ........................................ content
10.64.24.49:   kv_channels ..................................... 128
10.64.24.49:   lazy_mpu_init ................................... None
10.64.24.49:   load ............................................ None
10.64.24.49:   local_rank ...................................... None
10.64.24.49:   log_batch_size_to_tensorboard ................... False
10.64.24.49:   log_interval .................................... 10
10.64.24.49:   log_learning_rate_to_tensorboard ................ True
10.64.24.49:   log_loss_scale_to_tensorboard ................... True
10.64.24.49:   log_memory_to_tensorboard ....................... False
10.64.24.49:   log_num_zeros_in_grad ........................... False
10.64.24.49:   log_params_norm ................................. False
10.64.24.49:   log_timers_to_tensorboard ....................... False
10.64.24.49:   log_validation_ppl_to_tensorboard ............... False
10.64.24.49:   log_world_size_to_tensorboard ................... False
10.64.24.49:   loss_scale ...................................... None
10.64.24.49:   loss_scale_window ............................... 1000
10.64.24.49:   lr .............................................. 0.00015
10.64.24.49:   lr_decay_iters .................................. 320000
10.64.24.49:   lr_decay_samples ................................ None
10.64.24.49:   lr_decay_style .................................. cosine
10.64.24.49:   lr_warmup_fraction .............................. 0.01
10.64.24.49:   lr_warmup_init .................................. 0.0
10.64.24.49:   lr_warmup_iters ................................. 0
10.64.24.49:   lr_warmup_samples ............................... 0
10.64.24.49:   make_vocab_size_divisible_by .................... 128
10.64.24.49:   manual_gc ....................................... False
10.64.24.49:   manual_gc_eval .................................. True
10.64.24.49:   manual_gc_interval .............................. 0
10.64.24.49:   mask_factor ..................................... 1.0
10.64.24.49:   mask_prob ....................................... 0.15
10.64.24.49:   mask_type ....................................... random
10.64.24.49:   masked_softmax_fusion ........................... False
10.64.24.49:   max_position_embeddings ......................... 4096
10.64.24.49:   max_tokens_to_oom ............................... 12000
10.64.24.49:   merge_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/merges.txt
10.64.24.49:   micro_batch_size ................................ 2
10.64.24.49:   min_loss_scale .................................. 1.0
10.64.24.49:   min_lr .......................................... 1e-05
10.64.24.49:   nccl_communicator_config_path ................... None
10.64.24.49:   no_load_optim ................................... None
10.64.24.49:   no_load_rng ..................................... None
10.64.24.49:   no_persist_layer_norm ........................... False
10.64.24.49:   no_save_optim ................................... None
10.64.24.49:   no_save_rng ..................................... None
10.64.24.49:   norm_epsilon .................................... 1e-05
10.64.24.49:   normalization ................................... LayerNorm
10.64.24.49:   num_attention_heads ............................. 40
10.64.24.49:   num_channels .................................... 3
10.64.24.49:   num_classes ..................................... 1000
10.64.24.49:   num_experts ..................................... None
10.64.24.49:   num_layers ...................................... 40
10.64.24.49:   num_layers_per_virtual_pipeline_stage ........... None
10.64.24.49:   num_query_groups ................................ 1
10.64.24.49:   num_workers ..................................... 2
10.64.24.49:   onnx_safe ....................................... None
10.64.24.49:   openai_gelu ..................................... False
10.64.24.49:   optimizer ....................................... adam
10.64.24.49:   output_bert_embeddings .......................... False
10.64.24.49:   overlap_grad_reduce ............................. False
10.64.24.49:   overlap_p2p_comm ................................ False
10.64.24.49:   overlap_param_gather ............................ False
10.64.24.49:   override_opt_param_scheduler .................... False
10.64.24.49:   params_dtype .................................... torch.bfloat16
10.64.24.49:   patch_dim ....................................... 16
10.64.24.49:   perform_initialization .......................... True
10.64.24.49:   pipeline_model_parallel_size .................... 2
10.64.24.49:   pipeline_model_parallel_split_rank .............. None
10.64.24.49:   position_embedding_type ......................... learned_absolute
10.64.24.49:   profile ......................................... False
10.64.24.49:   profile_ranks ................................... [0]
10.64.24.49:   profile_step_end ................................ 12
10.64.24.49:   profile_step_start .............................. 10
10.64.24.49:   query_in_block_prob ............................. 0.1
10.64.24.49:   rampup_batch_size ............................... None
10.64.24.49:   rank ............................................ 0
10.64.24.49:   recompute_granularity ........................... None
10.64.24.49:   recompute_method ................................ None
10.64.24.49:   recompute_num_layers ............................ None
10.64.24.49:   reset_attention_mask ............................ False
10.64.24.49:   reset_position_ids .............................. False
10.64.24.49:   retriever_report_topk_accuracies ................ []
10.64.24.49:   retriever_score_scaling ......................... False
10.64.24.49:   retriever_seq_length ............................ 256
10.64.24.49:   retro_add_retriever ............................. False
10.64.24.49:   retro_cyclic_train_iters ........................ None
10.64.24.49:   retro_encoder_attention_dropout ................. 0.1
10.64.24.49:   retro_encoder_hidden_dropout .................... 0.1
10.64.24.49:   retro_encoder_layers ............................ 2
10.64.24.49:   retro_num_neighbors ............................. 2
10.64.24.49:   retro_num_retrieved_chunks ...................... 2
10.64.24.49:   retro_return_doc_ids ............................ False
10.64.24.49:   retro_verify_neighbor_count ..................... True
10.64.24.49:   retro_workdir ................................... None
10.64.24.49:   rotary_percent .................................. 1.0
10.64.24.49:   rotary_seq_len_interpolation_factor ............. None
10.64.24.49:   sample_rate ..................................... 1.0
10.64.24.49:   save ............................................ None
10.64.24.49:   save_interval ................................... 1000
10.64.24.49:   scatter_gather_tensors_in_pipeline .............. True
10.64.24.49:   seed ............................................ 1234
10.64.24.49:   seq_length ...................................... 4096
10.64.24.49:   sequence_packing ................................ False
10.64.24.49:   sequence_parallel ............................... True
10.64.24.49:   sgd_momentum .................................... 0.9
10.64.24.49:   short_seq_prob .................................. 0.1
10.64.24.49:   skip_train ...................................... False
10.64.24.49:   spec ............................................ None
10.64.24.49:   split ........................................... 949,50,1
10.64.24.49:   squared_relu .................................... False
10.64.24.49:   standalone_embedding_stage ...................... False
10.64.24.49:   start_weight_decay .............................. 0.01
10.64.24.49:   swiglu .......................................... False
10.64.24.49:   swin_backbone_type .............................. tiny
10.64.24.49:   tensor_model_parallel_size ...................... 2
10.64.24.49:   tensorboard_dir ................................. None
10.64.24.49:   tensorboard_log_interval ........................ 1
10.64.24.49:   tensorboard_queue_size .......................... 1000
10.64.24.49:   test_data_path .................................. None
10.64.24.49:   timing_log_level ................................ 2
10.64.24.49:   timing_log_option ............................... minmax
10.64.24.49:   titles_data_path ................................ None
10.64.24.49:   tokenizer_model ................................. None
10.64.24.49:   tokenizer_type .................................. GPT2BPETokenizer
10.64.24.49:   tp_comm_bulk_dgrad .............................. True
10.64.24.49:   tp_comm_bulk_wgrad .............................. True
10.64.24.49:   tp_comm_overlap ................................. False
10.64.24.49:   tp_comm_overlap_cfg ............................. None
10.64.24.49:   tp_comm_split_ag ................................ True
10.64.24.49:   tp_comm_split_rs ................................ True
10.64.24.49:   train_data_path ................................. None
10.64.24.49:   train_iters ..................................... 32
10.64.24.49:   train_samples ................................... None
10.64.24.49:   transformer_impl ................................ local
10.64.24.49:   transformer_pipeline_model_parallel_size ........ 2
10.64.24.49:   untie_embeddings_and_output_weights ............. False
10.64.24.49:   use_checkpoint_args ............................. False
10.64.24.49:   use_checkpoint_opt_param_scheduler .............. False
10.64.24.49:   use_cpu_initialization .......................... None
10.64.24.49:   use_distributed_optimizer ....................... True
10.64.24.49:   use_flash_attn .................................. True
10.64.24.49:   use_mcore_models ................................ False
10.64.24.49:   use_one_sent_docs ............................... False
10.64.24.49:   use_ring_exchange_p2p ........................... False
10.64.24.49:   use_rotary_position_embeddings .................. False
10.64.24.49:   valid_data_path ................................. None
10.64.24.49:   variable_seq_lengths ............................ False
10.64.24.49:   virtual_pipeline_model_parallel_size ............ None
10.64.24.49:   vision_backbone_type ............................ vit
10.64.24.49:   vision_pretraining .............................. False
10.64.24.49:   vision_pretraining_type ......................... classify
10.64.24.49:   vocab_extra_ids ................................. 0
10.64.24.49:   vocab_file ...................................... /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-LM/data/vocab.json
10.64.24.49:   vocab_size ...................................... None
10.64.24.49:   wandb_exp_name .................................. 
10.64.24.49:   wandb_project ................................... 
10.64.24.49:   wandb_save_dir .................................. 
10.64.24.49:   weight_decay .................................... 0.01
10.64.24.49:   weight_decay_incr_style ......................... constant
10.64.24.49:   world_size ...................................... 16
10.64.24.49: -------------------- end of arguments ---------------------
10.64.24.49: setting number of micro-batches to constant 64
10.64.24.49: > building GPT2BPETokenizer tokenizer ...
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49:  > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
10.64.24.49: > initializing torch distributed ...
10.64.24.49: > initialized tensor model parallel with size 2
10.64.24.49: > initialized pipeline model parallel with size 2
10.64.24.49: > setting random seeds to 1234 ...
10.64.24.49: > compiling dataset index builder ...
10.64.24.49: make: Entering directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: make: Nothing to be done for 'default'.
10.64.24.49: make: Leaving directory '/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/core/datasets'
10.64.24.49: >>> done with dataset index builder. Compilation time: 0.060 seconds
10.64.24.49: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
10.64.24.49: > compiling and loading fused kernels ...
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50: INFO:megatron.tokenizer.gpt2_tokenization:Special tokens {'<pad>': 50257}
10.64.24.49: >>> done with compiling and loading fused kernels. Compilation time: 8.817 seconds
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.49:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/initialize.py:354: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10.64.24.50:   output = bias_gelu(bias, input)
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.49:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py:106: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10.64.24.50:   start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
10.64.24.49: time to initialize megatron (seconds): 11.075
10.64.24.49: [after megatron is initialized] datetime: 2024-03-12 05:54:56 
10.64.24.49: building GPT model ...
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 1573350400
10.64.24.50:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1573350400
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
10.64.24.50: INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1573350400 elements):
10.64.24.50: /data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10.64.24.50:   storage = bucket.data.storage()._untyped()
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.49: NCCL version 2.19.3+cuda12.2
10.64.24.50: [E ProcessGroupNCCL.cpp:467] [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600016 milliseconds before timing out.
10.64.24.50: Traceback (most recent call last):
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/pretrain_gpt.py", line 310, in <module>
10.64.24.50:     pretrain(train_dataset_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 119, in pretrain
10.64.24.50: Traceback (most recent call last):
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/pretrain_gpt.py", line 310, in <module>
10.64.24.50:     model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 369, in setup_model_and_optimizer
10.64.24.50: Traceback (most recent call last):
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/pretrain_gpt.py", line 310, in <module>
10.64.24.50:     model = get_model(model_provider_func, model_type)
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 250, in get_model
10.64.24.50:     pretrain(train_dataset_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 119, in pretrain
10.64.24.50:     pretrain(train_dataset_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 119, in pretrain
10.64.24.50:     model = model_provider_func(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/pretrain_gpt.py", line 78, in model_provider
10.64.24.50:     model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 369, in setup_model_and_optimizer
10.64.24.50:     model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 369, in setup_model_and_optimizer
10.64.24.50:     model = megatron.model.GPTModel(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/model/gpt_model.py", line 107, in __init__
10.64.24.50:     model = get_model(model_provider_func, model_type)
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 250, in get_model
10.64.24.50:     model = model_provider_func(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/pretrain_gpt.py", line 78, in model_provider
10.64.24.50:     model = get_model(model_provider_func, model_type)
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 250, in get_model
10.64.24.50:     model = megatron.model.GPTModel(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/model/gpt_model.py", line 107, in __init__
10.64.24.50:     model = model_provider_func(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/pretrain_gpt.py", line 78, in model_provider
10.64.24.50:     self.initialize_word_embeddings()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/model/module.py", line 108, in initialize_word_embeddings
10.64.24.50:     self.initialize_word_embeddings()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/model/module.py", line 108, in initialize_word_embeddings
10.64.24.50:     model = megatron.model.GPTModel(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/model/gpt_model.py", line 107, in __init__
10.64.24.50:     self.initialize_word_embeddings()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/model/module.py", line 108, in initialize_word_embeddings
10.64.24.50:     torch.distributed.all_reduce(self.shared_embedding_or_output_weight().data,
10.64.24.50:     torch.distributed.all_reduce(self.shared_embedding_or_output_weight().data,  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
10.64.24.50: 
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
10.64.24.50:         return func(*args, **kwargs)return func(*args, **kwargs)
10.64.24.50: 
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
10.64.24.50:     torch.distributed.all_reduce(self.shared_embedding_or_output_weight().data,
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
10.64.24.50:     return func(*args, **kwargs)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
10.64.24.50:     work = group.allreduce([tensor], opts)
10.64.24.50:     work = group.allreduce([tensor], opts)
10.64.24.50: RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
10.64.24.50: Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
10.64.24.50: frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f59b31b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
10.64.24.50: frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f59b316b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
10.64.24.50: frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f5965b9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f5965b9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f5965b91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f5965b4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f5965b4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f5965b4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f5965b4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f5965b4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f5918ed73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f5918eda908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #12: <unknown function> + 0xe27d37 (0x7f5918ee6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f5918ee82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f5918eea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #15: <unknown function> + 0x4936f0a (0x7f5965b38f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #16: <unknown function> + 0x494766f (0x7f5965b4966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #17: <unknown function> + 0x4950b97 (0x7f5965b52b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #18: <unknown function> + 0x495f1ee (0x7f5965b611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #19: <unknown function> + 0xb8cc15 (0x7f596c322c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
10.64.24.50: frame #20: <unknown function> + 0x39c407 (0x7f596bb32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
10.64.24.50: frame #21: <unknown function> + 0x15fe0e (0x556c30b91e0e in /usr/bin/python)
10.64.24.50: frame #22: _PyObject_MakeTpCall + 0x25b (0x556c30b885eb in /usr/bin/python)
10.64.24.50: frame #23: <unknown function> + 0x16e7bb (0x556c30ba07bb in /usr/bin/python)
10.64.24.50: frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x556c30b808a2 in /usr/bin/python)
10.64.24.50: frame #25: _PyFunction_Vectorcall + 0x7c (0x556c30b9270c in /usr/bin/python)
10.64.24.50: frame #26: PyObject_Call + 0x122 (0x556c30ba1192 in /usr/bin/python)
10.64.24.50: frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x556c30b7d2c1 in /usr/bin/python)
10.64.24.50: frame #28: _PyFunction_Vectorcall + 0x7c (0x556c30b9270c in /usr/bin/python)
10.64.24.50: frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x556c30b7c0d1 in /usr/bin/python)
10.64.24.50: frame #30: <unknown function> + 0x16e4e1 (0x556c30ba04e1 in /usr/bin/python)
10.64.24.50: frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x556c30b808a2 in /usr/bin/python)
10.64.24.50: frame #32: _PyFunction_Vectorcall + 0x7c (0x556c30b9270c in /usr/bin/python)
10.64.24.50: frame #33: _PyObject_FastCallDictTstate + 0x16d (0x556c30b8782d in /usr/bin/python)
10.64.24.50: frame #34: <unknown function> + 0x16a744 (0x556c30b9c744 in /usr/bin/python)
10.64.24.50: frame #35: _PyObject_MakeTpCall + 0x1fc (0x556c30b8858c in /usr/bin/python)
10.64.24.50: frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x556c30b81908 in /usr/bin/python)
10.64.24.50: frame #37: _PyFunction_Vectorcall + 0x7c (0x556c30b9270c in /usr/bin/python)
10.64.24.50: frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x556c30b7c0d1 in /usr/bin/python)
10.64.24.50: frame #39: _PyFunction_Vectorcall + 0x7c (0x556c30b9270c in /usr/bin/python)
10.64.24.50: frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x556c30b7ae0d in /usr/bin/python)
10.64.24.50: frame #41: _PyFunction_Vectorcall + 0x7c (0x556c30b9270c in /usr/bin/python)
10.64.24.50: frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x556c30b7ae0d in /usr/bin/python)
10.64.24.50: frame #43: _PyFunction_Vectorcall + 0x7c (0x556c30b9270c in /usr/bin/python)
10.64.24.50: frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x556c30b7c0d1 in /usr/bin/python)
10.64.24.50: frame #45: <unknown function> + 0x239e56 (0x556c30c6be56 in /usr/bin/python)
10.64.24.50: frame #46: PyEval_EvalCode + 0x86 (0x556c30c6bcf6 in /usr/bin/python)
10.64.24.50: frame #47: <unknown function> + 0x2647d8 (0x556c30c967d8 in /usr/bin/python)
10.64.24.50: frame #48: <unknown function> + 0x25e0bb (0x556c30c900bb in /usr/bin/python)
10.64.24.50: frame #49: <unknown function> + 0x264525 (0x556c30c96525 in /usr/bin/python)
10.64.24.50: frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x556c30c95a08 in /usr/bin/python)
10.64.24.50: frame #51: _PyRun_AnyFileObject + 0x43 (0x556c30c95653 in /usr/bin/python)
10.64.24.50: frame #52: Py_RunMain + 0x2be (0x556c30c8841e in /usr/bin/python)
10.64.24.50: frame #53: Py_BytesMain + 0x2d (0x556c30c5ecad in /usr/bin/python)
10.64.24.50: frame #54: <unknown function> + 0x29d90 (0x7f59b4727d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
10.64.24.50: frame #55: __libc_start_main + 0x80 (0x7f59b4727e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
10.64.24.50: frame #56: _start + 0x25 (0x556c30c5eba5 in /usr/bin/python)
10.64.24.50: . This may indicate a possible application crash on rank 0 or a network set up issue.RuntimeError
10.64.24.50: : [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
10.64.24.50: Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
10.64.24.50: frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f833d1b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
10.64.24.50: frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f833d16b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
10.64.24.50: frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f82f999123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f82f999150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f82f9991816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f82f994a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f82f994a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f82f994a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f82f994a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f82f994a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f82accd73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f82accda908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #12: <unknown function> + 0xe27d37 (0x7f82acce6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f82acce82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f82accea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #15: <unknown function> + 0x4936f0a (0x7f82f9938f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #16: <unknown function> + 0x494766f (0x7f82f994966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #17: <unknown function> + 0x4950b97 (0x7f82f9952b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #18: <unknown function> + 0x495f1ee (0x7f82f99611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #19: <unknown function> + 0xb8cc15 (0x7f8300122c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
10.64.24.50: frame #20: <unknown function> + 0x39c407 (0x7f82ff932407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
10.64.24.50: frame #21: <unknown function> + 0x15fe0e (0x55d62df50e0e in /usr/bin/python)
10.64.24.50: frame #22: _PyObject_MakeTpCall + 0x25b (0x55d62df475eb in /usr/bin/python)
10.64.24.50: frame #23: <unknown function> + 0x16e7bb (0x55d62df5f7bb in /usr/bin/python)
10.64.24.50: frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x55d62df3f8a2 in /usr/bin/python)
10.64.24.50: frame #25: _PyFunction_Vectorcall + 0x7c (0x55d62df5170c in /usr/bin/python)
10.64.24.50: frame #26: PyObject_Call + 0x122 (0x55d62df60192 in /usr/bin/python)
10.64.24.50: frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x55d62df3c2c1 in /usr/bin/python)
10.64.24.50: frame #28: _PyFunction_Vectorcall + 0x7c (0x55d62df5170c in /usr/bin/python)
10.64.24.50: frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x55d62df3b0d1 in /usr/bin/python)
10.64.24.50: frame #30: <unknown function> + 0x16e4e1 (0x55d62df5f4e1 in /usr/bin/python)
10.64.24.50: frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x55d62df3f8a2 in /usr/bin/python)
10.64.24.50: frame #32: _PyFunction_Vectorcall + 0x7c (0x55d62df5170c in /usr/bin/python)
10.64.24.50: frame #33: _PyObject_FastCallDictTstate + 0x16d (0x55d62df4682d in /usr/bin/python)
10.64.24.50: frame #34: <unknown function> + 0x16a744 (0x55d62df5b744 in /usr/bin/python)
10.64.24.50: frame #35: _PyObject_MakeTpCall + 0x1fc (0x55d62df4758c in /usr/bin/python)
10.64.24.50: frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x55d62df40908 in /usr/bin/python)
10.64.24.50: frame #37: _PyFunction_Vectorcall + 0x7c (0x55d62df5170c in /usr/bin/python)
10.64.24.50: frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x55d62df3b0d1 in /usr/bin/python)
10.64.24.50: frame #39: _PyFunction_Vectorcall + 0x7c (0x55d62df5170c in /usr/bin/python)
10.64.24.50: frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x55d62df39e0d in /usr/bin/python)
10.64.24.50: frame #41: _PyFunction_Vectorcall + 0x7c (0x55d62df5170c in /usr/bin/python)
10.64.24.50: frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x55d62df39e0d in /usr/bin/python)
10.64.24.50: frame #43: _PyFunction_Vectorcall + 0x7c (0x55d62df5170c in /usr/bin/python)
10.64.24.50: frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x55d62df3b0d1 in /usr/bin/python)
10.64.24.50: frame #45: <unknown function> + 0x239e56 (0x55d62e02ae56 in /usr/bin/python)
10.64.24.50: frame #46: PyEval_EvalCode + 0x86 (0x55d62e02acf6 in /usr/bin/python)
10.64.24.50: frame #47: <unknown function> + 0x2647d8 (0x55d62e0557d8 in /usr/bin/python)
10.64.24.50: frame #48: <unknown function> + 0x25e0bb (0x55d62e04f0bb in /usr/bin/python)
10.64.24.50: frame #49: <unknown function> + 0x264525 (0x55d62e055525 in /usr/bin/python)
10.64.24.50: frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x55d62e054a08 in /usr/bin/python)
10.64.24.50: frame #51: _PyRun_AnyFileObject + 0x43 (0x55d62e054653 in /usr/bin/python)
10.64.24.50: frame #52: Py_RunMain + 0x2be (0x55d62e04741e in /usr/bin/python)
10.64.24.50: frame #53: Py_BytesMain + 0x2d (0x55d62e01dcad in /usr/bin/python)
10.64.24.50: frame #54: <unknown function> + 0x29d90 (0x7f8348487d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
10.64.24.50: frame #55: __libc_start_main + 0x80 (0x7f8348487e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
10.64.24.50: frame #56: _start + 0x25 (0x55d62e01dba5 in /usr/bin/python)
10.64.24.50: . This may indicate a possible application crash on rank 0 or a network set up issue.
10.64.24.50:     work = group.allreduce([tensor], opts)
10.64.24.50: RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
10.64.24.50: Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
10.64.24.50: frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f53ecfb0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
10.64.24.50: frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f53ecf6b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
10.64.24.50: frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f53a979123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f53a979150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f53a9791816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f53a974a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f53a974a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f53a974a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f53a974a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f53a974a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f535cad73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f535cada908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #12: <unknown function> + 0xe27d37 (0x7f535cae6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f535cae82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f535caea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #15: <unknown function> + 0x4936f0a (0x7f53a9738f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #16: <unknown function> + 0x494766f (0x7f53a974966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #17: <unknown function> + 0x4950b97 (0x7f53a9752b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #18: <unknown function> + 0x495f1ee (0x7f53a97611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #19: <unknown function> + 0xb8cc15 (0x7f53aff22c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
10.64.24.50: frame #20: <unknown function> + 0x39c407 (0x7f53af732407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
10.64.24.50: frame #21: <unknown function> + 0x15fe0e (0x55fa49a97e0e in /usr/bin/python)
10.64.24.50: frame #22: _PyObject_MakeTpCall + 0x25b (0x55fa49a8e5eb in /usr/bin/python)
10.64.24.50: frame #23: <unknown function> + 0x16e7bb (0x55fa49aa67bb in /usr/bin/python)
10.64.24.50: frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x55fa49a868a2 in /usr/bin/python)
10.64.24.50: frame #25: _PyFunction_Vectorcall + 0x7c (0x55fa49a9870c in /usr/bin/python)
10.64.24.50: frame #26: PyObject_Call + 0x122 (0x55fa49aa7192 in /usr/bin/python)
10.64.24.50: frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x55fa49a832c1 in /usr/bin/python)
10.64.24.50: frame #28: _PyFunction_Vectorcall + 0x7c (0x55fa49a9870c in /usr/bin/python)
10.64.24.50: frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x55fa49a820d1 in /usr/bin/python)
10.64.24.50: frame #30: <unknown function> + 0x16e4e1 (0x55fa49aa64e1 in /usr/bin/python)
10.64.24.50: frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x55fa49a868a2 in /usr/bin/python)
10.64.24.50: frame #32: _PyFunction_Vectorcall + 0x7c (0x55fa49a9870c in /usr/bin/python)
10.64.24.50: frame #33: _PyObject_FastCallDictTstate + 0x16d (0x55fa49a8d82d in /usr/bin/python)
10.64.24.50: frame #34: <unknown function> + 0x16a744 (0x55fa49aa2744 in /usr/bin/python)
10.64.24.50: frame #35: _PyObject_MakeTpCall + 0x1fc (0x55fa49a8e58c in /usr/bin/python)
10.64.24.50: frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x55fa49a87908 in /usr/bin/python)
10.64.24.50: frame #37: _PyFunction_Vectorcall + 0x7c (0x55fa49a9870c in /usr/bin/python)
10.64.24.50: frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x55fa49a820d1 in /usr/bin/python)
10.64.24.50: frame #39: _PyFunction_Vectorcall + 0x7c (0x55fa49a9870c in /usr/bin/python)
10.64.24.50: frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x55fa49a80e0d in /usr/bin/python)
10.64.24.50: frame #41: _PyFunction_Vectorcall + 0x7c (0x55fa49a9870c in /usr/bin/python)
10.64.24.50: frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x55fa49a80e0d in /usr/bin/python)
10.64.24.50: frame #43: _PyFunction_Vectorcall + 0x7c (0x55fa49a9870c in /usr/bin/python)
10.64.24.50: frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x55fa49a820d1 in /usr/bin/python)
10.64.24.50: frame #45: <unknown function> + 0x239e56 (0x55fa49b71e56 in /usr/bin/python)
10.64.24.50: frame #46: PyEval_EvalCode + 0x86 (0x55fa49b71cf6 in /usr/bin/python)
10.64.24.50: frame #47: <unknown function> + 0x2647d8 (0x55fa49b9c7d8 in /usr/bin/python)
10.64.24.50: frame #48: <unknown function> + 0x25e0bb (0x55fa49b960bb in /usr/bin/python)
10.64.24.50: frame #49: <unknown function> + 0x264525 (0x55fa49b9c525 in /usr/bin/python)
10.64.24.50: frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x55fa49b9ba08 in /usr/bin/python)
10.64.24.50: frame #51: _PyRun_AnyFileObject + 0x43 (0x55fa49b9b653 in /usr/bin/python)
10.64.24.50: frame #52: Py_RunMain + 0x2be (0x55fa49b8e41e in /usr/bin/python)
10.64.24.50: frame #53: Py_BytesMain + 0x2d (0x55fa49b64cad in /usr/bin/python)
10.64.24.50: frame #54: <unknown function> + 0x29d90 (0x7f53f828ed90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
10.64.24.50: frame #55: __libc_start_main + 0x80 (0x7f53f828ee40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
10.64.24.50: frame #56: _start + 0x25 (0x55fa49b64ba5 in /usr/bin/python)
10.64.24.50: . This may indicate a possible application crash on rank 0 or a network set up issue.
10.64.24.50: Traceback (most recent call last):
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/pretrain_gpt.py", line 310, in <module>
10.64.24.50:     pretrain(train_dataset_provider,
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 119, in pretrain
10.64.24.50:     model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 369, in setup_model_and_optimizer
10.64.24.50:     model = get_model(model_provider_func, model_type)
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/training.py", line 250, in get_model
10.64.24.50:     model = model_provider_func(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/pretrain_gpt.py", line 78, in model_provider
10.64.24.50:     model = megatron.model.GPTModel(
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/model/gpt_model.py", line 107, in __init__
10.64.24.50:     self.initialize_word_embeddings()
10.64.24.50:   File "/data/nolan/develop/bak/ht/hot_switch/gh/Megatron-llama/Megatron-LM/megatron/model/module.py", line 108, in initialize_word_embeddings
10.64.24.50:     torch.distributed.all_reduce(self.shared_embedding_or_output_weight().data,
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 47, in wrapper
10.64.24.50:     return func(*args, **kwargs)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2044, in all_reduce
10.64.24.50:     work = group.allreduce([tensor], opts)
10.64.24.50: RuntimeError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Socket Timeout
10.64.24.50: Exception raised from doWait at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/TCPStore.cpp:445 (most recent call first):
10.64.24.50: frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f54e17b0449 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
10.64.24.50: frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x6a (0x7f54e176b1d8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
10.64.24.50: frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x28c (0x7f54a8d9123c in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #3: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2a (0x7f54a8d9150a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #4: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x76 (0x7f54a8d91816 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #5: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f54a8d4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #6: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f54a8d4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #7: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f54a8d4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #8: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f54a8d4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #9: c10d::PrefixStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2f (0x7f54a8d4a2bf in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #10: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) + 0x1fa (0x7f545c0d73da in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #11: c10d::ProcessGroupNCCL::getNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x228 (0x7f545c0da908 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #12: <unknown function> + 0xe27d37 (0x7f545c0e6d37 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #13: c10d::ProcessGroupNCCL::allreduce_impl(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x21 (0x7f545c0e82e1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #14: c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) + 0x421 (0x7f545c0ea121 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
10.64.24.50: frame #15: <unknown function> + 0x4936f0a (0x7f54a8d38f0a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #16: <unknown function> + 0x494766f (0x7f54a8d4966f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #17: <unknown function> + 0x4950b97 (0x7f54a8d52b97 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #18: <unknown function> + 0x495f1ee (0x7f54a8d611ee in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
10.64.24.50: frame #19: <unknown function> + 0xb8cc15 (0x7f54af522c15 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
10.64.24.50: frame #20: <unknown function> + 0x39c407 (0x7f54aed32407 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
10.64.24.50: frame #21: <unknown function> + 0x15fe0e (0x55ee38545e0e in /usr/bin/python)
10.64.24.50: frame #22: _PyObject_MakeTpCall + 0x25b (0x55ee3853c5eb in /usr/bin/python)
10.64.24.50: frame #23: <unknown function> + 0x16e7bb (0x55ee385547bb in /usr/bin/python)
10.64.24.50: frame #24: _PyEval_EvalFrameDefault + 0x6152 (0x55ee385348a2 in /usr/bin/python)
10.64.24.50: frame #25: _PyFunction_Vectorcall + 0x7c (0x55ee3854670c in /usr/bin/python)
10.64.24.50: frame #26: PyObject_Call + 0x122 (0x55ee38555192 in /usr/bin/python)
10.64.24.50: frame #27: _PyEval_EvalFrameDefault + 0x2b71 (0x55ee385312c1 in /usr/bin/python)
10.64.24.50: frame #28: _PyFunction_Vectorcall + 0x7c (0x55ee3854670c in /usr/bin/python)
10.64.24.50: frame #29: _PyEval_EvalFrameDefault + 0x1981 (0x55ee385300d1 in /usr/bin/python)
10.64.24.50: frame #30: <unknown function> + 0x16e4e1 (0x55ee385544e1 in /usr/bin/python)
10.64.24.50: frame #31: _PyEval_EvalFrameDefault + 0x6152 (0x55ee385348a2 in /usr/bin/python)
10.64.24.50: frame #32: _PyFunction_Vectorcall + 0x7c (0x55ee3854670c in /usr/bin/python)
10.64.24.50: frame #33: _PyObject_FastCallDictTstate + 0x16d (0x55ee3853b82d in /usr/bin/python)
10.64.24.50: frame #34: <unknown function> + 0x16a744 (0x55ee38550744 in /usr/bin/python)
10.64.24.50: frame #35: _PyObject_MakeTpCall + 0x1fc (0x55ee3853c58c in /usr/bin/python)
10.64.24.50: frame #36: _PyEval_EvalFrameDefault + 0x71b8 (0x55ee38535908 in /usr/bin/python)
10.64.24.50: frame #37: _PyFunction_Vectorcall + 0x7c (0x55ee3854670c in /usr/bin/python)
10.64.24.50: frame #38: _PyEval_EvalFrameDefault + 0x1981 (0x55ee385300d1 in /usr/bin/python)
10.64.24.50: frame #39: _PyFunction_Vectorcall + 0x7c (0x55ee3854670c in /usr/bin/python)
10.64.24.50: frame #40: _PyEval_EvalFrameDefault + 0x6bd (0x55ee3852ee0d in /usr/bin/python)
10.64.24.50: frame #41: _PyFunction_Vectorcall + 0x7c (0x55ee3854670c in /usr/bin/python)
10.64.24.50: frame #42: _PyEval_EvalFrameDefault + 0x6bd (0x55ee3852ee0d in /usr/bin/python)
10.64.24.50: frame #43: _PyFunction_Vectorcall + 0x7c (0x55ee3854670c in /usr/bin/python)
10.64.24.50: frame #44: _PyEval_EvalFrameDefault + 0x1981 (0x55ee385300d1 in /usr/bin/python)
10.64.24.50: frame #45: <unknown function> + 0x239e56 (0x55ee3861fe56 in /usr/bin/python)
10.64.24.50: frame #46: PyEval_EvalCode + 0x86 (0x55ee3861fcf6 in /usr/bin/python)
10.64.24.50: frame #47: <unknown function> + 0x2647d8 (0x55ee3864a7d8 in /usr/bin/python)
10.64.24.50: frame #48: <unknown function> + 0x25e0bb (0x55ee386440bb in /usr/bin/python)
10.64.24.50: frame #49: <unknown function> + 0x264525 (0x55ee3864a525 in /usr/bin/python)
10.64.24.50: frame #50: _PyRun_SimpleFileObject + 0x1a8 (0x55ee38649a08 in /usr/bin/python)
10.64.24.50: frame #51: _PyRun_AnyFileObject + 0x43 (0x55ee38649653 in /usr/bin/python)
10.64.24.50: frame #52: Py_RunMain + 0x2be (0x55ee3863c41e in /usr/bin/python)
10.64.24.50: frame #53: Py_BytesMain + 0x2d (0x55ee38612cad in /usr/bin/python)
10.64.24.50: frame #54: <unknown function> + 0x29d90 (0x7f54f77fdd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
10.64.24.50: frame #55: __libc_start_main + 0x80 (0x7f54f77fde40 in /usr/lib/x86_64-linux-gnu/libc.so.6)
10.64.24.50: frame #56: _start + 0x25 (0x55ee38612ba5 in /usr/bin/python)
10.64.24.50: . This may indicate a possible application crash on rank 0 or a network set up issue.
10.64.24.50: [E ProcessGroupNCCL.cpp:467] [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600681 milliseconds before timing out.
10.64.24.50: [E ProcessGroupNCCL.cpp:467] [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600683 milliseconds before timing out.
10.64.24.50: [E ProcessGroupNCCL.cpp:467] [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600692 milliseconds before timing out.
10.64.24.50: [E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
10.64.24.50: [E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
10.64.24.50: [E ProcessGroupNCCL.cpp:852] [Rank 10] NCCL watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600683 milliseconds before timing out.
10.64.24.50: terminate called after throwing an instance of 'std::runtime_error'
10.64.24.50:   what():  [Rank 10] NCCL watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600683 milliseconds before timing out.
10.64.24.50: [E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
10.64.24.50: [E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
10.64.24.50: [E ProcessGroupNCCL.cpp:852] [Rank 8] NCCL watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600016 milliseconds before timing out.
10.64.24.50: terminate called after throwing an instance of 'std::runtime_error'
10.64.24.50:   what():  [Rank 8] NCCL watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600016 milliseconds before timing out.
10.64.24.50: [E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
10.64.24.50: [E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
10.64.24.50: [E ProcessGroupNCCL.cpp:852] [Rank 9] NCCL watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600692 milliseconds before timing out.
10.64.24.50: terminate called after throwing an instance of 'std::runtime_error'
10.64.24.50:   what():  [Rank 9] NCCL watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600692 milliseconds before timing out.
10.64.24.50: [E ProcessGroupNCCL.cpp:481] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
10.64.24.50: [E ProcessGroupNCCL.cpp:487] To avoid data inconsistency, we are taking the entire process down.
10.64.24.50: [E ProcessGroupNCCL.cpp:852] [Rank 11] NCCL watchdog thread terminated with exception: [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600681 milliseconds before timing out.
10.64.24.50: terminate called after throwing an instance of 'std::runtime_error'
10.64.24.50:   what():  [Rank 11] NCCL watchdog thread terminated with exception: [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600681 milliseconds before timing out.
10.64.24.50: [2024-03-12 06:04:58,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2453661 closing signal SIGTERM
10.64.24.50: [2024-03-12 06:04:58,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2453662 closing signal SIGTERM
10.64.24.50: [2024-03-12 06:04:58,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2453664 closing signal SIGTERM
10.64.24.50: [2024-03-12 06:05:01,380] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 2 (pid: 2453663) of binary: /usr/bin/python
10.64.24.50: Traceback (most recent call last):
10.64.24.50:   File "/usr/local/bin/torchrun", line 33, in <module>
10.64.24.50:     sys.exit(load_entry_point('torch==2.1.0a0+32f93b1', 'console_scripts', 'torchrun')())
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
10.64.24.50:     return f(*args, **kwargs)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
10.64.24.50:     run(args)
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
10.64.24.50:     elastic_launch(
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
10.64.24.50:     return launch_agent(self._config, self._entrypoint, list(args))
10.64.24.50:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
10.64.24.50:     raise ChildFailedError(
10.64.24.50: torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
10.64.24.50: ============================================================
10.64.24.50: pretrain_gpt.py FAILED
10.64.24.50: ------------------------------------------------------------
10.64.24.50: Failures:
10.64.24.50: [1]:
10.64.24.50:   time      : 2024-03-12_06:04:58
10.64.24.50:   host      : SYM206-GPU-A0204-P2-Node50
10.64.24.50:   rank      : 12 (local_rank: 4)
10.64.24.50:   exitcode  : 1 (pid: 2453665)
10.64.24.50:   error_file: <N/A>
10.64.24.50:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
10.64.24.50: [2]:
10.64.24.50:   time      : 2024-03-12_06:04:58
10.64.24.50:   host      : SYM206-GPU-A0204-P2-Node50
10.64.24.50:   rank      : 13 (local_rank: 5)
10.64.24.50:   exitcode  : 1 (pid: 2453666)
10.64.24.50:   error_file: <N/A>
10.64.24.50:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
10.64.24.50: [3]:
10.64.24.50:   time      : 2024-03-12_06:04:58
10.64.24.50:   host      : SYM206-GPU-A0204-P2-Node50
10.64.24.50:   rank      : 14 (local_rank: 6)
10.64.24.50:   exitcode  : 1 (pid: 2453667)
10.64.24.50:   error_file: <N/A>
10.64.24.50:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
10.64.24.50: [4]:
10.64.24.50:   time      : 2024-03-12_06:04:58
10.64.24.50:   host      : SYM206-GPU-A0204-P2-Node50
10.64.24.50:   rank      : 15 (local_rank: 7)
10.64.24.50:   exitcode  : 1 (pid: 2453668)
10.64.24.50:   error_file: <N/A>
10.64.24.50:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
10.64.24.50: ------------------------------------------------------------
10.64.24.50: Root Cause (first observed failure):
10.64.24.50: [0]:
10.64.24.50:   time      : 2024-03-12_06:04:58
10.64.24.50:   host      : SYM206-GPU-A0204-P2-Node50
10.64.24.50:   rank      : 10 (local_rank: 2)
10.64.24.50:   exitcode  : -6 (pid: 2453663)
10.64.24.50:   error_file: <N/A>
10.64.24.50:   traceback : Signal 6 (SIGABRT) received by PID 2453663
10.64.24.50: ============================================================
10.64.24.50: Writting log to logs/gpt/gpt_megatron_gpus16_13b_seqlen4096_gbs512_mbs2_dp2_tp2_pp4_pad_sp_node1.log...
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495680 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495672 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495655 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495693 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495662 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495656 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495681 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495687 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495667 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495637 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495643 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495642 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495666 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495668 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495647 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495682 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495683 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495663 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495664 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495689 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495669 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495694 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495676 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495692 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495677 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495686 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495657 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495648 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495688 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495649 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495665 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495678 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495652 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495654 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495638 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495653 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495633 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495635 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495639 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495634 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495646 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495674 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495673 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495675 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495684 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495650 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495645 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495659 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495636 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495641 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495640 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495658 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495651 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495644 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495661 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495660 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495685 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495691 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495695 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495690 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495696 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495679 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495670 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495671 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495681 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495672 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495656 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495655 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495662 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495687 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495693 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495680 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495637 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495647 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495642 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495668 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495682 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495666 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495667 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495643 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495683 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495669 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495676 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495689 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495664 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495692 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495694 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495663 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495648 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495686 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495677 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495657 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495665 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495678 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495688 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495649 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495633 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495638 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495635 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495639 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495654 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495634 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495653 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495652 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495645 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495674 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495650 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495659 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495675 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495673 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495646 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495684 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495636 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495658 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495651 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495641 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495644 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495640 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495661 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495660 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495679 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495685 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495691 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495670 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495671 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495696 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495695 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495690 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495662 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495672 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495687 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495693 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495681 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495680 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495656 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495655 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495637 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495666 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495647 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495642 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495667 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495668 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495682 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495643 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495683 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495676 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495669 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495663 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495689 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495664 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495692 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495694 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495648 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495677 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495657 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495686 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495649 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495678 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495688 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495665 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495633 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495635 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495638 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495639 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495654 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495634 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495653 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495652 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495674 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495645 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495650 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495646 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495673 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495659 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495675 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495684 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495636 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495658 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495641 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495644 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495651 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495640 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495660 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495661 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494700:1495679 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494701:1495685 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494694:1495670 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494698:1495690 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494699:1495691 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494695:1495696 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494696:1495671 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
10.64.24.49: 
10.64.24.49: SYM206-GPU-A0203-P2-Node49:1494697:1495695 [0] p2p_plugin.c:181 NCCL WARN NET/IB : Got async event : client reregistration
